---
title: "Random forest and pipeline"
subtitle: "with house price data"
execute:
  warning: false
  error: false
format:
  html:
    toc: true
    toc-location: right
    code-fold: show
    code-tools: true
    number-sections: true
    code-block-bg: true
    code-block-border-left: "#31BAE9"
---

with pipeline

# load package

```{python}
import os
#os.system('pip install xgboost')
```


```{python}
import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import explained_variance_score
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split

```

# data

## input data
```{python}
# Loading the data
df_train = pd.read_csv('./data/train.csv')
df_test = pd.read_csv('./data/test.csv')

# Store our test passenger IDs for easy access
Id = df_train['Id']


# Showing overview of the train dataset
df_train.head()

```
```{python}
#df_train.info()
```


```{python}
df_train['role'] = 'train'
df_test['role'] = 'test'

# Concatenate training and test sets
data = pd.concat([df_train.drop(['SalePrice'], axis=1), df_test])
```


## data EDA

in step 1



## Data Wrangling




## split data

```{python}

Y = df_train.SalePrice
X = df_train.drop(['SalePrice'], axis=1)



X_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)

X_train = X_train.drop('role', axis=1)
X_test = X_test.drop('role', axis=1)


```


```{python}
print(X_train.shape)
print(X_test.shape)
```

```{python}
print(Y_train.shape)
print(Y_test.shape)
```


## categorical_cols and numerical_cols


```{python}
categorical_cols = [cname for cname in X_train.columns 
                    if X_train[cname].nunique() < 10 and X_train[cname].dtype == "object"]
                    
                    
numerical_cols = numerical_cols = [cname for cname in X_train.columns 
                    if X_train[cname].dtype in ['int64', 'float64']]
```

```{python}
print("The total number of categorical columns:", len(categorical_cols))
print("The total number of numerical columns:", len(numerical_cols))
```


```{python}
my_cols = categorical_cols + numerical_cols
X_train = X_train[my_cols].copy()
X_test= X_test[my_cols].copy()


#X_final = df_test[my_cols].copy()
```


## Pipelines for Data Preprocessing

```{python}
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
```


```{python}
numerical_transformer = Pipeline(steps=[
    ('imputer_num', SimpleImputer(strategy='median')), 
    ('scaler', StandardScaler())
])
```




```{python}
from sklearn.preprocessing import OneHotEncoder

categorical_transformer = Pipeline(steps=[
    ('imputer_cat', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])
```


```{python}
from sklearn.compose import ColumnTransformer

preprocessor = ColumnTransformer(transformers=[
    ('num', numerical_transformer, numerical_cols),
    ('cat', categorical_transformer, categorical_cols)])
```



# model


## define model

random forest with  hyper parameter tuning
```{python}

from sklearn.ensemble import RandomForestRegressor
   

ml_model = RandomForestRegressor(random_state=0)
ml_model
```

## define pipline

```{python}
pipeline = Pipeline(
  steps=[
         ('preprocessor', preprocessor), 
         ('model', ml_model)
         ]
)
```




## train model

```{python}
pipeline.fit(X_train, Y_train)
```




```{python}
var=pipeline[:-1].get_feature_names_out()
var
```


```{python}
fitted_model=pipeline.steps[1][1]
```



variable importance 
```{python}
importances = fitted_model.feature_importances_
vi=pd.DataFrame({"variable":var,"importances":importances})
vi=vi.sort_values('importances',ascending=False)
vi
```

## Preformance


```{python}
Y_pred_dt =pipeline.predict(X_test) #always gets x and retuns y
```


R 2
```{python}
from sklearn.metrics import r2_score
r2_score(Y_test, Y_pred_dt)
```

MAE
```{python}
from sklearn.metrics import mean_absolute_error
mean_absolute_error(Y_test, Y_pred_dt)
```

RMSE
```{python}

from  math import sqrt
from sklearn.metrics import mean_squared_error
mse=mean_squared_error(Y_test, Y_pred_dt)
rmse=sqrt(mse)
rmse
```


## k-Fold Cross-Validation

```{python}
import numpy as np
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
```


```{python}
kf_dt = KFold(n_splits=5,shuffle=True)  
```

```{python}
cv_dt = cross_val_score(pipeline, X_train, Y_train, cv=kf_dt)
np.mean(cv_dt)
```

```{python}
cv_dt = cross_val_score(pipeline, X_train, Y_train, cv=kf_dt,scoring = 'neg_mean_squared_error')
np.mean(np.sqrt(np.abs(cv_dt)))
```


# predict vs real on testing data

```{python}
#result=pd.DataFrame({'predict':a},{'real':b})
result= pd.DataFrame(list(zip(Y_pred_dt.tolist(),Y_test.tolist())),columns=['predict','real'])

from plotnine import *

p=(
    ggplot(data=result)+aes(x="predict",y="real")+ geom_point()
)

p


```
# reference:

https://github.com/alicevillar/titanic-kaggle/blob/main/Titanic_DecisionTree.ipynb

https://scikit-learn.org/stable/modules/tree.html

https://github.com/codebasics/py/blob/master/ML/15_gridsearch/15_grid_search.ipynb

