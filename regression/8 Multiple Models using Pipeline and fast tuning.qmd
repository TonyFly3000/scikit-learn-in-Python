---
title: "Multiple models using pipeline and fast tuning"
subtitle: "with house price data"
execute:
  warning: false
  error: false
format:
  html:
    toc: true
    toc-location: right
    code-fold: show
    code-tools: true
    number-sections: true
    code-block-bg: true
    code-block-border-left: "#31BAE9"
---

with pipeline and tunning

# load package

```{python}
import os
#os.system('pip install xgboost')
```

```{python}
import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import re
import numpy as np
from sklearn import tree
from sklearn.model_selection import train_test_split
import time
from siuba import *

from sklearn.experimental import enable_halving_search_cv  # noqa
from sklearn.model_selection import HalvingGridSearchCV
```

# data

## input data

```{python}
# Loading the data
df_train = pd.read_csv('./data/train.csv')
df_test = pd.read_csv('./data/test.csv')

# Store our test passenger IDs for easy access
Id = df_train['Id']
Id_test = df_test['Id']

df_train = df_train >> select(~_.Id,~_.PoolQC,~_.Fence,~_.MiscFeature,~_.Alley)
df_test = df_test >> select(~_.Id,~_.PoolQC,~_.Fence,~_.MiscFeature,~_.Alley)

# Showing overview of the train dataset
df_train.head()

```



## data EDA

in step 1

## Data Wrangling



## Data Wrangling

```{python}
# Store target variable of training data in a safe place
#SalePrice_train = df_train.SalePrice



df_train['role'] = 'train'
df_test['role'] = 'test'

# Concatenate training and test sets
data = pd.concat([df_train.drop(['SalePrice'], axis=1), df_test])
```

```{python}
data.shape
```


## split data

```{python}

Y = df_train.SalePrice
X = df_train.drop(['SalePrice'], axis=1)


X_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)

X_train = X_train.drop('role', axis=1)
X_test = X_test.drop('role', axis=1)


```


```{python}
print(X_train.shape)
print(X_test.shape)
```

```{python}
print(Y_train.shape)
print(Y_test.shape)
```


```{python}
categorical_cols = [cname for cname in X_train.columns 
                    if X_train[cname].nunique() < 10 and X_train[cname].dtype == "object"]
                    
                    
numerical_cols = numerical_cols = [cname for cname in X_train.columns 
                    if X_train[cname].dtype in ['int64', 'float64']]
```

```{python}
print("The total number of categorical columns:", len(categorical_cols))
print("The total number of numerical columns:", len(numerical_cols))
```


```{python}
my_cols = categorical_cols + numerical_cols
X_train = X_train[my_cols].copy()
X_test= X_test[my_cols].copy()


#X_final = df_test[my_cols].copy()
```


## Pipelines for Data Preprocessing

```{python}
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
```


```{python}
numerical_transformer = Pipeline(steps=[
    ('imputer_num', SimpleImputer(strategy='median')), 
    ('scaler', StandardScaler())
])
```


```{python}
from sklearn.preprocessing import OneHotEncoder

categorical_transformer = Pipeline(steps=[
    ('imputer_cat', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])
```


```{python}
from sklearn.compose import ColumnTransformer

preprocessor = ColumnTransformer(transformers=[
    ('num', numerical_transformer, numerical_cols),
    ('cat', categorical_transformer, categorical_cols)])
```



# model



## define model


### XGB model
```{python}
from xgboost import XGBRegressor
xgb_model = XGBRegressor()
xgb_model
```

### Random Forest model
```{python}

from sklearn.ensemble import RandomForestRegressor
random_forest_model = RandomForestRegressor()
random_forest_model
```

### Linear Regression model

```{python}
from sklearn.linear_model import LinearRegression
LinearRegression_model = LinearRegression()
LinearRegression_model
```


## define pipline

```{python}
pipeline_xgb = Pipeline(
  steps=[
         ('preprocessor', preprocessor), 
         ('model', xgb_model)
         ]
)

pipeline_rf = Pipeline(
  steps=[
         ('preprocessor', preprocessor), 
         ('model', random_forest_model)
         ]
)

pipeline_lr = Pipeline(
  steps=[
         ('preprocessor', preprocessor), 
         ('model', LinearRegression_model)
         ]
)
```

## define GridSearch



```{python}         

parameters_xgb= {
        'model__learning_rate': [0.01, 0.02,0.08,0.1],
        'model__max_depth': [3, 5, 7,8,9,10,20],
        'model__min_child_weight': [1, 3,5,8],
        'model__subsample': [0.5, 0.7,0.9],
        
       # 'model__colsample__bytree': [0.5, 0.7],
       
        'model__n_estimators' : [100, 200, 500],
        'model__objective': ['reg:squarederror']
    }


Grid_xgb = HalvingGridSearchCV(pipeline_xgb
                ,parameters_xgb 
                #,scoring='neg_root_mean_squared_error'
                ,max_resources=100
                , cv=10, n_jobs=-1)
                
                
parameters_rf = {'model__max_depth':[20,30,40],
                 'model__n_estimators':[200,250,300],
                 'model__min_samples_leaf':[1,2,3]
                 }                
                

Grid_rf = HalvingGridSearchCV(pipeline_rf
                ,parameters_rf
                #,scoring='neg_root_mean_squared_error'
                ,max_resources=100
                , cv=10, n_jobs=-1)
                
                

```

## train model

```{python}
start_time = time.time()


#Grids = [Grid_xgb,Grid_rf,pipeline_xgb,pipeline_rf,pipeline_lr]


Grids = [pipeline_xgb,pipeline_rf]


for Grid in Grids:
    Grid.fit(X_train,Y_train)


end_time = time.time()
duration = end_time - start_time
duration

```



## Preformance

```{python}
#grid_dict = {0: 'XGB', 1: 'random forest', 2: 'XGB non tune',3: 'ramdon forest non tune',4:'linear regression non tune' }

grid_dict = {0: 'XGB', 1: 'random forest'}


for i, model in enumerate(Grids):
    print('{} Test Accuracy: {}'.format(grid_dict[i],
    model.score(X_test,Y_test)))
    #print('{} Best Params: {}'.format(grid_dict[i], model.best_params_))
```


```{python}
best_ml=pipeline_xgb
```

```{python}
#Using predict method to test the model
Y_pred_dt = best_ml.predict(X_test) #always gets x and retuns y
Y_pred_dt
```



R 2


```{python}
from sklearn.metrics import r2_score
r2_score(Y_test, Y_pred_dt)
```

MAE
```{python}
from sklearn.metrics import mean_absolute_error
mean_absolute_error(Y_test, Y_pred_dt)
```

RMSE
```{python}
from  math import sqrt
from sklearn.metrics import mean_squared_error
mse=mean_squared_error(Y_test, Y_pred_dt)
rmse=sqrt(mse)
rmse
```


## k-Fold Cross-Validation

```{python}
import numpy as np
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
```


```{python}
kf_dt = KFold(n_splits=5,shuffle=True)  
```



```{python}
#cv_dt = cross_val_score(optimised_model_pipeline, X_train, Y_train, cv=kf_dt,scoring = 'neg_mean_squared_error')
#np.mean(np.sqrt(np.abs(cv_dt)))
```


# reference:



https://scikit-learn.org/stable/modules/tree.html

https://github.com/codebasics/py/blob/master/ML/15_gridsearch/15_grid_search.ipynb

