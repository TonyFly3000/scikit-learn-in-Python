---
title: "Decision tree"
subtitle: "with hotel booking imbalanced data"
execute:
  warning: false
  error: false
format:
  html:
    toc: true
    toc-location: right
    code-fold: show
    code-tools: true
    number-sections: true
    code-block-bg: true
    code-block-border-left: "#31BAE9"
---

8590 booking with children.110800 bookings with no children.

Only 7% booking with children.Its imbalanced data.Why it causes problems?

The model cannot learn to predict the minority class well because of class imbalance.

Model is only able to learn a simple heuristic (e.g. always predict the dominate class) and it gets stuck in a sub optimal solution.

An accuracy of over 90% can be misleading because the model may not have predictive power on the rare class.

![](images/1.png){width="400"}

Since we have enough rare class data(at least 1K).Let handle imbalanced data with down sample before training.

![](images/2.png){width="400"}

If there is no enough rare class data, we will do over smaple method.

# load package

```{python}
import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import re
import numpy as np
from sklearn import tree
from sklearn.model_selection import train_test_split

from siuba.siu import call
from siuba import _, mutate, filter, group_by, summarize,show_query
```

# data

## download data


```{python}
import pandas as pd
#url='https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv'
hotels=pd.read_csv('data/hotels.csv')


```

```{python}
hotels.head()

```

## data EDA

Missing Data

```{python}
hotels.isnull().sum()
```

```{python}
#import math
#hotels=hotels>> filter(math.isnan(_.children)==False)
from siuba.siu import call
from siuba import _, mutate, filter, group_by, summarize,show_query
from siuba import *

hotels >> group_by(_.children)  >> summarize(n = _.shape[0])

```

```{python}
#import math
hotels=hotels>>mutate(children=if_else(_.children > 0, True, False))

# Create a boolean mask and apply it
mask = pd.notna(hotels['children'])
hotels = hotels[mask]

```

## Data Wrangling

```{python}
# Store target variable of training data in a safe place
children_train = hotels .children

# Concatenate training and test sets
data  = hotels
```

## categorical_cols and numerical_cols

```{python}
categorical_cols = [cname for cname in data 
                    if data[cname].nunique() < 10 and data[cname].dtype == "object"]
                    
                    
numerical_cols = numerical_cols = [cname for cname in data.columns 
                    if data[cname].dtype in ['int64', 'float64']]
```

```{python}
print("The total number of categorical columns:", len(categorical_cols))
print("The total number of numerical columns:", len(numerical_cols))
```

```{python}
data >> group_by(_.children)  >> summarize(n = _.shape[0])
```



## split data

```{python}
Y=data['children']
X=data.drop('children', axis=1)

X_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)
```

```{python}

from imblearn.under_sampling import RandomUnderSampler

ros=RandomUnderSampler(random_state=0)

X_train_resample,Y_train_resample=ros.fit_resample(X_train,Y_train)
```

before split

```{python}
Y.value_counts()
```

after split before downsample:

```{python}
Y_train.value_counts()
```

after split after downsample:

```{python}
Y_train_resample.value_counts()
```

```{python}
X_train_resample.shape
```

# model

## define model

```{python}
ml_model = tree.DecisionTreeClassifier()  
ml_model
```

## train model

train on resample data

```{python}
ml_model.fit(X_train_resample[numerical_cols],Y_train_resample)
```

variable importance

```{python}
importances = ml_model.feature_importances_
vi=pd.DataFrame({"variable":X_train_resample[numerical_cols].columns,"importances":importances})
#vi=vi.sort_values('importances',ascending=False)
#vi
```

## Preformance

```{python}
#Using predict method to test the model
Y_pred_dt = ml_model.predict(X_test[numerical_cols]) #always gets x and retuns y
```

```{python}
import collections, numpy
collections.Counter(Y_pred_dt)
```

predict 25% have children,the truth is only 7% have children,because we have make downsample before training.

```{python}
5977/(5977+17901)
```

```{python}
collections.Counter(Y_test)

1762/(22116+1762)
```

a)  Accuracy

```{python}
# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives
# Here is another way to find the accuracy score
from sklearn import metrics
accuracy = metrics.accuracy_score(Y_test,Y_pred_dt)  
accuracy
```

b)  Precision

```{python}
# Precision = true positive / true positive + false positive
precision_dt = metrics.precision_score(Y_test,Y_pred_dt)  
precision_dt
```

c)  Recall

```{python}
# Recall = true positive / true positive + false negative
recall_dt = metrics.recall_score(Y_test,Y_pred_dt)  
recall_dt
```

d)  Confusion matrix

```{python}
import seaborn as sns
confusion_matrix_dt = metrics.confusion_matrix(Y_test,Y_pred_dt)
confusion_matrix_dt
```

```{python}
from sklearn.metrics import ConfusionMatrixDisplay
ConfusionMatrixDisplay(confusion_matrix_dt).plot()
```

e)  AUC - ROC Curve

```{python}
auc_dt = metrics.roc_auc_score(Y_test, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score
auc_dt
```


```{python}
fpr, tpr, thresholds = metrics.roc_curve(Y_test, Y_pred_dt)
roc_auc = metrics.auc(fpr, tpr)

display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,
                                 estimator_name='example estimator')

display.plot()

plt.show()
```

## k-Fold Cross-Validation

```{python}
import numpy as np
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
```

```{python}
kf_dt = KFold(n_splits=5,shuffle=True)  
cv_dt = cross_val_score(ml_model, X_train[numerical_cols], Y_train, cv=kf_dt)
np.mean(cv_dt)
```

## save model

```{python}
from joblib import dump, load
dump(ml_model, 'trained_model_1.joblib') 
```

## load model

```{python}
ml_model_reload = load('trained_model_1.joblib') 
```

```{python}
Y_pred_dt = ml_model_reload.predict(X_test[numerical_cols]) #always gets x and retuns y

Y_pred_dt[0:5]
```

# reference:

imblearn package: https://imbalanced-learn.org/

https://www.youtube.com/watch?v=GR-OW5asKlk

https://www.youtube.com/watch?v=hl5EBA821t0
