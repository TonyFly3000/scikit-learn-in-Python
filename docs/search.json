[
  {
    "objectID": "model type/2 random forest.html",
    "href": "model type/2 random forest.html",
    "title": "Random forest",
    "section": "",
    "text": "1 Pros\n\nEasier to interpret than Neural Network\nFast training and making inference\n\n\n\n2 Cons\n\nThe size & inference speed of the random forest can sometimes be an issue\nRandom forests cannot learn and reuse internal representations\n\n\n\n3 reference:\nhttps://www.youtube.com/watch?v=w5gB8zyLx-8",
    "crumbs": [
      "model type",
      "Random forest"
    ]
  },
  {
    "objectID": "model type/3 gradient boosted trees.html",
    "href": "model type/3 gradient boosted trees.html",
    "title": "Gradient boosted trees",
    "section": "",
    "text": "1 Pros\n\nNative support for numerical and categorical features, and no necessary need for feature pre-processing.\nGBT are fast and light-weight and with great performance.\n\n\n\n2 Cons\n\nGBT can overfit.\nDecision tree trained sequentially -&gt; slower training.\nGBT cannot learn & reuse internal representations.Poor performance on image and long text.\n\n\n\n3 reference:\nhttps://www.youtube.com/watch?v=w5gB8zyLx-8",
    "crumbs": [
      "model type",
      "Gradient boosted trees"
    ]
  },
  {
    "objectID": "data/2 siuba.html",
    "href": "data/2 siuba.html",
    "title": "Data manipulation with siuba",
    "section": "",
    "text": "1 reference:\nhttps://siuba.org/",
    "crumbs": [
      "Data",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "regression/3 Random forest on house price data.html",
    "href": "regression/3 Random forest on house price data.html",
    "title": "Random forest and pipeline",
    "section": "",
    "text": "with pipeline",
    "crumbs": [
      "Regression",
      "Random forest and pipeline"
    ]
  },
  {
    "objectID": "regression/3 Random forest on house price data.html#input-data",
    "href": "regression/3 Random forest on house price data.html#input-data",
    "title": "Random forest and pipeline",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nId = df_train['Id']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nId\nMSSubClass\nMSZoning\nLotFrontage\nLotArea\nStreet\nAlley\nLotShape\nLandContour\nUtilities\n...\nPoolArea\nPoolQC\nFence\nMiscFeature\nMiscVal\nMoSold\nYrSold\nSaleType\nSaleCondition\nSalePrice\n\n\n\n\n0\n1\n60\nRL\n65.0\n8450\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n2\n2008\nWD\nNormal\n208500\n\n\n1\n2\n20\nRL\n80.0\n9600\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2007\nWD\nNormal\n181500\n\n\n2\n3\n60\nRL\n68.0\n11250\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n9\n2008\nWD\nNormal\n223500\n\n\n3\n4\n70\nRL\n60.0\n9550\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n2\n2006\nWD\nAbnorml\n140000\n\n\n4\n5\n60\nRL\n84.0\n14260\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n12\n2008\nWD\nNormal\n250000\n\n\n\n\n5 rows × 81 columns\n\n\n\n\n\n\nCode\n#df_train.info()\n\n\n\n\nCode\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['SalePrice'], axis=1), df_test])",
    "crumbs": [
      "Regression",
      "Random forest and pipeline"
    ]
  },
  {
    "objectID": "regression/3 Random forest on house price data.html#data-eda",
    "href": "regression/3 Random forest on house price data.html#data-eda",
    "title": "Random forest and pipeline",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\nin step 1",
    "crumbs": [
      "Regression",
      "Random forest and pipeline"
    ]
  },
  {
    "objectID": "regression/3 Random forest on house price data.html#data-wrangling",
    "href": "regression/3 Random forest on house price data.html#data-wrangling",
    "title": "Random forest and pipeline",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling",
    "crumbs": [
      "Regression",
      "Random forest and pipeline"
    ]
  },
  {
    "objectID": "regression/3 Random forest on house price data.html#split-data",
    "href": "regression/3 Random forest on house price data.html#split-data",
    "title": "Random forest and pipeline",
    "section": "2.4 split data",
    "text": "2.4 split data\n\n\nCode\nY = df_train.SalePrice\nX = df_train.drop(['SalePrice'], axis=1)\n\n\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\nX_train = X_train.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nprint(X_train.shape)\nprint(X_test.shape)\n\n\n(1168, 80)\n(292, 80)\n\n\n\n\nCode\nprint(Y_train.shape)\nprint(Y_test.shape)\n\n\n(1168,)\n(292,)",
    "crumbs": [
      "Regression",
      "Random forest and pipeline"
    ]
  },
  {
    "objectID": "regression/3 Random forest on house price data.html#pipelines-for-data-preprocessing",
    "href": "regression/3 Random forest on house price data.html#pipelines-for-data-preprocessing",
    "title": "Random forest and pipeline",
    "section": "2.6 Pipelines for Data Preprocessing",
    "text": "2.6 Pipelines for Data Preprocessing\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\nCode\nnumerical_transformer = Pipeline(steps=[\n    ('imputer_num', SimpleImputer(strategy='median')), \n    ('scaler', StandardScaler())\n])\n\n\n\n\nCode\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n\n\nCode\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)])",
    "crumbs": [
      "Regression",
      "Random forest and pipeline"
    ]
  },
  {
    "objectID": "regression/3 Random forest on house price data.html#define-model",
    "href": "regression/3 Random forest on house price data.html#define-model",
    "title": "Random forest and pipeline",
    "section": "3.1 define model",
    "text": "3.1 define model\nrandom forest with hyper parameter tuning\n\n\nCode\nfrom sklearn.ensemble import RandomForestRegressor\n   \n\nml_model = RandomForestRegressor(random_state=0)\nml_model\n\n\nRandomForestRegressor(random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestRegressor?Documentation for RandomForestRegressoriNot fittedRandomForestRegressor(random_state=0)",
    "crumbs": [
      "Regression",
      "Random forest and pipeline"
    ]
  },
  {
    "objectID": "regression/3 Random forest on house price data.html#define-pipline",
    "href": "regression/3 Random forest on house price data.html#define-pipline",
    "title": "Random forest and pipeline",
    "section": "3.2 define pipline",
    "text": "3.2 define pipline\n\n\nCode\npipeline = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', ml_model)\n         ]\n)",
    "crumbs": [
      "Regression",
      "Random forest and pipeline"
    ]
  },
  {
    "objectID": "regression/3 Random forest on house price data.html#train-model",
    "href": "regression/3 Random forest on house price data.html#train-model",
    "title": "Random forest and pipeline",
    "section": "3.3 train model",
    "text": "3.3 train model\n\n\nCode\npipeline.fit(X_train, Y_train)\n\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer_num',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['Id', 'MSSubClass',\n                                                   'LotFrontage', 'LotArea',\n                                                   'OverallQual', 'OverallCond',\n                                                   'YearBuilt', 'YearRemodAdd',\n                                                   'MasVnrArea', 'BsmtFinSF1',\n                                                   'BsmtFinSF2', 'BsmtUnfSF',\n                                                   'TotalBsmtSF', '1stFlrSF',\n                                                   '2ndFlrSF'...\n                                                   'LotConfig', 'LandSlope',\n                                                   'Condition1', 'Condition2',\n                                                   'BldgType', 'HouseStyle',\n                                                   'RoofStyle', 'RoofMatl',\n                                                   'MasVnrType', 'ExterQual',\n                                                   'ExterCond', 'Foundation',\n                                                   'BsmtQual', 'BsmtCond',\n                                                   'BsmtExposure',\n                                                   'BsmtFinType1',\n                                                   'BsmtFinType2', 'Heating',\n                                                   'HeatingQC', 'CentralAir',\n                                                   'Electrical', 'KitchenQual',\n                                                   'Functional', 'FireplaceQu', ...])])),\n                ('model', RandomForestRegressor(random_state=0))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiFittedPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer_num',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['Id', 'MSSubClass',\n                                                   'LotFrontage', 'LotArea',\n                                                   'OverallQual', 'OverallCond',\n                                                   'YearBuilt', 'YearRemodAdd',\n                                                   'MasVnrArea', 'BsmtFinSF1',\n                                                   'BsmtFinSF2', 'BsmtUnfSF',\n                                                   'TotalBsmtSF', '1stFlrSF',\n                                                   '2ndFlrSF'...\n                                                   'LotConfig', 'LandSlope',\n                                                   'Condition1', 'Condition2',\n                                                   'BldgType', 'HouseStyle',\n                                                   'RoofStyle', 'RoofMatl',\n                                                   'MasVnrType', 'ExterQual',\n                                                   'ExterCond', 'Foundation',\n                                                   'BsmtQual', 'BsmtCond',\n                                                   'BsmtExposure',\n                                                   'BsmtFinType1',\n                                                   'BsmtFinType2', 'Heating',\n                                                   'HeatingQC', 'CentralAir',\n                                                   'Electrical', 'KitchenQual',\n                                                   'Functional', 'FireplaceQu', ...])])),\n                ('model', RandomForestRegressor(random_state=0))])  preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('imputer_num',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('scaler', StandardScaler())]),\n                                 ['Id', 'MSSubClass', 'LotFrontage', 'LotArea',\n                                  'OverallQual', 'OverallCond', 'YearBuilt',\n                                  'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1',\n                                  'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF',\n                                  '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n                                  'GrLivArea', 'Bsm...\n                                 ['MSZoning', 'Street', 'Alley', 'LotShape',\n                                  'LandContour', 'Utilities', 'LotConfig',\n                                  'LandSlope', 'Condition1', 'Condition2',\n                                  'BldgType', 'HouseStyle', 'RoofStyle',\n                                  'RoofMatl', 'MasVnrType', 'ExterQual',\n                                  'ExterCond', 'Foundation', 'BsmtQual',\n                                  'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n                                  'BsmtFinType2', 'Heating', 'HeatingQC',\n                                  'CentralAir', 'Electrical', 'KitchenQual',\n                                  'Functional', 'FireplaceQu', ...])]) num['Id', 'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='median')  StandardScaler?Documentation for StandardScalerStandardScaler() cat['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'SaleType', 'SaleCondition']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='most_frequent')  OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore')  RandomForestRegressor?Documentation for RandomForestRegressorRandomForestRegressor(random_state=0) \n\n\n\n\nCode\nvar=pipeline[:-1].get_feature_names_out()\nvar\n\n\narray(['num__Id', 'num__MSSubClass', 'num__LotFrontage', 'num__LotArea',\n       'num__OverallQual', 'num__OverallCond', 'num__YearBuilt',\n       'num__YearRemodAdd', 'num__MasVnrArea', 'num__BsmtFinSF1',\n       'num__BsmtFinSF2', 'num__BsmtUnfSF', 'num__TotalBsmtSF',\n       'num__1stFlrSF', 'num__2ndFlrSF', 'num__LowQualFinSF',\n       'num__GrLivArea', 'num__BsmtFullBath', 'num__BsmtHalfBath',\n       'num__FullBath', 'num__HalfBath', 'num__BedroomAbvGr',\n       'num__KitchenAbvGr', 'num__TotRmsAbvGrd', 'num__Fireplaces',\n       'num__GarageYrBlt', 'num__GarageCars', 'num__GarageArea',\n       'num__WoodDeckSF', 'num__OpenPorchSF', 'num__EnclosedPorch',\n       'num__3SsnPorch', 'num__ScreenPorch', 'num__PoolArea',\n       'num__MiscVal', 'num__MoSold', 'num__YrSold',\n       'cat__MSZoning_C (all)', 'cat__MSZoning_FV', 'cat__MSZoning_RH',\n       'cat__MSZoning_RL', 'cat__MSZoning_RM', 'cat__Street_Grvl',\n       'cat__Street_Pave', 'cat__Alley_Grvl', 'cat__Alley_Pave',\n       'cat__LotShape_IR1', 'cat__LotShape_IR2', 'cat__LotShape_IR3',\n       'cat__LotShape_Reg', 'cat__LandContour_Bnk',\n       'cat__LandContour_HLS', 'cat__LandContour_Low',\n       'cat__LandContour_Lvl', 'cat__Utilities_AllPub',\n       'cat__Utilities_NoSeWa', 'cat__LotConfig_Corner',\n       'cat__LotConfig_CulDSac', 'cat__LotConfig_FR2',\n       'cat__LotConfig_FR3', 'cat__LotConfig_Inside',\n       'cat__LandSlope_Gtl', 'cat__LandSlope_Mod', 'cat__LandSlope_Sev',\n       'cat__Condition1_Artery', 'cat__Condition1_Feedr',\n       'cat__Condition1_Norm', 'cat__Condition1_PosA',\n       'cat__Condition1_PosN', 'cat__Condition1_RRAe',\n       'cat__Condition1_RRAn', 'cat__Condition1_RRNe',\n       'cat__Condition1_RRNn', 'cat__Condition2_Artery',\n       'cat__Condition2_Feedr', 'cat__Condition2_Norm',\n       'cat__Condition2_PosN', 'cat__Condition2_RRAe',\n       'cat__Condition2_RRAn', 'cat__Condition2_RRNn',\n       'cat__BldgType_1Fam', 'cat__BldgType_2fmCon',\n       'cat__BldgType_Duplex', 'cat__BldgType_Twnhs',\n       'cat__BldgType_TwnhsE', 'cat__HouseStyle_1.5Fin',\n       'cat__HouseStyle_1.5Unf', 'cat__HouseStyle_1Story',\n       'cat__HouseStyle_2.5Fin', 'cat__HouseStyle_2.5Unf',\n       'cat__HouseStyle_2Story', 'cat__HouseStyle_SFoyer',\n       'cat__HouseStyle_SLvl', 'cat__RoofStyle_Flat',\n       'cat__RoofStyle_Gable', 'cat__RoofStyle_Gambrel',\n       'cat__RoofStyle_Hip', 'cat__RoofStyle_Mansard',\n       'cat__RoofStyle_Shed', 'cat__RoofMatl_ClyTile',\n       'cat__RoofMatl_CompShg', 'cat__RoofMatl_Membran',\n       'cat__RoofMatl_Metal', 'cat__RoofMatl_Tar&Grv',\n       'cat__RoofMatl_WdShake', 'cat__RoofMatl_WdShngl',\n       'cat__MasVnrType_BrkCmn', 'cat__MasVnrType_BrkFace',\n       'cat__MasVnrType_Stone', 'cat__ExterQual_Ex', 'cat__ExterQual_Fa',\n       'cat__ExterQual_Gd', 'cat__ExterQual_TA', 'cat__ExterCond_Ex',\n       'cat__ExterCond_Fa', 'cat__ExterCond_Gd', 'cat__ExterCond_Po',\n       'cat__ExterCond_TA', 'cat__Foundation_BrkTil',\n       'cat__Foundation_CBlock', 'cat__Foundation_PConc',\n       'cat__Foundation_Slab', 'cat__Foundation_Stone',\n       'cat__Foundation_Wood', 'cat__BsmtQual_Ex', 'cat__BsmtQual_Fa',\n       'cat__BsmtQual_Gd', 'cat__BsmtQual_TA', 'cat__BsmtCond_Fa',\n       'cat__BsmtCond_Gd', 'cat__BsmtCond_Po', 'cat__BsmtCond_TA',\n       'cat__BsmtExposure_Av', 'cat__BsmtExposure_Gd',\n       'cat__BsmtExposure_Mn', 'cat__BsmtExposure_No',\n       'cat__BsmtFinType1_ALQ', 'cat__BsmtFinType1_BLQ',\n       'cat__BsmtFinType1_GLQ', 'cat__BsmtFinType1_LwQ',\n       'cat__BsmtFinType1_Rec', 'cat__BsmtFinType1_Unf',\n       'cat__BsmtFinType2_ALQ', 'cat__BsmtFinType2_BLQ',\n       'cat__BsmtFinType2_GLQ', 'cat__BsmtFinType2_LwQ',\n       'cat__BsmtFinType2_Rec', 'cat__BsmtFinType2_Unf',\n       'cat__Heating_Floor', 'cat__Heating_GasA', 'cat__Heating_GasW',\n       'cat__Heating_Grav', 'cat__Heating_OthW', 'cat__Heating_Wall',\n       'cat__HeatingQC_Ex', 'cat__HeatingQC_Fa', 'cat__HeatingQC_Gd',\n       'cat__HeatingQC_TA', 'cat__CentralAir_N', 'cat__CentralAir_Y',\n       'cat__Electrical_FuseA', 'cat__Electrical_FuseF',\n       'cat__Electrical_FuseP', 'cat__Electrical_Mix',\n       'cat__Electrical_SBrkr', 'cat__KitchenQual_Ex',\n       'cat__KitchenQual_Fa', 'cat__KitchenQual_Gd',\n       'cat__KitchenQual_TA', 'cat__Functional_Maj1',\n       'cat__Functional_Maj2', 'cat__Functional_Min1',\n       'cat__Functional_Min2', 'cat__Functional_Mod',\n       'cat__Functional_Sev', 'cat__Functional_Typ',\n       'cat__FireplaceQu_Ex', 'cat__FireplaceQu_Fa',\n       'cat__FireplaceQu_Gd', 'cat__FireplaceQu_Po',\n       'cat__FireplaceQu_TA', 'cat__GarageType_2Types',\n       'cat__GarageType_Attchd', 'cat__GarageType_Basment',\n       'cat__GarageType_BuiltIn', 'cat__GarageType_CarPort',\n       'cat__GarageType_Detchd', 'cat__GarageFinish_Fin',\n       'cat__GarageFinish_RFn', 'cat__GarageFinish_Unf',\n       'cat__GarageQual_Ex', 'cat__GarageQual_Fa', 'cat__GarageQual_Gd',\n       'cat__GarageQual_Po', 'cat__GarageQual_TA', 'cat__GarageCond_Ex',\n       'cat__GarageCond_Fa', 'cat__GarageCond_Gd', 'cat__GarageCond_Po',\n       'cat__GarageCond_TA', 'cat__PavedDrive_N', 'cat__PavedDrive_P',\n       'cat__PavedDrive_Y', 'cat__PoolQC_Ex', 'cat__PoolQC_Fa',\n       'cat__PoolQC_Gd', 'cat__Fence_GdPrv', 'cat__Fence_GdWo',\n       'cat__Fence_MnPrv', 'cat__Fence_MnWw', 'cat__MiscFeature_Gar2',\n       'cat__MiscFeature_Othr', 'cat__MiscFeature_Shed',\n       'cat__MiscFeature_TenC', 'cat__SaleType_COD', 'cat__SaleType_CWD',\n       'cat__SaleType_Con', 'cat__SaleType_ConLD', 'cat__SaleType_ConLI',\n       'cat__SaleType_ConLw', 'cat__SaleType_New', 'cat__SaleType_Oth',\n       'cat__SaleType_WD', 'cat__SaleCondition_Abnorml',\n       'cat__SaleCondition_AdjLand', 'cat__SaleCondition_Alloca',\n       'cat__SaleCondition_Family', 'cat__SaleCondition_Normal',\n       'cat__SaleCondition_Partial'], dtype=object)\n\n\n\n\nCode\nfitted_model=pipeline.steps[1][1]\n\n\nvariable importance\n\n\nCode\nimportances = fitted_model.feature_importances_\nvi=pd.DataFrame({\"variable\":var,\"importances\":importances})\nvi=vi.sort_values('importances',ascending=False)\nvi\n\n\n\n\n\n\n\n\n\n\nvariable\nimportances\n\n\n\n\n4\nnum__OverallQual\n5.665596e-01\n\n\n16\nnum__GrLivArea\n1.152691e-01\n\n\n14\nnum__2ndFlrSF\n4.922609e-02\n\n\n12\nnum__TotalBsmtSF\n4.073534e-02\n\n\n9\nnum__BsmtFinSF1\n2.143949e-02\n\n\n...\n...\n...\n\n\n78\ncat__Condition2_RRAn\n1.692313e-08\n\n\n77\ncat__Condition2_RRAe\n8.657450e-09\n\n\n216\ncat__SaleType_Con\n6.480665e-09\n\n\n55\ncat__Utilities_NoSeWa\n1.457866e-09\n\n\n123\ncat__Foundation_Wood\n2.625608e-10\n\n\n\n\n229 rows × 2 columns",
    "crumbs": [
      "Regression",
      "Random forest and pipeline"
    ]
  },
  {
    "objectID": "regression/3 Random forest on house price data.html#preformance",
    "href": "regression/3 Random forest on house price data.html#preformance",
    "title": "Random forest and pipeline",
    "section": "3.4 Preformance",
    "text": "3.4 Preformance\n\n\nCode\nY_pred_dt =pipeline.predict(X_test) #always gets x and retuns y\n\n\nR 2\n\n\nCode\nfrom sklearn.metrics import r2_score\nr2_score(Y_test, Y_pred_dt)\n\n\n0.8610158499188818\n\n\nMAE\n\n\nCode\nfrom sklearn.metrics import mean_absolute_error\nmean_absolute_error(Y_test, Y_pred_dt)\n\n\n19151.616438356163\n\n\nRMSE\n\n\nCode\nfrom  math import sqrt\nfrom sklearn.metrics import mean_squared_error\nmse=mean_squared_error(Y_test, Y_pred_dt)\nrmse=sqrt(mse)\nrmse\n\n\n31742.614883778195",
    "crumbs": [
      "Regression",
      "Random forest and pipeline"
    ]
  },
  {
    "objectID": "regression/3 Random forest on house price data.html#k-fold-cross-validation",
    "href": "regression/3 Random forest on house price data.html#k-fold-cross-validation",
    "title": "Random forest and pipeline",
    "section": "3.5 k-Fold Cross-Validation",
    "text": "3.5 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \n\n\n\n\nCode\ncv_dt = cross_val_score(pipeline, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.8354087162938729\n\n\n\n\nCode\ncv_dt = cross_val_score(pipeline, X_train, Y_train, cv=kf_dt,scoring = 'neg_mean_squared_error')\nnp.mean(np.sqrt(np.abs(cv_dt)))\n\n\n29854.074894412704",
    "crumbs": [
      "Regression",
      "Random forest and pipeline"
    ]
  },
  {
    "objectID": "regression/4 Random forest on house price data.html",
    "href": "regression/4 Random forest on house price data.html",
    "title": "Random forest and pipeline, hyperparameter tuning",
    "section": "",
    "text": "with pipeline and tunning",
    "crumbs": [
      "Regression",
      "Random forest and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/4 Random forest on house price data.html#input-data",
    "href": "regression/4 Random forest on house price data.html#input-data",
    "title": "Random forest and pipeline, hyperparameter tuning",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nId = df_train['Id']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nId\nMSSubClass\nMSZoning\nLotFrontage\nLotArea\nStreet\nAlley\nLotShape\nLandContour\nUtilities\n...\nPoolArea\nPoolQC\nFence\nMiscFeature\nMiscVal\nMoSold\nYrSold\nSaleType\nSaleCondition\nSalePrice\n\n\n\n\n0\n1\n60\nRL\n65.0\n8450\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n2\n2008\nWD\nNormal\n208500\n\n\n1\n2\n20\nRL\n80.0\n9600\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2007\nWD\nNormal\n181500\n\n\n2\n3\n60\nRL\n68.0\n11250\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n9\n2008\nWD\nNormal\n223500\n\n\n3\n4\n70\nRL\n60.0\n9550\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n2\n2006\nWD\nAbnorml\n140000\n\n\n4\n5\n60\nRL\n84.0\n14260\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n12\n2008\nWD\nNormal\n250000\n\n\n\n\n5 rows × 81 columns\n\n\n\n\n\n\nCode\n#df_train.info()\n\n\n\n\nCode\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['SalePrice'], axis=1), df_test])",
    "crumbs": [
      "Regression",
      "Random forest and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/4 Random forest on house price data.html#data-eda",
    "href": "regression/4 Random forest on house price data.html#data-eda",
    "title": "Random forest and pipeline, hyperparameter tuning",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\nin step 1",
    "crumbs": [
      "Regression",
      "Random forest and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/4 Random forest on house price data.html#data-wrangling",
    "href": "regression/4 Random forest on house price data.html#data-wrangling",
    "title": "Random forest and pipeline, hyperparameter tuning",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling",
    "crumbs": [
      "Regression",
      "Random forest and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/4 Random forest on house price data.html#split-data",
    "href": "regression/4 Random forest on house price data.html#split-data",
    "title": "Random forest and pipeline, hyperparameter tuning",
    "section": "2.4 split data",
    "text": "2.4 split data\n\n\nCode\nY = df_train.SalePrice\nX = df_train.drop(['SalePrice'], axis=1)\n\n\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\nX_train = X_train.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nprint(X_train.shape)\nprint(X_test.shape)\n\n\n(1168, 80)\n(292, 80)\n\n\n\n\nCode\nprint(Y_train.shape)\nprint(Y_test.shape)\n\n\n(1168,)\n(292,)\n\n\n\n\nCode\ncategorical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].nunique() &lt; 10 and X_train[cname].dtype == \"object\"]\n                    \n                    \nnumerical_cols = numerical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].dtype in ['int64', 'float64']]\n\n\n\n\nCode\nprint(\"The total number of categorical columns:\", len(categorical_cols))\nprint(\"The total number of numerical columns:\", len(numerical_cols))\n\n\nThe total number of categorical columns: 40\nThe total number of numerical columns: 37\n\n\n\n\nCode\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train[my_cols].copy()\nX_test= X_test[my_cols].copy()\n\n\nX_final = df_test[my_cols].copy()",
    "crumbs": [
      "Regression",
      "Random forest and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/4 Random forest on house price data.html#pipelines-for-data-preprocessing",
    "href": "regression/4 Random forest on house price data.html#pipelines-for-data-preprocessing",
    "title": "Random forest and pipeline, hyperparameter tuning",
    "section": "2.5 Pipelines for Data Preprocessing",
    "text": "2.5 Pipelines for Data Preprocessing\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\nCode\nnumerical_transformer = Pipeline(steps=[\n    ('imputer_num', SimpleImputer(strategy='median')), \n    ('scaler', StandardScaler())\n])\n\n\n\n\nCode\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n\n\nCode\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)])",
    "crumbs": [
      "Regression",
      "Random forest and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/4 Random forest on house price data.html#define-model",
    "href": "regression/4 Random forest on house price data.html#define-model",
    "title": "Random forest and pipeline, hyperparameter tuning",
    "section": "3.1 define model",
    "text": "3.1 define model\nrandom forest with hyper parameter tuning\n\n\nCode\nfrom sklearn.ensemble import RandomForestRegressor\n   \n\nml_model = RandomForestRegressor(random_state=0)\nml_model\n\n\nRandomForestRegressor(random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestRegressor?Documentation for RandomForestRegressoriNot fittedRandomForestRegressor(random_state=0)",
    "crumbs": [
      "Regression",
      "Random forest and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/4 Random forest on house price data.html#define-pipline",
    "href": "regression/4 Random forest on house price data.html#define-pipline",
    "title": "Random forest and pipeline, hyperparameter tuning",
    "section": "3.2 define pipline",
    "text": "3.2 define pipline\n\n\nCode\npipeline = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', ml_model)\n         ]\n)",
    "crumbs": [
      "Regression",
      "Random forest and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/4 Random forest on house price data.html#hyperparameter-tuning-set",
    "href": "regression/4 Random forest on house price data.html#hyperparameter-tuning-set",
    "title": "Random forest and pipeline, hyperparameter tuning",
    "section": "3.3 hyperparameter tuning set",
    "text": "3.3 hyperparameter tuning set\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {'model__max_depth':[20,30,40],\n                 'model__n_estimators':[200,250,300],\n                 'model__min_samples_leaf':[1,2,3]\n                 }\n                 \n              \nGridCV = GridSearchCV(pipeline, param_grid, n_jobs= -1, verbose=1)",
    "crumbs": [
      "Regression",
      "Random forest and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/4 Random forest on house price data.html#train-model",
    "href": "regression/4 Random forest on house price data.html#train-model",
    "title": "Random forest and pipeline, hyperparameter tuning",
    "section": "3.4 train model",
    "text": "3.4 train model\n\n\nCode\nGridCV.fit(X_train, Y_train)\n\n\nFitting 5 folds for each of 27 candidates, totalling 135 fits\n\n\nGridSearchCV(estimator=Pipeline(steps=[('preprocessor',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         Pipeline(steps=[('imputer_num',\n                                                                                          SimpleImputer(strategy='median')),\n                                                                                         ('scaler',\n                                                                                          StandardScaler())]),\n                                                                         ['Id',\n                                                                          'MSSubClass',\n                                                                          'LotFrontage',\n                                                                          'LotArea',\n                                                                          'OverallQual',\n                                                                          'OverallCond',\n                                                                          'YearBuilt',\n                                                                          'YearRemodAdd',\n                                                                          'MasVnrArea',\n                                                                          'BsmtFinSF1',\n                                                                          'BsmtFinSF2',\n                                                                          'BsmtUnfSF',\n                                                                          'TotalBsmtSF...\n                                                                          'Foundation',\n                                                                          'BsmtQual',\n                                                                          'BsmtCond',\n                                                                          'BsmtExposure',\n                                                                          'BsmtFinType1',\n                                                                          'BsmtFinType2',\n                                                                          'Heating',\n                                                                          'HeatingQC',\n                                                                          'CentralAir',\n                                                                          'Electrical',\n                                                                          'KitchenQual',\n                                                                          'Functional',\n                                                                          'FireplaceQu', ...])])),\n                                       ('model',\n                                        RandomForestRegressor(random_state=0))]),\n             n_jobs=-1,\n             param_grid={'model__max_depth': [20, 30, 40],\n                         'model__min_samples_leaf': [1, 2, 3],\n                         'model__n_estimators': [200, 250, 300]},\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(estimator=Pipeline(steps=[('preprocessor',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         Pipeline(steps=[('imputer_num',\n                                                                                          SimpleImputer(strategy='median')),\n                                                                                         ('scaler',\n                                                                                          StandardScaler())]),\n                                                                         ['Id',\n                                                                          'MSSubClass',\n                                                                          'LotFrontage',\n                                                                          'LotArea',\n                                                                          'OverallQual',\n                                                                          'OverallCond',\n                                                                          'YearBuilt',\n                                                                          'YearRemodAdd',\n                                                                          'MasVnrArea',\n                                                                          'BsmtFinSF1',\n                                                                          'BsmtFinSF2',\n                                                                          'BsmtUnfSF',\n                                                                          'TotalBsmtSF...\n                                                                          'Foundation',\n                                                                          'BsmtQual',\n                                                                          'BsmtCond',\n                                                                          'BsmtExposure',\n                                                                          'BsmtFinType1',\n                                                                          'BsmtFinType2',\n                                                                          'Heating',\n                                                                          'HeatingQC',\n                                                                          'CentralAir',\n                                                                          'Electrical',\n                                                                          'KitchenQual',\n                                                                          'Functional',\n                                                                          'FireplaceQu', ...])])),\n                                       ('model',\n                                        RandomForestRegressor(random_state=0))]),\n             n_jobs=-1,\n             param_grid={'model__max_depth': [20, 30, 40],\n                         'model__min_samples_leaf': [1, 2, 3],\n                         'model__n_estimators': [200, 250, 300]},\n             verbose=1) estimator: PipelinePipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer_num',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['Id', 'MSSubClass',\n                                                   'LotFrontage', 'LotArea',\n                                                   'OverallQual', 'OverallCond',\n                                                   'YearBuilt', 'YearRemodAdd',\n                                                   'MasVnrArea', 'BsmtFinSF1',\n                                                   'BsmtFinSF2', 'BsmtUnfSF',\n                                                   'TotalBsmtSF', '1stFlrSF',\n                                                   '2ndFlrSF'...\n                                                   'LotConfig', 'LandSlope',\n                                                   'Condition1', 'Condition2',\n                                                   'BldgType', 'HouseStyle',\n                                                   'RoofStyle', 'RoofMatl',\n                                                   'MasVnrType', 'ExterQual',\n                                                   'ExterCond', 'Foundation',\n                                                   'BsmtQual', 'BsmtCond',\n                                                   'BsmtExposure',\n                                                   'BsmtFinType1',\n                                                   'BsmtFinType2', 'Heating',\n                                                   'HeatingQC', 'CentralAir',\n                                                   'Electrical', 'KitchenQual',\n                                                   'Functional', 'FireplaceQu', ...])])),\n                ('model', RandomForestRegressor(random_state=0))])  preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('imputer_num',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('scaler', StandardScaler())]),\n                                 ['Id', 'MSSubClass', 'LotFrontage', 'LotArea',\n                                  'OverallQual', 'OverallCond', 'YearBuilt',\n                                  'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1',\n                                  'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF',\n                                  '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n                                  'GrLivArea', 'Bsm...\n                                 ['MSZoning', 'Street', 'Alley', 'LotShape',\n                                  'LandContour', 'Utilities', 'LotConfig',\n                                  'LandSlope', 'Condition1', 'Condition2',\n                                  'BldgType', 'HouseStyle', 'RoofStyle',\n                                  'RoofMatl', 'MasVnrType', 'ExterQual',\n                                  'ExterCond', 'Foundation', 'BsmtQual',\n                                  'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n                                  'BsmtFinType2', 'Heating', 'HeatingQC',\n                                  'CentralAir', 'Electrical', 'KitchenQual',\n                                  'Functional', 'FireplaceQu', ...])]) num['Id', 'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='median')  StandardScaler?Documentation for StandardScalerStandardScaler() cat['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'SaleType', 'SaleCondition']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='most_frequent')  OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore')  RandomForestRegressor?Documentation for RandomForestRegressorRandomForestRegressor(random_state=0) \n\n\n\n\nCode\nGridCV.best_params_\n\n\n{'model__max_depth': 20,\n 'model__min_samples_leaf': 1,\n 'model__n_estimators': 300}\n\n\n\n\nCode\nGridCV.best_score_\n\n\n0.8356691049766422\n\n\nbest model as pipeline\n\n\nCode\noptimised_model_pipeline = GridCV.best_estimator_\n\n\n\n\nCode\nvar=optimised_model_pipeline[:-1].get_feature_names_out()\n#var\n\n\n\n\nCode\nfitted_model=optimised_model_pipeline.steps[1][1]\n\n\nvariable importance\n\n\nCode\nimportances = fitted_model.feature_importances_\nvi=pd.DataFrame({\"variable\":var,\"importances\":importances})\nvi=vi.sort_values('importances',ascending=False)\nvi\n\n\n\n\n\n\n\n\n\n\nvariable\nimportances\n\n\n\n\n4\nnum__OverallQual\n5.549448e-01\n\n\n16\nnum__GrLivArea\n1.112510e-01\n\n\n9\nnum__BsmtFinSF1\n3.698242e-02\n\n\n12\nnum__TotalBsmtSF\n3.172121e-02\n\n\n14\nnum__2ndFlrSF\n2.840322e-02\n\n\n...\n...\n...\n\n\n116\ncat__ExterCond_Po\n2.580107e-08\n\n\n196\ncat__GarageCond_Ex\n1.632168e-08\n\n\n212\ncat__MiscFeature_Othr\n7.883686e-09\n\n\n54\ncat__Utilities_AllPub\n5.100970e-09\n\n\n102\ncat__RoofMatl_Roll\n1.013719e-11\n\n\n\n\n230 rows × 2 columns",
    "crumbs": [
      "Regression",
      "Random forest and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/4 Random forest on house price data.html#preformance",
    "href": "regression/4 Random forest on house price data.html#preformance",
    "title": "Random forest and pipeline, hyperparameter tuning",
    "section": "3.5 Preformance",
    "text": "3.5 Preformance\n\n\nCode\nY_pred_dt =optimised_model_pipeline.predict(X_test) #always gets x and retuns y\n\n\nR 2\n\n\nCode\nfrom sklearn.metrics import r2_score\nr2_score(Y_test, Y_pred_dt)\n\n\n0.9060762300832943\n\n\nMAE\n\n\nCode\nfrom sklearn.metrics import mean_absolute_error\nmean_absolute_error(Y_test, Y_pred_dt)\n\n\n16590.44897687198\n\n\nRMSE\n\n\nCode\nfrom  math import sqrt\nfrom sklearn.metrics import mean_squared_error\nmse=mean_squared_error(Y_test, Y_pred_dt)\nrmse=sqrt(mse)\nrmse\n\n\n25706.585324016956",
    "crumbs": [
      "Regression",
      "Random forest and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/4 Random forest on house price data.html#k-fold-cross-validation",
    "href": "regression/4 Random forest on house price data.html#k-fold-cross-validation",
    "title": "Random forest and pipeline, hyperparameter tuning",
    "section": "3.6 k-Fold Cross-Validation",
    "text": "3.6 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \n\n\n\n\nCode\ncv_dt = cross_val_score(optimised_model_pipeline, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.8401868489597248\n\n\n\n\nCode\ncv_dt = cross_val_score(optimised_model_pipeline, X_train, Y_train, cv=kf_dt,scoring = 'neg_mean_squared_error')\nnp.mean(np.sqrt(np.abs(cv_dt)))\n\n\n30709.02967606353",
    "crumbs": [
      "Regression",
      "Random forest and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\nhttps://tonyfly3000.github.io/scikit-learn-in-Python/\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "intro/3 install package.html",
    "href": "intro/3 install package.html",
    "title": "install pacakge",
    "section": "",
    "text": "1 install package\n\n\nCode\nimport os\nos.system('pip install -U scikit-learn')\n\n\n\n\n2 check one package version\n\n\nCode\nimport os\nos.system('pip show scikit-learn')\n\n\nName: scikit-learn\nVersion: 1.4.1.post1\nSummary: A set of python modules for machine learning and data mining\nHome-page: https://scikit-learn.org\nAuthor: \nAuthor-email: \nLicense: new BSD\nLocation: /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages\nRequires: joblib, numpy, scipy, threadpoolctl\nRequired-by: librosa\n\n\n0\n\n\n\n\n3 check all package version\n\n\nCode\nimport os\nos.system('pip list')\n\n\nPackage                      Version\n---------------------------- ------------\nabsl-py                      1.4.0\naioquic-mitmproxy            0.9.21.1\nannotated-types              0.6.0\nanyio                        4.0.0\nappdirs                      1.4.4\nappnope                      0.1.3\nargon2-cffi                  23.1.0\nargon2-cffi-bindings         21.2.0\narrow                        1.3.0\nasgiref                      3.7.2\nasttokens                    2.4.0\nastunparse                   1.6.3\nasync-lru                    2.0.4\nattrs                        23.1.0\naudioread                    3.0.1\nBabel                        2.13.0\nbackcall                     0.2.0\nbeautifulsoup4               4.12.2\nbleach                       6.1.0\nblinker                      1.7.0\nBrotli                       1.1.0\ncachetools                   5.3.1\ncertifi                      2023.7.22\ncffi                         1.16.0\ncharset-normalizer           3.3.0\nclick                        8.1.7\ncomm                         0.1.4\ncontourpy                    1.1.1\ncryptography                 41.0.7\ncycler                       0.12.1\ndacite                       1.8.1\ndebugpy                      1.8.0\ndecorator                    5.1.1\ndefusedxml                   0.7.1\ndill                         0.3.8\ndm-tree                      0.1.8\netils                        1.8.0\nexecuting                    2.0.0\nfastjsonschema               2.18.1\nfilelock                     3.12.4\nFlask                        3.0.0\nflatbuffers                  23.5.26\nfonttools                    4.43.1\nfqdn                         1.5.1\nfsspec                       2023.9.2\nfuture                       1.0.0\ngast                         0.5.4\ngoogle-auth                  2.23.3\ngoogle-auth-oauthlib         1.0.0\ngoogle-pasta                 0.2.0\ngoogleapis-common-protos     1.63.0\ngrpcio                       1.59.0\nh11                          0.14.0\nh2                           4.1.0\nh5py                         3.10.0\nhpack                        4.0.0\nhtmlmin                      0.1.12\nhtmltools                    0.5.1\nhuggingface-hub              0.17.3\nhyperframe                   6.0.1\nidna                         3.4\nImageHash                    4.3.1\nimportlib_resources          6.4.0\ninstall                      1.3.5\nipykernel                    6.25.2\nipython                      8.16.1\nipython-genutils             0.2.0\nipywidgets                   8.1.1\nisoduration                  20.11.0\nitsdangerous                 2.1.2\njedi                         0.19.1\nJinja2                       3.1.2\njoblib                       1.3.2\njson5                        0.9.14\njsonpointer                  2.4\njsonschema                   4.19.1\njsonschema-specifications    2023.7.1\njupyter                      1.0.0\njupyter_client               8.3.1\njupyter-console              6.6.3\njupyter_core                 5.3.2\njupyter-events               0.7.0\njupyter-lsp                  2.2.0\njupyter_server               2.7.3\njupyter_server_terminals     0.4.4\njupyterlab                   4.0.6\njupyterlab-pygments          0.2.2\njupyterlab_server            2.25.0\njupyterlab-widgets           3.0.9\nkaggle                       1.5.16\nkaitaistruct                 0.10\nkeras                        3.1.1\nKeras-Preprocessing          1.1.2\nkiwisolver                   1.4.5\nlazy_loader                  0.3\nldap3                        2.9.1\nlibclang                     16.0.6\nlibrosa                      0.10.1\nlinkify-it-py                2.0.3\nllvmlite                     0.41.0\nlxml                         4.9.3\nlzstring                     1.0.4\nMarkdown                     3.5\nmarkdown-it-py               3.0.0\nMarkupSafe                   2.1.3\nmatplotlib                   3.8.3\nmatplotlib-inline            0.1.6\nmdit-py-plugins              0.4.0\nmdurl                        0.1.2\nmissingno                    0.5.2\nmistune                      3.0.2\nmitmproxy                    10.1.5\nmitmproxy-macos              0.4.1\nmitmproxy_rs                 0.4.1\nmizani                       0.11.1\nml-dtypes                    0.3.2\nmpmath                       1.3.0\nmsgpack                      1.0.7\nmultimethod                  1.10\nmutagen                      1.47.0\nnamex                        0.0.7\nnbclient                     0.8.0\nnbconvert                    7.9.2\nnbformat                     5.9.2\nnest-asyncio                 1.5.8\nnetworkx                     3.1\nnltk                         3.8.1\nnotebook                     7.0.4\nnotebook_shim                0.2.3\nnumba                        0.58.0\nnumpy                        1.26.4\noauthlib                     3.2.2\nopencv-python                4.8.1.78\nopendatasets                 0.1.22\nopt-einsum                   3.3.0\noptree                       0.10.0\noutcome                      1.3.0.post0\noverrides                    7.4.0\npackaging                    23.2\npandas                       2.2.1\npandas-profiling             3.2.0\npandocfilters                1.5.0\nparso                        0.8.3\npasslib                      1.7.4\npatchworklib                 0.6.4\npatsy                        0.5.3\npexpect                      4.8.0\nphik                         0.12.3\npickleshare                  0.7.5\nPillow                       10.0.1\npip                          24.0\nplatformdirs                 3.11.0\nplotly                       5.20.0\nplotnine                     0.13.3\npooch                        1.7.0\nprometheus-client            0.17.1\npromise                      2.3\nprompt-toolkit               3.0.36\nprotobuf                     3.20.3\npsutil                       5.9.5\nptyprocess                   0.7.0\npublicsuffix2                2.20191221\npure-eval                    0.2.2\npyarrow                      15.0.2\npyasn1                       0.5.0\npyasn1-modules               0.3.0\npycparser                    2.21\npycryptodomex                3.20.0\npydantic                     1.10.13\npydantic_core                2.10.1\npydantic-settings            2.0.3\nPygments                     2.16.1\npylsqpack                    0.3.18\npyOpenSSL                    23.3.0\npyparsing                    3.1.1\npyperclip                    1.8.2\npypi-latest                  0.1.2\nPySocks                      1.7.1\npython-dateutil              2.8.2\npython-dotenv                1.0.0\npython-json-logger           2.0.7\npython-multipart             0.0.9\npython-slugify               8.0.1\npytz                         2023.3.post1\nPyWavelets                   1.4.1\nPyYAML                       6.0.1\npyzmq                        25.1.1\nqtconsole                    5.4.4\nQtPy                         2.4.0\nquestionary                  2.0.1\nreferencing                  0.30.2\nregex                        2023.10.3\nrequests                     2.31.0\nrequests-oauthlib            1.3.1\nrfc3339-validator            0.1.4\nrfc3986-validator            0.1.1\nrich                         13.7.1\nrpds-py                      0.10.4\nrsa                          4.9\nruamel.yaml                  0.18.5\nruamel.yaml.clib             0.2.8\nsafetensors                  0.4.0\nscikit-learn                 1.4.1.post1\nscipy                        1.12.0\nseaborn                      0.12.2\nselenium                     4.15.2\nSend2Trash                   1.8.2\nservice-identity             23.1.0\nsetuptools                   65.5.0\nshiny                        0.7.1\nshinylive                    0.2.2\nsiuba                        0.4.4\nsix                          1.16.0\nsniffio                      1.3.0\nsortedcontainers             2.4.0\nsoundfile                    0.12.1\nsoupsieve                    2.5\nsoxr                         0.3.7\nSQLAlchemy                   2.0.22\nstack-data                   0.6.3\nstarlette                    0.34.0\nstatsmodels                  0.14.0\nsweetviz                     2.3.1\nsympy                        1.12\ntangled-up-in-unicode        0.2.0\ntenacity                     8.2.3\ntensorboard                  2.16.2\ntensorboard-data-server      0.7.1\ntensorflow                   2.16.1\ntensorflow-datasets          4.9.4\ntensorflow_decision_forests  1.9.0\ntensorflow-estimator         2.14.0\ntensorflow-io-gcs-filesystem 0.34.0\ntensorflow-macos             2.14.0\ntensorflow-metadata          1.14.0\ntermcolor                    2.3.0\nterminado                    0.17.1\ntext-unidecode               1.3\ntf_keras                     2.16.0\nthreadpoolctl                3.2.0\ntinycss2                     1.2.1\ntokenizers                   0.14.1\ntoml                         0.10.2\ntorch                        2.1.0\ntorchvision                  0.16.0\ntornado                      6.3.3\ntqdm                         4.66.1\ntraitlets                    5.11.2\ntransformers                 4.34.0\ntrio                         0.23.1\ntrio-websocket               0.11.1\ntypeguard                    4.1.5\ntypes-python-dateutil        2.8.19.14\ntyping_extensions            4.8.0\ntzdata                       2023.3\nuc-micro-py                  1.0.3\nuri-template                 1.3.0\nurllib3                      2.0.6\nurwid-mitmproxy              2.1.2.1\nuvicorn                      0.27.1\nvisions                      0.7.5\nwatchfiles                   0.21.0\nwcwidth                      0.2.8\nwebcolors                    1.13\nwebencodings                 0.5.1\nwebsocket-client             1.6.4\nwebsockets                   12.0\nWerkzeug                     3.0.0\nwheel                        0.41.2\nwidgetsnbextension           4.0.9\nwordcloud                    1.9.2\nwrapt                        1.14.1\nwsproto                      1.2.0\nwurlitzer                    3.0.3\nxgboost                      2.0.3\nydata-profiling              4.6.0\nyt-dlp                       2023.12.30\nzipp                         3.18.1\nzstandard                    0.22.0\n\n\n0",
    "crumbs": [
      "Intro",
      "install pacakge"
    ]
  },
  {
    "objectID": "plot/2 plotnine.html",
    "href": "plot/2 plotnine.html",
    "title": "plotnine chart",
    "section": "",
    "text": "plotnine is an implementation of a grammar of graphics in Python based on ggplot2.\nCode\nimport plotnine\nprint(plotnine.__version__)\n\n\n0.13.3\nCode\nfrom plotnine import *\n\nprint(plotnine.__version__)\n\n\n0.13.3\nCode\nfrom plotnine import *\nimport seaborn as sns\nimport pandas as pd\n\n\n# Apply the default theme\n\n\n# Load an example dataset\ntips = sns.load_dataset(\"tips\")\ntips.head()\n\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4",
    "crumbs": [
      "Plot",
      "plotnine chart"
    ]
  },
  {
    "objectID": "plot/2 plotnine.html#color-by-group",
    "href": "plot/2 plotnine.html#color-by-group",
    "title": "plotnine chart",
    "section": "1.1 color by group",
    "text": "1.1 color by group\n\n\nCode\np=(\n    ggplot(data=tips)+aes(x=\"tip\",y=\"total_bill\")+ geom_point(aes(color=\"sex\"))\n)\n\np",
    "crumbs": [
      "Plot",
      "plotnine chart"
    ]
  },
  {
    "objectID": "plot/2 plotnine.html#size-by-group",
    "href": "plot/2 plotnine.html#size-by-group",
    "title": "plotnine chart",
    "section": "1.2 size by group",
    "text": "1.2 size by group\n\n\nCode\np=(\n    ggplot(data=tips)+aes(x=\"tip\",y=\"total_bill\",size=\"size\")+ geom_point()\n)\n\np",
    "crumbs": [
      "Plot",
      "plotnine chart"
    ]
  },
  {
    "objectID": "plot/2 plotnine.html#color-by-group-1",
    "href": "plot/2 plotnine.html#color-by-group-1",
    "title": "plotnine chart",
    "section": "2.1 color by group",
    "text": "2.1 color by group\n\n\nCode\np=(\n    ggplot(data=dowjones4)+aes(x=\"Date\",y=\"Price\")+ geom_line(aes(color=\"type\"))\n)\n\np",
    "crumbs": [
      "Plot",
      "plotnine chart"
    ]
  },
  {
    "objectID": "plot/2 plotnine.html#color-by-group-2",
    "href": "plot/2 plotnine.html#color-by-group-2",
    "title": "plotnine chart",
    "section": "3.1 color by group",
    "text": "3.1 color by group\n\n\nCode\np=(\n    ggplot(data=tips)+aes(x=\"tip\",fill = 'sex')+ geom_histogram(position = 'dodge')\n)\n\np",
    "crumbs": [
      "Plot",
      "plotnine chart"
    ]
  },
  {
    "objectID": "plot/2 plotnine.html#color-by-group-3",
    "href": "plot/2 plotnine.html#color-by-group-3",
    "title": "plotnine chart",
    "section": "5.1 color by group",
    "text": "5.1 color by group\n\n\nCode\np=(\n    ggplot(data=tips)+aes(x='day',y='tip',fill=\"sex\")+geom_boxplot()\n)\n\np",
    "crumbs": [
      "Plot",
      "plotnine chart"
    ]
  },
  {
    "objectID": "plot/2 plotnine.html#color-by-group-4",
    "href": "plot/2 plotnine.html#color-by-group-4",
    "title": "plotnine chart",
    "section": "6.1 color by group",
    "text": "6.1 color by group\n\n\nCode\np=(\n    ggplot(data=tips)+aes(x='day',y='tip',fill=\"sex\")+geom_jitter(position=position_jitterdodge())\n)\n\np",
    "crumbs": [
      "Plot",
      "plotnine chart"
    ]
  },
  {
    "objectID": "classification/1 decision tree on titanic data.html",
    "href": "classification/1 decision tree on titanic data.html",
    "title": "Decision tree",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split",
    "crumbs": [
      "Classification",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification/1 decision tree on titanic data.html#input-data",
    "href": "classification/1 decision tree on titanic data.html#input-data",
    "title": "Decision tree",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification/1 decision tree on titanic data.html#data-eda",
    "href": "classification/1 decision tree on titanic data.html#data-eda",
    "title": "Decision tree",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n\n\n0.7420382165605095\n0.18890814558058924",
    "crumbs": [
      "Classification",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification/1 decision tree on titanic data.html#data-wrangling",
    "href": "classification/1 decision tree on titanic data.html#data-wrangling",
    "title": "Decision tree",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1046 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1308 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Dealing with missing numerical variables\ndata['Age'] = data.Age.fillna(data.Age.median())\ndata['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1309 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1309 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\ndata = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\nSex_male\n\n\n\n\n0\n1\n3\nBraund, Mr. Owen Harris\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\nTrue\n\n\n1\n2\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\nFalse\n\n\n2\n3\n3\nHeikkinen, Miss. Laina\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\nFalse\n\n\n3\n4\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\nFalse\n\n\n4\n5\n3\nAllen, Mr. William Henry\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\nTrue\n\n\n\n\n\n\n\n\n\n\nCode\n# Select features columns\ndata = data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()\n\n\n\n\n\n\n\n\n\n\nSex_male\nFare\nAge\nPclass\nSibSp\nrole\n\n\n\n\n0\nTrue\n7.2500\n22.0\n3\n1\ntrain\n\n\n1\nFalse\n71.2833\n38.0\n1\n1\ntrain\n\n\n2\nFalse\n7.9250\n26.0\n3\n0\ntrain\n\n\n3\nFalse\n53.1000\n35.0\n1\n1\ntrain\n\n\n4\nTrue\n8.0500\n35.0\n3\n0\ntrain",
    "crumbs": [
      "Classification",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification/1 decision tree on titanic data.html#split-data",
    "href": "classification/1 decision tree on titanic data.html#split-data",
    "title": "Decision tree",
    "section": "2.4 split data",
    "text": "2.4 split data\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\nX_train = X_train.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 712 entries, 694 to 823\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Sex_male  712 non-null    bool   \n 1   Fare      712 non-null    float64\n 2   Age       712 non-null    float64\n 3   Pclass    712 non-null    int64  \n 4   SibSp     712 non-null    int64  \ndtypes: bool(1), float64(2), int64(2)\nmemory usage: 28.5 KB",
    "crumbs": [
      "Classification",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification/1 decision tree on titanic data.html#define-model",
    "href": "classification/1 decision tree on titanic data.html#define-model",
    "title": "Decision tree",
    "section": "3.1 define model",
    "text": "3.1 define model\n\n\nCode\nmodel_dt = tree.DecisionTreeClassifier(max_depth=3)   #model with deph 3\nmodel_dt\n\n\nDecisionTreeClassifier(max_depth=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriNot fittedDecisionTreeClassifier(max_depth=3)",
    "crumbs": [
      "Classification",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification/1 decision tree on titanic data.html#train-model",
    "href": "classification/1 decision tree on titanic data.html#train-model",
    "title": "Decision tree",
    "section": "3.2 train model",
    "text": "3.2 train model\n\n\nCode\nmodel_dt.fit(X_train,Y_train)\n\n\nDecisionTreeClassifier(max_depth=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier(max_depth=3) \n\n\nvariable importance\n\n\nCode\nimportances = model_dt.feature_importances_\nvi=pd.DataFrame({\"variable\":X_train.columns,\"importances\":importances})\nvi=vi.sort_values('importances',ascending=False)\nvi\n\n\n\n\n\n\n\n\n\n\nvariable\nimportances\n\n\n\n\n0\nSex_male\n0.659607\n\n\n3\nPclass\n0.200035\n\n\n1\nFare\n0.075849\n\n\n2\nAge\n0.064509\n\n\n4\nSibSp\n0.000000",
    "crumbs": [
      "Classification",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification/1 decision tree on titanic data.html#preformance",
    "href": "classification/1 decision tree on titanic data.html#preformance",
    "title": "Decision tree",
    "section": "3.3 Preformance",
    "text": "3.3 Preformance\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = model_dt.predict(X_test) #always gets x and retuns y\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_test,Y_pred_dt)  \naccuracy\n\n\n0.776536312849162\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_test,Y_pred_dt)  \nprecision_dt\n\n\n0.8148148148148148\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_test,Y_pred_dt)  \nrecall_dt\n\n\n0.5945945945945946\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_test,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[95, 10],\n       [30, 44]])\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_test, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.7496782496782497",
    "crumbs": [
      "Classification",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification/1 decision tree on titanic data.html#k-fold-cross-validation",
    "href": "classification/1 decision tree on titanic data.html#k-fold-cross-validation",
    "title": "Decision tree",
    "section": "3.4 k-Fold Cross-Validation",
    "text": "3.4 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(model_dt, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.8229291834925638",
    "crumbs": [
      "Classification",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification/0 titanic data.html#download-data",
    "href": "classification/0 titanic data.html#download-data",
    "title": "Titanic Dataset",
    "section": "2.1 download data",
    "text": "2.1 download data\nhttps://www.kaggle.com/c/titanic/data",
    "crumbs": [
      "Classification",
      "Titanic Dataset"
    ]
  },
  {
    "objectID": "classification/0 titanic data.html#input-data",
    "href": "classification/0 titanic data.html#input-data",
    "title": "Titanic Dataset",
    "section": "2.2 input data",
    "text": "2.2 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "Titanic Dataset"
    ]
  },
  {
    "objectID": "classification/0 titanic data.html#data-eda",
    "href": "classification/0 titanic data.html#data-eda",
    "title": "Titanic Dataset",
    "section": "2.3 data EDA",
    "text": "2.3 data EDA\n\n\nCode\ndf_train.describe()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\n\n\n\n\ncount\n891.000000\n891.000000\n891.000000\n714.000000\n891.000000\n891.000000\n891.000000\n\n\nmean\n446.000000\n0.383838\n2.308642\n29.699118\n0.523008\n0.381594\n32.204208\n\n\nstd\n257.353842\n0.486592\n0.836071\n14.526497\n1.102743\n0.806057\n49.693429\n\n\nmin\n1.000000\n0.000000\n1.000000\n0.420000\n0.000000\n0.000000\n0.000000\n\n\n25%\n223.500000\n0.000000\n2.000000\n20.125000\n0.000000\n0.000000\n7.910400\n\n\n50%\n446.000000\n0.000000\n3.000000\n28.000000\n0.000000\n0.000000\n14.454200\n\n\n75%\n668.500000\n1.000000\n3.000000\n38.000000\n1.000000\n0.000000\n31.000000\n\n\nmax\n891.000000\n1.000000\n3.000000\n80.000000\n8.000000\n6.000000\n512.329200\n\n\n\n\n\n\n\n\n\n\nCode\ndf_train.describe(include=[object])\n\n\n\n\n\n\n\n\n\n\nName\nSex\nTicket\nCabin\nEmbarked\n\n\n\n\ncount\n891\n891\n891\n204\n889\n\n\nunique\n891\n2\n681\n147\n3\n\n\ntop\nBraund, Mr. Owen Harris\nmale\n347082\nB96 B98\nS\n\n\nfreq\n1\n577\n7\n4\n644\n\n\n\n\n\n\n\n\nMissing Data\n\n\nCode\ndf_train.isnull().sum()\n\n\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\n\n\n\n\nCode\nimport seaborn as sns\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport sweetviz as sv\nmy_report = sv.analyze(df_train)\n\n\n\n\n\n\n\nCode\nmy_report.show_notebook()",
    "crumbs": [
      "Classification",
      "Titanic Dataset"
    ]
  },
  {
    "objectID": "classification/0 titanic data.html#feature-vs-target",
    "href": "classification/0 titanic data.html#feature-vs-target",
    "title": "Titanic Dataset",
    "section": "2.4 feature vs target",
    "text": "2.4 feature vs target\n\n\nCode\nmy_report2 = sv.analyze(df_train,target_feat='Survived')\n\n\n\n\n\n\n\nCode\nmy_report2.show_notebook()",
    "crumbs": [
      "Classification",
      "Titanic Dataset"
    ]
  },
  {
    "objectID": "classification/0 titanic data.html#compare-train-data-and-test-data",
    "href": "classification/0 titanic data.html#compare-train-data-and-test-data",
    "title": "Titanic Dataset",
    "section": "2.5 compare train data and test data",
    "text": "2.5 compare train data and test data\n\n\nCode\ncompare = sv.compare(source=df_train, compare=df_test)\n\n\n\n\n\n\n\nCode\ncompare.show_notebook()",
    "crumbs": [
      "Classification",
      "Titanic Dataset"
    ]
  },
  {
    "objectID": "classification/0 titanic data.html#data-dictionary",
    "href": "classification/0 titanic data.html#data-dictionary",
    "title": "Titanic Dataset",
    "section": "2.6 data dictionary",
    "text": "2.6 data dictionary\npclass: A proxy for socio-economic status (SES) 1st = Upper 2nd = Middle 3rd = Lower\nage: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\nsibsp: The dataset defines family relations in this way…\nSibling = brother, sister, stepbrother, stepsister\nSpouse = husband, wife (mistresses and fiancés were ignored)\nparch: The dataset defines family relations in this way…\nParent = mother, father\nChild = daughter, son, stepdaughter, stepson Some children travelled only with a nanny, therefore parch=0 for them.",
    "crumbs": [
      "Classification",
      "Titanic Dataset"
    ]
  },
  {
    "objectID": "classification/2 Logistic Regression on titanic data.html",
    "href": "classification/2 Logistic Regression on titanic data.html",
    "title": "Classification with Logistic Regression",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split",
    "crumbs": [
      "Classification",
      "Classification with Logistic Regression"
    ]
  },
  {
    "objectID": "classification/2 Logistic Regression on titanic data.html#input-data",
    "href": "classification/2 Logistic Regression on titanic data.html#input-data",
    "title": "Classification with Logistic Regression",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "Classification with Logistic Regression"
    ]
  },
  {
    "objectID": "classification/2 Logistic Regression on titanic data.html#data-eda",
    "href": "classification/2 Logistic Regression on titanic data.html#data-eda",
    "title": "Classification with Logistic Regression",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n\n\n0.7420382165605095\n0.18890814558058924",
    "crumbs": [
      "Classification",
      "Classification with Logistic Regression"
    ]
  },
  {
    "objectID": "classification/2 Logistic Regression on titanic data.html#data-wrangling",
    "href": "classification/2 Logistic Regression on titanic data.html#data-wrangling",
    "title": "Classification with Logistic Regression",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1046 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1308 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Dealing with missing numerical variables\ndata['Age'] = data.Age.fillna(data.Age.median())\ndata['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1309 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1309 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\ndata = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\nSex_male\n\n\n\n\n0\n1\n3\nBraund, Mr. Owen Harris\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\nTrue\n\n\n1\n2\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\nFalse\n\n\n2\n3\n3\nHeikkinen, Miss. Laina\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\nFalse\n\n\n3\n4\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\nFalse\n\n\n4\n5\n3\nAllen, Mr. William Henry\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\nTrue\n\n\n\n\n\n\n\n\n\n\nCode\n# Select features columns\ndata = data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()\n\n\n\n\n\n\n\n\n\n\nSex_male\nFare\nAge\nPclass\nSibSp\nrole\n\n\n\n\n0\nTrue\n7.2500\n22.0\n3\n1\ntrain\n\n\n1\nFalse\n71.2833\n38.0\n1\n1\ntrain\n\n\n2\nFalse\n7.9250\n26.0\n3\n0\ntrain\n\n\n3\nFalse\n53.1000\n35.0\n1\n1\ntrain\n\n\n4\nTrue\n8.0500\n35.0\n3\n0\ntrain",
    "crumbs": [
      "Classification",
      "Classification with Logistic Regression"
    ]
  },
  {
    "objectID": "classification/2 Logistic Regression on titanic data.html#split-data",
    "href": "classification/2 Logistic Regression on titanic data.html#split-data",
    "title": "Classification with Logistic Regression",
    "section": "2.4 split data",
    "text": "2.4 split data\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\nX_train = X_train.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 712 entries, 47 to 317\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Sex_male  712 non-null    bool   \n 1   Fare      712 non-null    float64\n 2   Age       712 non-null    float64\n 3   Pclass    712 non-null    int64  \n 4   SibSp     712 non-null    int64  \ndtypes: bool(1), float64(2), int64(2)\nmemory usage: 28.5 KB",
    "crumbs": [
      "Classification",
      "Classification with Logistic Regression"
    ]
  },
  {
    "objectID": "classification/2 Logistic Regression on titanic data.html#define-model",
    "href": "classification/2 Logistic Regression on titanic data.html#define-model",
    "title": "Classification with Logistic Regression",
    "section": "3.1 define model",
    "text": "3.1 define model\nThe solvers implemented in the class Logistic Regression are “liblinear”, “newton-cg”, “lbfgs”, “sag” and “saga”. According to Scikit Documentation: The “liblinear” solver was the one used by default for historical reasons before version 0.22. Since then, default use is lbfgs Algorithm.\n\n\nCode\nfrom sklearn.linear_model import LogisticRegression\nml_model = LogisticRegression(solver='liblinear')\nml_model\n\n\nLogisticRegression(solver='liblinear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniNot fittedLogisticRegression(solver='liblinear')",
    "crumbs": [
      "Classification",
      "Classification with Logistic Regression"
    ]
  },
  {
    "objectID": "classification/2 Logistic Regression on titanic data.html#train-model",
    "href": "classification/2 Logistic Regression on titanic data.html#train-model",
    "title": "Classification with Logistic Regression",
    "section": "3.2 train model",
    "text": "3.2 train model\n\n\nCode\nml_model.fit(X_train,Y_train)\n\n\nLogisticRegression(solver='liblinear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression(solver='liblinear') \n\n\nvariable importance\n\n\nCode\ncoefficients = ml_model.coef_[0]\n\nfeature_importance = pd.DataFrame({'Feature': X_train.columns, 'Importance': np.abs(coefficients)})\nfeature_importance = feature_importance.sort_values('Importance', ascending=False)\nfeature_importance\n\n\n\n\n\n\n\n\n\n\nFeature\nImportance\n\n\n\n\n0\nSex_male\n2.527450\n\n\n3\nPclass\n0.722334\n\n\n4\nSibSp\n0.325418\n\n\n2\nAge\n0.025424\n\n\n1\nFare\n0.004535",
    "crumbs": [
      "Classification",
      "Classification with Logistic Regression"
    ]
  },
  {
    "objectID": "classification/2 Logistic Regression on titanic data.html#preformance",
    "href": "classification/2 Logistic Regression on titanic data.html#preformance",
    "title": "Classification with Logistic Regression",
    "section": "3.3 Preformance",
    "text": "3.3 Preformance\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = ml_model.predict(X_test) #always gets x and retuns y\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_test,Y_pred_dt)  \naccuracy\n\n\n0.7932960893854749\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_test,Y_pred_dt)  \nprecision_dt\n\n\n0.6842105263157895\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_test,Y_pred_dt)  \nrecall_dt\n\n\n0.6724137931034483\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_test,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[103,  18],\n       [ 19,  39]])\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_test, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.761826731262468",
    "crumbs": [
      "Classification",
      "Classification with Logistic Regression"
    ]
  },
  {
    "objectID": "classification/2 Logistic Regression on titanic data.html#k-fold-cross-validation",
    "href": "classification/2 Logistic Regression on titanic data.html#k-fold-cross-validation",
    "title": "Classification with Logistic Regression",
    "section": "3.4 k-Fold Cross-Validation",
    "text": "3.4 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(ml_model, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.7921205555008373",
    "crumbs": [
      "Classification",
      "Classification with Logistic Regression"
    ]
  },
  {
    "objectID": "classification/3 Support Vector Machines on titanic data.html",
    "href": "classification/3 Support Vector Machines on titanic data.html",
    "title": "Classification with Support Vector Machines",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split",
    "crumbs": [
      "Classification",
      "Classification with Support Vector Machines"
    ]
  },
  {
    "objectID": "classification/3 Support Vector Machines on titanic data.html#input-data",
    "href": "classification/3 Support Vector Machines on titanic data.html#input-data",
    "title": "Classification with Support Vector Machines",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "Classification with Support Vector Machines"
    ]
  },
  {
    "objectID": "classification/3 Support Vector Machines on titanic data.html#data-eda",
    "href": "classification/3 Support Vector Machines on titanic data.html#data-eda",
    "title": "Classification with Support Vector Machines",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n\n\n0.7420382165605095\n0.18890814558058924",
    "crumbs": [
      "Classification",
      "Classification with Support Vector Machines"
    ]
  },
  {
    "objectID": "classification/3 Support Vector Machines on titanic data.html#data-wrangling",
    "href": "classification/3 Support Vector Machines on titanic data.html#data-wrangling",
    "title": "Classification with Support Vector Machines",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1046 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1308 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Dealing with missing numerical variables\ndata['Age'] = data.Age.fillna(data.Age.median())\ndata['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1309 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1309 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\ndata = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\nSex_male\n\n\n\n\n0\n1\n3\nBraund, Mr. Owen Harris\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\nTrue\n\n\n1\n2\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\nFalse\n\n\n2\n3\n3\nHeikkinen, Miss. Laina\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\nFalse\n\n\n3\n4\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\nFalse\n\n\n4\n5\n3\nAllen, Mr. William Henry\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\nTrue\n\n\n\n\n\n\n\n\n\n\nCode\n# Select features columns\ndata = data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()\n\n\n\n\n\n\n\n\n\n\nSex_male\nFare\nAge\nPclass\nSibSp\nrole\n\n\n\n\n0\nTrue\n7.2500\n22.0\n3\n1\ntrain\n\n\n1\nFalse\n71.2833\n38.0\n1\n1\ntrain\n\n\n2\nFalse\n7.9250\n26.0\n3\n0\ntrain\n\n\n3\nFalse\n53.1000\n35.0\n1\n1\ntrain\n\n\n4\nTrue\n8.0500\n35.0\n3\n0\ntrain",
    "crumbs": [
      "Classification",
      "Classification with Support Vector Machines"
    ]
  },
  {
    "objectID": "classification/3 Support Vector Machines on titanic data.html#split-data",
    "href": "classification/3 Support Vector Machines on titanic data.html#split-data",
    "title": "Classification with Support Vector Machines",
    "section": "2.4 split data",
    "text": "2.4 split data\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\nX_train = X_train.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 712 entries, 280 to 57\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Sex_male  712 non-null    bool   \n 1   Fare      712 non-null    float64\n 2   Age       712 non-null    float64\n 3   Pclass    712 non-null    int64  \n 4   SibSp     712 non-null    int64  \ndtypes: bool(1), float64(2), int64(2)\nmemory usage: 28.5 KB",
    "crumbs": [
      "Classification",
      "Classification with Support Vector Machines"
    ]
  },
  {
    "objectID": "classification/3 Support Vector Machines on titanic data.html#define-model",
    "href": "classification/3 Support Vector Machines on titanic data.html#define-model",
    "title": "Classification with Support Vector Machines",
    "section": "3.1 define model",
    "text": "3.1 define model\nThe solvers implemented in the class Logistic Regression are “liblinear”, “newton-cg”, “lbfgs”, “sag” and “saga”. According to Scikit Documentation: The “liblinear” solver was the one used by default for historical reasons before version 0.22. Since then, default use is lbfgs Algorithm.\n\n\nCode\nfrom sklearn import svm\nml_model = svm.SVC(kernel=\"linear\")\nml_model\n\n\nSVC(kernel='linear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  SVC?Documentation for SVCiNot fittedSVC(kernel='linear')",
    "crumbs": [
      "Classification",
      "Classification with Support Vector Machines"
    ]
  },
  {
    "objectID": "classification/3 Support Vector Machines on titanic data.html#train-model",
    "href": "classification/3 Support Vector Machines on titanic data.html#train-model",
    "title": "Classification with Support Vector Machines",
    "section": "3.2 train model",
    "text": "3.2 train model\n\n\nCode\nml_model.fit(X_train,Y_train)\n\n\nSVC(kernel='linear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  SVC?Documentation for SVCiFittedSVC(kernel='linear')",
    "crumbs": [
      "Classification",
      "Classification with Support Vector Machines"
    ]
  },
  {
    "objectID": "classification/3 Support Vector Machines on titanic data.html#preformance",
    "href": "classification/3 Support Vector Machines on titanic data.html#preformance",
    "title": "Classification with Support Vector Machines",
    "section": "3.3 Preformance",
    "text": "3.3 Preformance\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = ml_model.predict(X_test) #always gets x and retuns y\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_test,Y_pred_dt)  \naccuracy\n\n\n0.7430167597765364\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_test,Y_pred_dt)  \nprecision_dt\n\n\n0.7575757575757576\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_test,Y_pred_dt)  \nrecall_dt\n\n\n0.625\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_test,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[83, 16],\n       [30, 50]])\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_test, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.7316919191919191",
    "crumbs": [
      "Classification",
      "Classification with Support Vector Machines"
    ]
  },
  {
    "objectID": "classification/3 Support Vector Machines on titanic data.html#k-fold-cross-validation",
    "href": "classification/3 Support Vector Machines on titanic data.html#k-fold-cross-validation",
    "title": "Classification with Support Vector Machines",
    "section": "3.4 k-Fold Cross-Validation",
    "text": "3.4 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(ml_model, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.8005515611149413",
    "crumbs": [
      "Classification",
      "Classification with Support Vector Machines"
    ]
  },
  {
    "objectID": "classification/5 Random Forest on titanic dat.html",
    "href": "classification/5 Random Forest on titanic dat.html",
    "title": "Classification with Random Forest",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split",
    "crumbs": [
      "Classification",
      "Classification with Random Forest"
    ]
  },
  {
    "objectID": "classification/5 Random Forest on titanic dat.html#input-data",
    "href": "classification/5 Random Forest on titanic dat.html#input-data",
    "title": "Classification with Random Forest",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "Classification with Random Forest"
    ]
  },
  {
    "objectID": "classification/5 Random Forest on titanic dat.html#data-eda",
    "href": "classification/5 Random Forest on titanic dat.html#data-eda",
    "title": "Classification with Random Forest",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n\n\n0.7420382165605095\n0.18890814558058924",
    "crumbs": [
      "Classification",
      "Classification with Random Forest"
    ]
  },
  {
    "objectID": "classification/5 Random Forest on titanic dat.html#data-wrangling",
    "href": "classification/5 Random Forest on titanic dat.html#data-wrangling",
    "title": "Classification with Random Forest",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1046 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1308 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Dealing with missing numerical variables\ndata['Age'] = data.Age.fillna(data.Age.median())\ndata['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1309 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1309 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\ndata = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\nSex_male\n\n\n\n\n0\n1\n3\nBraund, Mr. Owen Harris\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\nTrue\n\n\n1\n2\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\nFalse\n\n\n2\n3\n3\nHeikkinen, Miss. Laina\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\nFalse\n\n\n3\n4\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\nFalse\n\n\n4\n5\n3\nAllen, Mr. William Henry\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\nTrue\n\n\n\n\n\n\n\n\n\n\nCode\n# Select features columns\ndata = data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()\n\n\n\n\n\n\n\n\n\n\nSex_male\nFare\nAge\nPclass\nSibSp\nrole\n\n\n\n\n0\nTrue\n7.2500\n22.0\n3\n1\ntrain\n\n\n1\nFalse\n71.2833\n38.0\n1\n1\ntrain\n\n\n2\nFalse\n7.9250\n26.0\n3\n0\ntrain\n\n\n3\nFalse\n53.1000\n35.0\n1\n1\ntrain\n\n\n4\nTrue\n8.0500\n35.0\n3\n0\ntrain",
    "crumbs": [
      "Classification",
      "Classification with Random Forest"
    ]
  },
  {
    "objectID": "classification/5 Random Forest on titanic dat.html#split-data",
    "href": "classification/5 Random Forest on titanic dat.html#split-data",
    "title": "Classification with Random Forest",
    "section": "2.4 split data",
    "text": "2.4 split data\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\nX_train = X_train.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 712 entries, 802 to 189\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Sex_male  712 non-null    bool   \n 1   Fare      712 non-null    float64\n 2   Age       712 non-null    float64\n 3   Pclass    712 non-null    int64  \n 4   SibSp     712 non-null    int64  \ndtypes: bool(1), float64(2), int64(2)\nmemory usage: 28.5 KB",
    "crumbs": [
      "Classification",
      "Classification with Random Forest"
    ]
  },
  {
    "objectID": "classification/5 Random Forest on titanic dat.html#define-model",
    "href": "classification/5 Random Forest on titanic dat.html#define-model",
    "title": "Classification with Random Forest",
    "section": "3.1 define model",
    "text": "3.1 define model\nThe solvers implemented in the class Logistic Regression are “liblinear”, “newton-cg”, “lbfgs”, “sag” and “saga”. According to Scikit Documentation: The “liblinear” solver was the one used by default for historical reasons before version 0.22. Since then, default use is lbfgs Algorithm.\n\n\nCode\nfrom sklearn.ensemble import RandomForestClassifier\nml_model = RandomForestClassifier()\nml_model\n\n\nRandomForestClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriNot fittedRandomForestClassifier()",
    "crumbs": [
      "Classification",
      "Classification with Random Forest"
    ]
  },
  {
    "objectID": "classification/5 Random Forest on titanic dat.html#train-model",
    "href": "classification/5 Random Forest on titanic dat.html#train-model",
    "title": "Classification with Random Forest",
    "section": "3.2 train model",
    "text": "3.2 train model\n\n\nCode\nml_model.fit(X_train,Y_train)\n\n\nRandomForestClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriFittedRandomForestClassifier()",
    "crumbs": [
      "Classification",
      "Classification with Random Forest"
    ]
  },
  {
    "objectID": "classification/5 Random Forest on titanic dat.html#preformance",
    "href": "classification/5 Random Forest on titanic dat.html#preformance",
    "title": "Classification with Random Forest",
    "section": "3.3 Preformance",
    "text": "3.3 Preformance\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = ml_model.predict(X_test) #always gets x and retuns y\nY_pred_dt\n\n\narray([0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n       1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,\n       0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n       1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,\n       0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,\n       0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n       0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,\n       1, 0, 0])\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_test,Y_pred_dt)  \naccuracy\n\n\n0.8435754189944135\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_test,Y_pred_dt)  \nprecision_dt\n\n\n0.7727272727272727\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_test,Y_pred_dt)  \nrecall_dt\n\n\n0.796875\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_test,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[100,  15],\n       [ 13,  51]])\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_test, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.8332201086956522",
    "crumbs": [
      "Classification",
      "Classification with Random Forest"
    ]
  },
  {
    "objectID": "classification/5 Random Forest on titanic dat.html#k-fold-cross-validation",
    "href": "classification/5 Random Forest on titanic dat.html#k-fold-cross-validation",
    "title": "Classification with Random Forest",
    "section": "3.4 k-Fold Cross-Validation",
    "text": "3.4 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(ml_model, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.7978233034571063",
    "crumbs": [
      "Classification",
      "Classification with Random Forest"
    ]
  },
  {
    "objectID": "classification/4 KNN on titanic data.html",
    "href": "classification/4 KNN on titanic data.html",
    "title": "Classification with K-Nearest Neighbors",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split",
    "crumbs": [
      "Classification",
      "Classification with K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "classification/4 KNN on titanic data.html#input-data",
    "href": "classification/4 KNN on titanic data.html#input-data",
    "title": "Classification with K-Nearest Neighbors",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "Classification with K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "classification/4 KNN on titanic data.html#data-eda",
    "href": "classification/4 KNN on titanic data.html#data-eda",
    "title": "Classification with K-Nearest Neighbors",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n\n\n0.7420382165605095\n0.18890814558058924",
    "crumbs": [
      "Classification",
      "Classification with K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "classification/4 KNN on titanic data.html#data-wrangling",
    "href": "classification/4 KNN on titanic data.html#data-wrangling",
    "title": "Classification with K-Nearest Neighbors",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1046 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1308 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Dealing with missing numerical variables\ndata['Age'] = data.Age.fillna(data.Age.median())\ndata['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1309 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1309 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\ndata = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\nSex_male\n\n\n\n\n0\n1\n3\nBraund, Mr. Owen Harris\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\nTrue\n\n\n1\n2\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\nFalse\n\n\n2\n3\n3\nHeikkinen, Miss. Laina\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\nFalse\n\n\n3\n4\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\nFalse\n\n\n4\n5\n3\nAllen, Mr. William Henry\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\nTrue\n\n\n\n\n\n\n\n\n\n\nCode\n# Select features columns\ndata = data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()\n\n\n\n\n\n\n\n\n\n\nSex_male\nFare\nAge\nPclass\nSibSp\nrole\n\n\n\n\n0\nTrue\n7.2500\n22.0\n3\n1\ntrain\n\n\n1\nFalse\n71.2833\n38.0\n1\n1\ntrain\n\n\n2\nFalse\n7.9250\n26.0\n3\n0\ntrain\n\n\n3\nFalse\n53.1000\n35.0\n1\n1\ntrain\n\n\n4\nTrue\n8.0500\n35.0\n3\n0\ntrain",
    "crumbs": [
      "Classification",
      "Classification with K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "classification/4 KNN on titanic data.html#split-data",
    "href": "classification/4 KNN on titanic data.html#split-data",
    "title": "Classification with K-Nearest Neighbors",
    "section": "2.4 split data",
    "text": "2.4 split data\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\nX_train = X_train.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 712 entries, 192 to 506\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Sex_male  712 non-null    bool   \n 1   Fare      712 non-null    float64\n 2   Age       712 non-null    float64\n 3   Pclass    712 non-null    int64  \n 4   SibSp     712 non-null    int64  \ndtypes: bool(1), float64(2), int64(2)\nmemory usage: 28.5 KB",
    "crumbs": [
      "Classification",
      "Classification with K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "classification/4 KNN on titanic data.html#define-model",
    "href": "classification/4 KNN on titanic data.html#define-model",
    "title": "Classification with K-Nearest Neighbors",
    "section": "3.1 define model",
    "text": "3.1 define model\nThe solvers implemented in the class Logistic Regression are “liblinear”, “newton-cg”, “lbfgs”, “sag” and “saga”. According to Scikit Documentation: The “liblinear” solver was the one used by default for historical reasons before version 0.22. Since then, default use is lbfgs Algorithm.\n\n\nCode\nfrom sklearn.neighbors import KNeighborsRegressor \nml_model = KNeighborsRegressor()\nml_model\n\n\nKNeighborsRegressor()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KNeighborsRegressor?Documentation for KNeighborsRegressoriNot fittedKNeighborsRegressor()",
    "crumbs": [
      "Classification",
      "Classification with K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "classification/4 KNN on titanic data.html#train-model",
    "href": "classification/4 KNN on titanic data.html#train-model",
    "title": "Classification with K-Nearest Neighbors",
    "section": "3.2 train model",
    "text": "3.2 train model\n\n\nCode\nml_model.fit(X_train,Y_train)\n\n\nKNeighborsRegressor()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KNeighborsRegressor?Documentation for KNeighborsRegressoriFittedKNeighborsRegressor()",
    "crumbs": [
      "Classification",
      "Classification with K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "classification/4 KNN on titanic data.html#preformance",
    "href": "classification/4 KNN on titanic data.html#preformance",
    "title": "Classification with K-Nearest Neighbors",
    "section": "3.3 Preformance",
    "text": "3.3 Preformance\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = ml_model.predict(X_test) #always gets x and retuns y\nY_pred_dt\n\n\narray([0.2, 0.2, 0.6, 0.4, 0.2, 0.8, 0.2, 0.4, 0. , 0.6, 0.4, 0.6, 0. ,\n       0.6, 0. , 0.6, 0. , 0.6, 1. , 0. , 0. , 0.8, 0.4, 0.8, 0.2, 0.2,\n       0.4, 0.2, 0. , 0.6, 0.4, 0. , 0.4, 0.6, 0. , 0.4, 0.8, 0. , 0.6,\n       0.6, 0.8, 0.4, 0.6, 0.6, 0.6, 0.2, 1. , 0.2, 0.8, 0.4, 0.4, 0.6,\n       0. , 0.2, 0.8, 0.6, 0. , 0.4, 1. , 0. , 0. , 0.4, 0.8, 0. , 0.4,\n       0.4, 0.4, 0.8, 0. , 0.4, 0.8, 0.2, 0.4, 1. , 0.8, 0.2, 0.2, 0.4,\n       0.4, 0. , 0.8, 0. , 0.2, 0.2, 0. , 0.4, 0. , 0.4, 0.4, 0.6, 0.2,\n       0. , 0.4, 0. , 0. , 0. , 0. , 0.8, 0. , 0.6, 0. , 0.4, 0.4, 1. ,\n       0.8, 0. , 0.2, 0.6, 0.6, 0.2, 0.2, 0.2, 0. , 0.2, 0. , 0. , 0. ,\n       0.2, 0.6, 0.2, 0.4, 0.8, 0.2, 0.4, 0.8, 0. , 0.4, 0.2, 0. , 0.2,\n       1. , 0.2, 0.2, 0.4, 0.6, 0.4, 0.6, 0.2, 0.4, 0.4, 0. , 0.2, 0.4,\n       0. , 0.8, 0.8, 0.6, 0.6, 0.2, 0.4, 0.2, 0.4, 0. , 0.4, 0.8, 0.2,\n       1. , 0. , 0.2, 0. , 0.8, 1. , 0.2, 0.2, 0.2, 0.8, 0.4, 0.4, 0. ,\n       0.2, 0.6, 0.2, 0.2, 0.8, 0.6, 0.2, 0.8, 0.2, 0.4])\n\n\n\n\nCode\n# its criteria is to round to 1 when higher than 0.5\nY_pred_dt = np.round(Y_pred_dt)  \n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_test,Y_pred_dt)  \naccuracy\n\n\n0.6983240223463687\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_test,Y_pred_dt)  \nprecision_dt\n\n\n0.6491228070175439\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_test,Y_pred_dt)  \nrecall_dt\n\n\n0.5211267605633803\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_test,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[88, 20],\n       [34, 37]])\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_test, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.6679707876890976",
    "crumbs": [
      "Classification",
      "Classification with K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "classification/4 KNN on titanic data.html#k-fold-cross-validation",
    "href": "classification/4 KNN on titanic data.html#k-fold-cross-validation",
    "title": "Classification with K-Nearest Neighbors",
    "section": "3.4 k-Fold Cross-Validation",
    "text": "3.4 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(ml_model, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.12900438323146735",
    "crumbs": [
      "Classification",
      "Classification with K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html",
    "href": "plot/1 seaborn.html",
    "title": "seaborn chart",
    "section": "",
    "text": "Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.\nCode\nimport seaborn as sns\nprint(sns.__version__)\n\n\n0.12.2\nCode\n# Import seaborn\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib\n\n# Apply the default theme\n#sns.set_theme()\n\n# Load an example dataset\ntips = sns.load_dataset(\"tips\")\ntips.head()\n\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html#color-by-group",
    "href": "plot/1 seaborn.html#color-by-group",
    "title": "seaborn chart",
    "section": "1.1 color by group",
    "text": "1.1 color by group\n\n\nCode\nsns.scatterplot(data=tips,x='tip',y='total_bill',hue='sex')",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html#size-by-group",
    "href": "plot/1 seaborn.html#size-by-group",
    "title": "seaborn chart",
    "section": "1.2 size by group",
    "text": "1.2 size by group\n\n\nCode\nsns.scatterplot(data=tips,x='tip',y='total_bill',size='size')",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html#color-by-group-1",
    "href": "plot/1 seaborn.html#color-by-group-1",
    "title": "seaborn chart",
    "section": "2.1 color by group",
    "text": "2.1 color by group\n\n\nCode\nimport random\nfrom siuba import _, mutate, filter, group_by, summarize,show_query\nfrom siuba import *\n\ndowjones2=dowjones&gt;&gt;mutate(type='old')\n\ndowjones3=dowjones&gt;&gt;mutate(Price=_.Price+random.random()*200,type='new')\n\ndowjones4=pd.concat([dowjones2, dowjones3], ignore_index = True)&gt;&gt; arrange(_.Date)\n\n\n\n\nCode\ndowjones4.head()\n\n\n\n\n\n\n\n\n\n\nDate\nPrice\ntype\n\n\n\n\n0\n1914-12-01\n55.000000\nold\n\n\n649\n1914-12-01\n125.660482\nnew\n\n\n1\n1915-01-01\n56.550000\nold\n\n\n650\n1915-01-01\n127.210482\nnew\n\n\n2\n1915-02-01\n56.000000\nold\n\n\n\n\n\n\n\n\n\n\nCode\nsns.lineplot(data=dowjones4,x='Date',y='Price',hue='type')",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html#color-by-group-2",
    "href": "plot/1 seaborn.html#color-by-group-2",
    "title": "seaborn chart",
    "section": "3.1 color by group",
    "text": "3.1 color by group\n\n\nCode\nsns.histplot(data=tips,x='tip',hue='sex',multiple=\"dodge\")",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html#color-by-group-3",
    "href": "plot/1 seaborn.html#color-by-group-3",
    "title": "seaborn chart",
    "section": "5.1 color by group",
    "text": "5.1 color by group\n\n\nCode\nsns.boxplot(data=tips,x='day',y='tip',hue='sex')",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html#color-by-group-4",
    "href": "plot/1 seaborn.html#color-by-group-4",
    "title": "seaborn chart",
    "section": "6.1 color by group",
    "text": "6.1 color by group\n\n\nCode\nsns.stripplot(data=tips,x='day',y='tip',hue='sex',dodge=True)\n\n\n\n\n\n\n\n\n\njoin plot\n\n\nCode\nsns.jointplot(data=tips,x='total_bill',y='tip',kind='reg')",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "intro/4 data structure in Python .html#singular",
    "href": "intro/4 data structure in Python .html#singular",
    "title": "data structure in Python",
    "section": "1.1 singular",
    "text": "1.1 singular\n\n\nCode\na=1\ntype(a)\n\n\nint\n\n\n\n\nCode\na=1.3\ntype(a)\n\n\nfloat\n\n\n\n\nCode\na='hell'\ntype(a)\n\n\nstr",
    "crumbs": [
      "Intro",
      "data structure in Python"
    ]
  },
  {
    "objectID": "intro/4 data structure in Python .html#list",
    "href": "intro/4 data structure in Python .html#list",
    "title": "data structure in Python",
    "section": "1.2 list",
    "text": "1.2 list\n\n\nCode\na=[1,2,3]\n\na\n\n\n[1, 2, 3]\n\n\n\n\nCode\ntype(a) \n\n\nlist\n\n\n\n\nCode\nfruits = ['orange', 'apple', 'pear', 'banana', 'kiwi', 'apple', 'banana','apple']\n\n\n\n1.2.1 find length of the list with len()\n\n\nCode\nlen(fruits)\n\n\n8\n\n\n\n\n1.2.2 find how many time in the list with count()\n\n\nCode\nfruits.count('apple')\n\n\n3\n\n\n\n\n1.2.3 find locaiton of on the list with index()\nshow the first ‘apple’ index. python list start at 0\n\n\nCode\nfruits.index('apple')\n\n\n1\n\n\nall ‘apple’ in the list\n\n\nCode\n[index for index, value in enumerate(fruits) if value == 'apple']\n\n\n[1, 5, 7]",
    "crumbs": [
      "Intro",
      "data structure in Python"
    ]
  },
  {
    "objectID": "intro/4 data structure in Python .html#reverse-the-list",
    "href": "intro/4 data structure in Python .html#reverse-the-list",
    "title": "data structure in Python",
    "section": "1.3 reverse the list",
    "text": "1.3 reverse the list\n\n\nCode\nfruits.reverse()\nfruits\n\n\n['apple', 'banana', 'apple', 'kiwi', 'banana', 'pear', 'apple', 'orange']\n\n\n\n1.3.1 sort the list\n\n\nCode\nfruits.sort()\nfruits\n\n\n['apple', 'apple', 'apple', 'banana', 'banana', 'kiwi', 'orange', 'pear']\n\n\n\n\n1.3.2 add element on the list\n\n\nCode\nfruits.append('grape')\nfruits\n\n\n['apple',\n 'apple',\n 'apple',\n 'banana',\n 'banana',\n 'kiwi',\n 'orange',\n 'pear',\n 'grape']",
    "crumbs": [
      "Intro",
      "data structure in Python"
    ]
  },
  {
    "objectID": "intro/4 data structure in Python .html#drop-last-element",
    "href": "intro/4 data structure in Python .html#drop-last-element",
    "title": "data structure in Python",
    "section": "1.4 drop last element",
    "text": "1.4 drop last element\n\n\nCode\nfruits.pop()\n\nfruits\n\n\n['apple', 'apple', 'apple', 'banana', 'banana', 'kiwi', 'orange', 'pear']\n\n\n\n1.4.1 List Comprehensions\nusing loop:\n\n\nCode\nsquares = []\nfor x in range(10):\n  squares.append(x**2)\n  \nsquares\n\n\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n\n\nusing List Comprehensions\n\n\nCode\nsquares = [x**2 for x in range(10)]\nsquares\n\n\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]",
    "crumbs": [
      "Intro",
      "data structure in Python"
    ]
  },
  {
    "objectID": "intro/4 data structure in Python .html#tuples",
    "href": "intro/4 data structure in Python .html#tuples",
    "title": "data structure in Python",
    "section": "1.5 Tuples",
    "text": "1.5 Tuples\n\n\nCode\nfruits = ('orange', 'apple', 'pear', 'banana', 'kiwi', 'apple', 'banana','apple')\n\nfruits\n\n\n('orange', 'apple', 'pear', 'banana', 'kiwi', 'apple', 'banana', 'apple')\n\n\n\n\nCode\ntype(fruits)\n\n\ntuple\n\n\ntuple can not be modified.",
    "crumbs": [
      "Intro",
      "data structure in Python"
    ]
  },
  {
    "objectID": "intro/4 data structure in Python .html#sets",
    "href": "intro/4 data structure in Python .html#sets",
    "title": "data structure in Python",
    "section": "1.6 Sets",
    "text": "1.6 Sets\nA set is an unordered collection with no duplicate elements.\n\n\nCode\nbasket = {'apple', 'orange', 'apple', 'pear', 'orange', 'banana'}\n\nbasket\n\n\n{'apple', 'banana', 'orange', 'pear'}\n\n\n\n\nCode\ntype(basket)\n\n\nset",
    "crumbs": [
      "Intro",
      "data structure in Python"
    ]
  },
  {
    "objectID": "intro/4 data structure in Python .html#dictionaries",
    "href": "intro/4 data structure in Python .html#dictionaries",
    "title": "data structure in Python",
    "section": "1.7 Dictionaries",
    "text": "1.7 Dictionaries\n\n\nCode\ntel = {'jack': 4098, 'sape': 4139}\n\ntel\n\n\n{'jack': 4098, 'sape': 4139}\n\n\n\n\nCode\ntype(tel)\n\n\ndict\n\n\n\n\nCode\ntel['jack']\n\n\n4098",
    "crumbs": [
      "Intro",
      "data structure in Python"
    ]
  },
  {
    "objectID": "regression/0 house price data.html#download-data",
    "href": "regression/0 house price data.html#download-data",
    "title": "Housing Prices Dataset",
    "section": "2.1 download data",
    "text": "2.1 download data\nhttps://www.kaggle.com/competitions/house-prices-advanced-regression-techniques",
    "crumbs": [
      "Regression",
      "Housing Prices Dataset"
    ]
  },
  {
    "objectID": "regression/0 house price data.html#input-data",
    "href": "regression/0 house price data.html#input-data",
    "title": "Housing Prices Dataset",
    "section": "2.2 input data",
    "text": "2.2 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nId = df_train['Id']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nId\nMSSubClass\nMSZoning\nLotFrontage\nLotArea\nStreet\nAlley\nLotShape\nLandContour\nUtilities\n...\nPoolArea\nPoolQC\nFence\nMiscFeature\nMiscVal\nMoSold\nYrSold\nSaleType\nSaleCondition\nSalePrice\n\n\n\n\n0\n1\n60\nRL\n65.0\n8450\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n2\n2008\nWD\nNormal\n208500\n\n\n1\n2\n20\nRL\n80.0\n9600\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2007\nWD\nNormal\n181500\n\n\n2\n3\n60\nRL\n68.0\n11250\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n9\n2008\nWD\nNormal\n223500\n\n\n3\n4\n70\nRL\n60.0\n9550\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n2\n2006\nWD\nAbnorml\n140000\n\n\n4\n5\n60\nRL\n84.0\n14260\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n12\n2008\nWD\nNormal\n250000\n\n\n\n\n5 rows × 81 columns\n\n\n\n\n\n\nCode\n#df_train.info()\n\n\n\n\nCode\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['SalePrice'], axis=1), df_test])",
    "crumbs": [
      "Regression",
      "Housing Prices Dataset"
    ]
  },
  {
    "objectID": "regression/0 house price data.html#data-eda",
    "href": "regression/0 house price data.html#data-eda",
    "title": "Housing Prices Dataset",
    "section": "2.3 data EDA",
    "text": "2.3 data EDA\n\n\nCode\ndf_train.describe()\n\n\n\n\n\n\n\n\n\n\nId\nMSSubClass\nLotFrontage\nLotArea\nOverallQual\nOverallCond\nYearBuilt\nYearRemodAdd\nMasVnrArea\nBsmtFinSF1\n...\nWoodDeckSF\nOpenPorchSF\nEnclosedPorch\n3SsnPorch\nScreenPorch\nPoolArea\nMiscVal\nMoSold\nYrSold\nSalePrice\n\n\n\n\ncount\n1460.000000\n1460.000000\n1201.000000\n1460.000000\n1460.000000\n1460.000000\n1460.000000\n1460.000000\n1452.000000\n1460.000000\n...\n1460.000000\n1460.000000\n1460.000000\n1460.000000\n1460.000000\n1460.000000\n1460.000000\n1460.000000\n1460.000000\n1460.000000\n\n\nmean\n730.500000\n56.897260\n70.049958\n10516.828082\n6.099315\n5.575342\n1971.267808\n1984.865753\n103.685262\n443.639726\n...\n94.244521\n46.660274\n21.954110\n3.409589\n15.060959\n2.758904\n43.489041\n6.321918\n2007.815753\n180921.195890\n\n\nstd\n421.610009\n42.300571\n24.284752\n9981.264932\n1.382997\n1.112799\n30.202904\n20.645407\n181.066207\n456.098091\n...\n125.338794\n66.256028\n61.119149\n29.317331\n55.757415\n40.177307\n496.123024\n2.703626\n1.328095\n79442.502883\n\n\nmin\n1.000000\n20.000000\n21.000000\n1300.000000\n1.000000\n1.000000\n1872.000000\n1950.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n2006.000000\n34900.000000\n\n\n25%\n365.750000\n20.000000\n59.000000\n7553.500000\n5.000000\n5.000000\n1954.000000\n1967.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n5.000000\n2007.000000\n129975.000000\n\n\n50%\n730.500000\n50.000000\n69.000000\n9478.500000\n6.000000\n5.000000\n1973.000000\n1994.000000\n0.000000\n383.500000\n...\n0.000000\n25.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n6.000000\n2008.000000\n163000.000000\n\n\n75%\n1095.250000\n70.000000\n80.000000\n11601.500000\n7.000000\n6.000000\n2000.000000\n2004.000000\n166.000000\n712.250000\n...\n168.000000\n68.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n8.000000\n2009.000000\n214000.000000\n\n\nmax\n1460.000000\n190.000000\n313.000000\n215245.000000\n10.000000\n9.000000\n2010.000000\n2010.000000\n1600.000000\n5644.000000\n...\n857.000000\n547.000000\n552.000000\n508.000000\n480.000000\n738.000000\n15500.000000\n12.000000\n2010.000000\n755000.000000\n\n\n\n\n8 rows × 38 columns\n\n\n\n\n\n\nCode\ndf_train.describe(include=[object])\n\n\n\n\n\n\n\n\n\n\nMSZoning\nStreet\nAlley\nLotShape\nLandContour\nUtilities\nLotConfig\nLandSlope\nNeighborhood\nCondition1\n...\nGarageFinish\nGarageQual\nGarageCond\nPavedDrive\nPoolQC\nFence\nMiscFeature\nSaleType\nSaleCondition\nrole\n\n\n\n\ncount\n1460\n1460\n91\n1460\n1460\n1460\n1460\n1460\n1460\n1460\n...\n1379\n1379\n1379\n1460\n7\n281\n54\n1460\n1460\n1460\n\n\nunique\n5\n2\n2\n4\n4\n2\n5\n3\n25\n9\n...\n3\n5\n5\n3\n3\n4\n4\n9\n6\n1\n\n\ntop\nRL\nPave\nGrvl\nReg\nLvl\nAllPub\nInside\nGtl\nNAmes\nNorm\n...\nUnf\nTA\nTA\nY\nGd\nMnPrv\nShed\nWD\nNormal\ntrain\n\n\nfreq\n1151\n1454\n50\n925\n1311\n1459\n1052\n1382\n225\n1260\n...\n605\n1311\n1326\n1340\n3\n157\n49\n1267\n1198\n1460\n\n\n\n\n4 rows × 44 columns\n\n\n\n\nMissing Data\n\n\nCode\ndf_train.isnull().sum()\n\n\nId                 0\nMSSubClass         0\nMSZoning           0\nLotFrontage      259\nLotArea            0\n                ... \nYrSold             0\nSaleType           0\nSaleCondition      0\nSalePrice          0\nrole               0\nLength: 82, dtype: int64\n\n\n\n\nCode\nimport seaborn as sns\nsns.histplot(data=df_train,x='SalePrice')\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport sweetviz as sv\nmy_report = sv.analyze(df_train)\n\n\n\n\n\n\n\nCode\nmy_report.show_notebook()",
    "crumbs": [
      "Regression",
      "Housing Prices Dataset"
    ]
  },
  {
    "objectID": "regression/0 house price data.html#feature-vs-target",
    "href": "regression/0 house price data.html#feature-vs-target",
    "title": "Housing Prices Dataset",
    "section": "2.4 feature vs target",
    "text": "2.4 feature vs target\n\n\nCode\nmy_report2 = sv.analyze(df_train,target_feat='SalePrice')\n\n\n\n\n\n\n\nCode\nmy_report2.show_notebook()",
    "crumbs": [
      "Regression",
      "Housing Prices Dataset"
    ]
  },
  {
    "objectID": "regression/0 house price data.html#compare-train-data-and-test-data",
    "href": "regression/0 house price data.html#compare-train-data-and-test-data",
    "title": "Housing Prices Dataset",
    "section": "2.5 compare train data and test data",
    "text": "2.5 compare train data and test data\n\n\nCode\ncompare = sv.compare(source=df_train, compare=df_test)\n\n\n\n\n\n\n\nCode\ncompare.show_notebook()",
    "crumbs": [
      "Regression",
      "Housing Prices Dataset"
    ]
  },
  {
    "objectID": "regression/0 house price data.html#data-dictionary",
    "href": "regression/0 house price data.html#data-dictionary",
    "title": "Housing Prices Dataset",
    "section": "2.6 data dictionary",
    "text": "2.6 data dictionary\nSalePrice - the property’s sale price in dollars. This is the target variable that you’re trying to predict.\nMSSubClass: The building class\nMSZoning: The general zoning classification\nLotFrontage: Linear feet of street connected to property\nLotArea: Lot size in square feet\nStreet: Type of road access\nAlley: Type of alley access\nLotShape: General shape of property\nLandContour: Flatness of the property\nUtilities: Type of utilities available\nLotConfig: Lot configuration\nLandSlope: Slope of property\nNeighborhood: Physical locations within Ames city limits\nCondition1: Proximity to main road or railroad\nCondition2: Proximity to main road or railroad (if a second is present)\nBldgType: Type of dwelling\nHouseStyle: Style of dwelling\nOverallQual: Overall material and finish quality\nOverallCond: Overall condition rating\nYearBuilt: Original construction date\nYearRemodAdd: Remodel date\nRoofStyle: Type of roof\nRoofMatl: Roof material\nExterior1st: Exterior covering on house\nExterior2nd: Exterior covering on house (if more than one material)\nMasVnrType: Masonry veneer type\nMasVnrArea: Masonry veneer area in square feet\nExterQual: Exterior material quality\nExterCond: Present condition of the material on the exterior\nFoundation: Type of foundation\nBsmtQual: Height of the basement\nBsmtCond: General condition of the basement\nBsmtExposure: Walkout or garden level basement walls\nBsmtFinType1: Quality of basement finished area\nBsmtFinSF1: Type 1 finished square feet\nBsmtFinType2: Quality of second finished area (if present)\nBsmtFinSF2: Type 2 finished square feet\nBsmtUnfSF: Unfinished square feet of basement area\nTotalBsmtSF: Total square feet of basement area\nHeating: Type of heating\nHeatingQC: Heating quality and condition\nCentralAir: Central air conditioning\nElectrical: Electrical system\n1stFlrSF: First Floor square feet\n2ndFlrSF: Second floor square feet\nLowQualFinSF: Low quality finished square feet (all floors)\nGrLivArea: Above grade (ground) living area square feet\nBsmtFullBath: Basement full bathrooms\nBsmtHalfBath: Basement half bathrooms\nFullBath: Full bathrooms above grade\nHalfBath: Half baths above grade\nBedroom: Number of bedrooms above basement level\nKitchen: Number of kitchens\nKitchenQual: Kitchen quality\nTotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\nFunctional: Home functionality rating\nFireplaces: Number of fireplaces\nFireplaceQu: Fireplace quality\nGarageType: Garage location\nGarageYrBlt: Year garage was built\nGarageFinish: Interior finish of the garage\nGarageCars: Size of garage in car capacity\nGarageArea: Size of garage in square feet\nGarageQual: Garage quality\nGarageCond: Garage condition\nPavedDrive: Paved driveway\nWoodDeckSF: Wood deck area in square feet\nOpenPorchSF: Open porch area in square feet\nEnclosedPorch: Enclosed porch area in square feet\n3SsnPorch: Three season porch area in square feet\nScreenPorch: Screen porch area in square feet\nPoolArea: Pool area in square feet\nPoolQC: Pool quality\nFence: Fence quality\nMiscFeature: Miscellaneous feature not covered in other categories\nMiscVal: $Value of miscellaneous feature\nMoSold: Month Sold\nYrSold: Year Sold\nSaleType: Type of sale\nSaleCondition: Condition of sale",
    "crumbs": [
      "Regression",
      "Housing Prices Dataset"
    ]
  },
  {
    "objectID": "regression/5 XGboost on house price data.html",
    "href": "regression/5 XGboost on house price data.html",
    "title": "XGboost and pipeline, hyperparameter tuning",
    "section": "",
    "text": "with pipeline and tunning",
    "crumbs": [
      "Regression",
      "XGboost and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/5 XGboost on house price data.html#input-data",
    "href": "regression/5 XGboost on house price data.html#input-data",
    "title": "XGboost and pipeline, hyperparameter tuning",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nId = df_train['Id']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nId\nMSSubClass\nMSZoning\nLotFrontage\nLotArea\nStreet\nAlley\nLotShape\nLandContour\nUtilities\n...\nPoolArea\nPoolQC\nFence\nMiscFeature\nMiscVal\nMoSold\nYrSold\nSaleType\nSaleCondition\nSalePrice\n\n\n\n\n0\n1\n60\nRL\n65.0\n8450\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n2\n2008\nWD\nNormal\n208500\n\n\n1\n2\n20\nRL\n80.0\n9600\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2007\nWD\nNormal\n181500\n\n\n2\n3\n60\nRL\n68.0\n11250\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n9\n2008\nWD\nNormal\n223500\n\n\n3\n4\n70\nRL\n60.0\n9550\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n2\n2006\nWD\nAbnorml\n140000\n\n\n4\n5\n60\nRL\n84.0\n14260\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n12\n2008\nWD\nNormal\n250000\n\n\n\n\n5 rows × 81 columns\n\n\n\n\n\n\nCode\n#df_train.info()\n\n\n\n\nCode\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['SalePrice'], axis=1), df_test])",
    "crumbs": [
      "Regression",
      "XGboost and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/5 XGboost on house price data.html#data-eda",
    "href": "regression/5 XGboost on house price data.html#data-eda",
    "title": "XGboost and pipeline, hyperparameter tuning",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\nin step 1",
    "crumbs": [
      "Regression",
      "XGboost and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/5 XGboost on house price data.html#data-wrangling",
    "href": "regression/5 XGboost on house price data.html#data-wrangling",
    "title": "XGboost and pipeline, hyperparameter tuning",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling",
    "crumbs": [
      "Regression",
      "XGboost and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/5 XGboost on house price data.html#split-data",
    "href": "regression/5 XGboost on house price data.html#split-data",
    "title": "XGboost and pipeline, hyperparameter tuning",
    "section": "2.4 split data",
    "text": "2.4 split data\n\n\nCode\nY = df_train.SalePrice\nX = df_train.drop(['SalePrice'], axis=1)\n\n\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\nX_train = X_train.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nprint(X_train.shape)\nprint(X_test.shape)\n\n\n(1168, 80)\n(292, 80)\n\n\n\n\nCode\nprint(Y_train.shape)\nprint(Y_test.shape)\n\n\n(1168,)\n(292,)\n\n\n\n\nCode\ncategorical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].nunique() &lt; 10 and X_train[cname].dtype == \"object\"]\n                    \n                    \nnumerical_cols = numerical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].dtype in ['int64', 'float64']]\n\n\n\n\nCode\nprint(\"The total number of categorical columns:\", len(categorical_cols))\nprint(\"The total number of numerical columns:\", len(numerical_cols))\n\n\nThe total number of categorical columns: 40\nThe total number of numerical columns: 37\n\n\n\n\nCode\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train[my_cols].copy()\nX_test= X_test[my_cols].copy()\n\n\nX_final = df_test[my_cols].copy()",
    "crumbs": [
      "Regression",
      "XGboost and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/5 XGboost on house price data.html#pipelines-for-data-preprocessing",
    "href": "regression/5 XGboost on house price data.html#pipelines-for-data-preprocessing",
    "title": "XGboost and pipeline, hyperparameter tuning",
    "section": "2.5 Pipelines for Data Preprocessing",
    "text": "2.5 Pipelines for Data Preprocessing\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\nCode\nnumerical_transformer = Pipeline(steps=[\n    ('imputer_num', SimpleImputer(strategy='median')), \n    ('scaler', StandardScaler())\n])\n\n\n\n\nCode\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n\n\nCode\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)])",
    "crumbs": [
      "Regression",
      "XGboost and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/5 XGboost on house price data.html#define-model",
    "href": "regression/5 XGboost on house price data.html#define-model",
    "title": "XGboost and pipeline, hyperparameter tuning",
    "section": "3.1 define model",
    "text": "3.1 define model\nrandom forest with hyper parameter tuning\n\n\nCode\nimport xgboost\nprint(xgboost.__version__)\n\n\n2.0.3\n\n\nrandom forest with hyper parameter tuning\n\n\nCode\nfrom xgboost import XGBRegressor\n\nml_model = XGBRegressor()\nml_model\n\n\nXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=None, n_jobs=None,\n             num_parallel_tree=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. XGBRegressoriNot fittedXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=None, n_jobs=None,\n             num_parallel_tree=None, random_state=None, ...)",
    "crumbs": [
      "Regression",
      "XGboost and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/5 XGboost on house price data.html#define-pipline",
    "href": "regression/5 XGboost on house price data.html#define-pipline",
    "title": "XGboost and pipeline, hyperparameter tuning",
    "section": "3.2 define pipline",
    "text": "3.2 define pipline\n\n\nCode\npipeline = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', ml_model)\n         ]\n)",
    "crumbs": [
      "Regression",
      "XGboost and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/5 XGboost on house price data.html#hyperparameter-tuning-set",
    "href": "regression/5 XGboost on house price data.html#hyperparameter-tuning-set",
    "title": "XGboost and pipeline, hyperparameter tuning",
    "section": "3.3 hyperparameter tuning set",
    "text": "3.3 hyperparameter tuning set\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\n\n\nparam_grid = {\n        'model__learning_rate': [0.01, 0.1],\n        'model__max_depth': [3, 5, 7, 10],\n        'model__min_child_weight': [1, 3, 5],\n        'model__subsample': [0.5, 0.7],\n       # 'model__colsample__bytree': [0.5, 0.7],\n        'model__n_estimators' : [100, 200, 500],\n        'model__objective': ['reg:squarederror']\n    }\n\n\n\n\nCode\nGridCV = GridSearchCV(pipeline, param_grid, n_jobs= -1, verbose=1)",
    "crumbs": [
      "Regression",
      "XGboost and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/5 XGboost on house price data.html#train-model",
    "href": "regression/5 XGboost on house price data.html#train-model",
    "title": "XGboost and pipeline, hyperparameter tuning",
    "section": "3.4 train model",
    "text": "3.4 train model\n\n\nCode\nGridCV.fit(X_train, Y_train)\n\n\nFitting 5 folds for each of 144 candidates, totalling 720 fits\n\n\nGridSearchCV(estimator=Pipeline(steps=[('preprocessor',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         Pipeline(steps=[('imputer_num',\n                                                                                          SimpleImputer(strategy='median')),\n                                                                                         ('scaler',\n                                                                                          StandardScaler())]),\n                                                                         ['Id',\n                                                                          'MSSubClass',\n                                                                          'LotFrontage',\n                                                                          'LotArea',\n                                                                          'OverallQual',\n                                                                          'OverallCond',\n                                                                          'YearBuilt',\n                                                                          'YearRemodAdd',\n                                                                          'MasVnrArea',\n                                                                          'BsmtFinSF1',\n                                                                          'BsmtFinSF2',\n                                                                          'BsmtUnfSF',\n                                                                          'TotalBsmtSF...\n                                                     monotone_constraints=None,\n                                                     multi_strategy=None,\n                                                     n_estimators=None,\n                                                     n_jobs=None,\n                                                     num_parallel_tree=None,\n                                                     random_state=None, ...))]),\n             n_jobs=-1,\n             param_grid={'model__learning_rate': [0.01, 0.1],\n                         'model__max_depth': [3, 5, 7, 10],\n                         'model__min_child_weight': [1, 3, 5],\n                         'model__n_estimators': [100, 200, 500],\n                         'model__objective': ['reg:squarederror'],\n                         'model__subsample': [0.5, 0.7]},\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(estimator=Pipeline(steps=[('preprocessor',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         Pipeline(steps=[('imputer_num',\n                                                                                          SimpleImputer(strategy='median')),\n                                                                                         ('scaler',\n                                                                                          StandardScaler())]),\n                                                                         ['Id',\n                                                                          'MSSubClass',\n                                                                          'LotFrontage',\n                                                                          'LotArea',\n                                                                          'OverallQual',\n                                                                          'OverallCond',\n                                                                          'YearBuilt',\n                                                                          'YearRemodAdd',\n                                                                          'MasVnrArea',\n                                                                          'BsmtFinSF1',\n                                                                          'BsmtFinSF2',\n                                                                          'BsmtUnfSF',\n                                                                          'TotalBsmtSF...\n                                                     monotone_constraints=None,\n                                                     multi_strategy=None,\n                                                     n_estimators=None,\n                                                     n_jobs=None,\n                                                     num_parallel_tree=None,\n                                                     random_state=None, ...))]),\n             n_jobs=-1,\n             param_grid={'model__learning_rate': [0.01, 0.1],\n                         'model__max_depth': [3, 5, 7, 10],\n                         'model__min_child_weight': [1, 3, 5],\n                         'model__n_estimators': [100, 200, 500],\n                         'model__objective': ['reg:squarederror'],\n                         'model__subsample': [0.5, 0.7]},\n             verbose=1) estimator: PipelinePipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer_num',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['Id', 'MSSubClass',\n                                                   'LotFrontage', 'LotArea',\n                                                   'OverallQual', 'OverallCond',\n                                                   'YearBuilt', 'YearRemodAdd',\n                                                   'MasVnrArea', 'BsmtFinSF1',\n                                                   'BsmtFinSF2', 'BsmtUnfSF',\n                                                   'TotalBsmtSF', '1stFlrSF',\n                                                   '2ndFlrSF'...\n                              feature_types=None, gamma=None, grow_policy=None,\n                              importance_type=None,\n                              interaction_constraints=None, learning_rate=None,\n                              max_bin=None, max_cat_threshold=None,\n                              max_cat_to_onehot=None, max_delta_step=None,\n                              max_depth=None, max_leaves=None,\n                              min_child_weight=None, missing=nan,\n                              monotone_constraints=None, multi_strategy=None,\n                              n_estimators=None, n_jobs=None,\n                              num_parallel_tree=None, random_state=None, ...))])  preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('imputer_num',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('scaler', StandardScaler())]),\n                                 ['Id', 'MSSubClass', 'LotFrontage', 'LotArea',\n                                  'OverallQual', 'OverallCond', 'YearBuilt',\n                                  'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1',\n                                  'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF',\n                                  '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n                                  'GrLivArea', 'Bsm...\n                                 ['MSZoning', 'Street', 'Alley', 'LotShape',\n                                  'LandContour', 'Utilities', 'LotConfig',\n                                  'LandSlope', 'Condition1', 'Condition2',\n                                  'BldgType', 'HouseStyle', 'RoofStyle',\n                                  'RoofMatl', 'MasVnrType', 'ExterQual',\n                                  'ExterCond', 'Foundation', 'BsmtQual',\n                                  'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n                                  'BsmtFinType2', 'Heating', 'HeatingQC',\n                                  'CentralAir', 'Electrical', 'KitchenQual',\n                                  'Functional', 'FireplaceQu', ...])]) num['Id', 'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='median')  StandardScaler?Documentation for StandardScalerStandardScaler() cat['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'SaleType', 'SaleCondition']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='most_frequent')  OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore') XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=None, n_jobs=None,\n             num_parallel_tree=None, random_state=None, ...) \n\n\n\n\nCode\nGridCV.best_params_\n\n\n{'model__learning_rate': 0.01,\n 'model__max_depth': 5,\n 'model__min_child_weight': 5,\n 'model__n_estimators': 500,\n 'model__objective': 'reg:squarederror',\n 'model__subsample': 0.5}\n\n\n\n\nCode\nGridCV.best_score_\n\n\n0.8682297036551574\n\n\nbest model as pipeline\n\n\nCode\noptimised_model_pipeline = GridCV.best_estimator_\n\n\n\n\nCode\nvar=optimised_model_pipeline[:-1].get_feature_names_out()\n#var\n\n\n\n\nCode\nfitted_model=optimised_model_pipeline.steps[1][1]\n\n\nvariable importance\n\n\nCode\nimportances = fitted_model.feature_importances_\nvi=pd.DataFrame({\"variable\":var,\"importances\":importances})\nvi=vi.sort_values('importances',ascending=False)\nvi\n\n\n\n\n\n\n\n\n\n\nvariable\nimportances\n\n\n\n\n4\nnum__OverallQual\n0.189310\n\n\n26\nnum__GarageCars\n0.069107\n\n\n126\ncat__BsmtQual_Ex\n0.038735\n\n\n19\nnum__FullBath\n0.036401\n\n\n114\ncat__ExterQual_TA\n0.031176\n\n\n...\n...\n...\n\n\n112\ncat__ExterQual_Fa\n0.000000\n\n\n43\ncat__Street_Pave\n0.000000\n\n\n45\ncat__Alley_Pave\n0.000000\n\n\n106\ncat__RoofMatl_WdShake\n0.000000\n\n\n115\ncat__ExterCond_Ex\n0.000000\n\n\n\n\n230 rows × 2 columns",
    "crumbs": [
      "Regression",
      "XGboost and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/5 XGboost on house price data.html#preformance",
    "href": "regression/5 XGboost on house price data.html#preformance",
    "title": "XGboost and pipeline, hyperparameter tuning",
    "section": "3.5 Preformance",
    "text": "3.5 Preformance\n\n\nCode\nY_pred_dt =optimised_model_pipeline.predict(X_test) #always gets x and retuns y\n\n\nR 2\n\n\nCode\nfrom sklearn.metrics import r2_score\nr2_score(Y_test, Y_pred_dt)\n\n\n0.9095407797423984\n\n\nMAE\n\n\nCode\nfrom sklearn.metrics import mean_absolute_error\nmean_absolute_error(Y_test, Y_pred_dt)\n\n\n15227.14426369863\n\n\nRMSE\n\n\nCode\nfrom  math import sqrt\nfrom sklearn.metrics import mean_squared_error\nmse=mean_squared_error(Y_test, Y_pred_dt)\nrmse=sqrt(mse)\nrmse\n\n\n22960.975221159788",
    "crumbs": [
      "Regression",
      "XGboost and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/5 XGboost on house price data.html#k-fold-cross-validation",
    "href": "regression/5 XGboost on house price data.html#k-fold-cross-validation",
    "title": "XGboost and pipeline, hyperparameter tuning",
    "section": "3.6 k-Fold Cross-Validation",
    "text": "3.6 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \n\n\n\n\nCode\ncv_dt = cross_val_score(optimised_model_pipeline, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.8683986008609523\n\n\n\n\nCode\ncv_dt = cross_val_score(optimised_model_pipeline, X_train, Y_train, cv=kf_dt,scoring = 'neg_mean_squared_error')\nnp.mean(np.sqrt(np.abs(cv_dt)))\n\n\n30095.130277299217",
    "crumbs": [
      "Regression",
      "XGboost and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/2 Decision Tree on house price data copy.html",
    "href": "regression/2 Decision Tree on house price data copy.html",
    "title": "Decision tree",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split",
    "crumbs": [
      "Regression",
      "Decision tree"
    ]
  },
  {
    "objectID": "regression/2 Decision Tree on house price data copy.html#input-data",
    "href": "regression/2 Decision Tree on house price data copy.html#input-data",
    "title": "Decision tree",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nId = df_train['Id']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nId\nMSSubClass\nMSZoning\nLotFrontage\nLotArea\nStreet\nAlley\nLotShape\nLandContour\nUtilities\n...\nPoolArea\nPoolQC\nFence\nMiscFeature\nMiscVal\nMoSold\nYrSold\nSaleType\nSaleCondition\nSalePrice\n\n\n\n\n0\n1\n60\nRL\n65.0\n8450\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n2\n2008\nWD\nNormal\n208500\n\n\n1\n2\n20\nRL\n80.0\n9600\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2007\nWD\nNormal\n181500\n\n\n2\n3\n60\nRL\n68.0\n11250\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n9\n2008\nWD\nNormal\n223500\n\n\n3\n4\n70\nRL\n60.0\n9550\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n2\n2006\nWD\nAbnorml\n140000\n\n\n4\n5\n60\nRL\n84.0\n14260\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n12\n2008\nWD\nNormal\n250000\n\n\n\n\n5 rows × 81 columns\n\n\n\n\n\n\nCode\n#df_train.info()\n\n\n\n\nCode\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['SalePrice'], axis=1), df_test])",
    "crumbs": [
      "Regression",
      "Decision tree"
    ]
  },
  {
    "objectID": "regression/2 Decision Tree on house price data copy.html#data-eda",
    "href": "regression/2 Decision Tree on house price data copy.html#data-eda",
    "title": "Decision tree",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\nin step 1",
    "crumbs": [
      "Regression",
      "Decision tree"
    ]
  },
  {
    "objectID": "regression/2 Decision Tree on house price data copy.html#data-wrangling",
    "href": "regression/2 Decision Tree on house price data copy.html#data-wrangling",
    "title": "Decision tree",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling",
    "crumbs": [
      "Regression",
      "Decision tree"
    ]
  },
  {
    "objectID": "regression/2 Decision Tree on house price data copy.html#split-data",
    "href": "regression/2 Decision Tree on house price data copy.html#split-data",
    "title": "Decision tree",
    "section": "2.4 split data",
    "text": "2.4 split data\n\n\nCode\nY = df_train.SalePrice\nX = df_train.drop(['SalePrice'], axis=1)\n\n\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\nX_train = X_train.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nprint(X_train.shape)\nprint(X_test.shape)\n\n\n(1168, 80)\n(292, 80)\n\n\n\n\nCode\nprint(Y_train.shape)\nprint(Y_test.shape)\n\n\n(1168,)\n(292,)\n\n\n\n\nCode\ncategorical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].nunique() &lt; 10 and X_train[cname].dtype == \"object\"]\n                    \n                    \nnumerical_cols = numerical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].dtype in ['int64', 'float64']]\n\n\n\n\nCode\nprint(\"The total number of categorical columns:\", len(categorical_cols))\nprint(\"The total number of numerical columns:\", len(numerical_cols))\n\n\nThe total number of categorical columns: 40\nThe total number of numerical columns: 37\n\n\n\n\nCode\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train[my_cols].copy()\nX_test= X_test[my_cols].copy()\n\n\nX_final = df_test[my_cols].copy()",
    "crumbs": [
      "Regression",
      "Decision tree"
    ]
  },
  {
    "objectID": "regression/2 Decision Tree on house price data copy.html#pipelines-for-data-preprocessing",
    "href": "regression/2 Decision Tree on house price data copy.html#pipelines-for-data-preprocessing",
    "title": "Decision tree",
    "section": "2.5 Pipelines for Data Preprocessing",
    "text": "2.5 Pipelines for Data Preprocessing\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\nCode\nnumerical_transformer = Pipeline(steps=[\n    ('imputer_num', SimpleImputer(strategy='median')), \n    ('scaler', StandardScaler())\n])\n\n\n\n\nCode\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n\n\nCode\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)])",
    "crumbs": [
      "Regression",
      "Decision tree"
    ]
  },
  {
    "objectID": "regression/2 Decision Tree on house price data copy.html#define-model",
    "href": "regression/2 Decision Tree on house price data copy.html#define-model",
    "title": "Decision tree",
    "section": "3.1 define model",
    "text": "3.1 define model\n\n\nCode\nml_model = DecisionTreeRegressor(random_state=0)\nml_model\n\n\nDecisionTreeRegressor(random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeRegressor?Documentation for DecisionTreeRegressoriNot fittedDecisionTreeRegressor(random_state=0)",
    "crumbs": [
      "Regression",
      "Decision tree"
    ]
  },
  {
    "objectID": "regression/2 Decision Tree on house price data copy.html#define-pipline",
    "href": "regression/2 Decision Tree on house price data copy.html#define-pipline",
    "title": "Decision tree",
    "section": "3.2 define pipline",
    "text": "3.2 define pipline\n\n\nCode\npipeline = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model_dt', ml_model)\n         ]\n)",
    "crumbs": [
      "Regression",
      "Decision tree"
    ]
  },
  {
    "objectID": "regression/2 Decision Tree on house price data copy.html#train-model",
    "href": "regression/2 Decision Tree on house price data copy.html#train-model",
    "title": "Decision tree",
    "section": "3.3 train model",
    "text": "3.3 train model\n\n\nCode\npipeline.fit(X_train, Y_train)\n\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer_num',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['Id', 'MSSubClass',\n                                                   'LotFrontage', 'LotArea',\n                                                   'OverallQual', 'OverallCond',\n                                                   'YearBuilt', 'YearRemodAdd',\n                                                   'MasVnrArea', 'BsmtFinSF1',\n                                                   'BsmtFinSF2', 'BsmtUnfSF',\n                                                   'TotalBsmtSF', '1stFlrSF',\n                                                   '2ndFlrSF'...\n                                                   'LotConfig', 'LandSlope',\n                                                   'Condition1', 'Condition2',\n                                                   'BldgType', 'HouseStyle',\n                                                   'RoofStyle', 'RoofMatl',\n                                                   'MasVnrType', 'ExterQual',\n                                                   'ExterCond', 'Foundation',\n                                                   'BsmtQual', 'BsmtCond',\n                                                   'BsmtExposure',\n                                                   'BsmtFinType1',\n                                                   'BsmtFinType2', 'Heating',\n                                                   'HeatingQC', 'CentralAir',\n                                                   'Electrical', 'KitchenQual',\n                                                   'Functional', 'FireplaceQu', ...])])),\n                ('model_dt', DecisionTreeRegressor(random_state=0))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiFittedPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer_num',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['Id', 'MSSubClass',\n                                                   'LotFrontage', 'LotArea',\n                                                   'OverallQual', 'OverallCond',\n                                                   'YearBuilt', 'YearRemodAdd',\n                                                   'MasVnrArea', 'BsmtFinSF1',\n                                                   'BsmtFinSF2', 'BsmtUnfSF',\n                                                   'TotalBsmtSF', '1stFlrSF',\n                                                   '2ndFlrSF'...\n                                                   'LotConfig', 'LandSlope',\n                                                   'Condition1', 'Condition2',\n                                                   'BldgType', 'HouseStyle',\n                                                   'RoofStyle', 'RoofMatl',\n                                                   'MasVnrType', 'ExterQual',\n                                                   'ExterCond', 'Foundation',\n                                                   'BsmtQual', 'BsmtCond',\n                                                   'BsmtExposure',\n                                                   'BsmtFinType1',\n                                                   'BsmtFinType2', 'Heating',\n                                                   'HeatingQC', 'CentralAir',\n                                                   'Electrical', 'KitchenQual',\n                                                   'Functional', 'FireplaceQu', ...])])),\n                ('model_dt', DecisionTreeRegressor(random_state=0))])  preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('imputer_num',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('scaler', StandardScaler())]),\n                                 ['Id', 'MSSubClass', 'LotFrontage', 'LotArea',\n                                  'OverallQual', 'OverallCond', 'YearBuilt',\n                                  'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1',\n                                  'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF',\n                                  '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n                                  'GrLivArea', 'Bsm...\n                                 ['MSZoning', 'Street', 'Alley', 'LotShape',\n                                  'LandContour', 'Utilities', 'LotConfig',\n                                  'LandSlope', 'Condition1', 'Condition2',\n                                  'BldgType', 'HouseStyle', 'RoofStyle',\n                                  'RoofMatl', 'MasVnrType', 'ExterQual',\n                                  'ExterCond', 'Foundation', 'BsmtQual',\n                                  'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n                                  'BsmtFinType2', 'Heating', 'HeatingQC',\n                                  'CentralAir', 'Electrical', 'KitchenQual',\n                                  'Functional', 'FireplaceQu', ...])]) num['Id', 'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='median')  StandardScaler?Documentation for StandardScalerStandardScaler() cat['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'SaleType', 'SaleCondition']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='most_frequent')  OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore')  DecisionTreeRegressor?Documentation for DecisionTreeRegressorDecisionTreeRegressor(random_state=0) \n\n\n\n\nCode\nfitted_model=pipeline.steps[1][1]\n\n\n\n\nCode\nvar=pipeline[:-1].get_feature_names_out()\nvar\n\n\narray(['num__Id', 'num__MSSubClass', 'num__LotFrontage', 'num__LotArea',\n       'num__OverallQual', 'num__OverallCond', 'num__YearBuilt',\n       'num__YearRemodAdd', 'num__MasVnrArea', 'num__BsmtFinSF1',\n       'num__BsmtFinSF2', 'num__BsmtUnfSF', 'num__TotalBsmtSF',\n       'num__1stFlrSF', 'num__2ndFlrSF', 'num__LowQualFinSF',\n       'num__GrLivArea', 'num__BsmtFullBath', 'num__BsmtHalfBath',\n       'num__FullBath', 'num__HalfBath', 'num__BedroomAbvGr',\n       'num__KitchenAbvGr', 'num__TotRmsAbvGrd', 'num__Fireplaces',\n       'num__GarageYrBlt', 'num__GarageCars', 'num__GarageArea',\n       'num__WoodDeckSF', 'num__OpenPorchSF', 'num__EnclosedPorch',\n       'num__3SsnPorch', 'num__ScreenPorch', 'num__PoolArea',\n       'num__MiscVal', 'num__MoSold', 'num__YrSold',\n       'cat__MSZoning_C (all)', 'cat__MSZoning_FV', 'cat__MSZoning_RH',\n       'cat__MSZoning_RL', 'cat__MSZoning_RM', 'cat__Street_Grvl',\n       'cat__Street_Pave', 'cat__Alley_Grvl', 'cat__Alley_Pave',\n       'cat__LotShape_IR1', 'cat__LotShape_IR2', 'cat__LotShape_IR3',\n       'cat__LotShape_Reg', 'cat__LandContour_Bnk',\n       'cat__LandContour_HLS', 'cat__LandContour_Low',\n       'cat__LandContour_Lvl', 'cat__Utilities_AllPub',\n       'cat__Utilities_NoSeWa', 'cat__LotConfig_Corner',\n       'cat__LotConfig_CulDSac', 'cat__LotConfig_FR2',\n       'cat__LotConfig_FR3', 'cat__LotConfig_Inside',\n       'cat__LandSlope_Gtl', 'cat__LandSlope_Mod', 'cat__LandSlope_Sev',\n       'cat__Condition1_Artery', 'cat__Condition1_Feedr',\n       'cat__Condition1_Norm', 'cat__Condition1_PosA',\n       'cat__Condition1_PosN', 'cat__Condition1_RRAe',\n       'cat__Condition1_RRAn', 'cat__Condition1_RRNe',\n       'cat__Condition1_RRNn', 'cat__Condition2_Feedr',\n       'cat__Condition2_Norm', 'cat__Condition2_PosA',\n       'cat__Condition2_PosN', 'cat__Condition2_RRAe',\n       'cat__Condition2_RRAn', 'cat__Condition2_RRNn',\n       'cat__BldgType_1Fam', 'cat__BldgType_2fmCon',\n       'cat__BldgType_Duplex', 'cat__BldgType_Twnhs',\n       'cat__BldgType_TwnhsE', 'cat__HouseStyle_1.5Fin',\n       'cat__HouseStyle_1.5Unf', 'cat__HouseStyle_1Story',\n       'cat__HouseStyle_2.5Fin', 'cat__HouseStyle_2.5Unf',\n       'cat__HouseStyle_2Story', 'cat__HouseStyle_SFoyer',\n       'cat__HouseStyle_SLvl', 'cat__RoofStyle_Flat',\n       'cat__RoofStyle_Gable', 'cat__RoofStyle_Gambrel',\n       'cat__RoofStyle_Hip', 'cat__RoofStyle_Mansard',\n       'cat__RoofStyle_Shed', 'cat__RoofMatl_ClyTile',\n       'cat__RoofMatl_CompShg', 'cat__RoofMatl_Membran',\n       'cat__RoofMatl_Metal', 'cat__RoofMatl_Roll',\n       'cat__RoofMatl_Tar&Grv', 'cat__RoofMatl_WdShake',\n       'cat__RoofMatl_WdShngl', 'cat__MasVnrType_BrkCmn',\n       'cat__MasVnrType_BrkFace', 'cat__MasVnrType_Stone',\n       'cat__ExterQual_Ex', 'cat__ExterQual_Fa', 'cat__ExterQual_Gd',\n       'cat__ExterQual_TA', 'cat__ExterCond_Ex', 'cat__ExterCond_Fa',\n       'cat__ExterCond_Gd', 'cat__ExterCond_Po', 'cat__ExterCond_TA',\n       'cat__Foundation_BrkTil', 'cat__Foundation_CBlock',\n       'cat__Foundation_PConc', 'cat__Foundation_Slab',\n       'cat__Foundation_Stone', 'cat__Foundation_Wood',\n       'cat__BsmtQual_Ex', 'cat__BsmtQual_Fa', 'cat__BsmtQual_Gd',\n       'cat__BsmtQual_TA', 'cat__BsmtCond_Fa', 'cat__BsmtCond_Gd',\n       'cat__BsmtCond_Po', 'cat__BsmtCond_TA', 'cat__BsmtExposure_Av',\n       'cat__BsmtExposure_Gd', 'cat__BsmtExposure_Mn',\n       'cat__BsmtExposure_No', 'cat__BsmtFinType1_ALQ',\n       'cat__BsmtFinType1_BLQ', 'cat__BsmtFinType1_GLQ',\n       'cat__BsmtFinType1_LwQ', 'cat__BsmtFinType1_Rec',\n       'cat__BsmtFinType1_Unf', 'cat__BsmtFinType2_ALQ',\n       'cat__BsmtFinType2_BLQ', 'cat__BsmtFinType2_GLQ',\n       'cat__BsmtFinType2_LwQ', 'cat__BsmtFinType2_Rec',\n       'cat__BsmtFinType2_Unf', 'cat__Heating_Floor', 'cat__Heating_GasA',\n       'cat__Heating_GasW', 'cat__Heating_Grav', 'cat__Heating_OthW',\n       'cat__Heating_Wall', 'cat__HeatingQC_Ex', 'cat__HeatingQC_Fa',\n       'cat__HeatingQC_Gd', 'cat__HeatingQC_Po', 'cat__HeatingQC_TA',\n       'cat__CentralAir_N', 'cat__CentralAir_Y', 'cat__Electrical_FuseA',\n       'cat__Electrical_FuseF', 'cat__Electrical_FuseP',\n       'cat__Electrical_Mix', 'cat__Electrical_SBrkr',\n       'cat__KitchenQual_Ex', 'cat__KitchenQual_Fa',\n       'cat__KitchenQual_Gd', 'cat__KitchenQual_TA',\n       'cat__Functional_Maj1', 'cat__Functional_Maj2',\n       'cat__Functional_Min1', 'cat__Functional_Min2',\n       'cat__Functional_Mod', 'cat__Functional_Sev',\n       'cat__Functional_Typ', 'cat__FireplaceQu_Ex',\n       'cat__FireplaceQu_Fa', 'cat__FireplaceQu_Gd',\n       'cat__FireplaceQu_Po', 'cat__FireplaceQu_TA',\n       'cat__GarageType_2Types', 'cat__GarageType_Attchd',\n       'cat__GarageType_Basment', 'cat__GarageType_BuiltIn',\n       'cat__GarageType_CarPort', 'cat__GarageType_Detchd',\n       'cat__GarageFinish_Fin', 'cat__GarageFinish_RFn',\n       'cat__GarageFinish_Unf', 'cat__GarageQual_Ex',\n       'cat__GarageQual_Fa', 'cat__GarageQual_Gd', 'cat__GarageQual_Po',\n       'cat__GarageQual_TA', 'cat__GarageCond_Ex', 'cat__GarageCond_Fa',\n       'cat__GarageCond_Gd', 'cat__GarageCond_Po', 'cat__GarageCond_TA',\n       'cat__PavedDrive_N', 'cat__PavedDrive_P', 'cat__PavedDrive_Y',\n       'cat__PoolQC_Ex', 'cat__PoolQC_Fa', 'cat__PoolQC_Gd',\n       'cat__Fence_GdPrv', 'cat__Fence_GdWo', 'cat__Fence_MnPrv',\n       'cat__Fence_MnWw', 'cat__MiscFeature_Gar2',\n       'cat__MiscFeature_Othr', 'cat__MiscFeature_Shed',\n       'cat__SaleType_COD', 'cat__SaleType_CWD', 'cat__SaleType_Con',\n       'cat__SaleType_ConLD', 'cat__SaleType_ConLI',\n       'cat__SaleType_ConLw', 'cat__SaleType_New', 'cat__SaleType_Oth',\n       'cat__SaleType_WD', 'cat__SaleCondition_Abnorml',\n       'cat__SaleCondition_AdjLand', 'cat__SaleCondition_Alloca',\n       'cat__SaleCondition_Family', 'cat__SaleCondition_Normal',\n       'cat__SaleCondition_Partial'], dtype=object)\n\n\nvariable importance\n\n\nCode\nimportances = fitted_model.feature_importances_\nvi=pd.DataFrame({\"variable\":var,\"importances\":importances})\nvi=vi.sort_values('importances',ascending=False)\nvi\n\n\n\n\n\n\n\n\n\n\nvariable\nimportances\n\n\n\n\n4\nnum__OverallQual\n0.618841\n\n\n16\nnum__GrLivArea\n0.086857\n\n\n14\nnum__2ndFlrSF\n0.072518\n\n\n12\nnum__TotalBsmtSF\n0.035454\n\n\n26\nnum__GarageCars\n0.023574\n\n\n...\n...\n...\n\n\n110\ncat__ExterQual_Ex\n0.000000\n\n\n168\ncat__KitchenQual_Fa\n0.000000\n\n\n70\ncat__Condition1_RRAn\n0.000000\n\n\n71\ncat__Condition1_RRNe\n0.000000\n\n\n68\ncat__Condition1_PosN\n0.000000\n\n\n\n\n230 rows × 2 columns",
    "crumbs": [
      "Regression",
      "Decision tree"
    ]
  },
  {
    "objectID": "regression/2 Decision Tree on house price data copy.html#preformance",
    "href": "regression/2 Decision Tree on house price data copy.html#preformance",
    "title": "Decision tree",
    "section": "3.4 Preformance",
    "text": "3.4 Preformance\n\n\nCode\nY_pred_dt =pipeline.predict(X_test) #always gets x and retuns y\n\n\nR 2\n\n\nCode\nfrom sklearn.metrics import r2_score\nr2_score(Y_test, Y_pred_dt)\n\n\n0.786080833023575\n\n\nMAE\n\n\nCode\nfrom sklearn.metrics import mean_absolute_error\nmean_absolute_error(Y_test, Y_pred_dt)\n\n\n24463.89383561644\n\n\nRMSE\n\n\nCode\nfrom  math import sqrt\nfrom sklearn.metrics import mean_squared_error\nmse=mean_squared_error(Y_test, Y_pred_dt)\nrmse=sqrt(mse)\nrmse\n\n\n33873.177831444824",
    "crumbs": [
      "Regression",
      "Decision tree"
    ]
  },
  {
    "objectID": "regression/2 Decision Tree on house price data copy.html#k-fold-cross-validation",
    "href": "regression/2 Decision Tree on house price data copy.html#k-fold-cross-validation",
    "title": "Decision tree",
    "section": "3.5 k-Fold Cross-Validation",
    "text": "3.5 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \n\n\n\n\nCode\ncv_dt = cross_val_score(pipeline, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.6440603651748669\n\n\n\n\nCode\ncv_dt = cross_val_score(pipeline, X_train, Y_train, cv=kf_dt,scoring = 'neg_mean_squared_error')\nnp.mean(np.sqrt(np.abs(cv_dt)))\n\n\n46489.563013617146",
    "crumbs": [
      "Regression",
      "Decision tree"
    ]
  },
  {
    "objectID": "data/1 Pandas.html",
    "href": "data/1 Pandas.html",
    "title": "Data manipulation with Pandas",
    "section": "",
    "text": "Code\nimport sys\nprint(sys.version)\n\n\n3.11.4 (v3.11.4:d2340ef257, Jun  6 2023, 19:15:51) [Clang 13.0.0 (clang-1300.0.29.30)]\nCode\nimport pandas as pd\nprint('pandas version', pd.__version__)\n\n\npandas version 2.2.1",
    "crumbs": [
      "Data",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data/1 Pandas.html#reading-from-parquet",
    "href": "data/1 Pandas.html#reading-from-parquet",
    "title": "Data manipulation with Pandas",
    "section": "0.1 Reading from parquet",
    "text": "0.1 Reading from parquet\n\n\nCode\ndf = pd.read_parquet(\"data/Combined_Flights_2022.parquet\")",
    "crumbs": [
      "Data",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data/1 Pandas.html#get-first-3",
    "href": "data/1 Pandas.html#get-first-3",
    "title": "Data manipulation with Pandas",
    "section": "0.2 get first 3",
    "text": "0.2 get first 3\n\n\nCode\ndf.head(3)\n\n\n\n\n\n\n\n\n\n\nFlightDate\nAirline\nOrigin\nDest\nCancelled\nDiverted\nCRSDepTime\nDepTime\nDepDelayMinutes\nDepDelay\n...\nWheelsOff\nWheelsOn\nTaxiIn\nCRSArrTime\nArrDelay\nArrDel15\nArrivalDelayGroups\nArrTimeBlk\nDistanceGroup\nDivAirportLandings\n\n\n\n\n0\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nGJT\nDEN\nFalse\nFalse\n1133\n1123.0\n0.0\n-10.0\n...\n1140.0\n1220.0\n8.0\n1245\n-17.0\n0.0\n-2.0\n1200-1259\n1\n0\n\n\n1\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nHRL\nIAH\nFalse\nFalse\n732\n728.0\n0.0\n-4.0\n...\n744.0\n839.0\n9.0\n849\n-1.0\n0.0\n-1.0\n0800-0859\n2\n0\n\n\n2\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nDRO\nDEN\nFalse\nFalse\n1529\n1514.0\n0.0\n-15.0\n...\n1535.0\n1622.0\n14.0\n1639\n-3.0\n0.0\n-1.0\n1600-1659\n2\n0\n\n\n\n\n3 rows × 61 columns",
    "crumbs": [
      "Data",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data/1 Pandas.html#get-last-3",
    "href": "data/1 Pandas.html#get-last-3",
    "title": "Data manipulation with Pandas",
    "section": "0.3 get last 3",
    "text": "0.3 get last 3\n\n\nCode\ndf.tail(3)\n\n\n\n\n\n\n\n\n\n\nFlightDate\nAirline\nOrigin\nDest\nCancelled\nDiverted\nCRSDepTime\nDepTime\nDepDelayMinutes\nDepDelay\n...\nWheelsOff\nWheelsOn\nTaxiIn\nCRSArrTime\nArrDelay\nArrDel15\nArrivalDelayGroups\nArrTimeBlk\nDistanceGroup\nDivAirportLandings\n\n\n\n\n590539\n2022-03-08\nRepublic Airlines\nALB\nORD\nFalse\nFalse\n1700\n2318.0\n378.0\n378.0\n...\n2337.0\n52.0\n7.0\n1838\n381.0\n1.0\n12.0\n1800-1859\n3\n0\n\n\n590540\n2022-03-25\nRepublic Airlines\nEWR\nPIT\nFalse\nTrue\n2129\n2322.0\n113.0\n113.0\n...\n2347.0\n933.0\n6.0\n2255\nNaN\nNaN\nNaN\n2200-2259\n2\n1\n\n\n590541\n2022-03-07\nRepublic Airlines\nEWR\nRDU\nFalse\nTrue\n1154\n1148.0\n0.0\n-6.0\n...\n1201.0\n1552.0\n4.0\n1333\nNaN\nNaN\nNaN\n1300-1359\n2\n1\n\n\n\n\n3 rows × 61 columns",
    "crumbs": [
      "Data",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data/1 Pandas.html#get-ramdon-5",
    "href": "data/1 Pandas.html#get-ramdon-5",
    "title": "Data manipulation with Pandas",
    "section": "0.4 get ramdon 5",
    "text": "0.4 get ramdon 5\n\n\nCode\ndf.sample(5, random_state=42)\n\n\n\n\n\n\n\n\n\n\nFlightDate\nAirline\nOrigin\nDest\nCancelled\nDiverted\nCRSDepTime\nDepTime\nDepDelayMinutes\nDepDelay\n...\nWheelsOff\nWheelsOn\nTaxiIn\nCRSArrTime\nArrDelay\nArrDel15\nArrivalDelayGroups\nArrTimeBlk\nDistanceGroup\nDivAirportLandings\n\n\n\n\n324021\n2022-03-19\nSkyWest Airlines Inc.\nASE\nDEN\nFalse\nFalse\n1831\n1826.0\n0.0\n-5.0\n...\n1845.0\n1916.0\n7.0\n1933\n-10.0\n0.0\n-1.0\n1900-1959\n1\n0\n\n\n34739\n2022-02-16\nSkyWest Airlines Inc.\nTYS\nDEN\nFalse\nFalse\n1605\n1605.0\n0.0\n0.0\n...\n1614.0\n1728.0\n44.0\n1737\n35.0\n1.0\n2.0\n1700-1759\n5\n0\n\n\n304494\n2022-01-18\nAmerican Airlines Inc.\nLAX\nOGG\nFalse\nFalse\n1719\n1714.0\n0.0\n-5.0\n...\n1728.0\n2042.0\n10.0\n2049\n3.0\n0.0\n0.0\n2000-2059\n10\n0\n\n\n205451\n2022-01-23\nSouthwest Airlines Co.\nLAS\nBWI\nFalse\nFalse\n1515\n1533.0\n18.0\n18.0\n...\n1548.0\n2259.0\n3.0\n2235\n27.0\n1.0\n1.0\n2200-2259\n9\n0\n\n\n173176\n2022-07-01\nDelta Air Lines Inc.\nDTW\nRDU\nFalse\nFalse\n715\n709.0\n0.0\n-6.0\n...\n721.0\n834.0\n3.0\n853\n-16.0\n0.0\n-2.0\n0800-0859\n3\n0\n\n\n\n\n5 rows × 61 columns",
    "crumbs": [
      "Data",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data/1 Pandas.html#get-columns-names",
    "href": "data/1 Pandas.html#get-columns-names",
    "title": "Data manipulation with Pandas",
    "section": "0.5 get columns names",
    "text": "0.5 get columns names\n\n\nCode\ndf.columns\n\n\nIndex(['FlightDate', 'Airline', 'Origin', 'Dest', 'Cancelled', 'Diverted',\n       'CRSDepTime', 'DepTime', 'DepDelayMinutes', 'DepDelay', 'ArrTime',\n       'ArrDelayMinutes', 'AirTime', 'CRSElapsedTime', 'ActualElapsedTime',\n       'Distance', 'Year', 'Quarter', 'Month', 'DayofMonth', 'DayOfWeek',\n       'Marketing_Airline_Network', 'Operated_or_Branded_Code_Share_Partners',\n       'DOT_ID_Marketing_Airline', 'IATA_Code_Marketing_Airline',\n       'Flight_Number_Marketing_Airline', 'Operating_Airline',\n       'DOT_ID_Operating_Airline', 'IATA_Code_Operating_Airline',\n       'Tail_Number', 'Flight_Number_Operating_Airline', 'OriginAirportID',\n       'OriginAirportSeqID', 'OriginCityMarketID', 'OriginCityName',\n       'OriginState', 'OriginStateFips', 'OriginStateName', 'OriginWac',\n       'DestAirportID', 'DestAirportSeqID', 'DestCityMarketID', 'DestCityName',\n       'DestState', 'DestStateFips', 'DestStateName', 'DestWac', 'DepDel15',\n       'DepartureDelayGroups', 'DepTimeBlk', 'TaxiOut', 'WheelsOff',\n       'WheelsOn', 'TaxiIn', 'CRSArrTime', 'ArrDelay', 'ArrDel15',\n       'ArrivalDelayGroups', 'ArrTimeBlk', 'DistanceGroup',\n       'DivAirportLandings'],\n      dtype='object')",
    "crumbs": [
      "Data",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data/1 Pandas.html#get-row-index",
    "href": "data/1 Pandas.html#get-row-index",
    "title": "Data manipulation with Pandas",
    "section": "0.6 get row index",
    "text": "0.6 get row index\n\n\nCode\ndf.index\n\n\nIndex([     0,      1,      2,      3,      4,      5,      6,      7,      8,\n            9,\n       ...\n       590532, 590533, 590534, 590535, 590536, 590537, 590538, 590539, 590540,\n       590541],\n      dtype='int64', length=4078318)",
    "crumbs": [
      "Data",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data/1 Pandas.html#get-long-info",
    "href": "data/1 Pandas.html#get-long-info",
    "title": "Data manipulation with Pandas",
    "section": "0.7 get long info",
    "text": "0.7 get long info\n\n\nCode\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 4078318 entries, 0 to 590541\nData columns (total 61 columns):\n #   Column                                   Dtype         \n---  ------                                   -----         \n 0   FlightDate                               datetime64[us]\n 1   Airline                                  object        \n 2   Origin                                   object        \n 3   Dest                                     object        \n 4   Cancelled                                bool          \n 5   Diverted                                 bool          \n 6   CRSDepTime                               int64         \n 7   DepTime                                  float64       \n 8   DepDelayMinutes                          float64       \n 9   DepDelay                                 float64       \n 10  ArrTime                                  float64       \n 11  ArrDelayMinutes                          float64       \n 12  AirTime                                  float64       \n 13  CRSElapsedTime                           float64       \n 14  ActualElapsedTime                        float64       \n 15  Distance                                 float64       \n 16  Year                                     int64         \n 17  Quarter                                  int64         \n 18  Month                                    int64         \n 19  DayofMonth                               int64         \n 20  DayOfWeek                                int64         \n 21  Marketing_Airline_Network                object        \n 22  Operated_or_Branded_Code_Share_Partners  object        \n 23  DOT_ID_Marketing_Airline                 int64         \n 24  IATA_Code_Marketing_Airline              object        \n 25  Flight_Number_Marketing_Airline          int64         \n 26  Operating_Airline                        object        \n 27  DOT_ID_Operating_Airline                 int64         \n 28  IATA_Code_Operating_Airline              object        \n 29  Tail_Number                              object        \n 30  Flight_Number_Operating_Airline          int64         \n 31  OriginAirportID                          int64         \n 32  OriginAirportSeqID                       int64         \n 33  OriginCityMarketID                       int64         \n 34  OriginCityName                           object        \n 35  OriginState                              object        \n 36  OriginStateFips                          int64         \n 37  OriginStateName                          object        \n 38  OriginWac                                int64         \n 39  DestAirportID                            int64         \n 40  DestAirportSeqID                         int64         \n 41  DestCityMarketID                         int64         \n 42  DestCityName                             object        \n 43  DestState                                object        \n 44  DestStateFips                            int64         \n 45  DestStateName                            object        \n 46  DestWac                                  int64         \n 47  DepDel15                                 float64       \n 48  DepartureDelayGroups                     float64       \n 49  DepTimeBlk                               object        \n 50  TaxiOut                                  float64       \n 51  WheelsOff                                float64       \n 52  WheelsOn                                 float64       \n 53  TaxiIn                                   float64       \n 54  CRSArrTime                               int64         \n 55  ArrDelay                                 float64       \n 56  ArrDel15                                 float64       \n 57  ArrivalDelayGroups                       float64       \n 58  ArrTimeBlk                               object        \n 59  DistanceGroup                            int64         \n 60  DivAirportLandings                       int64         \ndtypes: bool(2), datetime64[us](1), float64(18), int64(23), object(17)\nmemory usage: 1.8+ GB",
    "crumbs": [
      "Data",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data/1 Pandas.html#get-short-info",
    "href": "data/1 Pandas.html#get-short-info",
    "title": "Data manipulation with Pandas",
    "section": "0.8 get short info",
    "text": "0.8 get short info\n\n\nCode\ndf.info(verbose=False)\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 4078318 entries, 0 to 590541\nColumns: 61 entries, FlightDate to DivAirportLandings\ndtypes: bool(2), datetime64[us](1), float64(18), int64(23), object(17)\nmemory usage: 1.8+ GB",
    "crumbs": [
      "Data",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data/1 Pandas.html#summary-of-numeric-column",
    "href": "data/1 Pandas.html#summary-of-numeric-column",
    "title": "Data manipulation with Pandas",
    "section": "0.9 summary of numeric column",
    "text": "0.9 summary of numeric column\n\n\nCode\ndf.describe()\n\n\n\n\n\n\n\n\n\n\nFlightDate\nCRSDepTime\nDepTime\nDepDelayMinutes\nDepDelay\nArrTime\nArrDelayMinutes\nAirTime\nCRSElapsedTime\nActualElapsedTime\n...\nTaxiOut\nWheelsOff\nWheelsOn\nTaxiIn\nCRSArrTime\nArrDelay\nArrDel15\nArrivalDelayGroups\nDistanceGroup\nDivAirportLandings\n\n\n\n\ncount\n4078318\n4.078318e+06\n3.957885e+06\n3.957823e+06\n3.957823e+06\n3.954079e+06\n3.944916e+06\n3.944916e+06\n4.078318e+06\n3.944916e+06\n...\n3.955652e+06\n3.955652e+06\n3.954076e+06\n3.954076e+06\n4.078318e+06\n3.944916e+06\n3.944916e+06\n3.944916e+06\n4.078318e+06\n4.078318e+06\n\n\nmean\n2022-04-18 12:10:43.903101\n1.329587e+03\n1.334374e+03\n1.601494e+01\n1.309049e+01\n1.457886e+03\n1.578307e+01\n1.110075e+02\n1.413211e+02\n1.358624e+02\n...\n1.697375e+01\n1.356576e+03\n1.455073e+03\n7.894387e+00\n1.486058e+03\n7.528486e+00\n2.164715e-01\n-6.256103e-02\n3.663516e+00\n3.685098e-03\n\n\nmin\n2022-01-01 00:00:00\n1.000000e+00\n1.000000e+00\n0.000000e+00\n-7.800000e+01\n1.000000e+00\n0.000000e+00\n8.000000e+00\n-4.800000e+01\n1.400000e+01\n...\n1.000000e+00\n1.000000e+00\n1.000000e+00\n1.000000e+00\n1.000000e+00\n-1.000000e+02\n0.000000e+00\n-2.000000e+00\n1.000000e+00\n0.000000e+00\n\n\n25%\n2022-02-25 00:00:00\n9.140000e+02\n9.170000e+02\n0.000000e+00\n-5.000000e+00\n1.046000e+03\n0.000000e+00\n6.000000e+01\n8.900000e+01\n8.300000e+01\n...\n1.100000e+01\n9.320000e+02\n1.044000e+03\n4.000000e+00\n1.103000e+03\n-1.400000e+01\n0.000000e+00\n-1.000000e+00\n2.000000e+00\n0.000000e+00\n\n\n50%\n2022-04-19 00:00:00\n1.320000e+03\n1.325000e+03\n0.000000e+00\n-2.000000e+00\n1.500000e+03\n0.000000e+00\n9.400000e+01\n1.240000e+02\n1.190000e+02\n...\n1.500000e+01\n1.338000e+03\n1.456000e+03\n6.000000e+00\n1.513000e+03\n-5.000000e+00\n0.000000e+00\n-1.000000e+00\n3.000000e+00\n0.000000e+00\n\n\n75%\n2022-06-11 00:00:00\n1.735000e+03\n1.744000e+03\n1.100000e+01\n1.100000e+01\n1.914000e+03\n1.000000e+01\n1.410000e+02\n1.710000e+02\n1.670000e+02\n...\n1.900000e+01\n1.758000e+03\n1.909000e+03\n9.000000e+00\n1.920000e+03\n1.000000e+01\n0.000000e+00\n0.000000e+00\n5.000000e+00\n0.000000e+00\n\n\nmax\n2022-07-31 00:00:00\n2.359000e+03\n2.400000e+03\n7.223000e+03\n7.223000e+03\n2.400000e+03\n7.232000e+03\n7.270000e+02\n6.900000e+02\n7.640000e+02\n...\n2.210000e+02\n2.400000e+03\n2.400000e+03\n2.900000e+02\n2.359000e+03\n7.232000e+03\n1.000000e+00\n1.200000e+01\n1.100000e+01\n9.000000e+00\n\n\nstd\nNaN\n4.904801e+02\n5.056219e+02\n5.231498e+01\n5.332016e+01\n5.431841e+02\n5.198424e+01\n6.996246e+01\n7.179635e+01\n7.185501e+01\n...\n9.495407e+00\n5.075580e+02\n5.378428e+02\n6.663118e+00\n5.185078e+02\n5.524625e+01\n4.118393e-01\n2.487442e+00\n2.320848e+00\n1.141331e-01\n\n\n\n\n8 rows × 42 columns",
    "crumbs": [
      "Data",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data/1 Pandas.html#summary-of-categorical-column",
    "href": "data/1 Pandas.html#summary-of-categorical-column",
    "title": "Data manipulation with Pandas",
    "section": "0.10 summary of categorical column",
    "text": "0.10 summary of categorical column\n\n\nCode\ndf[[\"Airline\"]].describe()\n\n\n\n\n\n\n\n\n\n\nAirline\n\n\n\n\ncount\n4078318\n\n\nunique\n21\n\n\ntop\nSouthwest Airlines Co.\n\n\nfreq\n731925",
    "crumbs": [
      "Data",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "model type/1 decision tree.html",
    "href": "model type/1 decision tree.html",
    "title": "Decision tree",
    "section": "",
    "text": "1 Pros\n\nEasier to interpret than Neural Network\nFast training and making inference\n\n\n\n2 Cons\n\nProne to overfitting\n\n\n\n3 reference:\nhttps://www.youtube.com/watch?v=6DlWndLbk90",
    "crumbs": [
      "model type",
      "Decision tree"
    ]
  },
  {
    "objectID": "data/2 siuba.html#comparison",
    "href": "data/2 siuba.html#comparison",
    "title": "Data manipulation with siuba",
    "section": "1 Comparison",
    "text": "1 Comparison",
    "crumbs": [
      "Data",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data/2 siuba.html#package-download",
    "href": "data/2 siuba.html#package-download",
    "title": "Data manipulation with siuba",
    "section": "2 Package download",
    "text": "2 Package download\n\n\nCode\nimport os\nos.system('pip install siuba')",
    "crumbs": [
      "Data",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data/2 siuba.html#load-package",
    "href": "data/2 siuba.html#load-package",
    "title": "Data manipulation with siuba",
    "section": "3 load package",
    "text": "3 load package\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nimport seaborn as sns\n\nfrom siuba import _, mutate, filter, group_by, summarize,show_query\nfrom siuba import *\nfrom siuba.data import mtcars,penguins\n\n\n\n\nCode\nimport os\nos.system('pip show siuba')\n\n\nName: siuba\nVersion: 0.4.4\nSummary: A package for quick, scrappy analyses with pandas and SQL\nHome-page: https://github.com/machow/siuba\nAuthor: Michael Chow\nAuthor-email: mc_al_gh_siuba@fastmail.com\nLicense: MIT\nLocation: /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages\nRequires: numpy, pandas, PyYAML, SQLAlchemy\nRequired-by: \n\n\n0\n\n\n\n\nCode\nsmall_mtcars = mtcars &gt;&gt; select(_.cyl, _.mpg, _.hp)&gt;&gt; head(5)\nsmall_penguins=penguins&gt;&gt; head(5)",
    "crumbs": [
      "Data",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data/2 siuba.html#select",
    "href": "data/2 siuba.html#select",
    "title": "Data manipulation with siuba",
    "section": "4 select",
    "text": "4 select\n\n\nCode\nsmall_mtcars &gt;&gt; select(_.cyl, _.mpg)\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\n\n\n\n\n0\n6\n21.0\n\n\n1\n6\n21.0\n\n\n2\n4\n22.8\n\n\n3\n6\n21.4\n\n\n4\n8\n18.7\n\n\n\n\n\n\n\n\nexclude\n\n\nCode\nsmall_mtcars &gt;&gt; select(~_.cyl)\n\n\n\n\n\n\n\n\n\n\nmpg\nhp\n\n\n\n\n0\n21.0\n110\n\n\n1\n21.0\n110\n\n\n2\n22.8\n93\n\n\n3\n21.4\n110\n\n\n4\n18.7\n175",
    "crumbs": [
      "Data",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data/2 siuba.html#renaming",
    "href": "data/2 siuba.html#renaming",
    "title": "Data manipulation with siuba",
    "section": "5 Renaming",
    "text": "5 Renaming\n\n\nCode\nsmall_mtcars &gt;&gt; rename(new_name_mpg = _.mpg)\n\n\n\n\n\n\n\n\n\n\ncyl\nnew_name_mpg\nhp\n\n\n\n\n0\n6\n21.0\n110\n\n\n1\n6\n21.0\n110\n\n\n2\n4\n22.8\n93\n\n\n3\n6\n21.4\n110\n\n\n4\n8\n18.7\n175",
    "crumbs": [
      "Data",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data/2 siuba.html#mutate",
    "href": "data/2 siuba.html#mutate",
    "title": "Data manipulation with siuba",
    "section": "6 Mutate",
    "text": "6 Mutate\n\n\nCode\nmtcars.head()&gt;&gt; mutate(gear2 = _.gear+1)\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\ngear2\n\n\n\n\n0\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n5\n\n\n1\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n5\n\n\n2\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n5\n\n\n3\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n4\n\n\n4\n18.7\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n4",
    "crumbs": [
      "Data",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data/2 siuba.html#filter",
    "href": "data/2 siuba.html#filter",
    "title": "Data manipulation with siuba",
    "section": "7 Filter",
    "text": "7 Filter\n\n\nCode\nmtcars&gt;&gt; filter(_.gear ==4)\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n7\n24.4\n4\n146.7\n62\n3.69\n3.190\n20.00\n1\n0\n4\n2\n\n\n8\n22.8\n4\n140.8\n95\n3.92\n3.150\n22.90\n1\n0\n4\n2\n\n\n9\n19.2\n6\n167.6\n123\n3.92\n3.440\n18.30\n1\n0\n4\n4\n\n\n10\n17.8\n6\n167.6\n123\n3.92\n3.440\n18.90\n1\n0\n4\n4\n\n\n17\n32.4\n4\n78.7\n66\n4.08\n2.200\n19.47\n1\n1\n4\n1\n\n\n18\n30.4\n4\n75.7\n52\n4.93\n1.615\n18.52\n1\n1\n4\n2\n\n\n19\n33.9\n4\n71.1\n65\n4.22\n1.835\n19.90\n1\n1\n4\n1\n\n\n25\n27.3\n4\n79.0\n66\n4.08\n1.935\n18.90\n1\n1\n4\n1\n\n\n31\n21.4\n4\n121.0\n109\n4.11\n2.780\n18.60\n1\n1\n4\n2\n\n\n\n\n\n\n\n\nFilters with OR conditions\n\n\nCode\nmtcars &gt;&gt; filter((_.cyl == 4) | (_.gear == 5))\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n2\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n7\n24.4\n4\n146.7\n62\n3.69\n3.190\n20.00\n1\n0\n4\n2\n\n\n8\n22.8\n4\n140.8\n95\n3.92\n3.150\n22.90\n1\n0\n4\n2\n\n\n17\n32.4\n4\n78.7\n66\n4.08\n2.200\n19.47\n1\n1\n4\n1\n\n\n18\n30.4\n4\n75.7\n52\n4.93\n1.615\n18.52\n1\n1\n4\n2\n\n\n19\n33.9\n4\n71.1\n65\n4.22\n1.835\n19.90\n1\n1\n4\n1\n\n\n20\n21.5\n4\n120.1\n97\n3.70\n2.465\n20.01\n1\n0\n3\n1\n\n\n25\n27.3\n4\n79.0\n66\n4.08\n1.935\n18.90\n1\n1\n4\n1\n\n\n26\n26.0\n4\n120.3\n91\n4.43\n2.140\n16.70\n0\n1\n5\n2\n\n\n27\n30.4\n4\n95.1\n113\n3.77\n1.513\n16.90\n1\n1\n5\n2\n\n\n28\n15.8\n8\n351.0\n264\n4.22\n3.170\n14.50\n0\n1\n5\n4\n\n\n29\n19.7\n6\n145.0\n175\n3.62\n2.770\n15.50\n0\n1\n5\n6\n\n\n30\n15.0\n8\n301.0\n335\n3.54\n3.570\n14.60\n0\n1\n5\n8\n\n\n31\n21.4\n4\n121.0\n109\n4.11\n2.780\n18.60\n1\n1\n4\n2\n\n\n\n\n\n\n\n\nDropping NA values",
    "crumbs": [
      "Data",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data/2 siuba.html#group-by",
    "href": "data/2 siuba.html#group-by",
    "title": "Data manipulation with siuba",
    "section": "8 group by",
    "text": "8 group by\n\n8.1 average,min,max,sum\n\n\nCode\ntbl_query = (mtcars\n  &gt;&gt; group_by(_.cyl)\n  &gt;&gt; summarize(avg_hp = _.hp.mean()\n              ,min_hp=_.hp.min()\n              ,max_hp=_.hp.max()\n              ,totol_disp=_.disp.sum()\n  )\n  )\n\ntbl_query\n\n\n\n\n\n\n\n\n\n\ncyl\navg_hp\nmin_hp\nmax_hp\ntotol_disp\n\n\n\n\n0\n4\n82.636364\n52\n113\n1156.5\n\n\n1\n6\n122.285714\n105\n175\n1283.2\n\n\n2\n8\n209.214286\n150\n335\n4943.4\n\n\n\n\n\n\n\n\n\n\n8.2 count\n\n\nCode\nmtcars &gt;&gt; group_by(_.cyl)  &gt;&gt; summarize(n = _.shape[0])\n\n\n\n\n\n\n\n\n\n\ncyl\nn\n\n\n\n\n0\n4\n11\n\n\n1\n6\n7\n\n\n2\n8\n14",
    "crumbs": [
      "Data",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data/2 siuba.html#order",
    "href": "data/2 siuba.html#order",
    "title": "Data manipulation with siuba",
    "section": "9 order",
    "text": "9 order\n\n\nCode\nsmall_mtcars &gt;&gt; arrange(_.hp)\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\nhp\n\n\n\n\n2\n4\n22.8\n93\n\n\n0\n6\n21.0\n110\n\n\n1\n6\n21.0\n110\n\n\n3\n6\n21.4\n110\n\n\n4\n8\n18.7\n175\n\n\n\n\n\n\n\n\nSort in descending order\n\n\nCode\nsmall_mtcars &gt;&gt; arrange(-_.hp)\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\nhp\n\n\n\n\n4\n8\n18.7\n175\n\n\n0\n6\n21.0\n110\n\n\n1\n6\n21.0\n110\n\n\n3\n6\n21.4\n110\n\n\n2\n4\n22.8\n93\n\n\n\n\n\n\n\n\nArrange by multiple variables\n\n\nCode\nsmall_mtcars &gt;&gt; arrange(_.cyl, -_.mpg)\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\nhp\n\n\n\n\n2\n4\n22.8\n93\n\n\n3\n6\n21.4\n110\n\n\n0\n6\n21.0\n110\n\n\n1\n6\n21.0\n110\n\n\n4\n8\n18.7\n175",
    "crumbs": [
      "Data",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data/2 siuba.html#using-siuba-with-database",
    "href": "data/2 siuba.html#using-siuba-with-database",
    "title": "Data manipulation with siuba",
    "section": "10 using siuba with database",
    "text": "10 using siuba with database\n\n10.1 set up a sqlite database, with an mtcars table.\n\n\nCode\nfrom sqlalchemy import create_engine\nfrom siuba.sql import LazyTbl\nfrom siuba import _, group_by, summarize, show_query, collect \nfrom siuba.data import mtcars\n\n# copy in to sqlite, using the pandas .to_sql() method\nengine = create_engine(\"sqlite:///:memory:\")\nmtcars.to_sql(\"mtcars\", engine, if_exists = \"replace\")\n\n\n32\n\n\n\n\n10.2 create table\n\n\nCode\n# Create a lazy SQL DataFrame\ntbl_mtcars = LazyTbl(engine, \"mtcars\")\ntbl_mtcars\n\n\n\n# Source: lazy query\n# DB Conn: Engine(sqlite:///:memory:)\n# Preview:\n\n\n\n\n\n\n\nindex\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\n0\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\n1\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\n2\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n3\n3\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n4\n4\n18.7\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\n\n\n# .. may have more rows\n\n\n\n\n\n10.3 create query\n\n\nCode\n# connect with siuba\n\ntbl_query = (tbl_mtcars\n  &gt;&gt; group_by(_.mpg)\n  &gt;&gt; summarize(avg_hp = _.hp.mean())\n  )\n\ntbl_query\n\n\n\n# Source: lazy query\n# DB Conn: Engine(sqlite:///:memory:)\n# Preview:\n\n\n\n\n\n\n\nmpg\navg_hp\n\n\n\n\n0\n10.4\n210.0\n\n\n1\n13.3\n245.0\n\n\n2\n14.3\n245.0\n\n\n3\n14.7\n230.0\n\n\n4\n15.0\n335.0\n\n\n\n\n# .. may have more rows\n\n\n\n\n\n10.4 show query\n\n\nCode\n tbl_query &gt;&gt; show_query()\n\n\nSELECT mtcars.mpg, avg(mtcars.hp) AS avg_hp \nFROM mtcars GROUP BY mtcars.mpg\n\n\n\n# Source: lazy query\n# DB Conn: Engine(sqlite:///:memory:)\n# Preview:\n\n\n\n\n\n\n\nmpg\navg_hp\n\n\n\n\n0\n10.4\n210.0\n\n\n1\n13.3\n245.0\n\n\n2\n14.3\n245.0\n\n\n3\n14.7\n230.0\n\n\n4\n15.0\n335.0\n\n\n\n\n# .. may have more rows\n\n\n\n\n\n10.5 Collect to DataFrame\nbecause lazy expressions,the collect function is actually running the sql.\n\n\nCode\ndata=tbl_query &gt;&gt; collect()\nprint(data)\n\n\n     mpg  avg_hp\n0   10.4   210.0\n1   13.3   245.0\n2   14.3   245.0\n3   14.7   230.0\n4   15.0   335.0\n5   15.2   165.0\n6   15.5   150.0\n7   15.8   264.0\n8   16.4   180.0\n9   17.3   180.0\n10  17.8   123.0\n11  18.1   105.0\n12  18.7   175.0\n13  19.2   149.0\n14  19.7   175.0\n15  21.0   110.0\n16  21.4   109.5\n17  21.5    97.0\n18  22.8    94.0\n19  24.4    62.0\n20  26.0    91.0\n21  27.3    66.0\n22  30.4    82.5\n23  32.4    66.0\n24  33.9    65.0",
    "crumbs": [
      "Data",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data/2 siuba.html#reference",
    "href": "data/2 siuba.html#reference",
    "title": "Data manipulation with siuba",
    "section": "11 reference:",
    "text": "11 reference:\nhttps://siuba.org/",
    "crumbs": [
      "Data",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data/2 siuba.html#select-column",
    "href": "data/2 siuba.html#select-column",
    "title": "Data manipulation with siuba",
    "section": "4 select column",
    "text": "4 select column\n\n\nCode\nsmall_mtcars &gt;&gt; select(_.cyl, _.mpg)\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\n\n\n\n\n0\n6\n21.0\n\n\n1\n6\n21.0\n\n\n2\n4\n22.8\n\n\n3\n6\n21.4\n\n\n4\n8\n18.7\n\n\n\n\n\n\n\n\nexclude\n\n\nCode\nsmall_mtcars &gt;&gt; select(~_.cyl)\n\n\n\n\n\n\n\n\n\n\nmpg\nhp\n\n\n\n\n0\n21.0\n110\n\n\n1\n21.0\n110\n\n\n2\n22.8\n93\n\n\n3\n21.4\n110\n\n\n4\n18.7\n175",
    "crumbs": [
      "Data",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data/2 siuba.html#renaming-column",
    "href": "data/2 siuba.html#renaming-column",
    "title": "Data manipulation with siuba",
    "section": "5 Renaming column",
    "text": "5 Renaming column\n\n\nCode\nsmall_mtcars &gt;&gt; rename(new_name_mpg = _.mpg)\n\n\n\n\n\n\n\n\n\n\ncyl\nnew_name_mpg\nhp\n\n\n\n\n0\n6\n21.0\n110\n\n\n1\n6\n21.0\n110\n\n\n2\n4\n22.8\n93\n\n\n3\n6\n21.4\n110\n\n\n4\n8\n18.7\n175",
    "crumbs": [
      "Data",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data/2 siuba.html#filter-rows",
    "href": "data/2 siuba.html#filter-rows",
    "title": "Data manipulation with siuba",
    "section": "7 Filter rows",
    "text": "7 Filter rows\n\n\nCode\nmtcars&gt;&gt; filter(_.gear ==4)\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n7\n24.4\n4\n146.7\n62\n3.69\n3.190\n20.00\n1\n0\n4\n2\n\n\n8\n22.8\n4\n140.8\n95\n3.92\n3.150\n22.90\n1\n0\n4\n2\n\n\n9\n19.2\n6\n167.6\n123\n3.92\n3.440\n18.30\n1\n0\n4\n4\n\n\n10\n17.8\n6\n167.6\n123\n3.92\n3.440\n18.90\n1\n0\n4\n4\n\n\n17\n32.4\n4\n78.7\n66\n4.08\n2.200\n19.47\n1\n1\n4\n1\n\n\n18\n30.4\n4\n75.7\n52\n4.93\n1.615\n18.52\n1\n1\n4\n2\n\n\n19\n33.9\n4\n71.1\n65\n4.22\n1.835\n19.90\n1\n1\n4\n1\n\n\n25\n27.3\n4\n79.0\n66\n4.08\n1.935\n18.90\n1\n1\n4\n1\n\n\n31\n21.4\n4\n121.0\n109\n4.11\n2.780\n18.60\n1\n1\n4\n2\n\n\n\n\n\n\n\n\n\n7.1 Filters with AND conditions\n\n\nCode\nmtcars &gt;&gt; filter((_.cyl &gt;4) & (_.gear == 5))\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n28\n15.8\n8\n351.0\n264\n4.22\n3.17\n14.5\n0\n1\n5\n4\n\n\n29\n19.7\n6\n145.0\n175\n3.62\n2.77\n15.5\n0\n1\n5\n6\n\n\n30\n15.0\n8\n301.0\n335\n3.54\n3.57\n14.6\n0\n1\n5\n8\n\n\n\n\n\n\n\n\n\n\n7.2 Filters with OR conditions\n\n\nCode\nmtcars &gt;&gt; filter((_.cyl == 6) | (_.gear == 5))\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n3\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n5\n18.1\n6\n225.0\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n\n\n9\n19.2\n6\n167.6\n123\n3.92\n3.440\n18.30\n1\n0\n4\n4\n\n\n10\n17.8\n6\n167.6\n123\n3.92\n3.440\n18.90\n1\n0\n4\n4\n\n\n26\n26.0\n4\n120.3\n91\n4.43\n2.140\n16.70\n0\n1\n5\n2\n\n\n27\n30.4\n4\n95.1\n113\n3.77\n1.513\n16.90\n1\n1\n5\n2\n\n\n28\n15.8\n8\n351.0\n264\n4.22\n3.170\n14.50\n0\n1\n5\n4\n\n\n29\n19.7\n6\n145.0\n175\n3.62\n2.770\n15.50\n0\n1\n5\n6\n\n\n30\n15.0\n8\n301.0\n335\n3.54\n3.570\n14.60\n0\n1\n5\n8\n\n\n\n\n\n\n\n\n\n\n7.3 Dropping NA values",
    "crumbs": [
      "Data",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data/2 siuba.html#order-rows",
    "href": "data/2 siuba.html#order-rows",
    "title": "Data manipulation with siuba",
    "section": "9 order rows",
    "text": "9 order rows\n\n\nCode\nsmall_mtcars &gt;&gt; arrange(_.hp)\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\nhp\n\n\n\n\n2\n4\n22.8\n93\n\n\n0\n6\n21.0\n110\n\n\n1\n6\n21.0\n110\n\n\n3\n6\n21.4\n110\n\n\n4\n8\n18.7\n175\n\n\n\n\n\n\n\n\n\n9.1 Sort in descending order\n\n\nCode\nsmall_mtcars &gt;&gt; arrange(-_.hp)\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\nhp\n\n\n\n\n4\n8\n18.7\n175\n\n\n0\n6\n21.0\n110\n\n\n1\n6\n21.0\n110\n\n\n3\n6\n21.4\n110\n\n\n2\n4\n22.8\n93\n\n\n\n\n\n\n\n\n\n\n9.2 Arrange by multiple variables\n\n\nCode\nsmall_mtcars &gt;&gt; arrange(_.cyl, -_.mpg)\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\nhp\n\n\n\n\n2\n4\n22.8\n93\n\n\n3\n6\n21.4\n110\n\n\n0\n6\n21.0\n110\n\n\n1\n6\n21.0\n110\n\n\n4\n8\n18.7\n175",
    "crumbs": [
      "Data",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/2 siuba.html#comparison",
    "href": "data manipulation/2 siuba.html#comparison",
    "title": "Data manipulation with siuba",
    "section": "1 Comparison",
    "text": "1 Comparison",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/2 siuba.html#package-download",
    "href": "data manipulation/2 siuba.html#package-download",
    "title": "Data manipulation with siuba",
    "section": "2 Package download",
    "text": "2 Package download\n\n\nCode\nimport os\nos.system('pip install siuba')",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/2 siuba.html#load-package",
    "href": "data manipulation/2 siuba.html#load-package",
    "title": "Data manipulation with siuba",
    "section": "3 load package",
    "text": "3 load package\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nimport seaborn as sns\n\nfrom siuba.data import mtcars,penguins\n\nfrom siuba.siu import call\nfrom siuba import _, mutate, filter, group_by, summarize,show_query\nfrom siuba import *\n\nfrom siuba.data import mtcars,penguins\n\n\n\n\nCode\nimport os\nos.system('pip show siuba')\n\n\nName: siuba\nVersion: 0.4.4\nSummary: A package for quick, scrappy analyses with pandas and SQL\nHome-page: https://github.com/machow/siuba\nAuthor: Michael Chow\nAuthor-email: mc_al_gh_siuba@fastmail.com\nLicense: MIT\nLocation: /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages\nRequires: numpy, pandas, PyYAML, SQLAlchemy\nRequired-by: \n\n\n0\n\n\n\n\nCode\nsmall_mtcars = mtcars &gt;&gt; select(_.cyl, _.mpg, _.hp)&gt;&gt; head(5)\nsmall_penguins=penguins&gt;&gt; head(5)",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/2 siuba.html#select-column",
    "href": "data manipulation/2 siuba.html#select-column",
    "title": "Data manipulation with siuba",
    "section": "4 select column",
    "text": "4 select column\n\n\nCode\nsmall_mtcars &gt;&gt; select(_.cyl, _.mpg)\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\n\n\n\n\n0\n6\n21.0\n\n\n1\n6\n21.0\n\n\n2\n4\n22.8\n\n\n3\n6\n21.4\n\n\n4\n8\n18.7",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/2 siuba.html#renaming-column",
    "href": "data manipulation/2 siuba.html#renaming-column",
    "title": "Data manipulation with siuba",
    "section": "6 Renaming column",
    "text": "6 Renaming column\n\n\nCode\nsmall_mtcars &gt;&gt; rename(new_name_mpg = _.mpg)\n\n\n\n\n\n\n\n\n\n\ncyl\nnew_name_mpg\nhp\n\n\n\n\n0\n6\n21.0\n110\n\n\n1\n6\n21.0\n110\n\n\n2\n4\n22.8\n93\n\n\n3\n6\n21.4\n110\n\n\n4\n8\n18.7\n175",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/2 siuba.html#create-column",
    "href": "data manipulation/2 siuba.html#create-column",
    "title": "Data manipulation with siuba",
    "section": "7 Create column",
    "text": "7 Create column\n\n7.1 Mutate\n\n\nCode\nmtcars.head()&gt;&gt; mutate(gear2 = _.gear+1\n                      ,gear3=if_else(_.gear &gt; 3, \"long\", \"short\")\n                       ,qsec2=case_when({\n                                          _.qsec &lt;= 17: \"short\",\n                                          _.qsec &lt;= 18: \"Medium\",\n                                          True: \"long\"\n                                                     })\n                       )\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\ngear2\ngear3\nqsec2\n\n\n\n\n0\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n5\nlong\nshort\n\n\n1\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n5\nlong\nMedium\n\n\n2\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n5\nlong\nlong\n\n\n3\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n4\nshort\nlong\n\n\n4\n18.7\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n4\nshort\nMedium\n\n\n\n\n\n\n\n\n\n\n7.2 Transmute,create column and only keep this column\n\n\nCode\nmtcars.head()&gt;&gt; transmute(gear2 = _.gear+1)\n\n\n\n\n\n\n\n\n\n\ngear2\n\n\n\n\n0\n5\n\n\n1\n5\n\n\n2\n5\n\n\n3\n4\n\n\n4\n4",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/2 siuba.html#filter-rows",
    "href": "data manipulation/2 siuba.html#filter-rows",
    "title": "Data manipulation with siuba",
    "section": "8 Filter rows",
    "text": "8 Filter rows\n\n\nCode\nmtcars&gt;&gt; filter(_.gear ==4)\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n7\n24.4\n4\n146.7\n62\n3.69\n3.190\n20.00\n1\n0\n4\n2\n\n\n8\n22.8\n4\n140.8\n95\n3.92\n3.150\n22.90\n1\n0\n4\n2\n\n\n9\n19.2\n6\n167.6\n123\n3.92\n3.440\n18.30\n1\n0\n4\n4\n\n\n10\n17.8\n6\n167.6\n123\n3.92\n3.440\n18.90\n1\n0\n4\n4\n\n\n17\n32.4\n4\n78.7\n66\n4.08\n2.200\n19.47\n1\n1\n4\n1\n\n\n18\n30.4\n4\n75.7\n52\n4.93\n1.615\n18.52\n1\n1\n4\n2\n\n\n19\n33.9\n4\n71.1\n65\n4.22\n1.835\n19.90\n1\n1\n4\n1\n\n\n25\n27.3\n4\n79.0\n66\n4.08\n1.935\n18.90\n1\n1\n4\n1\n\n\n31\n21.4\n4\n121.0\n109\n4.11\n2.780\n18.60\n1\n1\n4\n2\n\n\n\n\n\n\n\n\n\n8.1 Filters with AND conditions\n\n\nCode\nmtcars &gt;&gt; filter((_.cyl &gt;4) & (_.gear == 5))\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n28\n15.8\n8\n351.0\n264\n4.22\n3.17\n14.5\n0\n1\n5\n4\n\n\n29\n19.7\n6\n145.0\n175\n3.62\n2.77\n15.5\n0\n1\n5\n6\n\n\n30\n15.0\n8\n301.0\n335\n3.54\n3.57\n14.6\n0\n1\n5\n8\n\n\n\n\n\n\n\n\n\n\n8.2 Filters with OR conditions\n\n\nCode\nmtcars &gt;&gt; filter((_.cyl == 6) | (_.gear == 5))\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n3\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n5\n18.1\n6\n225.0\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n\n\n9\n19.2\n6\n167.6\n123\n3.92\n3.440\n18.30\n1\n0\n4\n4\n\n\n10\n17.8\n6\n167.6\n123\n3.92\n3.440\n18.90\n1\n0\n4\n4\n\n\n26\n26.0\n4\n120.3\n91\n4.43\n2.140\n16.70\n0\n1\n5\n2\n\n\n27\n30.4\n4\n95.1\n113\n3.77\n1.513\n16.90\n1\n1\n5\n2\n\n\n28\n15.8\n8\n351.0\n264\n4.22\n3.170\n14.50\n0\n1\n5\n4\n\n\n29\n19.7\n6\n145.0\n175\n3.62\n2.770\n15.50\n0\n1\n5\n6\n\n\n30\n15.0\n8\n301.0\n335\n3.54\n3.570\n14.60\n0\n1\n5\n8\n\n\n\n\n\n\n\n\n\n\n8.3 Dropping NA values",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/2 siuba.html#group-by",
    "href": "data manipulation/2 siuba.html#group-by",
    "title": "Data manipulation with siuba",
    "section": "9 group by",
    "text": "9 group by\n\n9.1 average,min,max,sum\n\n\nCode\ntbl_query = (mtcars\n  &gt;&gt; group_by(_.cyl)\n  &gt;&gt; summarize(avg_hp = _.hp.mean()\n              ,min_hp=_.hp.min()\n              ,max_hp=_.hp.max()\n              ,totol_disp=_.disp.sum()\n  )\n  )\n\ntbl_query\n\n\n\n\n\n\n\n\n\n\ncyl\navg_hp\nmin_hp\nmax_hp\ntotol_disp\n\n\n\n\n0\n4\n82.636364\n52\n113\n1156.5\n\n\n1\n6\n122.285714\n105\n175\n1283.2\n\n\n2\n8\n209.214286\n150\n335\n4943.4\n\n\n\n\n\n\n\n\n\n\n9.2 count\n\n\nCode\nmtcars &gt;&gt; group_by(_.cyl)  &gt;&gt; summarize(n = _.shape[0])\n\n\n\n\n\n\n\n\n\n\ncyl\nn\n\n\n\n\n0\n4\n11\n\n\n1\n6\n7\n\n\n2\n8\n14",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/2 siuba.html#order-rows",
    "href": "data manipulation/2 siuba.html#order-rows",
    "title": "Data manipulation with siuba",
    "section": "10 order rows",
    "text": "10 order rows\n\n\nCode\nsmall_mtcars &gt;&gt; arrange(_.hp)\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\nhp\n\n\n\n\n2\n4\n22.8\n93\n\n\n0\n6\n21.0\n110\n\n\n1\n6\n21.0\n110\n\n\n3\n6\n21.4\n110\n\n\n4\n8\n18.7\n175\n\n\n\n\n\n\n\n\n\n10.1 Sort in descending order\n\n\nCode\nsmall_mtcars &gt;&gt; arrange(-_.hp)\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\nhp\n\n\n\n\n4\n8\n18.7\n175\n\n\n0\n6\n21.0\n110\n\n\n1\n6\n21.0\n110\n\n\n3\n6\n21.4\n110\n\n\n2\n4\n22.8\n93\n\n\n\n\n\n\n\n\n\n\n10.2 Arrange by multiple variables\n\n\nCode\nsmall_mtcars &gt;&gt; arrange(_.cyl, -_.mpg)\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\nhp\n\n\n\n\n2\n4\n22.8\n93\n\n\n3\n6\n21.4\n110\n\n\n0\n6\n21.0\n110\n\n\n1\n6\n21.0\n110\n\n\n4\n8\n18.7\n175",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/2 siuba.html#join",
    "href": "data manipulation/2 siuba.html#join",
    "title": "Data manipulation with siuba",
    "section": "11 join",
    "text": "11 join\n\n\nCode\nlhs = pd.DataFrame({'id': [1,2,3], 'val': ['lhs.1', 'lhs.2', 'lhs.3']})\nrhs = pd.DataFrame({'id': [1,2,4], 'val': ['rhs.1', 'rhs.2', 'rhs.3']})\n\n\n\n\nCode\nlhs\n\n\n\n\n\n\n\n\n\n\nid\nval\n\n\n\n\n0\n1\nlhs.1\n\n\n1\n2\nlhs.2\n\n\n2\n3\nlhs.3\n\n\n\n\n\n\n\n\n\n\nCode\nrhs\n\n\n\n\n\n\n\n\n\n\nid\nval\n\n\n\n\n0\n1\nrhs.1\n\n\n1\n2\nrhs.2\n\n\n2\n4\nrhs.3\n\n\n\n\n\n\n\n\n\n11.1 inner_join\n\n\nCode\nresult=lhs &gt;&gt; inner_join(_, rhs, on=\"id\")\nresult\n\n\n\n\n\n\n\n\n\n\nid\nval_x\nval_y\n\n\n\n\n0\n1\nlhs.1\nrhs.1\n\n\n1\n2\nlhs.2\nrhs.2\n\n\n\n\n\n\n\n\n\n\n11.2 full join\n\n\nCode\nresult=rhs &gt;&gt; full_join(_, lhs, on=\"id\")\nresult\n\n\n\n\n\n\n\n\n\n\nid\nval_x\nval_y\n\n\n\n\n0\n1\nrhs.1\nlhs.1\n\n\n1\n2\nrhs.2\nlhs.2\n\n\n2\n3\nNaN\nlhs.3\n\n\n3\n4\nrhs.3\nNaN\n\n\n\n\n\n\n\n\n\n\n11.3 left join\n\n\nCode\nresult=lhs &gt;&gt; left_join(_, rhs, on=\"id\")\nresult\n\n\n\n\n\n\n\n\n\n\nid\nval_x\nval_y\n\n\n\n\n0\n1\nlhs.1\nrhs.1\n\n\n1\n2\nlhs.2\nrhs.2\n\n\n2\n3\nlhs.3\nNaN\n\n\n\n\n\n\n\n\n\n\n11.4 anti join\nkeep data in left which not in right\n\n\nCode\nresult=lhs &gt;&gt; anti_join(_, rhs, on=\"id\")\nresult\n\n\n\n\n\n\n\n\n\n\nid\nval\n\n\n\n\n2\n3\nlhs.3\n\n\n\n\n\n\n\n\nkeep data in right which not in left\n\n\nCode\nresult=rhs &gt;&gt; anti_join(_, lhs, on=\"id\")\nresult\n\n\n\n\n\n\n\n\n\n\nid\nval\n\n\n\n\n2\n4\nrhs.3",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/2 siuba.html#using-siuba-with-database",
    "href": "data manipulation/2 siuba.html#using-siuba-with-database",
    "title": "Data manipulation with siuba",
    "section": "15 using siuba with database",
    "text": "15 using siuba with database\n\n15.1 set up a sqlite database, with an mtcars table.\n\n\nCode\nfrom sqlalchemy import create_engine\nfrom siuba.sql import LazyTbl\nfrom siuba import _, group_by, summarize, show_query, collect \nfrom siuba.data import mtcars\n\n# copy in to sqlite, using the pandas .to_sql() method\nengine = create_engine(\"sqlite:///:memory:\")\nmtcars.to_sql(\"mtcars\", engine, if_exists = \"replace\")\n\n\n32\n\n\n\n\n15.2 create table\n\n\nCode\n# Create a lazy SQL DataFrame\ntbl_mtcars = LazyTbl(engine, \"mtcars\")\ntbl_mtcars\n\n\n\n# Source: lazy query\n# DB Conn: Engine(sqlite:///:memory:)\n# Preview:\n\n\n\n\n\n\n\nindex\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\n0\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\n1\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\n2\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n3\n3\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n4\n4\n18.7\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\n\n\n# .. may have more rows\n\n\n\n\n\n15.3 create query\n\n\nCode\n# connect with siuba\n\ntbl_query = (tbl_mtcars\n  &gt;&gt; group_by(_.mpg)\n  &gt;&gt; summarize(avg_hp = _.hp.mean())\n  )\n\ntbl_query\n\n\n\n# Source: lazy query\n# DB Conn: Engine(sqlite:///:memory:)\n# Preview:\n\n\n\n\n\n\n\nmpg\navg_hp\n\n\n\n\n0\n10.4\n210.0\n\n\n1\n13.3\n245.0\n\n\n2\n14.3\n245.0\n\n\n3\n14.7\n230.0\n\n\n4\n15.0\n335.0\n\n\n\n\n# .. may have more rows\n\n\n\n\n\n15.4 show query\n\n\nCode\n tbl_query &gt;&gt; show_query()\n\n\nSELECT mtcars.mpg, avg(mtcars.hp) AS avg_hp \nFROM mtcars GROUP BY mtcars.mpg\n\n\n\n# Source: lazy query\n# DB Conn: Engine(sqlite:///:memory:)\n# Preview:\n\n\n\n\n\n\n\nmpg\navg_hp\n\n\n\n\n0\n10.4\n210.0\n\n\n1\n13.3\n245.0\n\n\n2\n14.3\n245.0\n\n\n3\n14.7\n230.0\n\n\n4\n15.0\n335.0\n\n\n\n\n# .. may have more rows\n\n\n\n\n\n15.5 Collect to DataFrame\nbecause lazy expressions,the collect function is actually running the sql.\n\n\nCode\ndata=tbl_query &gt;&gt; collect()\nprint(data)\n\n\n     mpg  avg_hp\n0   10.4   210.0\n1   13.3   245.0\n2   14.3   245.0\n3   14.7   230.0\n4   15.0   335.0\n5   15.2   165.0\n6   15.5   150.0\n7   15.8   264.0\n8   16.4   180.0\n9   17.3   180.0\n10  17.8   123.0\n11  18.1   105.0\n12  18.7   175.0\n13  19.2   149.0\n14  19.7   175.0\n15  21.0   110.0\n16  21.4   109.5\n17  21.5    97.0\n18  22.8    94.0\n19  24.4    62.0\n20  26.0    91.0\n21  27.3    66.0\n22  30.4    82.5\n23  32.4    66.0\n24  33.9    65.0",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/2 siuba.html#reference",
    "href": "data manipulation/2 siuba.html#reference",
    "title": "Data manipulation with siuba",
    "section": "16 reference:",
    "text": "16 reference:\nhttps://siuba.org/",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html",
    "href": "data manipulation/1 Pandas.html",
    "title": "Data manipulation with Pandas",
    "section": "",
    "text": "pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.\nCode\nimport sys\nprint(sys.version)\n\n\n3.11.4 (v3.11.4:d2340ef257, Jun  6 2023, 19:15:51) [Clang 13.0.0 (clang-1300.0.29.30)]\nCode\nimport pandas as pd\nprint('pandas version', pd.__version__)\n\n\npandas version 2.2.1",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#reading-from-parquet",
    "href": "data manipulation/1 Pandas.html#reading-from-parquet",
    "title": "Data manipulation with Pandas",
    "section": "0.1 Reading from parquet",
    "text": "0.1 Reading from parquet\n\n\nCode\nbig_df = pd.read_parquet(\"data/Combined_Flights_2022.parquet\")\n\ndf=big_df.head(10)",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#get-first-3",
    "href": "data manipulation/1 Pandas.html#get-first-3",
    "title": "Data manipulation with Pandas",
    "section": "0.2 get first 3",
    "text": "0.2 get first 3\n\n\nCode\ndf.head(3)\n\n\n\n\n\n\n\n\n\n\nFlightDate\nAirline\nOrigin\nDest\nCancelled\nDiverted\nCRSDepTime\nDepTime\nDepDelayMinutes\nDepDelay\n...\nWheelsOff\nWheelsOn\nTaxiIn\nCRSArrTime\nArrDelay\nArrDel15\nArrivalDelayGroups\nArrTimeBlk\nDistanceGroup\nDivAirportLandings\n\n\n\n\n0\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nGJT\nDEN\nFalse\nFalse\n1133\n1123.0\n0.0\n-10.0\n...\n1140.0\n1220.0\n8.0\n1245\n-17.0\n0.0\n-2.0\n1200-1259\n1\n0\n\n\n1\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nHRL\nIAH\nFalse\nFalse\n732\n728.0\n0.0\n-4.0\n...\n744.0\n839.0\n9.0\n849\n-1.0\n0.0\n-1.0\n0800-0859\n2\n0\n\n\n2\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nDRO\nDEN\nFalse\nFalse\n1529\n1514.0\n0.0\n-15.0\n...\n1535.0\n1622.0\n14.0\n1639\n-3.0\n0.0\n-1.0\n1600-1659\n2\n0\n\n\n\n\n3 rows × 61 columns",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#get-last-3",
    "href": "data manipulation/1 Pandas.html#get-last-3",
    "title": "Data manipulation with Pandas",
    "section": "0.3 get last 3",
    "text": "0.3 get last 3\n\n\nCode\ndf.tail(3)\n\n\n\n\n\n\n\n\n\n\nFlightDate\nAirline\nOrigin\nDest\nCancelled\nDiverted\nCRSDepTime\nDepTime\nDepDelayMinutes\nDepDelay\n...\nWheelsOff\nWheelsOn\nTaxiIn\nCRSArrTime\nArrDelay\nArrDel15\nArrivalDelayGroups\nArrTimeBlk\nDistanceGroup\nDivAirportLandings\n\n\n\n\n590539\n2022-03-08\nRepublic Airlines\nALB\nORD\nFalse\nFalse\n1700\n2318.0\n378.0\n378.0\n...\n2337.0\n52.0\n7.0\n1838\n381.0\n1.0\n12.0\n1800-1859\n3\n0\n\n\n590540\n2022-03-25\nRepublic Airlines\nEWR\nPIT\nFalse\nTrue\n2129\n2322.0\n113.0\n113.0\n...\n2347.0\n933.0\n6.0\n2255\nNaN\nNaN\nNaN\n2200-2259\n2\n1\n\n\n590541\n2022-03-07\nRepublic Airlines\nEWR\nRDU\nFalse\nTrue\n1154\n1148.0\n0.0\n-6.0\n...\n1201.0\n1552.0\n4.0\n1333\nNaN\nNaN\nNaN\n1300-1359\n2\n1\n\n\n\n\n3 rows × 61 columns",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#get-ramdon-5",
    "href": "data manipulation/1 Pandas.html#get-ramdon-5",
    "title": "Data manipulation with Pandas",
    "section": "0.4 get ramdon 5",
    "text": "0.4 get ramdon 5\n\n\nCode\ndf.sample(5, random_state=42)\n\n\n\n\n\n\n\n\n\n\nFlightDate\nAirline\nOrigin\nDest\nCancelled\nDiverted\nCRSDepTime\nDepTime\nDepDelayMinutes\nDepDelay\n...\nWheelsOff\nWheelsOn\nTaxiIn\nCRSArrTime\nArrDelay\nArrDel15\nArrivalDelayGroups\nArrTimeBlk\nDistanceGroup\nDivAirportLandings\n\n\n\n\n324021\n2022-03-19\nSkyWest Airlines Inc.\nASE\nDEN\nFalse\nFalse\n1831\n1826.0\n0.0\n-5.0\n...\n1845.0\n1916.0\n7.0\n1933\n-10.0\n0.0\n-1.0\n1900-1959\n1\n0\n\n\n34739\n2022-02-16\nSkyWest Airlines Inc.\nTYS\nDEN\nFalse\nFalse\n1605\n1605.0\n0.0\n0.0\n...\n1614.0\n1728.0\n44.0\n1737\n35.0\n1.0\n2.0\n1700-1759\n5\n0\n\n\n304494\n2022-01-18\nAmerican Airlines Inc.\nLAX\nOGG\nFalse\nFalse\n1719\n1714.0\n0.0\n-5.0\n...\n1728.0\n2042.0\n10.0\n2049\n3.0\n0.0\n0.0\n2000-2059\n10\n0\n\n\n205451\n2022-01-23\nSouthwest Airlines Co.\nLAS\nBWI\nFalse\nFalse\n1515\n1533.0\n18.0\n18.0\n...\n1548.0\n2259.0\n3.0\n2235\n27.0\n1.0\n1.0\n2200-2259\n9\n0\n\n\n173176\n2022-07-01\nDelta Air Lines Inc.\nDTW\nRDU\nFalse\nFalse\n715\n709.0\n0.0\n-6.0\n...\n721.0\n834.0\n3.0\n853\n-16.0\n0.0\n-2.0\n0800-0859\n3\n0\n\n\n\n\n5 rows × 61 columns",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#get-columns-names",
    "href": "data manipulation/1 Pandas.html#get-columns-names",
    "title": "Data manipulation with Pandas",
    "section": "0.5 get columns names",
    "text": "0.5 get columns names\n\n\nCode\ndf.columns\n\n\nIndex(['FlightDate', 'Airline', 'Origin', 'Dest', 'Cancelled', 'Diverted',\n       'CRSDepTime', 'DepTime', 'DepDelayMinutes', 'DepDelay', 'ArrTime',\n       'ArrDelayMinutes', 'AirTime', 'CRSElapsedTime', 'ActualElapsedTime',\n       'Distance', 'Year', 'Quarter', 'Month', 'DayofMonth', 'DayOfWeek',\n       'Marketing_Airline_Network', 'Operated_or_Branded_Code_Share_Partners',\n       'DOT_ID_Marketing_Airline', 'IATA_Code_Marketing_Airline',\n       'Flight_Number_Marketing_Airline', 'Operating_Airline',\n       'DOT_ID_Operating_Airline', 'IATA_Code_Operating_Airline',\n       'Tail_Number', 'Flight_Number_Operating_Airline', 'OriginAirportID',\n       'OriginAirportSeqID', 'OriginCityMarketID', 'OriginCityName',\n       'OriginState', 'OriginStateFips', 'OriginStateName', 'OriginWac',\n       'DestAirportID', 'DestAirportSeqID', 'DestCityMarketID', 'DestCityName',\n       'DestState', 'DestStateFips', 'DestStateName', 'DestWac', 'DepDel15',\n       'DepartureDelayGroups', 'DepTimeBlk', 'TaxiOut', 'WheelsOff',\n       'WheelsOn', 'TaxiIn', 'CRSArrTime', 'ArrDelay', 'ArrDel15',\n       'ArrivalDelayGroups', 'ArrTimeBlk', 'DistanceGroup',\n       'DivAirportLandings'],\n      dtype='object')",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#get-row-index",
    "href": "data manipulation/1 Pandas.html#get-row-index",
    "title": "Data manipulation with Pandas",
    "section": "0.6 get row index",
    "text": "0.6 get row index\n\n\nCode\ndf.index\n\n\nIndex([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype='int64')",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#get-long-info",
    "href": "data manipulation/1 Pandas.html#get-long-info",
    "title": "Data manipulation with Pandas",
    "section": "0.7 get long info",
    "text": "0.7 get long info\n\n\nCode\ndf.shape\n\n\n(10, 61)\n\n\n\n\nCode\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 10 entries, 0 to 9\nData columns (total 61 columns):\n #   Column                                   Non-Null Count  Dtype         \n---  ------                                   --------------  -----         \n 0   FlightDate                               10 non-null     datetime64[us]\n 1   Airline                                  10 non-null     object        \n 2   Origin                                   10 non-null     object        \n 3   Dest                                     10 non-null     object        \n 4   Cancelled                                10 non-null     bool          \n 5   Diverted                                 10 non-null     bool          \n 6   CRSDepTime                               10 non-null     int64         \n 7   DepTime                                  10 non-null     float64       \n 8   DepDelayMinutes                          10 non-null     float64       \n 9   DepDelay                                 10 non-null     float64       \n 10  ArrTime                                  10 non-null     float64       \n 11  ArrDelayMinutes                          10 non-null     float64       \n 12  AirTime                                  10 non-null     float64       \n 13  CRSElapsedTime                           10 non-null     float64       \n 14  ActualElapsedTime                        10 non-null     float64       \n 15  Distance                                 10 non-null     float64       \n 16  Year                                     10 non-null     int64         \n 17  Quarter                                  10 non-null     int64         \n 18  Month                                    10 non-null     int64         \n 19  DayofMonth                               10 non-null     int64         \n 20  DayOfWeek                                10 non-null     int64         \n 21  Marketing_Airline_Network                10 non-null     object        \n 22  Operated_or_Branded_Code_Share_Partners  10 non-null     object        \n 23  DOT_ID_Marketing_Airline                 10 non-null     int64         \n 24  IATA_Code_Marketing_Airline              10 non-null     object        \n 25  Flight_Number_Marketing_Airline          10 non-null     int64         \n 26  Operating_Airline                        10 non-null     object        \n 27  DOT_ID_Operating_Airline                 10 non-null     int64         \n 28  IATA_Code_Operating_Airline              10 non-null     object        \n 29  Tail_Number                              10 non-null     object        \n 30  Flight_Number_Operating_Airline          10 non-null     int64         \n 31  OriginAirportID                          10 non-null     int64         \n 32  OriginAirportSeqID                       10 non-null     int64         \n 33  OriginCityMarketID                       10 non-null     int64         \n 34  OriginCityName                           10 non-null     object        \n 35  OriginState                              10 non-null     object        \n 36  OriginStateFips                          10 non-null     int64         \n 37  OriginStateName                          10 non-null     object        \n 38  OriginWac                                10 non-null     int64         \n 39  DestAirportID                            10 non-null     int64         \n 40  DestAirportSeqID                         10 non-null     int64         \n 41  DestCityMarketID                         10 non-null     int64         \n 42  DestCityName                             10 non-null     object        \n 43  DestState                                10 non-null     object        \n 44  DestStateFips                            10 non-null     int64         \n 45  DestStateName                            10 non-null     object        \n 46  DestWac                                  10 non-null     int64         \n 47  DepDel15                                 10 non-null     float64       \n 48  DepartureDelayGroups                     10 non-null     float64       \n 49  DepTimeBlk                               10 non-null     object        \n 50  TaxiOut                                  10 non-null     float64       \n 51  WheelsOff                                10 non-null     float64       \n 52  WheelsOn                                 10 non-null     float64       \n 53  TaxiIn                                   10 non-null     float64       \n 54  CRSArrTime                               10 non-null     int64         \n 55  ArrDelay                                 10 non-null     float64       \n 56  ArrDel15                                 10 non-null     float64       \n 57  ArrivalDelayGroups                       10 non-null     float64       \n 58  ArrTimeBlk                               10 non-null     object        \n 59  DistanceGroup                            10 non-null     int64         \n 60  DivAirportLandings                       10 non-null     int64         \ndtypes: bool(2), datetime64[us](1), float64(18), int64(23), object(17)\nmemory usage: 4.7+ KB",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#get-short-info",
    "href": "data manipulation/1 Pandas.html#get-short-info",
    "title": "Data manipulation with Pandas",
    "section": "0.8 get short info",
    "text": "0.8 get short info\n\n\nCode\ndf.info(verbose=False)\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 10 entries, 0 to 9\nColumns: 61 entries, FlightDate to DivAirportLandings\ndtypes: bool(2), datetime64[us](1), float64(18), int64(23), object(17)\nmemory usage: 4.7+ KB",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#summary-of-numeric-column",
    "href": "data manipulation/1 Pandas.html#summary-of-numeric-column",
    "title": "Data manipulation with Pandas",
    "section": "0.9 summary of numeric column",
    "text": "0.9 summary of numeric column\n\n\nCode\ndf.describe()\n\n\n\n\n\n\n\n\n\n\nFlightDate\nCRSDepTime\nDepTime\nDepDelayMinutes\nDepDelay\nArrTime\nArrDelayMinutes\nAirTime\nCRSElapsedTime\nActualElapsedTime\n...\nTaxiOut\nWheelsOff\nWheelsOn\nTaxiIn\nCRSArrTime\nArrDelay\nArrDel15\nArrivalDelayGroups\nDistanceGroup\nDivAirportLandings\n\n\n\n\ncount\n10\n10.000000\n10.000000\n10.0\n10.00000\n10.000000\n10.000000\n10.000000\n10.00000\n10.000000\n...\n10.00000\n10.000000\n10.000000\n10.000000\n10.000000\n10.000000\n10.0\n10.000000\n10.000000\n10.0\n\n\nmean\n2022-04-04 00:00:00\n1256.500000\n1249.600000\n0.0\n-6.90000\n1390.000000\n1.100000\n58.400000\n84.00000\n84.400000\n...\n18.00000\n1275.600000\n1378.000000\n8.000000\n1400.500000\n-6.500000\n0.0\n-1.000000\n2.000000\n0.0\n\n\nmin\n2022-04-04 00:00:00\n732.000000\n728.000000\n0.0\n-15.00000\n848.000000\n0.000000\n26.000000\n52.00000\n42.000000\n...\n11.00000\n744.000000\n839.000000\n4.000000\n849.000000\n-18.000000\n0.0\n-2.000000\n1.000000\n0.0\n\n\n25%\n2022-04-04 00:00:00\n998.500000\n993.250000\n0.0\n-10.00000\n1230.500000\n0.000000\n41.750000\n70.00000\n67.750000\n...\n16.00000\n1047.500000\n1223.500000\n5.250000\n1241.250000\n-12.500000\n0.0\n-1.000000\n1.250000\n0.0\n\n\n50%\n2022-04-04 00:00:00\n1134.000000\n1129.000000\n0.0\n-6.00000\n1281.000000\n0.000000\n52.000000\n74.50000\n78.500000\n...\n17.00000\n1147.000000\n1249.000000\n7.000000\n1275.500000\n-7.000000\n0.0\n-1.000000\n2.000000\n0.0\n\n\n75%\n2022-04-04 00:00:00\n1432.250000\n1426.000000\n0.0\n-3.25000\n1538.500000\n0.000000\n59.250000\n89.25000\n82.750000\n...\n20.50000\n1442.000000\n1534.000000\n8.750000\n1584.750000\n-1.250000\n0.0\n-1.000000\n2.000000\n0.0\n\n\nmax\n2022-04-04 00:00:00\n2139.000000\n2136.000000\n0.0\n0.00000\n2218.000000\n6.000000\n136.000000\n157.00000\n174.000000\n...\n25.00000\n2147.000000\n2213.000000\n16.000000\n2231.000000\n6.000000\n0.0\n0.000000\n4.000000\n0.0\n\n\nstd\nNaN\n396.759163\n396.390156\n0.0\n4.72464\n370.127906\n2.330951\n30.638393\n29.81424\n35.693759\n...\n3.91578\n387.953949\n371.973416\n4.082483\n375.500925\n8.669871\n0.0\n0.666667\n0.942809\n0.0\n\n\n\n\n8 rows × 42 columns",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#summary-of-categorical-column",
    "href": "data manipulation/1 Pandas.html#summary-of-categorical-column",
    "title": "Data manipulation with Pandas",
    "section": "0.10 summary of categorical column",
    "text": "0.10 summary of categorical column\n\n\nCode\ndf[[\"Airline\"]].describe()\n\n\n\n\n\n\n\n\n\n\nAirline\n\n\n\n\ncount\n10\n\n\nunique\n1\n\n\ntop\nCommutair Aka Champlain Enterprises, Inc.\n\n\nfreq\n10",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/2 siuba.html#reshape-tables",
    "href": "data manipulation/2 siuba.html#reshape-tables",
    "title": "Data manipulation with siuba",
    "section": "12 Reshape tables",
    "text": "12 Reshape tables\n\n\nCode\ncosts = pd.DataFrame({\n    'id': [1,2],\n    'price_x': [.1, .2],\n    'price_y': [.4, .5],\n    'price_z': [.7, .8]\n})\n\ncosts\n\n\n\n\n\n\n\n\n\n\nid\nprice_x\nprice_y\nprice_z\n\n\n\n\n0\n1\n0.1\n0.4\n0.7\n\n\n1\n2\n0.2\n0.5\n0.8\n\n\n\n\n\n\n\n\n\n12.1 Gather data long\nBelow 3 method will give same result\n\n\nCode\n# selecting each variable manually\ncosts &gt;&gt; gather('measure', 'value', _.price_x, _.price_y, _.price_z)\n\n\n\n\n\n\n\n\n\n\nid\nmeasure\nvalue\n\n\n\n\n0\n1\nprice_x\n0.1\n\n\n1\n2\nprice_x\n0.2\n\n\n2\n1\nprice_y\n0.4\n\n\n3\n2\nprice_y\n0.5\n\n\n4\n1\nprice_z\n0.7\n\n\n5\n2\nprice_z\n0.8\n\n\n\n\n\n\n\n\n\n\nCode\n# selecting variables using a slice\ncosts &gt;&gt; gather('measure', 'value', _[\"price_x\":\"price_z\"])\n\n\n\n\n\n\n\n\n\n\nid\nmeasure\nvalue\n\n\n\n\n0\n1\nprice_x\n0.1\n\n\n1\n2\nprice_x\n0.2\n\n\n2\n1\nprice_y\n0.4\n\n\n3\n2\nprice_y\n0.5\n\n\n4\n1\nprice_z\n0.7\n\n\n5\n2\nprice_z\n0.8\n\n\n\n\n\n\n\n\n\n\nCode\n# selecting by excluding id\ncosts &gt;&gt; gather('measure', 'value', -_.id)\n\n\n\n\n\n\n\n\n\n\nid\nmeasure\nvalue\n\n\n\n\n0\n1\nprice_x\n0.1\n\n\n1\n2\nprice_x\n0.2\n\n\n2\n1\nprice_y\n0.4\n\n\n3\n2\nprice_y\n0.5\n\n\n4\n1\nprice_z\n0.7\n\n\n5\n2\nprice_z\n0.8\n\n\n\n\n\n\n\n\n\n\n12.2 Spread data wide",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/2 siuba.html#string",
    "href": "data manipulation/2 siuba.html#string",
    "title": "Data manipulation with siuba",
    "section": "13 string",
    "text": "13 string\n\n\nCode\ndf = pd.DataFrame({'text': ['abc', 'DDD','1243c','aeEe'], 'num': [3, 4,7,8]})\n\ndf\n\n\n\n\n\n\n\n\n\n\ntext\nnum\n\n\n\n\n0\nabc\n3\n\n\n1\nDDD\n4\n\n\n2\n1243c\n7\n\n\n3\naeEe\n8\n\n\n\n\n\n\n\n\n\n13.1 upper case\n\n\nCode\ndf&gt;&gt; mutate(text_new=_.text.str.upper())\n\n\n\n\n\n\n\n\n\n\ntext\nnum\ntext_new\n\n\n\n\n0\nabc\n3\nABC\n\n\n1\nDDD\n4\nDDD\n\n\n2\n1243c\n7\n1243C\n\n\n3\naeEe\n8\nAEEE\n\n\n\n\n\n\n\n\n\n\n13.2 lower case\n\n\nCode\ndf&gt;&gt; mutate(text_new=_.text.str.lower())\n\n\n\n\n\n\n\n\n\n\ntext\nnum\ntext_new\n\n\n\n\n0\nabc\n3\nabc\n\n\n1\nDDD\n4\nddd\n\n\n2\n1243c\n7\n1243c\n\n\n3\naeEe\n8\naeee\n\n\n\n\n\n\n\n\n\n\n13.3 match\n\n\nCode\ndf&gt;&gt; mutate(text_new1=if_else(_.text== \"abc\",'T','F')\n            ,text_new2=if_else(_.text.str.startswith(\"a\"),'T','F')\n            ,text_new3=if_else(_.text.str.endswith(\"c\"),'T','F')\n            ,text_new4=if_else(_.text.str.contains(\"4\"),'T','F')\n\n)\n\n\n\n\n\n\n\n\n\n\ntext\nnum\ntext_new1\ntext_new2\ntext_new3\ntext_new4\n\n\n\n\n0\nabc\n3\nT\nT\nT\nF\n\n\n1\nDDD\n4\nF\nF\nF\nF\n\n\n2\n1243c\n7\nF\nF\nT\nT\n\n\n3\naeEe\n8\nF\nT\nF\nF\n\n\n\n\n\n\n\n\n\n\n13.4 concatenation\n\n\nCode\ndf&gt;&gt; mutate(text_new1=_.text+' is '+_.text\n)\n\n\n\n\n\n\n\n\n\n\ntext\nnum\ntext_new1\n\n\n\n\n0\nabc\n3\nabc is abc\n\n\n1\nDDD\n4\nDDD is DDD\n\n\n2\n1243c\n7\n1243c is 1243c\n\n\n3\naeEe\n8\naeEe is aeEe\n\n\n\n\n\n\n\n\n\n\n13.5 replace\nUse .str.replace(…, regex=True) with regular expressions to replace patterns in strings.\nFor example, the code below uses “p.”, where . is called a wildcard–which matches any character.\n\n\nCode\ndf&gt;&gt; mutate(text_new1=_.text.str.replace(\"a.\", \"XX\", regex=True)\n)\n\n\n\n\n\n\n\n\n\n\ntext\nnum\ntext_new1\n\n\n\n\n0\nabc\n3\nXXc\n\n\n1\nDDD\n4\nDDD\n\n\n2\n1243c\n7\n1243c\n\n\n3\naeEe\n8\nXXEe\n\n\n\n\n\n\n\n\n\n\n13.6 extract\nUse str.extract() with a regular expression to pull out a matching piece of text.\nFor example the regular expression “^(.*) ” contains the following pieces:\n\na matches the literal letter “a”\n.* has a . which matches anything, and * which modifies it to apply 0 or more times.\n\n\n\nCode\ndf&gt;&gt; mutate(text_new1=_.text.str.extract(\"a(.*)\")\n            ,text_new2=_.text.str.extract(\"(.*)c\")\n)\n\n\n\n\n\n\n\n\n\n\ntext\nnum\ntext_new1\ntext_new2\n\n\n\n\n0\nabc\n3\nbc\nab\n\n\n1\nDDD\n4\nNaN\nNaN\n\n\n2\n1243c\n7\nNaN\n1243\n\n\n3\naeEe\n8\neEe\nNaN",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/2 siuba.html#date",
    "href": "data manipulation/2 siuba.html#date",
    "title": "Data manipulation with siuba",
    "section": "14 date",
    "text": "14 date\n\n\nCode\ndf_dates = pd.DataFrame({\n    \"dates\": pd.to_datetime([\"2021-01-02\", \"2021-02-03\"]),\n    \"raw\": [\"2023-04-05 06:07:08\", \"2024-05-06 07:08:09\"],\n})\ndf_dates\n\n\n\n\n\n\n\n\n\n\ndates\nraw\n\n\n\n\n0\n2021-01-02\n2023-04-05 06:07:08\n\n\n1\n2021-02-03\n2024-05-06 07:08:09\n\n\n\n\n\n\n\n\n\n\nCode\nfrom datetime import datetime\n\ndf_date=df_dates&gt;&gt;mutate(month=_.dates.dt.month_name()\n                  ,date_format_raw = call(pd.to_datetime, _.raw)\n                  ,date_format_raw_year=_.date_format_raw.dt.year\n\n)\n\ndf_date\n\n\n\n\n\n\n\n\n\n\ndates\nraw\nmonth\ndate_format_raw\ndate_format_raw_year\n\n\n\n\n0\n2021-01-02\n2023-04-05 06:07:08\nJanuary\n2023-04-05 06:07:08\n2023\n\n\n1\n2021-02-03\n2024-05-06 07:08:09\nFebruary\n2024-05-06 07:08:09\n2024\n\n\n\n\n\n\n\n\n\n\nCode\ndf_date.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2 entries, 0 to 1\nData columns (total 5 columns):\n #   Column                Non-Null Count  Dtype         \n---  ------                --------------  -----         \n 0   dates                 2 non-null      datetime64[ns]\n 1   raw                   2 non-null      object        \n 2   month                 2 non-null      object        \n 3   date_format_raw       2 non-null      datetime64[ns]\n 4   date_format_raw_year  2 non-null      int32         \ndtypes: datetime64[ns](2), int32(1), object(2)\nmemory usage: 204.0+ bytes",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/2 siuba.html#comparison-with-different-python-dataframe-package",
    "href": "data manipulation/2 siuba.html#comparison-with-different-python-dataframe-package",
    "title": "Data manipulation with siuba",
    "section": "1 Comparison with different python dataframe package",
    "text": "1 Comparison with different python dataframe package",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "plot/3 plotly.html",
    "href": "plot/3 plotly.html",
    "title": "Plotly chart",
    "section": "",
    "text": "Plotly’s Python graphing library makes interactive, publication-quality graphs. Examples of how to make line plots, scatter plots, area charts, bar charts, error bars, box plots, histograms, heatmaps, subplots, multiple-axes, polar charts, and bubble charts.\nCode\nimport plotly\nprint(plotly.__version__)\n\n\n5.20.0\nCode\nfrom plotnine import *\nimport seaborn as sns\nimport plotly.express as px\nimport pandas as pd\n\n# Apply the default theme\n\n\n# Load an example dataset\ntips = sns.load_dataset(\"tips\")\ntips.head()\n\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4",
    "crumbs": [
      "Plot",
      "Plotly chart"
    ]
  },
  {
    "objectID": "plot/3 plotly.html#color-by-group",
    "href": "plot/3 plotly.html#color-by-group",
    "title": "Plotly chart",
    "section": "1.1 color by group",
    "text": "1.1 color by group\n\n\nCode\nfig = px.scatter(tips,x=\"tip\", y=\"total_bill\",color=\"sex\")\nfig.show()",
    "crumbs": [
      "Plot",
      "Plotly chart"
    ]
  },
  {
    "objectID": "plot/3 plotly.html#size-by-group",
    "href": "plot/3 plotly.html#size-by-group",
    "title": "Plotly chart",
    "section": "1.2 size by group",
    "text": "1.2 size by group\n\n\nCode\nfig = px.scatter(tips,x=\"tip\", y=\"total_bill\",size=\"size\")\nfig.show()",
    "crumbs": [
      "Plot",
      "Plotly chart"
    ]
  },
  {
    "objectID": "plot/3 plotly.html#color-by-group-1",
    "href": "plot/3 plotly.html#color-by-group-1",
    "title": "Plotly chart",
    "section": "2.1 color by group",
    "text": "2.1 color by group\n\n\nCode\nimport random\nfrom siuba import _, mutate, filter, group_by, summarize,show_query\nfrom siuba import *\n\ndowjones2=dowjones&gt;&gt;mutate(type='old')\n\ndowjones3=dowjones&gt;&gt;mutate(Price=_.Price+random.random()*200,type='new')\n\ndowjones4=pd.concat([dowjones2, dowjones3], ignore_index = True)&gt;&gt; arrange(_.Date)\n\ndf = px.data.gapminder().query(\"continent=='Oceania'\")\n\n\n\n\nCode\ndowjones4.head()\n\n\n\n\n\n\n\n\n\n\nDate\nPrice\ntype\n\n\n\n\n0\n1914-12-01\n55.000000\nold\n\n\n649\n1914-12-01\n81.210288\nnew\n\n\n1\n1915-01-01\n56.550000\nold\n\n\n650\n1915-01-01\n82.760288\nnew\n\n\n2\n1915-02-01\n56.000000\nold\n\n\n\n\n\n\n\n\n\n\nCode\nimport plotly.express as px\n\nfig = px.line(dowjones4, x=\"Date\", y=\"Price\", color='type')\nfig.show()",
    "crumbs": [
      "Plot",
      "Plotly chart"
    ]
  },
  {
    "objectID": "plot/3 plotly.html#color-by-group-2",
    "href": "plot/3 plotly.html#color-by-group-2",
    "title": "Plotly chart",
    "section": "3.1 color by group",
    "text": "3.1 color by group\n\n\nCode\nfig = px.histogram(tips, x=\"total_bill\", color='sex', barmode='group')\nfig.show()",
    "crumbs": [
      "Plot",
      "Plotly chart"
    ]
  },
  {
    "objectID": "plot/3 plotly.html#color-by-group-3",
    "href": "plot/3 plotly.html#color-by-group-3",
    "title": "Plotly chart",
    "section": "5.1 color by group",
    "text": "5.1 color by group\n\n\nCode\nfig = px.box(tips, y=\"total_bill\",x='sex',color='sex')\nfig.show()",
    "crumbs": [
      "Plot",
      "Plotly chart"
    ]
  },
  {
    "objectID": "plot/3 plotly.html#color-by-group-4",
    "href": "plot/3 plotly.html#color-by-group-4",
    "title": "Plotly chart",
    "section": "6.1 color by group",
    "text": "6.1 color by group\n\n\nCode\nfig = px.strip(tips,x=\"day\", y=\"total_bill\",color='sex')\nfig.show()",
    "crumbs": [
      "Plot",
      "Plotly chart"
    ]
  },
  {
    "objectID": "data manipulation/0 data structure in Python .html#singular",
    "href": "data manipulation/0 data structure in Python .html#singular",
    "title": "Data structure in Python",
    "section": "1.1 singular",
    "text": "1.1 singular\n\n\nCode\na=1\ntype(a)\n\n\nint\n\n\n\n\nCode\na=1.3\ntype(a)\n\n\nfloat\n\n\n\n\nCode\na='hell'\ntype(a)\n\n\nstr",
    "crumbs": [
      "data manipulation",
      "Data structure in Python"
    ]
  },
  {
    "objectID": "data manipulation/0 data structure in Python .html#list",
    "href": "data manipulation/0 data structure in Python .html#list",
    "title": "Data structure in Python",
    "section": "1.2 list",
    "text": "1.2 list\n\n\nCode\na=[1,2,3]\n\na\n\n\n[1, 2, 3]\n\n\n\n\nCode\ntype(a) \n\n\nlist\n\n\n\n\nCode\nfruits = ['orange', 'apple', 'pear', 'banana', 'kiwi', 'apple', 'banana','apple']\n\n\n\n1.2.1 find length of the list with len()\n\n\nCode\nlen(fruits)\n\n\n8\n\n\n\n\n1.2.2 find how many time in the list with count()\n\n\nCode\nfruits.count('apple')\n\n\n3\n\n\n\n\n1.2.3 find locaiton of on the list with index()\nshow the first ‘apple’ index. python list start at 0\n\n\nCode\nfruits.index('apple')\n\n\n1\n\n\nall ‘apple’ in the list\n\n\nCode\n[index for index, value in enumerate(fruits) if value == 'apple']\n\n\n[1, 5, 7]\n\n\n\n\n1.2.4 reverse the list\n\n\nCode\nfruits.reverse()\nfruits\n\n\n['apple', 'banana', 'apple', 'kiwi', 'banana', 'pear', 'apple', 'orange']\n\n\n\n\n1.2.5 sort the list\n\n\nCode\nfruits.sort()\nfruits\n\n\n['apple', 'apple', 'apple', 'banana', 'banana', 'kiwi', 'orange', 'pear']\n\n\n\n\n1.2.6 add element on the list\n\n\nCode\nfruits.append('grape')\nfruits\n\n\n['apple',\n 'apple',\n 'apple',\n 'banana',\n 'banana',\n 'kiwi',\n 'orange',\n 'pear',\n 'grape']\n\n\n\n\n1.2.7 drop last element\n\n\nCode\nfruits.pop()\n\nfruits\n\n\n['apple', 'apple', 'apple', 'banana', 'banana', 'kiwi', 'orange', 'pear']\n\n\n\n\n1.2.8 List Comprehensions\nusing loop:\n\n\nCode\nsquares = []\nfor x in range(10):\n  squares.append(x**2)\n  \nsquares\n\n\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n\n\nusing List Comprehensions\n\n\nCode\nsquares = [x**2 for x in range(10)]\nsquares\n\n\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]",
    "crumbs": [
      "data manipulation",
      "Data structure in Python"
    ]
  },
  {
    "objectID": "data manipulation/0 data structure in Python .html#reverse-the-list",
    "href": "data manipulation/0 data structure in Python .html#reverse-the-list",
    "title": "Data structure in Python",
    "section": "1.3 reverse the list",
    "text": "1.3 reverse the list\n\n\nCode\nfruits.reverse()\nfruits\n\n\n['apple', 'banana', 'apple', 'kiwi', 'banana', 'pear', 'apple', 'orange']\n\n\n\n1.3.1 sort the list\n\n\nCode\nfruits.sort()\nfruits\n\n\n['apple', 'apple', 'apple', 'banana', 'banana', 'kiwi', 'orange', 'pear']\n\n\n\n\n1.3.2 add element on the list\n\n\nCode\nfruits.append('grape')\nfruits\n\n\n['apple',\n 'apple',\n 'apple',\n 'banana',\n 'banana',\n 'kiwi',\n 'orange',\n 'pear',\n 'grape']",
    "crumbs": [
      "data manipulation",
      "Data structure in Python"
    ]
  },
  {
    "objectID": "data manipulation/0 data structure in Python .html#drop-last-element",
    "href": "data manipulation/0 data structure in Python .html#drop-last-element",
    "title": "Data structure in Python",
    "section": "1.4 drop last element",
    "text": "1.4 drop last element\n\n\nCode\nfruits.pop()\n\nfruits\n\n\n['apple', 'apple', 'apple', 'banana', 'banana', 'kiwi', 'orange', 'pear']\n\n\n\n1.4.1 List Comprehensions\nusing loop:\n\n\nCode\nsquares = []\nfor x in range(10):\n  squares.append(x**2)\n  \nsquares\n\n\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n\n\nusing List Comprehensions\n\n\nCode\nsquares = [x**2 for x in range(10)]\nsquares\n\n\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]",
    "crumbs": [
      "data manipulation",
      "Data structure in Python"
    ]
  },
  {
    "objectID": "data manipulation/0 data structure in Python .html#tuples",
    "href": "data manipulation/0 data structure in Python .html#tuples",
    "title": "Data structure in Python",
    "section": "1.3 Tuples",
    "text": "1.3 Tuples\n\n\nCode\nfruits = ('orange', 'apple', 'pear', 'banana', 'kiwi', 'apple', 'banana','apple')\n\nfruits\n\n\n('orange', 'apple', 'pear', 'banana', 'kiwi', 'apple', 'banana', 'apple')\n\n\n\n\nCode\ntype(fruits)\n\n\ntuple\n\n\ntuple can not be modified.",
    "crumbs": [
      "data manipulation",
      "Data structure in Python"
    ]
  },
  {
    "objectID": "data manipulation/0 data structure in Python .html#sets",
    "href": "data manipulation/0 data structure in Python .html#sets",
    "title": "Data structure in Python",
    "section": "1.4 Sets",
    "text": "1.4 Sets\nA set is an unordered collection with no duplicate elements.\n\n\nCode\nbasket = {'apple', 'orange', 'apple', 'pear', 'orange', 'banana'}\n\nbasket\n\n\n{'apple', 'banana', 'orange', 'pear'}\n\n\n\n\nCode\ntype(basket)\n\n\nset",
    "crumbs": [
      "data manipulation",
      "Data structure in Python"
    ]
  },
  {
    "objectID": "data manipulation/0 data structure in Python .html#dictionaries",
    "href": "data manipulation/0 data structure in Python .html#dictionaries",
    "title": "Data structure in Python",
    "section": "1.5 Dictionaries",
    "text": "1.5 Dictionaries\n\n\nCode\ntel = {'jack': 4098, 'sape': 4139}\n\ntel\n\n\n{'jack': 4098, 'sape': 4139}\n\n\n\n\nCode\ntype(tel)\n\n\ndict\n\n\n\n\nCode\ntel['jack']\n\n\n4098",
    "crumbs": [
      "data manipulation",
      "Data structure in Python"
    ]
  },
  {
    "objectID": "classification/1 decision tree and hyperparameter tuning.html",
    "href": "classification/1 decision tree and hyperparameter tuning.html",
    "title": "Decision tree and hyperparameter tuning",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nimport time",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/1 decision tree and hyperparameter tuning.html#input-data",
    "href": "classification/1 decision tree and hyperparameter tuning.html#input-data",
    "title": "Decision tree and hyperparameter tuning",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/1 decision tree and hyperparameter tuning.html#data-eda",
    "href": "classification/1 decision tree and hyperparameter tuning.html#data-eda",
    "title": "Decision tree and hyperparameter tuning",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n\n\n0.7420382165605095\n0.18890814558058924",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/1 decision tree and hyperparameter tuning.html#data-wrangling",
    "href": "classification/1 decision tree and hyperparameter tuning.html#data-wrangling",
    "title": "Decision tree and hyperparameter tuning",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1046 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1308 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Dealing with missing numerical variables\ndata['Age'] = data.Age.fillna(data.Age.median())\ndata['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1309 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1309 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\ndata = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\nSex_male\n\n\n\n\n0\n1\n3\nBraund, Mr. Owen Harris\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\nTrue\n\n\n1\n2\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\nFalse\n\n\n2\n3\n3\nHeikkinen, Miss. Laina\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\nFalse\n\n\n3\n4\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\nFalse\n\n\n4\n5\n3\nAllen, Mr. William Henry\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\nTrue\n\n\n\n\n\n\n\n\n\n\nCode\n# Select features columns\ndata = data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()\n\n\n\n\n\n\n\n\n\n\nSex_male\nFare\nAge\nPclass\nSibSp\nrole\n\n\n\n\n0\nTrue\n7.2500\n22.0\n3\n1\ntrain\n\n\n1\nFalse\n71.2833\n38.0\n1\n1\ntrain\n\n\n2\nFalse\n7.9250\n26.0\n3\n0\ntrain\n\n\n3\nFalse\n53.1000\n35.0\n1\n1\ntrain\n\n\n4\nTrue\n8.0500\n35.0\n3\n0\ntrain",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/1 decision tree and hyperparameter tuning.html#split-data",
    "href": "classification/1 decision tree and hyperparameter tuning.html#split-data",
    "title": "Decision tree and hyperparameter tuning",
    "section": "2.4 split data",
    "text": "2.4 split data\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\nX_train = X_train.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 712 entries, 384 to 855\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Sex_male  712 non-null    bool   \n 1   Fare      712 non-null    float64\n 2   Age       712 non-null    float64\n 3   Pclass    712 non-null    int64  \n 4   SibSp     712 non-null    int64  \ndtypes: bool(1), float64(2), int64(2)\nmemory usage: 28.5 KB",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/1 decision tree and hyperparameter tuning.html#define-model",
    "href": "classification/1 decision tree and hyperparameter tuning.html#define-model",
    "title": "Decision tree and hyperparameter tuning",
    "section": "3.1 define model",
    "text": "3.1 define model\n\n\nCode\nmodel_dt = DecisionTreeClassifier()   #model with deph 3\nmodel_dt\n\n\nDecisionTreeClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriNot fittedDecisionTreeClassifier() \n\n\ndefalt hyper-parameters:\n\n\nCode\nmodel_dt.get_params()\n\n\n{'ccp_alpha': 0.0,\n 'class_weight': None,\n 'criterion': 'gini',\n 'max_depth': None,\n 'max_features': None,\n 'max_leaf_nodes': None,\n 'min_impurity_decrease': 0.0,\n 'min_samples_leaf': 1,\n 'min_samples_split': 2,\n 'min_weight_fraction_leaf': 0.0,\n 'monotonic_cst': None,\n 'random_state': None,\n 'splitter': 'best'}",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/1 decision tree and hyperparameter tuning.html#define-parameters",
    "href": "classification/1 decision tree and hyperparameter tuning.html#define-parameters",
    "title": "Decision tree and hyperparameter tuning",
    "section": "3.2 define parameters",
    "text": "3.2 define parameters\n\n\nCode\nparameters = {'criterion':['gini','entropy'],\n              'max_depth':np.arange(1,21).tolist()[0::2],\n              'min_samples_split':np.arange(2,11).tolist()[0::2],\n              'max_leaf_nodes':np.arange(3,26).tolist()[0::2]}",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/1 decision tree and hyperparameter tuning.html#define-grids",
    "href": "classification/1 decision tree and hyperparameter tuning.html#define-grids",
    "title": "Decision tree and hyperparameter tuning",
    "section": "3.3 define GridS",
    "text": "3.3 define GridS\ndefault cv is 5\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\nG1=GridSearchCV(DecisionTreeClassifier(), parameters, cv=10, n_jobs=-1)",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/1 decision tree and hyperparameter tuning.html#train-model",
    "href": "classification/1 decision tree and hyperparameter tuning.html#train-model",
    "title": "Decision tree and hyperparameter tuning",
    "section": "3.4 train model",
    "text": "3.4 train model\n\n\nCode\nstart_time = time.time()\n\nG1.fit(X_train,Y_train)\n\nend_time = time.time()\n\nduration = end_time - start_time\nduration\n\n\n3.4475159645080566\n\n\nbest parameters\n\n\nCode\nG1.best_params_\n\n\n{'criterion': 'gini',\n 'max_depth': 5,\n 'max_leaf_nodes': 7,\n 'min_samples_split': 2}\n\n\nbest model\n\n\nCode\nmodel_dt = G1.best_estimator_\n\n\nvariable importance\n\n\nCode\nimportances = model_dt.feature_importances_\nvi=pd.DataFrame({\"variable\":X_train.columns,\"importances\":importances})\nvi=vi.sort_values('importances',ascending=False)\nvi\n\n\n\n\n\n\n\n\n\n\nvariable\nimportances\n\n\n\n\n0\nSex_male\n0.633276\n\n\n3\nPclass\n0.224033\n\n\n2\nAge\n0.054571\n\n\n1\nFare\n0.046240\n\n\n4\nSibSp\n0.041881",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/1 decision tree and hyperparameter tuning.html#preformance",
    "href": "classification/1 decision tree and hyperparameter tuning.html#preformance",
    "title": "Decision tree and hyperparameter tuning",
    "section": "3.5 Preformance",
    "text": "3.5 Preformance\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = model_dt.predict(X_test) #always gets x and retuns y\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_test,Y_pred_dt)  \naccuracy\n\n\n0.7877094972067039\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_test,Y_pred_dt)  \nprecision_dt\n\n\n0.8\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_test,Y_pred_dt)  \nrecall_dt\n\n\n0.6486486486486487\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_test,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[93, 12],\n       [26, 48]])\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_test, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.7671814671814672",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/1 decision tree and hyperparameter tuning.html#k-fold-cross-validation",
    "href": "classification/1 decision tree and hyperparameter tuning.html#k-fold-cross-validation",
    "title": "Decision tree and hyperparameter tuning",
    "section": "3.6 k-Fold Cross-Validation",
    "text": "3.6 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(model_dt, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.818831872352999",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/2 Decision Tree on house price data.html",
    "href": "regression/2 Decision Tree on house price data.html",
    "title": "Decision tree",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split",
    "crumbs": [
      "Regression",
      "Decision tree"
    ]
  },
  {
    "objectID": "regression/2 Decision Tree on house price data.html#input-data",
    "href": "regression/2 Decision Tree on house price data.html#input-data",
    "title": "Decision tree",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nId = df_train['Id']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nId\nMSSubClass\nMSZoning\nLotFrontage\nLotArea\nStreet\nAlley\nLotShape\nLandContour\nUtilities\n...\nPoolArea\nPoolQC\nFence\nMiscFeature\nMiscVal\nMoSold\nYrSold\nSaleType\nSaleCondition\nSalePrice\n\n\n\n\n0\n1\n60\nRL\n65.0\n8450\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n2\n2008\nWD\nNormal\n208500\n\n\n1\n2\n20\nRL\n80.0\n9600\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2007\nWD\nNormal\n181500\n\n\n2\n3\n60\nRL\n68.0\n11250\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n9\n2008\nWD\nNormal\n223500\n\n\n3\n4\n70\nRL\n60.0\n9550\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n2\n2006\nWD\nAbnorml\n140000\n\n\n4\n5\n60\nRL\n84.0\n14260\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n12\n2008\nWD\nNormal\n250000\n\n\n\n\n5 rows × 81 columns\n\n\n\n\n\n\nCode\n#df_train.info()\n\n\n\n\nCode\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['SalePrice'], axis=1), df_test])",
    "crumbs": [
      "Regression",
      "Decision tree"
    ]
  },
  {
    "objectID": "regression/2 Decision Tree on house price data.html#data-eda",
    "href": "regression/2 Decision Tree on house price data.html#data-eda",
    "title": "Decision tree",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\nin step 1",
    "crumbs": [
      "Regression",
      "Decision tree"
    ]
  },
  {
    "objectID": "regression/2 Decision Tree on house price data.html#data-wrangling",
    "href": "regression/2 Decision Tree on house price data.html#data-wrangling",
    "title": "Decision tree",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling",
    "crumbs": [
      "Regression",
      "Decision tree"
    ]
  },
  {
    "objectID": "regression/2 Decision Tree on house price data.html#split-data",
    "href": "regression/2 Decision Tree on house price data.html#split-data",
    "title": "Decision tree",
    "section": "2.4 split data",
    "text": "2.4 split data\n\n\nCode\nY = df_train.SalePrice\nX = df_train.drop(['SalePrice'], axis=1)\n\n\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\nX_train = X_train.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nprint(X_train.shape)\nprint(X_test.shape)\n\n\n(1168, 80)\n(292, 80)\n\n\n\n\nCode\nprint(Y_train.shape)\nprint(Y_test.shape)\n\n\n(1168,)\n(292,)\n\n\n\n\nCode\ncategorical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].nunique() &lt; 10 and X_train[cname].dtype == \"object\"]\n                    \n                    \nnumerical_cols = numerical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].dtype in ['int64', 'float64']]\n\n\n\n\nCode\nprint(\"The total number of categorical columns:\", len(categorical_cols))\nprint(\"The total number of numerical columns:\", len(numerical_cols))\n\n\nThe total number of categorical columns: 40\nThe total number of numerical columns: 37\n\n\n\n\nCode\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train[my_cols].copy()\nX_test= X_test[my_cols].copy()\n\n\nX_final = df_test[my_cols].copy()",
    "crumbs": [
      "Regression",
      "Decision tree"
    ]
  },
  {
    "objectID": "regression/2 Decision Tree on house price data.html#pipelines-for-data-preprocessing",
    "href": "regression/2 Decision Tree on house price data.html#pipelines-for-data-preprocessing",
    "title": "Decision tree",
    "section": "2.5 Pipelines for Data Preprocessing",
    "text": "2.5 Pipelines for Data Preprocessing\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\nCode\nnumerical_transformer = Pipeline(steps=[\n    ('imputer_num', SimpleImputer(strategy='median')), \n    ('scaler', StandardScaler())\n])\n\n\n\n\nCode\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n\n\nCode\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)])",
    "crumbs": [
      "Regression",
      "Decision tree"
    ]
  },
  {
    "objectID": "regression/2 Decision Tree on house price data.html#define-model",
    "href": "regression/2 Decision Tree on house price data.html#define-model",
    "title": "Decision tree",
    "section": "3.1 define model",
    "text": "3.1 define model\n\n\nCode\nml_model = DecisionTreeRegressor(random_state=0)\nml_model\n\n\nDecisionTreeRegressor(random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeRegressor?Documentation for DecisionTreeRegressoriNot fittedDecisionTreeRegressor(random_state=0)",
    "crumbs": [
      "Regression",
      "Decision tree"
    ]
  },
  {
    "objectID": "regression/2 Decision Tree on house price data.html#define-pipline",
    "href": "regression/2 Decision Tree on house price data.html#define-pipline",
    "title": "Decision tree",
    "section": "3.2 define pipline",
    "text": "3.2 define pipline\n\n\nCode\npipeline = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model_dt', ml_model)\n         ]\n)",
    "crumbs": [
      "Regression",
      "Decision tree"
    ]
  },
  {
    "objectID": "regression/2 Decision Tree on house price data.html#train-model",
    "href": "regression/2 Decision Tree on house price data.html#train-model",
    "title": "Decision tree",
    "section": "3.3 train model",
    "text": "3.3 train model\n\n\nCode\npipeline.fit(X_train, Y_train)\n\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer_num',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['Id', 'MSSubClass',\n                                                   'LotFrontage', 'LotArea',\n                                                   'OverallQual', 'OverallCond',\n                                                   'YearBuilt', 'YearRemodAdd',\n                                                   'MasVnrArea', 'BsmtFinSF1',\n                                                   'BsmtFinSF2', 'BsmtUnfSF',\n                                                   'TotalBsmtSF', '1stFlrSF',\n                                                   '2ndFlrSF'...\n                                                   'LotConfig', 'LandSlope',\n                                                   'Condition1', 'Condition2',\n                                                   'BldgType', 'HouseStyle',\n                                                   'RoofStyle', 'RoofMatl',\n                                                   'MasVnrType', 'ExterQual',\n                                                   'ExterCond', 'Foundation',\n                                                   'BsmtQual', 'BsmtCond',\n                                                   'BsmtExposure',\n                                                   'BsmtFinType1',\n                                                   'BsmtFinType2', 'Heating',\n                                                   'HeatingQC', 'CentralAir',\n                                                   'Electrical', 'KitchenQual',\n                                                   'Functional', 'FireplaceQu', ...])])),\n                ('model_dt', DecisionTreeRegressor(random_state=0))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiFittedPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer_num',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['Id', 'MSSubClass',\n                                                   'LotFrontage', 'LotArea',\n                                                   'OverallQual', 'OverallCond',\n                                                   'YearBuilt', 'YearRemodAdd',\n                                                   'MasVnrArea', 'BsmtFinSF1',\n                                                   'BsmtFinSF2', 'BsmtUnfSF',\n                                                   'TotalBsmtSF', '1stFlrSF',\n                                                   '2ndFlrSF'...\n                                                   'LotConfig', 'LandSlope',\n                                                   'Condition1', 'Condition2',\n                                                   'BldgType', 'HouseStyle',\n                                                   'RoofStyle', 'RoofMatl',\n                                                   'MasVnrType', 'ExterQual',\n                                                   'ExterCond', 'Foundation',\n                                                   'BsmtQual', 'BsmtCond',\n                                                   'BsmtExposure',\n                                                   'BsmtFinType1',\n                                                   'BsmtFinType2', 'Heating',\n                                                   'HeatingQC', 'CentralAir',\n                                                   'Electrical', 'KitchenQual',\n                                                   'Functional', 'FireplaceQu', ...])])),\n                ('model_dt', DecisionTreeRegressor(random_state=0))])  preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('imputer_num',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('scaler', StandardScaler())]),\n                                 ['Id', 'MSSubClass', 'LotFrontage', 'LotArea',\n                                  'OverallQual', 'OverallCond', 'YearBuilt',\n                                  'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1',\n                                  'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF',\n                                  '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n                                  'GrLivArea', 'Bsm...\n                                 ['MSZoning', 'Street', 'Alley', 'LotShape',\n                                  'LandContour', 'Utilities', 'LotConfig',\n                                  'LandSlope', 'Condition1', 'Condition2',\n                                  'BldgType', 'HouseStyle', 'RoofStyle',\n                                  'RoofMatl', 'MasVnrType', 'ExterQual',\n                                  'ExterCond', 'Foundation', 'BsmtQual',\n                                  'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n                                  'BsmtFinType2', 'Heating', 'HeatingQC',\n                                  'CentralAir', 'Electrical', 'KitchenQual',\n                                  'Functional', 'FireplaceQu', ...])]) num['Id', 'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='median')  StandardScaler?Documentation for StandardScalerStandardScaler() cat['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'SaleType', 'SaleCondition']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='most_frequent')  OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore')  DecisionTreeRegressor?Documentation for DecisionTreeRegressorDecisionTreeRegressor(random_state=0) \n\n\n\n\nCode\nfitted_model=pipeline.steps[1][1]\n\n\n\n\nCode\nvar=pipeline[:-1].get_feature_names_out()\nvar\n\n\narray(['num__Id', 'num__MSSubClass', 'num__LotFrontage', 'num__LotArea',\n       'num__OverallQual', 'num__OverallCond', 'num__YearBuilt',\n       'num__YearRemodAdd', 'num__MasVnrArea', 'num__BsmtFinSF1',\n       'num__BsmtFinSF2', 'num__BsmtUnfSF', 'num__TotalBsmtSF',\n       'num__1stFlrSF', 'num__2ndFlrSF', 'num__LowQualFinSF',\n       'num__GrLivArea', 'num__BsmtFullBath', 'num__BsmtHalfBath',\n       'num__FullBath', 'num__HalfBath', 'num__BedroomAbvGr',\n       'num__KitchenAbvGr', 'num__TotRmsAbvGrd', 'num__Fireplaces',\n       'num__GarageYrBlt', 'num__GarageCars', 'num__GarageArea',\n       'num__WoodDeckSF', 'num__OpenPorchSF', 'num__EnclosedPorch',\n       'num__3SsnPorch', 'num__ScreenPorch', 'num__PoolArea',\n       'num__MiscVal', 'num__MoSold', 'num__YrSold',\n       'cat__MSZoning_C (all)', 'cat__MSZoning_FV', 'cat__MSZoning_RH',\n       'cat__MSZoning_RL', 'cat__MSZoning_RM', 'cat__Street_Grvl',\n       'cat__Street_Pave', 'cat__Alley_Grvl', 'cat__Alley_Pave',\n       'cat__LotShape_IR1', 'cat__LotShape_IR2', 'cat__LotShape_IR3',\n       'cat__LotShape_Reg', 'cat__LandContour_Bnk',\n       'cat__LandContour_HLS', 'cat__LandContour_Low',\n       'cat__LandContour_Lvl', 'cat__Utilities_AllPub',\n       'cat__Utilities_NoSeWa', 'cat__LotConfig_Corner',\n       'cat__LotConfig_CulDSac', 'cat__LotConfig_FR2',\n       'cat__LotConfig_FR3', 'cat__LotConfig_Inside',\n       'cat__LandSlope_Gtl', 'cat__LandSlope_Mod', 'cat__LandSlope_Sev',\n       'cat__Condition1_Artery', 'cat__Condition1_Feedr',\n       'cat__Condition1_Norm', 'cat__Condition1_PosA',\n       'cat__Condition1_PosN', 'cat__Condition1_RRAe',\n       'cat__Condition1_RRAn', 'cat__Condition1_RRNe',\n       'cat__Condition1_RRNn', 'cat__Condition2_Artery',\n       'cat__Condition2_Feedr', 'cat__Condition2_Norm',\n       'cat__Condition2_PosN', 'cat__Condition2_RRAn',\n       'cat__Condition2_RRNn', 'cat__BldgType_1Fam',\n       'cat__BldgType_2fmCon', 'cat__BldgType_Duplex',\n       'cat__BldgType_Twnhs', 'cat__BldgType_TwnhsE',\n       'cat__HouseStyle_1.5Fin', 'cat__HouseStyle_1.5Unf',\n       'cat__HouseStyle_1Story', 'cat__HouseStyle_2.5Fin',\n       'cat__HouseStyle_2.5Unf', 'cat__HouseStyle_2Story',\n       'cat__HouseStyle_SFoyer', 'cat__HouseStyle_SLvl',\n       'cat__RoofStyle_Flat', 'cat__RoofStyle_Gable',\n       'cat__RoofStyle_Gambrel', 'cat__RoofStyle_Hip',\n       'cat__RoofStyle_Mansard', 'cat__RoofStyle_Shed',\n       'cat__RoofMatl_ClyTile', 'cat__RoofMatl_CompShg',\n       'cat__RoofMatl_Membran', 'cat__RoofMatl_Metal',\n       'cat__RoofMatl_Roll', 'cat__RoofMatl_Tar&Grv',\n       'cat__RoofMatl_WdShake', 'cat__RoofMatl_WdShngl',\n       'cat__MasVnrType_BrkCmn', 'cat__MasVnrType_BrkFace',\n       'cat__MasVnrType_Stone', 'cat__ExterQual_Ex', 'cat__ExterQual_Fa',\n       'cat__ExterQual_Gd', 'cat__ExterQual_TA', 'cat__ExterCond_Ex',\n       'cat__ExterCond_Fa', 'cat__ExterCond_Gd', 'cat__ExterCond_Po',\n       'cat__ExterCond_TA', 'cat__Foundation_BrkTil',\n       'cat__Foundation_CBlock', 'cat__Foundation_PConc',\n       'cat__Foundation_Slab', 'cat__Foundation_Stone',\n       'cat__Foundation_Wood', 'cat__BsmtQual_Ex', 'cat__BsmtQual_Fa',\n       'cat__BsmtQual_Gd', 'cat__BsmtQual_TA', 'cat__BsmtCond_Fa',\n       'cat__BsmtCond_Gd', 'cat__BsmtCond_Po', 'cat__BsmtCond_TA',\n       'cat__BsmtExposure_Av', 'cat__BsmtExposure_Gd',\n       'cat__BsmtExposure_Mn', 'cat__BsmtExposure_No',\n       'cat__BsmtFinType1_ALQ', 'cat__BsmtFinType1_BLQ',\n       'cat__BsmtFinType1_GLQ', 'cat__BsmtFinType1_LwQ',\n       'cat__BsmtFinType1_Rec', 'cat__BsmtFinType1_Unf',\n       'cat__BsmtFinType2_ALQ', 'cat__BsmtFinType2_BLQ',\n       'cat__BsmtFinType2_GLQ', 'cat__BsmtFinType2_LwQ',\n       'cat__BsmtFinType2_Rec', 'cat__BsmtFinType2_Unf',\n       'cat__Heating_Floor', 'cat__Heating_GasA', 'cat__Heating_GasW',\n       'cat__Heating_Grav', 'cat__Heating_OthW', 'cat__Heating_Wall',\n       'cat__HeatingQC_Ex', 'cat__HeatingQC_Fa', 'cat__HeatingQC_Gd',\n       'cat__HeatingQC_Po', 'cat__HeatingQC_TA', 'cat__CentralAir_N',\n       'cat__CentralAir_Y', 'cat__Electrical_FuseA',\n       'cat__Electrical_FuseF', 'cat__Electrical_FuseP',\n       'cat__Electrical_Mix', 'cat__Electrical_SBrkr',\n       'cat__KitchenQual_Ex', 'cat__KitchenQual_Fa',\n       'cat__KitchenQual_Gd', 'cat__KitchenQual_TA',\n       'cat__Functional_Maj1', 'cat__Functional_Maj2',\n       'cat__Functional_Min1', 'cat__Functional_Min2',\n       'cat__Functional_Mod', 'cat__Functional_Sev',\n       'cat__Functional_Typ', 'cat__FireplaceQu_Ex',\n       'cat__FireplaceQu_Fa', 'cat__FireplaceQu_Gd',\n       'cat__FireplaceQu_Po', 'cat__FireplaceQu_TA',\n       'cat__GarageType_2Types', 'cat__GarageType_Attchd',\n       'cat__GarageType_Basment', 'cat__GarageType_BuiltIn',\n       'cat__GarageType_CarPort', 'cat__GarageType_Detchd',\n       'cat__GarageFinish_Fin', 'cat__GarageFinish_RFn',\n       'cat__GarageFinish_Unf', 'cat__GarageQual_Ex',\n       'cat__GarageQual_Fa', 'cat__GarageQual_Gd', 'cat__GarageQual_Po',\n       'cat__GarageQual_TA', 'cat__GarageCond_Ex', 'cat__GarageCond_Fa',\n       'cat__GarageCond_Gd', 'cat__GarageCond_Po', 'cat__GarageCond_TA',\n       'cat__PavedDrive_N', 'cat__PavedDrive_P', 'cat__PavedDrive_Y',\n       'cat__PoolQC_Ex', 'cat__PoolQC_Fa', 'cat__PoolQC_Gd',\n       'cat__Fence_GdPrv', 'cat__Fence_GdWo', 'cat__Fence_MnPrv',\n       'cat__Fence_MnWw', 'cat__MiscFeature_Gar2',\n       'cat__MiscFeature_Othr', 'cat__MiscFeature_Shed',\n       'cat__SaleType_COD', 'cat__SaleType_CWD', 'cat__SaleType_Con',\n       'cat__SaleType_ConLD', 'cat__SaleType_ConLI',\n       'cat__SaleType_ConLw', 'cat__SaleType_New', 'cat__SaleType_Oth',\n       'cat__SaleType_WD', 'cat__SaleCondition_Abnorml',\n       'cat__SaleCondition_AdjLand', 'cat__SaleCondition_Alloca',\n       'cat__SaleCondition_Family', 'cat__SaleCondition_Normal',\n       'cat__SaleCondition_Partial'], dtype=object)\n\n\nvariable importance\n\n\nCode\nimportances = fitted_model.feature_importances_\nvi=pd.DataFrame({\"variable\":var,\"importances\":importances})\nvi=vi.sort_values('importances',ascending=False)\nvi\n\n\n\n\n\n\n\n\n\n\nvariable\nimportances\n\n\n\n\n4\nnum__OverallQual\n0.632886\n\n\n16\nnum__GrLivArea\n0.111301\n\n\n14\nnum__2ndFlrSF\n0.040809\n\n\n12\nnum__TotalBsmtSF\n0.035467\n\n\n2\nnum__LotFrontage\n0.020744\n\n\n...\n...\n...\n\n\n151\ncat__Heating_Grav\n0.000000\n\n\n150\ncat__Heating_GasW\n0.000000\n\n\n149\ncat__Heating_GasA\n0.000000\n\n\n148\ncat__Heating_Floor\n0.000000\n\n\n167\ncat__KitchenQual_Fa\n0.000000\n\n\n\n\n229 rows × 2 columns",
    "crumbs": [
      "Regression",
      "Decision tree"
    ]
  },
  {
    "objectID": "regression/2 Decision Tree on house price data.html#preformance",
    "href": "regression/2 Decision Tree on house price data.html#preformance",
    "title": "Decision tree",
    "section": "3.4 Preformance",
    "text": "3.4 Preformance\n\n\nCode\nY_pred_dt =pipeline.predict(X_test) #always gets x and retuns y\n\n\nR 2\n\n\nCode\nfrom sklearn.metrics import r2_score\nr2_score(Y_test, Y_pred_dt)\n\n\n0.7396321741155943\n\n\nMAE\n\n\nCode\nfrom sklearn.metrics import mean_absolute_error\nmean_absolute_error(Y_test, Y_pred_dt)\n\n\n23549.56506849315\n\n\nRMSE\n\n\nCode\nfrom  math import sqrt\nfrom sklearn.metrics import mean_squared_error\nmse=mean_squared_error(Y_test, Y_pred_dt)\nrmse=sqrt(mse)\nrmse\n\n\n37360.29917045853",
    "crumbs": [
      "Regression",
      "Decision tree"
    ]
  },
  {
    "objectID": "regression/2 Decision Tree on house price data.html#k-fold-cross-validation",
    "href": "regression/2 Decision Tree on house price data.html#k-fold-cross-validation",
    "title": "Decision tree",
    "section": "3.5 k-Fold Cross-Validation",
    "text": "3.5 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \n\n\n\n\nCode\ncv_dt = cross_val_score(pipeline, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.7170706989156083\n\n\n\n\nCode\ncv_dt = cross_val_score(pipeline, X_train, Y_train, cv=kf_dt,scoring = 'neg_mean_squared_error')\nnp.mean(np.sqrt(np.abs(cv_dt)))\n\n\n43050.55798060925",
    "crumbs": [
      "Regression",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification/1.2 decision tree and hyperparameter tuning.html",
    "href": "classification/1.2 decision tree and hyperparameter tuning.html",
    "title": "Decision tree and hyperparameter tuning",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nimport time",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/1.2 decision tree and hyperparameter tuning.html#input-data",
    "href": "classification/1.2 decision tree and hyperparameter tuning.html#input-data",
    "title": "Decision tree and hyperparameter tuning",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/1.2 decision tree and hyperparameter tuning.html#data-eda",
    "href": "classification/1.2 decision tree and hyperparameter tuning.html#data-eda",
    "title": "Decision tree and hyperparameter tuning",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n\n\n0.7420382165605095\n0.18890814558058924",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/1.2 decision tree and hyperparameter tuning.html#data-wrangling",
    "href": "classification/1.2 decision tree and hyperparameter tuning.html#data-wrangling",
    "title": "Decision tree and hyperparameter tuning",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1046 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1308 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Dealing with missing numerical variables\ndata['Age'] = data.Age.fillna(data.Age.median())\ndata['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1309 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1309 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\ndata = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\nSex_male\n\n\n\n\n0\n1\n3\nBraund, Mr. Owen Harris\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\nTrue\n\n\n1\n2\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\nFalse\n\n\n2\n3\n3\nHeikkinen, Miss. Laina\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\nFalse\n\n\n3\n4\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\nFalse\n\n\n4\n5\n3\nAllen, Mr. William Henry\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\nTrue\n\n\n\n\n\n\n\n\n\n\nCode\n# Select features columns\ndata = data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()\n\n\n\n\n\n\n\n\n\n\nSex_male\nFare\nAge\nPclass\nSibSp\nrole\n\n\n\n\n0\nTrue\n7.2500\n22.0\n3\n1\ntrain\n\n\n1\nFalse\n71.2833\n38.0\n1\n1\ntrain\n\n\n2\nFalse\n7.9250\n26.0\n3\n0\ntrain\n\n\n3\nFalse\n53.1000\n35.0\n1\n1\ntrain\n\n\n4\nTrue\n8.0500\n35.0\n3\n0\ntrain",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/1.2 decision tree and hyperparameter tuning.html#split-data",
    "href": "classification/1.2 decision tree and hyperparameter tuning.html#split-data",
    "title": "Decision tree and hyperparameter tuning",
    "section": "2.4 split data",
    "text": "2.4 split data\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\nX_train = X_train.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 712 entries, 320 to 518\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Sex_male  712 non-null    bool   \n 1   Fare      712 non-null    float64\n 2   Age       712 non-null    float64\n 3   Pclass    712 non-null    int64  \n 4   SibSp     712 non-null    int64  \ndtypes: bool(1), float64(2), int64(2)\nmemory usage: 28.5 KB",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/1.2 decision tree and hyperparameter tuning.html#define-model",
    "href": "classification/1.2 decision tree and hyperparameter tuning.html#define-model",
    "title": "Decision tree and hyperparameter tuning",
    "section": "3.1 define model",
    "text": "3.1 define model\n\n\nCode\nmodel_dt = DecisionTreeClassifier()   #model with deph 3\nmodel_dt\n\n\nDecisionTreeClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriNot fittedDecisionTreeClassifier() \n\n\ndefalt hyper-parameters:\n\n\nCode\nmodel_dt.get_params()\n\n\n{'ccp_alpha': 0.0,\n 'class_weight': None,\n 'criterion': 'gini',\n 'max_depth': None,\n 'max_features': None,\n 'max_leaf_nodes': None,\n 'min_impurity_decrease': 0.0,\n 'min_samples_leaf': 1,\n 'min_samples_split': 2,\n 'min_weight_fraction_leaf': 0.0,\n 'monotonic_cst': None,\n 'random_state': None,\n 'splitter': 'best'}",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/1.2 decision tree and hyperparameter tuning.html#define-parameters",
    "href": "classification/1.2 decision tree and hyperparameter tuning.html#define-parameters",
    "title": "Decision tree and hyperparameter tuning",
    "section": "3.2 define parameters",
    "text": "3.2 define parameters\n\n\nCode\nparameters = {'criterion':['gini','entropy'],\n              'max_depth':np.arange(1,21).tolist()[0::2],\n              'min_samples_split':np.arange(2,11).tolist()[0::2],\n              'max_leaf_nodes':np.arange(3,26).tolist()[0::2]}\n\n\nall parameters combinations\n\n\nCode\nimport itertools\na = parameters.values()\ncombinations = list(itertools.product(*a))\n\n\n1200 combinations\n\n\nCode\nlen(combinations)\n\n\n1200\n\n\n\n\nCode\ncombinations[0:5]\n\n\n[('gini', 1, 2, 3),\n ('gini', 1, 2, 5),\n ('gini', 1, 2, 7),\n ('gini', 1, 2, 9),\n ('gini', 1, 2, 11)]",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/1.2 decision tree and hyperparameter tuning.html#define-grids",
    "href": "classification/1.2 decision tree and hyperparameter tuning.html#define-grids",
    "title": "Decision tree and hyperparameter tuning",
    "section": "3.3 define GridS",
    "text": "3.3 define GridS\n\n3.3.1 GridSearchCV\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\n\n# default cv is 5\nG1=GridSearchCV(DecisionTreeClassifier()\n                ,parameters\n                ,scoring='accuracy'\n                , cv=10, n_jobs=-1)\n\n\n\n\n3.3.2 RandomizedSearchCV\nmake a very small RandomizedSearch with 3 parameters combinations\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV\nG2=RandomizedSearchCV(DecisionTreeClassifier()\n                      ,parameters\n                    ,scoring='accuracy'\n                    ,n_iter = 3, cv=10, n_jobs=-1)",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/1.2 decision tree and hyperparameter tuning.html#train-model",
    "href": "classification/1.2 decision tree and hyperparameter tuning.html#train-model",
    "title": "Decision tree and hyperparameter tuning",
    "section": "3.4 train model",
    "text": "3.4 train model\n\n3.4.1 train GridSearchCV\n\n\nCode\nstart_time = time.time()\n\nG1.fit(X_train,Y_train)\n\nend_time = time.time()\nduration = end_time - start_time\nduration\n\n\n5.29243016242981\n\n\n\n\n3.4.2 train RandomizedSearchCV\n\n\nCode\nstart_time = time.time()\n\nG2.fit(X_train,Y_train)\n\nend_time = time.time()\nduration = end_time - start_time\nduration\n\n\n0.02891397476196289\n\n\n\n3.4.2.1 tunning result\n\n3.4.2.1.1 GridSearchCV\n\n\nCode\n# get the parameter names\ncolumn_results = [f\"param_{name}\" for name in parameters.keys()]\ncolumn_results += [\"mean_test_score\", \"std_test_score\", \"rank_test_score\"]\n\ncv_results = pd.DataFrame(G1.cv_results_)\ncv_results = cv_results[column_results].sort_values(\n    \"mean_test_score\", ascending=False\n)\n\n\ndef shorten_param(param_name):\n    if \"__\" in param_name:\n        return param_name.rsplit(\"__\", 1)[1]\n    return param_name\n\n\ncv_results = cv_results.rename(shorten_param, axis=1)\ncv_results.head()\n\n\n\n\n\n\n\n\n\n\nparam_criterion\nparam_max_depth\nparam_min_samples_split\nparam_max_leaf_nodes\nmean_test_score\nstd_test_score\nrank_test_score\n\n\n\n\n328\ngini\n11\n8\n13\n0.837089\n0.038231\n1\n\n\n506\ngini\n17\n4\n13\n0.837089\n0.038231\n1\n\n\n446\ngini\n15\n4\n13\n0.837089\n0.038231\n1\n\n\n387\ngini\n13\n6\n13\n0.837089\n0.038231\n1\n\n\n386\ngini\n13\n4\n13\n0.837089\n0.038231\n1\n\n\n\n\n\n\n\n\n\n\n3.4.2.1.2 RandomizedSearchCV\n\n\nCode\n# get the parameter names\ncolumn_results = [f\"param_{name}\" for name in parameters.keys()]\ncolumn_results += [\"mean_test_score\", \"std_test_score\", \"rank_test_score\"]\n\ncv_results = pd.DataFrame(G2.cv_results_)\ncv_results = cv_results[column_results].sort_values(\n    \"mean_test_score\", ascending=False\n)\n\n\ndef shorten_param(param_name):\n    if \"__\" in param_name:\n        return param_name.rsplit(\"__\", 1)[1]\n    return param_name\n\n\ncv_results = cv_results.rename(shorten_param, axis=1)\ncv_results.head()\n\n\n\n\n\n\n\n\n\n\nparam_criterion\nparam_max_depth\nparam_min_samples_split\nparam_max_leaf_nodes\nmean_test_score\nstd_test_score\nrank_test_score\n\n\n\n\n0\nentropy\n13\n8\n21\n0.830067\n0.037145\n1\n\n\n2\nentropy\n17\n8\n23\n0.827250\n0.038026\n2\n\n\n1\ngini\n1\n2\n13\n0.787989\n0.029869\n3\n\n\n\n\n\n\n\n\n\n\n\n3.4.2.2 tunning best parameters\n\n\nCode\nG1.best_params_\n\n\n{'criterion': 'gini',\n 'max_depth': 7,\n 'max_leaf_nodes': 13,\n 'min_samples_split': 2}\n\n\n\n\nCode\nG2.best_params_\n\n\n{'min_samples_split': 8,\n 'max_leaf_nodes': 21,\n 'max_depth': 13,\n 'criterion': 'entropy'}\n\n\n\n\n3.4.2.3 tunning best model\n\n\nCode\nmodel_dt = G1.best_estimator_\n\n\n\n\nCode\nmodel_dt2 = G2.best_estimator_\n\n\n\n\n3.4.2.4 variable importance\n\n\nCode\nimportances = model_dt.feature_importances_\nvi=pd.DataFrame({\"variable\":X_train.columns,\"importances\":importances})\nvi=vi.sort_values('importances',ascending=False)\nvi\n\n\n\n\n\n\n\n\n\n\nvariable\nimportances\n\n\n\n\n0\nSex_male\n0.525262\n\n\n3\nPclass\n0.141914\n\n\n2\nAge\n0.134637\n\n\n1\nFare\n0.121324\n\n\n4\nSibSp\n0.076863",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/1.2 decision tree and hyperparameter tuning.html#preformance",
    "href": "classification/1.2 decision tree and hyperparameter tuning.html#preformance",
    "title": "Decision tree and hyperparameter tuning",
    "section": "3.5 Preformance",
    "text": "3.5 Preformance\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = model_dt.predict(X_test) #always gets x and retuns y\nY_pred_dt2 = model_dt2.predict(X_test) #always gets x and retuns y\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_test,Y_pred_dt)  \naccuracy\n\n\n0.770949720670391\n\n\n\n\nCode\naccuracy2 = metrics.accuracy_score(Y_test,Y_pred_dt2)  \naccuracy2\n\n\n0.7541899441340782\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_test,Y_pred_dt)  \nprecision_dt\n\n\n0.8301886792452831\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_test,Y_pred_dt)  \nrecall_dt\n\n\n0.5789473684210527\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_test,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[94,  9],\n       [32, 44]])\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_test, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.7457843638221768",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/1.2 decision tree and hyperparameter tuning.html#k-fold-cross-validation",
    "href": "classification/1.2 decision tree and hyperparameter tuning.html#k-fold-cross-validation",
    "title": "Decision tree and hyperparameter tuning",
    "section": "3.6 k-Fold Cross-Validation",
    "text": "3.6 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(model_dt, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.8216487737614498",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/1.1 decision tree on titanic data.html",
    "href": "classification/1.1 decision tree on titanic data.html",
    "title": "Decision tree",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split",
    "crumbs": [
      "Classification",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification/1.1 decision tree on titanic data.html#input-data",
    "href": "classification/1.1 decision tree on titanic data.html#input-data",
    "title": "Decision tree",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification/1.1 decision tree on titanic data.html#data-eda",
    "href": "classification/1.1 decision tree on titanic data.html#data-eda",
    "title": "Decision tree",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n\n\n0.7420382165605095\n0.18890814558058924",
    "crumbs": [
      "Classification",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification/1.1 decision tree on titanic data.html#data-wrangling",
    "href": "classification/1.1 decision tree on titanic data.html#data-wrangling",
    "title": "Decision tree",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1046 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1308 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Dealing with missing numerical variables\ndata['Age'] = data.Age.fillna(data.Age.median())\ndata['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1309 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1309 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\ndata = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\nSex_male\n\n\n\n\n0\n1\n3\nBraund, Mr. Owen Harris\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\nTrue\n\n\n1\n2\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\nFalse\n\n\n2\n3\n3\nHeikkinen, Miss. Laina\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\nFalse\n\n\n3\n4\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\nFalse\n\n\n4\n5\n3\nAllen, Mr. William Henry\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\nTrue\n\n\n\n\n\n\n\n\n\n\nCode\n# Select features columns\ndata = data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()\n\n\n\n\n\n\n\n\n\n\nSex_male\nFare\nAge\nPclass\nSibSp\nrole\n\n\n\n\n0\nTrue\n7.2500\n22.0\n3\n1\ntrain\n\n\n1\nFalse\n71.2833\n38.0\n1\n1\ntrain\n\n\n2\nFalse\n7.9250\n26.0\n3\n0\ntrain\n\n\n3\nFalse\n53.1000\n35.0\n1\n1\ntrain\n\n\n4\nTrue\n8.0500\n35.0\n3\n0\ntrain",
    "crumbs": [
      "Classification",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification/1.1 decision tree on titanic data.html#split-data",
    "href": "classification/1.1 decision tree on titanic data.html#split-data",
    "title": "Decision tree",
    "section": "2.4 split data",
    "text": "2.4 split data\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\nX_train = X_train.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 712 entries, 330 to 466\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Sex_male  712 non-null    bool   \n 1   Fare      712 non-null    float64\n 2   Age       712 non-null    float64\n 3   Pclass    712 non-null    int64  \n 4   SibSp     712 non-null    int64  \ndtypes: bool(1), float64(2), int64(2)\nmemory usage: 28.5 KB",
    "crumbs": [
      "Classification",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification/1.1 decision tree on titanic data.html#define-model",
    "href": "classification/1.1 decision tree on titanic data.html#define-model",
    "title": "Decision tree",
    "section": "3.1 define model",
    "text": "3.1 define model\n\n\nCode\nmodel_dt = tree.DecisionTreeClassifier(max_depth=3)   #model with deph 3\nmodel_dt\n\n\nDecisionTreeClassifier(max_depth=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriNot fittedDecisionTreeClassifier(max_depth=3)",
    "crumbs": [
      "Classification",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification/1.1 decision tree on titanic data.html#train-model",
    "href": "classification/1.1 decision tree on titanic data.html#train-model",
    "title": "Decision tree",
    "section": "3.2 train model",
    "text": "3.2 train model\n\n\nCode\nmodel_dt.fit(X_train,Y_train)\n\n\nDecisionTreeClassifier(max_depth=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier(max_depth=3) \n\n\nvariable importance\n\n\nCode\nimportances = model_dt.feature_importances_\nvi=pd.DataFrame({\"variable\":X_train.columns,\"importances\":importances})\nvi=vi.sort_values('importances',ascending=False)\nvi\n\n\n\n\n\n\n\n\n\n\nvariable\nimportances\n\n\n\n\n0\nSex_male\n0.637694\n\n\n3\nPclass\n0.198801\n\n\n2\nAge\n0.071437\n\n\n1\nFare\n0.071284\n\n\n4\nSibSp\n0.020785",
    "crumbs": [
      "Classification",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification/1.1 decision tree on titanic data.html#preformance",
    "href": "classification/1.1 decision tree on titanic data.html#preformance",
    "title": "Decision tree",
    "section": "3.3 Preformance",
    "text": "3.3 Preformance\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = model_dt.predict(X_test) #always gets x and retuns y\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_test,Y_pred_dt)  \naccuracy\n\n\n0.8156424581005587\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_test,Y_pred_dt)  \nprecision_dt\n\n\n0.7887323943661971\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_test,Y_pred_dt)  \nrecall_dt\n\n\n0.7567567567567568\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_test,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[90, 15],\n       [18, 56]])\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_test, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.806949806949807",
    "crumbs": [
      "Classification",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification/1.1 decision tree on titanic data.html#k-fold-cross-validation",
    "href": "classification/1.1 decision tree on titanic data.html#k-fold-cross-validation",
    "title": "Decision tree",
    "section": "3.4 k-Fold Cross-Validation",
    "text": "3.4 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(model_dt, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.8160445188614203",
    "crumbs": [
      "Classification",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification/4 Support Vector Machines on titanic data.html",
    "href": "classification/4 Support Vector Machines on titanic data.html",
    "title": "Support Vector Machines",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split",
    "crumbs": [
      "Classification",
      "Support Vector Machines"
    ]
  },
  {
    "objectID": "classification/4 Support Vector Machines on titanic data.html#input-data",
    "href": "classification/4 Support Vector Machines on titanic data.html#input-data",
    "title": "Support Vector Machines",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "Support Vector Machines"
    ]
  },
  {
    "objectID": "classification/4 Support Vector Machines on titanic data.html#data-eda",
    "href": "classification/4 Support Vector Machines on titanic data.html#data-eda",
    "title": "Support Vector Machines",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n\n\n0.7420382165605095\n0.18890814558058924",
    "crumbs": [
      "Classification",
      "Support Vector Machines"
    ]
  },
  {
    "objectID": "classification/4 Support Vector Machines on titanic data.html#data-wrangling",
    "href": "classification/4 Support Vector Machines on titanic data.html#data-wrangling",
    "title": "Support Vector Machines",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1046 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1308 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Dealing with missing numerical variables\ndata['Age'] = data.Age.fillna(data.Age.median())\ndata['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1309 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1309 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\ndata = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\nSex_male\n\n\n\n\n0\n1\n3\nBraund, Mr. Owen Harris\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\nTrue\n\n\n1\n2\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\nFalse\n\n\n2\n3\n3\nHeikkinen, Miss. Laina\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\nFalse\n\n\n3\n4\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\nFalse\n\n\n4\n5\n3\nAllen, Mr. William Henry\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\nTrue\n\n\n\n\n\n\n\n\n\n\nCode\n# Select features columns\ndata = data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()\n\n\n\n\n\n\n\n\n\n\nSex_male\nFare\nAge\nPclass\nSibSp\nrole\n\n\n\n\n0\nTrue\n7.2500\n22.0\n3\n1\ntrain\n\n\n1\nFalse\n71.2833\n38.0\n1\n1\ntrain\n\n\n2\nFalse\n7.9250\n26.0\n3\n0\ntrain\n\n\n3\nFalse\n53.1000\n35.0\n1\n1\ntrain\n\n\n4\nTrue\n8.0500\n35.0\n3\n0\ntrain",
    "crumbs": [
      "Classification",
      "Support Vector Machines"
    ]
  },
  {
    "objectID": "classification/4 Support Vector Machines on titanic data.html#split-data",
    "href": "classification/4 Support Vector Machines on titanic data.html#split-data",
    "title": "Support Vector Machines",
    "section": "2.4 split data",
    "text": "2.4 split data\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\nX_train = X_train.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 712 entries, 65 to 380\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Sex_male  712 non-null    bool   \n 1   Fare      712 non-null    float64\n 2   Age       712 non-null    float64\n 3   Pclass    712 non-null    int64  \n 4   SibSp     712 non-null    int64  \ndtypes: bool(1), float64(2), int64(2)\nmemory usage: 28.5 KB",
    "crumbs": [
      "Classification",
      "Support Vector Machines"
    ]
  },
  {
    "objectID": "classification/4 Support Vector Machines on titanic data.html#define-model",
    "href": "classification/4 Support Vector Machines on titanic data.html#define-model",
    "title": "Support Vector Machines",
    "section": "3.1 define model",
    "text": "3.1 define model\nThe solvers implemented in the class Logistic Regression are “liblinear”, “newton-cg”, “lbfgs”, “sag” and “saga”. According to Scikit Documentation: The “liblinear” solver was the one used by default for historical reasons before version 0.22. Since then, default use is lbfgs Algorithm.\n\n\nCode\nfrom sklearn import svm\nml_model = svm.SVC(kernel=\"linear\")\nml_model\n\n\nSVC(kernel='linear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  SVC?Documentation for SVCiNot fittedSVC(kernel='linear')",
    "crumbs": [
      "Classification",
      "Support Vector Machines"
    ]
  },
  {
    "objectID": "classification/4 Support Vector Machines on titanic data.html#train-model",
    "href": "classification/4 Support Vector Machines on titanic data.html#train-model",
    "title": "Support Vector Machines",
    "section": "3.2 train model",
    "text": "3.2 train model\n\n\nCode\nml_model.fit(X_train,Y_train)\n\n\nSVC(kernel='linear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  SVC?Documentation for SVCiFittedSVC(kernel='linear')",
    "crumbs": [
      "Classification",
      "Support Vector Machines"
    ]
  },
  {
    "objectID": "classification/4 Support Vector Machines on titanic data.html#preformance",
    "href": "classification/4 Support Vector Machines on titanic data.html#preformance",
    "title": "Support Vector Machines",
    "section": "3.3 Preformance",
    "text": "3.3 Preformance\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = ml_model.predict(X_test) #always gets x and retuns y\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_test,Y_pred_dt)  \naccuracy\n\n\n0.7653631284916201\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_test,Y_pred_dt)  \nprecision_dt\n\n\n0.7\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_test,Y_pred_dt)  \nrecall_dt\n\n\n0.6363636363636364\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_test,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[95, 18],\n       [24, 42]])\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_test, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.7385358004827031",
    "crumbs": [
      "Classification",
      "Support Vector Machines"
    ]
  },
  {
    "objectID": "classification/4 Support Vector Machines on titanic data.html#k-fold-cross-validation",
    "href": "classification/4 Support Vector Machines on titanic data.html#k-fold-cross-validation",
    "title": "Support Vector Machines",
    "section": "3.4 k-Fold Cross-Validation",
    "text": "3.4 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(ml_model, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.7921107061952132",
    "crumbs": [
      "Classification",
      "Support Vector Machines"
    ]
  },
  {
    "objectID": "classification/5 KNN on titanic data.html",
    "href": "classification/5 KNN on titanic data.html",
    "title": "K-Nearest Neighbors",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split",
    "crumbs": [
      "Classification",
      "K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "classification/5 KNN on titanic data.html#input-data",
    "href": "classification/5 KNN on titanic data.html#input-data",
    "title": "K-Nearest Neighbors",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "classification/5 KNN on titanic data.html#data-eda",
    "href": "classification/5 KNN on titanic data.html#data-eda",
    "title": "K-Nearest Neighbors",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n\n\n0.7420382165605095\n0.18890814558058924",
    "crumbs": [
      "Classification",
      "K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "classification/5 KNN on titanic data.html#data-wrangling",
    "href": "classification/5 KNN on titanic data.html#data-wrangling",
    "title": "K-Nearest Neighbors",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1046 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1308 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Dealing with missing numerical variables\ndata['Age'] = data.Age.fillna(data.Age.median())\ndata['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1309 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1309 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\ndata = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\nSex_male\n\n\n\n\n0\n1\n3\nBraund, Mr. Owen Harris\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\nTrue\n\n\n1\n2\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\nFalse\n\n\n2\n3\n3\nHeikkinen, Miss. Laina\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\nFalse\n\n\n3\n4\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\nFalse\n\n\n4\n5\n3\nAllen, Mr. William Henry\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\nTrue\n\n\n\n\n\n\n\n\n\n\nCode\n# Select features columns\ndata = data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()\n\n\n\n\n\n\n\n\n\n\nSex_male\nFare\nAge\nPclass\nSibSp\nrole\n\n\n\n\n0\nTrue\n7.2500\n22.0\n3\n1\ntrain\n\n\n1\nFalse\n71.2833\n38.0\n1\n1\ntrain\n\n\n2\nFalse\n7.9250\n26.0\n3\n0\ntrain\n\n\n3\nFalse\n53.1000\n35.0\n1\n1\ntrain\n\n\n4\nTrue\n8.0500\n35.0\n3\n0\ntrain",
    "crumbs": [
      "Classification",
      "K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "classification/5 KNN on titanic data.html#split-data",
    "href": "classification/5 KNN on titanic data.html#split-data",
    "title": "K-Nearest Neighbors",
    "section": "2.4 split data",
    "text": "2.4 split data\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\nX_train = X_train.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 712 entries, 451 to 188\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Sex_male  712 non-null    bool   \n 1   Fare      712 non-null    float64\n 2   Age       712 non-null    float64\n 3   Pclass    712 non-null    int64  \n 4   SibSp     712 non-null    int64  \ndtypes: bool(1), float64(2), int64(2)\nmemory usage: 28.5 KB",
    "crumbs": [
      "Classification",
      "K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "classification/5 KNN on titanic data.html#define-model",
    "href": "classification/5 KNN on titanic data.html#define-model",
    "title": "K-Nearest Neighbors",
    "section": "3.1 define model",
    "text": "3.1 define model\nThe solvers implemented in the class Logistic Regression are “liblinear”, “newton-cg”, “lbfgs”, “sag” and “saga”. According to Scikit Documentation: The “liblinear” solver was the one used by default for historical reasons before version 0.22. Since then, default use is lbfgs Algorithm.\n\n\nCode\nfrom sklearn.neighbors import KNeighborsRegressor \nml_model = KNeighborsRegressor()\nml_model\n\n\nKNeighborsRegressor()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KNeighborsRegressor?Documentation for KNeighborsRegressoriNot fittedKNeighborsRegressor()",
    "crumbs": [
      "Classification",
      "K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "classification/5 KNN on titanic data.html#train-model",
    "href": "classification/5 KNN on titanic data.html#train-model",
    "title": "K-Nearest Neighbors",
    "section": "3.2 train model",
    "text": "3.2 train model\n\n\nCode\nml_model.fit(X_train,Y_train)\n\n\nKNeighborsRegressor()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KNeighborsRegressor?Documentation for KNeighborsRegressoriFittedKNeighborsRegressor()",
    "crumbs": [
      "Classification",
      "K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "classification/5 KNN on titanic data.html#preformance",
    "href": "classification/5 KNN on titanic data.html#preformance",
    "title": "K-Nearest Neighbors",
    "section": "3.3 Preformance",
    "text": "3.3 Preformance\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = ml_model.predict(X_test) #always gets x and retuns y\nY_pred_dt\n\n\narray([0.8, 0.4, 0.2, 0. , 0.2, 0. , 0. , 0.6, 0.2, 0. , 0.8, 0.2, 0.6,\n       0.6, 0.4, 1. , 0.8, 0. , 1. , 0.2, 0.2, 0.8, 0.4, 0. , 0.8, 0. ,\n       0.6, 0.8, 1. , 0.2, 0.2, 0.6, 0. , 0.2, 0.8, 1. , 0.2, 0.6, 0.6,\n       0.6, 0. , 0. , 0.2, 0.2, 0.8, 0.2, 0.4, 0.4, 0.2, 0.2, 0.4, 0.4,\n       0.4, 0.4, 1. , 1. , 0. , 0. , 0. , 1. , 0.4, 0.2, 0.8, 0.4, 0.2,\n       0.2, 0. , 0.6, 0.6, 0.2, 0.2, 0.2, 0.2, 0.8, 0.2, 0.2, 0.4, 0.8,\n       0.2, 0.2, 0.2, 1. , 0.2, 0.6, 0.6, 0.4, 0. , 0.4, 0.2, 0.4, 0.2,\n       0.4, 0. , 0.2, 0. , 0.6, 0.2, 0.2, 0.6, 0. , 0.6, 0.4, 0.4, 0.4,\n       0. , 1. , 0.2, 0.4, 0.2, 0. , 0. , 0.2, 0.2, 0. , 0.2, 0. , 0. ,\n       0. , 1. , 0.2, 0.2, 0.2, 0.8, 0.6, 0. , 0.2, 0.4, 0.6, 0.2, 0.8,\n       0. , 0.6, 0. , 0.6, 0. , 0.8, 0.2, 0.2, 0. , 0. , 0.6, 0.8, 0.6,\n       0.6, 0.6, 0.6, 0.4, 0.8, 0.6, 0.2, 0.8, 0.2, 0.4, 0. , 0.2, 0.8,\n       0.6, 0.6, 0.8, 0.2, 0.8, 0.2, 0.2, 0.8, 0. , 0.2, 0.6, 0. , 0.4,\n       0.2, 0.6, 0.8, 0.6, 0. , 0. , 0. , 0.2, 0. , 0.2])\n\n\n\n\nCode\n# its criteria is to round to 1 when higher than 0.5\nY_pred_dt = np.round(Y_pred_dt)  \n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_test,Y_pred_dt)  \naccuracy\n\n\n0.6983240223463687\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_test,Y_pred_dt)  \nprecision_dt\n\n\n0.6129032258064516\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_test,Y_pred_dt)  \nrecall_dt\n\n\n0.5588235294117647\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_test,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[87, 24],\n       [30, 38]])\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_test, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.6713036565977742",
    "crumbs": [
      "Classification",
      "K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "classification/5 KNN on titanic data.html#k-fold-cross-validation",
    "href": "classification/5 KNN on titanic data.html#k-fold-cross-validation",
    "title": "K-Nearest Neighbors",
    "section": "3.4 k-Fold Cross-Validation",
    "text": "3.4 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(ml_model, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.10856888813345673",
    "crumbs": [
      "Classification",
      "K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "classification/3 Logistic Regression on titanic data.html",
    "href": "classification/3 Logistic Regression on titanic data.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split",
    "crumbs": [
      "Classification",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "classification/3 Logistic Regression on titanic data.html#input-data",
    "href": "classification/3 Logistic Regression on titanic data.html#input-data",
    "title": "Logistic Regression",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "classification/3 Logistic Regression on titanic data.html#data-eda",
    "href": "classification/3 Logistic Regression on titanic data.html#data-eda",
    "title": "Logistic Regression",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n\n\n0.7420382165605095\n0.18890814558058924",
    "crumbs": [
      "Classification",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "classification/3 Logistic Regression on titanic data.html#data-wrangling",
    "href": "classification/3 Logistic Regression on titanic data.html#data-wrangling",
    "title": "Logistic Regression",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1046 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1308 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Dealing with missing numerical variables\ndata['Age'] = data.Age.fillna(data.Age.median())\ndata['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1309 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1309 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\ndata = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\nSex_male\n\n\n\n\n0\n1\n3\nBraund, Mr. Owen Harris\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\nTrue\n\n\n1\n2\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\nFalse\n\n\n2\n3\n3\nHeikkinen, Miss. Laina\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\nFalse\n\n\n3\n4\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\nFalse\n\n\n4\n5\n3\nAllen, Mr. William Henry\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\nTrue\n\n\n\n\n\n\n\n\n\n\nCode\n# Select features columns\ndata = data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()\n\n\n\n\n\n\n\n\n\n\nSex_male\nFare\nAge\nPclass\nSibSp\nrole\n\n\n\n\n0\nTrue\n7.2500\n22.0\n3\n1\ntrain\n\n\n1\nFalse\n71.2833\n38.0\n1\n1\ntrain\n\n\n2\nFalse\n7.9250\n26.0\n3\n0\ntrain\n\n\n3\nFalse\n53.1000\n35.0\n1\n1\ntrain\n\n\n4\nTrue\n8.0500\n35.0\n3\n0\ntrain",
    "crumbs": [
      "Classification",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "classification/3 Logistic Regression on titanic data.html#split-data",
    "href": "classification/3 Logistic Regression on titanic data.html#split-data",
    "title": "Logistic Regression",
    "section": "2.4 split data",
    "text": "2.4 split data\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\nX_train = X_train.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 712 entries, 791 to 454\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Sex_male  712 non-null    bool   \n 1   Fare      712 non-null    float64\n 2   Age       712 non-null    float64\n 3   Pclass    712 non-null    int64  \n 4   SibSp     712 non-null    int64  \ndtypes: bool(1), float64(2), int64(2)\nmemory usage: 28.5 KB",
    "crumbs": [
      "Classification",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "classification/3 Logistic Regression on titanic data.html#define-model",
    "href": "classification/3 Logistic Regression on titanic data.html#define-model",
    "title": "Logistic Regression",
    "section": "3.1 define model",
    "text": "3.1 define model\nThe solvers implemented in the class Logistic Regression are “liblinear”, “newton-cg”, “lbfgs”, “sag” and “saga”. According to Scikit Documentation: The “liblinear” solver was the one used by default for historical reasons before version 0.22. Since then, default use is lbfgs Algorithm.\n\n\nCode\nfrom sklearn.linear_model import LogisticRegression\nml_model = LogisticRegression(solver='liblinear')\nml_model\n\n\nLogisticRegression(solver='liblinear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniNot fittedLogisticRegression(solver='liblinear')",
    "crumbs": [
      "Classification",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "classification/3 Logistic Regression on titanic data.html#train-model",
    "href": "classification/3 Logistic Regression on titanic data.html#train-model",
    "title": "Logistic Regression",
    "section": "3.2 train model",
    "text": "3.2 train model\n\n\nCode\nml_model.fit(X_train,Y_train)\n\n\nLogisticRegression(solver='liblinear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression(solver='liblinear') \n\n\nvariable importance\n\n\nCode\ncoefficients = ml_model.coef_[0]\n\nfeature_importance = pd.DataFrame({'Feature': X_train.columns, 'Importance': np.abs(coefficients)})\nfeature_importance = feature_importance.sort_values('Importance', ascending=False)\nfeature_importance\n\n\n\n\n\n\n\n\n\n\nFeature\nImportance\n\n\n\n\n0\nSex_male\n2.343351\n\n\n3\nPclass\n0.793533\n\n\n4\nSibSp\n0.277801\n\n\n2\nAge\n0.027368\n\n\n1\nFare\n0.004206",
    "crumbs": [
      "Classification",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "classification/3 Logistic Regression on titanic data.html#preformance",
    "href": "classification/3 Logistic Regression on titanic data.html#preformance",
    "title": "Logistic Regression",
    "section": "3.3 Preformance",
    "text": "3.3 Preformance\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = ml_model.predict(X_test) #always gets x and retuns y\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_test,Y_pred_dt)  \naccuracy\n\n\n0.8491620111731844\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_test,Y_pred_dt)  \nprecision_dt\n\n\n0.84\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_test,Y_pred_dt)  \nrecall_dt\n\n\n0.6885245901639344\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_test,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[110,   8],\n       [ 19,  42]])\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_test, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.8103639899972214",
    "crumbs": [
      "Classification",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "classification/3 Logistic Regression on titanic data.html#k-fold-cross-validation",
    "href": "classification/3 Logistic Regression on titanic data.html#k-fold-cross-validation",
    "title": "Logistic Regression",
    "section": "3.4 k-Fold Cross-Validation",
    "text": "3.4 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(ml_model, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.7795035949965528",
    "crumbs": [
      "Classification",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "classification/6 Random Forest on titanic dat.html",
    "href": "classification/6 Random Forest on titanic dat.html",
    "title": "Classification with Random Forest",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split",
    "crumbs": [
      "Classification",
      "Classification with Random Forest"
    ]
  },
  {
    "objectID": "classification/6 Random Forest on titanic dat.html#input-data",
    "href": "classification/6 Random Forest on titanic dat.html#input-data",
    "title": "Classification with Random Forest",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "Classification with Random Forest"
    ]
  },
  {
    "objectID": "classification/6 Random Forest on titanic dat.html#data-eda",
    "href": "classification/6 Random Forest on titanic dat.html#data-eda",
    "title": "Classification with Random Forest",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n\n\n0.7420382165605095\n0.18890814558058924",
    "crumbs": [
      "Classification",
      "Classification with Random Forest"
    ]
  },
  {
    "objectID": "classification/6 Random Forest on titanic dat.html#data-wrangling",
    "href": "classification/6 Random Forest on titanic dat.html#data-wrangling",
    "title": "Classification with Random Forest",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1046 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1308 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Dealing with missing numerical variables\ndata['Age'] = data.Age.fillna(data.Age.median())\ndata['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1309 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1309 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\ndata = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\nSex_male\n\n\n\n\n0\n1\n3\nBraund, Mr. Owen Harris\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\nTrue\n\n\n1\n2\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\nFalse\n\n\n2\n3\n3\nHeikkinen, Miss. Laina\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\nFalse\n\n\n3\n4\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\nFalse\n\n\n4\n5\n3\nAllen, Mr. William Henry\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\nTrue\n\n\n\n\n\n\n\n\n\n\nCode\n# Select features columns\ndata = data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()\n\n\n\n\n\n\n\n\n\n\nSex_male\nFare\nAge\nPclass\nSibSp\nrole\n\n\n\n\n0\nTrue\n7.2500\n22.0\n3\n1\ntrain\n\n\n1\nFalse\n71.2833\n38.0\n1\n1\ntrain\n\n\n2\nFalse\n7.9250\n26.0\n3\n0\ntrain\n\n\n3\nFalse\n53.1000\n35.0\n1\n1\ntrain\n\n\n4\nTrue\n8.0500\n35.0\n3\n0\ntrain",
    "crumbs": [
      "Classification",
      "Classification with Random Forest"
    ]
  },
  {
    "objectID": "classification/6 Random Forest on titanic dat.html#split-data",
    "href": "classification/6 Random Forest on titanic dat.html#split-data",
    "title": "Classification with Random Forest",
    "section": "2.4 split data",
    "text": "2.4 split data\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\nX_train = X_train.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 712 entries, 710 to 744\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Sex_male  712 non-null    bool   \n 1   Fare      712 non-null    float64\n 2   Age       712 non-null    float64\n 3   Pclass    712 non-null    int64  \n 4   SibSp     712 non-null    int64  \ndtypes: bool(1), float64(2), int64(2)\nmemory usage: 28.5 KB",
    "crumbs": [
      "Classification",
      "Classification with Random Forest"
    ]
  },
  {
    "objectID": "classification/6 Random Forest on titanic dat.html#define-model",
    "href": "classification/6 Random Forest on titanic dat.html#define-model",
    "title": "Classification with Random Forest",
    "section": "3.1 define model",
    "text": "3.1 define model\nThe solvers implemented in the class Logistic Regression are “liblinear”, “newton-cg”, “lbfgs”, “sag” and “saga”. According to Scikit Documentation: The “liblinear” solver was the one used by default for historical reasons before version 0.22. Since then, default use is lbfgs Algorithm.\n\n\nCode\nfrom sklearn.ensemble import RandomForestClassifier\nml_model = RandomForestClassifier()\nml_model\n\n\nRandomForestClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriNot fittedRandomForestClassifier()",
    "crumbs": [
      "Classification",
      "Classification with Random Forest"
    ]
  },
  {
    "objectID": "classification/6 Random Forest on titanic dat.html#train-model",
    "href": "classification/6 Random Forest on titanic dat.html#train-model",
    "title": "Classification with Random Forest",
    "section": "3.2 train model",
    "text": "3.2 train model\n\n\nCode\nml_model.fit(X_train,Y_train)\n\n\nRandomForestClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriFittedRandomForestClassifier()",
    "crumbs": [
      "Classification",
      "Classification with Random Forest"
    ]
  },
  {
    "objectID": "classification/6 Random Forest on titanic dat.html#preformance",
    "href": "classification/6 Random Forest on titanic dat.html#preformance",
    "title": "Classification with Random Forest",
    "section": "3.3 Preformance",
    "text": "3.3 Preformance\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = ml_model.predict(X_test) #always gets x and retuns y\nY_pred_dt\n\n\narray([1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,\n       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n       0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,\n       0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n       0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n       0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n       0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n       0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n       0, 0, 1])\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_test,Y_pred_dt)  \naccuracy\n\n\n0.8156424581005587\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_test,Y_pred_dt)  \nprecision_dt\n\n\n0.7761194029850746\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_test,Y_pred_dt)  \nrecall_dt\n\n\n0.7428571428571429\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_test,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[94, 15],\n       [18, 52]])\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_test, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.8026212319790301",
    "crumbs": [
      "Classification",
      "Classification with Random Forest"
    ]
  },
  {
    "objectID": "classification/6 Random Forest on titanic dat.html#k-fold-cross-validation",
    "href": "classification/6 Random Forest on titanic dat.html#k-fold-cross-validation",
    "title": "Classification with Random Forest",
    "section": "3.4 k-Fold Cross-Validation",
    "text": "3.4 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(ml_model, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.8160346695557962",
    "crumbs": [
      "Classification",
      "Classification with Random Forest"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#get-first-3-rows",
    "href": "data manipulation/1 Pandas.html#get-first-3-rows",
    "title": "Data manipulation with Pandas",
    "section": "0.2 get first 3 rows",
    "text": "0.2 get first 3 rows\n\n\nCode\ndf.head(3)\n\n\n\n\n\n\n\n\n\n\nFlightDate\nAirline\nOrigin\nDest\nCancelled\nDiverted\nCRSDepTime\nDepTime\nDepDelayMinutes\nDepDelay\n...\nWheelsOff\nWheelsOn\nTaxiIn\nCRSArrTime\nArrDelay\nArrDel15\nArrivalDelayGroups\nArrTimeBlk\nDistanceGroup\nDivAirportLandings\n\n\n\n\n0\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nGJT\nDEN\nFalse\nFalse\n1133\n1123.0\n0.0\n-10.0\n...\n1140.0\n1220.0\n8.0\n1245\n-17.0\n0.0\n-2.0\n1200-1259\n1\n0\n\n\n1\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nHRL\nIAH\nFalse\nFalse\n732\n728.0\n0.0\n-4.0\n...\n744.0\n839.0\n9.0\n849\n-1.0\n0.0\n-1.0\n0800-0859\n2\n0\n\n\n2\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nDRO\nDEN\nFalse\nFalse\n1529\n1514.0\n0.0\n-15.0\n...\n1535.0\n1622.0\n14.0\n1639\n-3.0\n0.0\n-1.0\n1600-1659\n2\n0\n\n\n\n\n3 rows × 61 columns",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#get-last-3-rows",
    "href": "data manipulation/1 Pandas.html#get-last-3-rows",
    "title": "Data manipulation with Pandas",
    "section": "0.3 get last 3 rows",
    "text": "0.3 get last 3 rows\n\n\nCode\ndf.tail(3)\n\n\n\n\n\n\n\n\n\n\nFlightDate\nAirline\nOrigin\nDest\nCancelled\nDiverted\nCRSDepTime\nDepTime\nDepDelayMinutes\nDepDelay\n...\nWheelsOff\nWheelsOn\nTaxiIn\nCRSArrTime\nArrDelay\nArrDel15\nArrivalDelayGroups\nArrTimeBlk\nDistanceGroup\nDivAirportLandings\n\n\n\n\n7\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nTYS\nIAH\nFalse\nFalse\n1129\n1117.0\n0.0\n-12.0\n...\n1139.0\n1255.0\n16.0\n1306\n5.0\n0.0\n0.0\n1300-1359\n4\n0\n\n\n8\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nIAH\nAEX\nFalse\nFalse\n1424\n1414.0\n0.0\n-10.0\n...\n1430.0\n1507.0\n6.0\n1524\n-11.0\n0.0\n-1.0\n1500-1559\n1\n0\n\n\n9\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nIAH\nMOB\nFalse\nFalse\n954\n947.0\n0.0\n-7.0\n...\n1004.0\n1104.0\n6.0\n1121\n-11.0\n0.0\n-1.0\n1100-1159\n2\n0\n\n\n\n\n3 rows × 61 columns",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#get-ramdon-5-rows",
    "href": "data manipulation/1 Pandas.html#get-ramdon-5-rows",
    "title": "Data manipulation with Pandas",
    "section": "0.4 get ramdon 5 rows",
    "text": "0.4 get ramdon 5 rows\n\n\nCode\ndf.sample(5, random_state=42)\n\n\n\n\n\n\n\n\n\n\nFlightDate\nAirline\nOrigin\nDest\nCancelled\nDiverted\nCRSDepTime\nDepTime\nDepDelayMinutes\nDepDelay\n...\nWheelsOff\nWheelsOn\nTaxiIn\nCRSArrTime\nArrDelay\nArrDel15\nArrivalDelayGroups\nArrTimeBlk\nDistanceGroup\nDivAirportLandings\n\n\n\n\n8\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nIAH\nAEX\nFalse\nFalse\n1424\n1414.0\n0.0\n-10.0\n...\n1430.0\n1507.0\n6.0\n1524\n-11.0\n0.0\n-1.0\n1500-1559\n1\n0\n\n\n1\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nHRL\nIAH\nFalse\nFalse\n732\n728.0\n0.0\n-4.0\n...\n744.0\n839.0\n9.0\n849\n-1.0\n0.0\n-1.0\n0800-0859\n2\n0\n\n\n5\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nDEN\nTUL\nFalse\nFalse\n955\n952.0\n0.0\n-3.0\n...\n1017.0\n1234.0\n4.0\n1240\n-2.0\n0.0\n-1.0\n1200-1259\n3\n0\n\n\n0\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nGJT\nDEN\nFalse\nFalse\n1133\n1123.0\n0.0\n-10.0\n...\n1140.0\n1220.0\n8.0\n1245\n-17.0\n0.0\n-2.0\n1200-1259\n1\n0\n\n\n7\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nTYS\nIAH\nFalse\nFalse\n1129\n1117.0\n0.0\n-12.0\n...\n1139.0\n1255.0\n16.0\n1306\n5.0\n0.0\n0.0\n1300-1359\n4\n0\n\n\n\n\n5 rows × 61 columns",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "classification/1.3 decision tree and fast tuning.html",
    "href": "classification/1.3 decision tree and fast tuning.html",
    "title": "Decision tree and fast tuning",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nimport time",
    "crumbs": [
      "Classification",
      "Decision tree and fast tuning"
    ]
  },
  {
    "objectID": "classification/1.3 decision tree and fast tuning.html#input-data",
    "href": "classification/1.3 decision tree and fast tuning.html#input-data",
    "title": "Decision tree and fast tuning",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "Decision tree and fast tuning"
    ]
  },
  {
    "objectID": "classification/1.3 decision tree and fast tuning.html#data-eda",
    "href": "classification/1.3 decision tree and fast tuning.html#data-eda",
    "title": "Decision tree and fast tuning",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n\n\n0.7420382165605095\n0.18890814558058924",
    "crumbs": [
      "Classification",
      "Decision tree and fast tuning"
    ]
  },
  {
    "objectID": "classification/1.3 decision tree and fast tuning.html#data-wrangling",
    "href": "classification/1.3 decision tree and fast tuning.html#data-wrangling",
    "title": "Decision tree and fast tuning",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1046 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1308 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Dealing with missing numerical variables\ndata['Age'] = data.Age.fillna(data.Age.median())\ndata['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1309 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1309 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\ndata = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\nSex_male\n\n\n\n\n0\n1\n3\nBraund, Mr. Owen Harris\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\nTrue\n\n\n1\n2\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\nFalse\n\n\n2\n3\n3\nHeikkinen, Miss. Laina\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\nFalse\n\n\n3\n4\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\nFalse\n\n\n4\n5\n3\nAllen, Mr. William Henry\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\nTrue\n\n\n\n\n\n\n\n\n\n\nCode\n# Select features columns\ndata = data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()\n\n\n\n\n\n\n\n\n\n\nSex_male\nFare\nAge\nPclass\nSibSp\nrole\n\n\n\n\n0\nTrue\n7.2500\n22.0\n3\n1\ntrain\n\n\n1\nFalse\n71.2833\n38.0\n1\n1\ntrain\n\n\n2\nFalse\n7.9250\n26.0\n3\n0\ntrain\n\n\n3\nFalse\n53.1000\n35.0\n1\n1\ntrain\n\n\n4\nTrue\n8.0500\n35.0\n3\n0\ntrain",
    "crumbs": [
      "Classification",
      "Decision tree and fast tuning"
    ]
  },
  {
    "objectID": "classification/1.3 decision tree and fast tuning.html#split-data",
    "href": "classification/1.3 decision tree and fast tuning.html#split-data",
    "title": "Decision tree and fast tuning",
    "section": "2.4 split data",
    "text": "2.4 split data\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\nX_train = X_train.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 712 entries, 701 to 753\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Sex_male  712 non-null    bool   \n 1   Fare      712 non-null    float64\n 2   Age       712 non-null    float64\n 3   Pclass    712 non-null    int64  \n 4   SibSp     712 non-null    int64  \ndtypes: bool(1), float64(2), int64(2)\nmemory usage: 28.5 KB",
    "crumbs": [
      "Classification",
      "Decision tree and fast tuning"
    ]
  },
  {
    "objectID": "classification/1.3 decision tree and fast tuning.html#define-model",
    "href": "classification/1.3 decision tree and fast tuning.html#define-model",
    "title": "Decision tree and fast tuning",
    "section": "3.1 define model",
    "text": "3.1 define model\n\n\nCode\nmodel_dt = DecisionTreeClassifier()   #model with deph 3\nmodel_dt\n\n\nDecisionTreeClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriNot fittedDecisionTreeClassifier() \n\n\ndefalt hyper-parameters:\n\n\nCode\nmodel_dt.get_params()\n\n\n{'ccp_alpha': 0.0,\n 'class_weight': None,\n 'criterion': 'gini',\n 'max_depth': None,\n 'max_features': None,\n 'max_leaf_nodes': None,\n 'min_impurity_decrease': 0.0,\n 'min_samples_leaf': 1,\n 'min_samples_split': 2,\n 'min_weight_fraction_leaf': 0.0,\n 'monotonic_cst': None,\n 'random_state': None,\n 'splitter': 'best'}",
    "crumbs": [
      "Classification",
      "Decision tree and fast tuning"
    ]
  },
  {
    "objectID": "classification/1.3 decision tree and fast tuning.html#define-parameters",
    "href": "classification/1.3 decision tree and fast tuning.html#define-parameters",
    "title": "Decision tree and fast tuning",
    "section": "3.2 define parameters",
    "text": "3.2 define parameters\n\n\nCode\nparameters = {'criterion':['gini','entropy'],\n              'max_depth':np.arange(1,21).tolist()[0::2],\n              'min_samples_split':np.arange(2,11).tolist()[0::2],\n              'max_leaf_nodes':np.arange(3,26).tolist()[0::2]}\n\n\nall parameters combinations\n\n\nCode\nimport itertools\na = parameters.values()\ncombinations = list(itertools.product(*a))\n\n\n1200 combinations\n\n\nCode\nlen(combinations)\n\n\n1200\n\n\n\n\nCode\ncombinations[0:5]\n\n\n[('gini', 1, 2, 3),\n ('gini', 1, 2, 5),\n ('gini', 1, 2, 7),\n ('gini', 1, 2, 9),\n ('gini', 1, 2, 11)]",
    "crumbs": [
      "Classification",
      "Decision tree and fast tuning"
    ]
  },
  {
    "objectID": "classification/1.3 decision tree and fast tuning.html#define-grids",
    "href": "classification/1.3 decision tree and fast tuning.html#define-grids",
    "title": "Decision tree and fast tuning",
    "section": "3.3 define GridS",
    "text": "3.3 define GridS\nusing Random Search limit to 100 combinations\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV\nG1=RandomizedSearchCV(DecisionTreeClassifier(), param_distributions=parameters,n_iter = 100, cv=10, n_jobs=-1)",
    "crumbs": [
      "Classification",
      "Decision tree and fast tuning"
    ]
  },
  {
    "objectID": "classification/1.3 decision tree and fast tuning.html#train-model",
    "href": "classification/1.3 decision tree and fast tuning.html#train-model",
    "title": "Decision tree and fast tuning",
    "section": "3.4 train model",
    "text": "3.4 train model\n\n\nCode\nstart_time = time.time()\n\nG1.fit(X_train,Y_train)\n\nend_time = time.time()\n\nduration = end_time - start_time\nduration\n\n\n2.8857500553131104\n\n\nbest parameters\n\n\nCode\nG1.best_params_\n\n\n{'min_samples_split': 2,\n 'max_leaf_nodes': 21,\n 'max_depth': 7,\n 'criterion': 'gini'}\n\n\nbest model\n\n\nCode\nmodel_dt = G1.best_estimator_\n\n\nvariable importance\n\n\nCode\nimportances = model_dt.feature_importances_\nvi=pd.DataFrame({\"variable\":X_train.columns,\"importances\":importances})\nvi=vi.sort_values('importances',ascending=False)\nvi\n\n\n\n\n\n\n\n\n\n\nvariable\nimportances\n\n\n\n\n0\nSex_male\n0.506731\n\n\n3\nPclass\n0.151602\n\n\n2\nAge\n0.144835\n\n\n1\nFare\n0.134373\n\n\n4\nSibSp\n0.062458",
    "crumbs": [
      "Classification",
      "Decision tree and fast tuning"
    ]
  },
  {
    "objectID": "classification/1.3 decision tree and fast tuning.html#preformance",
    "href": "classification/1.3 decision tree and fast tuning.html#preformance",
    "title": "Decision tree and fast tuning",
    "section": "3.5 Preformance",
    "text": "3.5 Preformance\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = model_dt.predict(X_test) #always gets x and retuns y\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_test,Y_pred_dt)  \naccuracy\n\n\n0.8100558659217877\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_test,Y_pred_dt)  \nprecision_dt\n\n\n0.75\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_test,Y_pred_dt)  \nrecall_dt\n\n\n0.75\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_test,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[94, 17],\n       [17, 51]])\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_test, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.7984234234234234",
    "crumbs": [
      "Classification",
      "Decision tree and fast tuning"
    ]
  },
  {
    "objectID": "classification/1.3 decision tree and fast tuning.html#k-fold-cross-validation",
    "href": "classification/1.3 decision tree and fast tuning.html#k-fold-cross-validation",
    "title": "Decision tree and fast tuning",
    "section": "3.6 k-Fold Cross-Validation",
    "text": "3.6 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(model_dt, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.8315177779966513",
    "crumbs": [
      "Classification",
      "Decision tree and fast tuning"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#select-column-by-column-names",
    "href": "data manipulation/1 Pandas.html#select-column-by-column-names",
    "title": "Data manipulation with Pandas",
    "section": "1.1 select column by column names",
    "text": "1.1 select column by column names\n\n\nCode\ndf[['FlightDate','Airline']]\n\n\n\n\n\n\n\n\n\n\nFlightDate\nAirline\n\n\n\n\n0\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\n\n\n1\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\n\n\n2\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\n\n\n3\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\n\n\n4\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\n\n\n5\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\n\n\n6\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\n\n\n7\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\n\n\n8\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\n\n\n9\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#first-3-columns",
    "href": "data manipulation/1 Pandas.html#first-3-columns",
    "title": "Data manipulation with Pandas",
    "section": "1.2 first 3 columns",
    "text": "1.2 first 3 columns\n\n\nCode\ndf[df.columns[:3]]\n\n\n\n\n\n\n\n\n\n\nFlightDate\nAirline\nOrigin\n\n\n\n\n0\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nGJT\n\n\n1\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nHRL\n\n\n2\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nDRO\n\n\n3\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nIAH\n\n\n4\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nDRO\n\n\n5\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nDEN\n\n\n6\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nIAH\n\n\n7\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nTYS\n\n\n8\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nIAH\n\n\n9\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nIAH",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#last-3-columns",
    "href": "data manipulation/1 Pandas.html#last-3-columns",
    "title": "Data manipulation with Pandas",
    "section": "1.3 last 3 columns",
    "text": "1.3 last 3 columns\n\n\nCode\ndf[df.columns[-3:]]\n\n\n\n\n\n\n\n\n\n\nArrTimeBlk\nDistanceGroup\nDivAirportLandings\n\n\n\n\n0\n1200-1259\n1\n0\n\n\n1\n0800-0859\n2\n0\n\n\n2\n1600-1659\n2\n0\n\n\n3\n1600-1659\n2\n0\n\n\n4\n1200-1259\n2\n0\n\n\n5\n1200-1259\n3\n0\n\n\n6\n2200-2259\n1\n0\n\n\n7\n1300-1359\n4\n0\n\n\n8\n1500-1559\n1\n0\n\n\n9\n1100-1159\n2\n0",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#select-3rd-columns",
    "href": "data manipulation/1 Pandas.html#select-3rd-columns",
    "title": "Data manipulation with Pandas",
    "section": "1.4 select 3rd columns",
    "text": "1.4 select 3rd columns\n\n\nCode\ndf.iloc[:,2]\n\n\n0    GJT\n1    HRL\n2    DRO\n3    IAH\n4    DRO\n5    DEN\n6    IAH\n7    TYS\n8    IAH\n9    IAH\nName: Origin, dtype: object",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#select-int-data-type-column",
    "href": "data manipulation/1 Pandas.html#select-int-data-type-column",
    "title": "Data manipulation with Pandas",
    "section": "1.5 select int data type column",
    "text": "1.5 select int data type column\n\n\nCode\ndf.select_dtypes('int')\n\n\n\n\n\n\n\n\n\n\nCRSDepTime\nYear\nQuarter\nMonth\nDayofMonth\nDayOfWeek\nDOT_ID_Marketing_Airline\nFlight_Number_Marketing_Airline\nDOT_ID_Operating_Airline\nFlight_Number_Operating_Airline\n...\nOriginStateFips\nOriginWac\nDestAirportID\nDestAirportSeqID\nDestCityMarketID\nDestStateFips\nDestWac\nCRSArrTime\nDistanceGroup\nDivAirportLandings\n\n\n\n\n0\n1133\n2022\n2\n4\n4\n1\n19977\n4301\n20445\n4301\n...\n8\n82\n11292\n1129202\n30325\n8\n82\n1245\n1\n0\n\n\n1\n732\n2022\n2\n4\n4\n1\n19977\n4299\n20445\n4299\n...\n48\n74\n12266\n1226603\n31453\n48\n74\n849\n2\n0\n\n\n2\n1529\n2022\n2\n4\n4\n1\n19977\n4298\n20445\n4298\n...\n8\n82\n11292\n1129202\n30325\n8\n82\n1639\n2\n0\n\n\n3\n1435\n2022\n2\n4\n4\n1\n19977\n4296\n20445\n4296\n...\n48\n74\n11973\n1197302\n31973\n28\n53\n1605\n2\n0\n\n\n4\n1135\n2022\n2\n4\n4\n1\n19977\n4295\n20445\n4295\n...\n8\n82\n11292\n1129202\n30325\n8\n82\n1245\n2\n0\n\n\n5\n955\n2022\n2\n4\n4\n1\n19977\n4294\n20445\n4294\n...\n8\n82\n15370\n1537002\n34653\n40\n73\n1240\n3\n0\n\n\n6\n2139\n2022\n2\n4\n4\n1\n19977\n4293\n20445\n4293\n...\n48\n74\n12915\n1291503\n31205\n22\n72\n2231\n1\n0\n\n\n7\n1129\n2022\n2\n4\n4\n1\n19977\n4292\n20445\n4292\n...\n47\n54\n12266\n1226603\n31453\n48\n74\n1306\n4\n0\n\n\n8\n1424\n2022\n2\n4\n4\n1\n19977\n4291\n20445\n4291\n...\n48\n74\n10185\n1018502\n30185\n22\n72\n1524\n1\n0\n\n\n9\n954\n2022\n2\n4\n4\n1\n19977\n4290\n20445\n4290\n...\n48\n74\n13422\n1342202\n30562\n1\n51\n1121\n2\n0\n\n\n\n\n10 rows × 23 columns",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#select-3-rows-and-4-column-cells",
    "href": "data manipulation/1 Pandas.html#select-3-rows-and-4-column-cells",
    "title": "Data manipulation with Pandas",
    "section": "2.1 select 3 rows and 4 column cells",
    "text": "2.1 select 3 rows and 4 column cells\n\n\nCode\ndf.iloc[2, 3]\n\n\n'DEN'",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#select-first-5-rows-and-first-5-column-cells",
    "href": "data manipulation/1 Pandas.html#select-first-5-rows-and-first-5-column-cells",
    "title": "Data manipulation with Pandas",
    "section": "2.2 select first 5 rows and first 5 column cells",
    "text": "2.2 select first 5 rows and first 5 column cells\n\n\nCode\ndf.iloc[:5, :5] \n\n\n\n\n\n\n\n\n\n\nFlightDate\nAirline\nOrigin\nDest\nCancelled\n\n\n\n\n0\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nGJT\nDEN\nFalse\n\n\n1\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nHRL\nIAH\nFalse\n\n\n2\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nDRO\nDEN\nFalse\n\n\n3\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nIAH\nGPT\nFalse\n\n\n4\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nDRO\nDEN\nFalse",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#select-3rd-rows",
    "href": "data manipulation/1 Pandas.html#select-3rd-rows",
    "title": "Data manipulation with Pandas",
    "section": "2.3 select 3rd rows",
    "text": "2.3 select 3rd rows\n\n\nCode\ndf.iloc[[2]]\n\n\n\n\n\n\n\n\n\n\nFlightDate\nAirline\nOrigin\nDest\nCancelled\nDiverted\nCRSDepTime\nDepTime\nDepDelayMinutes\nDepDelay\n...\nWheelsOff\nWheelsOn\nTaxiIn\nCRSArrTime\nArrDelay\nArrDel15\nArrivalDelayGroups\nArrTimeBlk\nDistanceGroup\nDivAirportLandings\n\n\n\n\n2\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nDRO\nDEN\nFalse\nFalse\n1529\n1514.0\n0.0\n-15.0\n...\n1535.0\n1622.0\n14.0\n1639\n-3.0\n0.0\n-1.0\n1600-1659\n2\n0\n\n\n\n\n1 rows × 61 columns",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "classification/6.1 Random Forest.html",
    "href": "classification/6.1 Random Forest.html",
    "title": "Random Forest",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split",
    "crumbs": [
      "Classification",
      "Random Forest"
    ]
  },
  {
    "objectID": "classification/6.1 Random Forest.html#input-data",
    "href": "classification/6.1 Random Forest.html#input-data",
    "title": "Random Forest",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "Random Forest"
    ]
  },
  {
    "objectID": "classification/6.1 Random Forest.html#data-eda",
    "href": "classification/6.1 Random Forest.html#data-eda",
    "title": "Random Forest",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n\n\n0.7420382165605095\n0.18890814558058924",
    "crumbs": [
      "Classification",
      "Random Forest"
    ]
  },
  {
    "objectID": "classification/6.1 Random Forest.html#data-wrangling",
    "href": "classification/6.1 Random Forest.html#data-wrangling",
    "title": "Random Forest",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1046 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1308 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Dealing with missing numerical variables\ndata['Age'] = data.Age.fillna(data.Age.median())\ndata['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1309 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1309 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\ndata = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\nSex_male\n\n\n\n\n0\n1\n3\nBraund, Mr. Owen Harris\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\nTrue\n\n\n1\n2\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\nFalse\n\n\n2\n3\n3\nHeikkinen, Miss. Laina\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\nFalse\n\n\n3\n4\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\nFalse\n\n\n4\n5\n3\nAllen, Mr. William Henry\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\nTrue\n\n\n\n\n\n\n\n\n\n\nCode\n# Select features columns\ndata = data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()\n\n\n\n\n\n\n\n\n\n\nSex_male\nFare\nAge\nPclass\nSibSp\nrole\n\n\n\n\n0\nTrue\n7.2500\n22.0\n3\n1\ntrain\n\n\n1\nFalse\n71.2833\n38.0\n1\n1\ntrain\n\n\n2\nFalse\n7.9250\n26.0\n3\n0\ntrain\n\n\n3\nFalse\n53.1000\n35.0\n1\n1\ntrain\n\n\n4\nTrue\n8.0500\n35.0\n3\n0\ntrain",
    "crumbs": [
      "Classification",
      "Random Forest"
    ]
  },
  {
    "objectID": "classification/6.1 Random Forest.html#split-data",
    "href": "classification/6.1 Random Forest.html#split-data",
    "title": "Random Forest",
    "section": "2.4 split data",
    "text": "2.4 split data\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\nX_train = X_train.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 712 entries, 206 to 819\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Sex_male  712 non-null    bool   \n 1   Fare      712 non-null    float64\n 2   Age       712 non-null    float64\n 3   Pclass    712 non-null    int64  \n 4   SibSp     712 non-null    int64  \ndtypes: bool(1), float64(2), int64(2)\nmemory usage: 28.5 KB",
    "crumbs": [
      "Classification",
      "Random Forest"
    ]
  },
  {
    "objectID": "classification/6.1 Random Forest.html#define-model",
    "href": "classification/6.1 Random Forest.html#define-model",
    "title": "Random Forest",
    "section": "3.1 define model",
    "text": "3.1 define model\nThe solvers implemented in the class Logistic Regression are “liblinear”, “newton-cg”, “lbfgs”, “sag” and “saga”. According to Scikit Documentation: The “liblinear” solver was the one used by default for historical reasons before version 0.22. Since then, default use is lbfgs Algorithm.\n\n\nCode\nfrom sklearn.ensemble import RandomForestClassifier\nml_model = RandomForestClassifier()\nml_model\n\n\nRandomForestClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriNot fittedRandomForestClassifier()",
    "crumbs": [
      "Classification",
      "Random Forest"
    ]
  },
  {
    "objectID": "classification/6.1 Random Forest.html#train-model",
    "href": "classification/6.1 Random Forest.html#train-model",
    "title": "Random Forest",
    "section": "3.2 train model",
    "text": "3.2 train model\n\n\nCode\nml_model.fit(X_train,Y_train)\n\n\nRandomForestClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriFittedRandomForestClassifier()",
    "crumbs": [
      "Classification",
      "Random Forest"
    ]
  },
  {
    "objectID": "classification/6.1 Random Forest.html#preformance",
    "href": "classification/6.1 Random Forest.html#preformance",
    "title": "Random Forest",
    "section": "3.3 Preformance",
    "text": "3.3 Preformance\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = ml_model.predict(X_test) #always gets x and retuns y\nY_pred_dt\n\n\narray([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n       0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,\n       0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,\n       0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,\n       1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n       0, 1, 0])\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_test,Y_pred_dt)  \naccuracy\n\n\n0.8044692737430168\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_test,Y_pred_dt)  \nprecision_dt\n\n\n0.7288135593220338\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_test,Y_pred_dt)  \nrecall_dt\n\n\n0.6935483870967742\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_test,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[101,  16],\n       [ 19,  43]])\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_test, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.7783981251723188",
    "crumbs": [
      "Classification",
      "Random Forest"
    ]
  },
  {
    "objectID": "classification/6.1 Random Forest.html#k-fold-cross-validation",
    "href": "classification/6.1 Random Forest.html#k-fold-cross-validation",
    "title": "Random Forest",
    "section": "3.4 k-Fold Cross-Validation",
    "text": "3.4 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(ml_model, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.8089825667290457",
    "crumbs": [
      "Classification",
      "Random Forest"
    ]
  },
  {
    "objectID": "classification/6.2 Random Forest.html",
    "href": "classification/6.2 Random Forest.html",
    "title": "Random Forest with pipeline and hyperparameter tuning",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nimport time",
    "crumbs": [
      "Classification",
      "Random Forest with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/6.2 Random Forest.html#input-data",
    "href": "classification/6.2 Random Forest.html#input-data",
    "title": "Random Forest with pipeline and hyperparameter tuning",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "Random Forest with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/6.2 Random Forest.html#data-eda",
    "href": "classification/6.2 Random Forest.html#data-eda",
    "title": "Random Forest with pipeline and hyperparameter tuning",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n\n\n0.7420382165605095\n0.18890814558058924",
    "crumbs": [
      "Classification",
      "Random Forest with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/6.2 Random Forest.html#data-wrangling",
    "href": "classification/6.2 Random Forest.html#data-wrangling",
    "title": "Random Forest with pipeline and hyperparameter tuning",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1046 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1308 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Dealing with missing numerical variables\ndata['Age'] = data.Age.fillna(data.Age.median())\ndata['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1309 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1309 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\ndata = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\nSex_male\n\n\n\n\n0\n1\n3\nBraund, Mr. Owen Harris\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\nTrue\n\n\n1\n2\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\nFalse\n\n\n2\n3\n3\nHeikkinen, Miss. Laina\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\nFalse\n\n\n3\n4\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\nFalse\n\n\n4\n5\n3\nAllen, Mr. William Henry\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\nTrue\n\n\n\n\n\n\n\n\n\n\nCode\n# Select features columns\ndata = data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()\n\n\n\n\n\n\n\n\n\n\nSex_male\nFare\nAge\nPclass\nSibSp\nrole\n\n\n\n\n0\nTrue\n7.2500\n22.0\n3\n1\ntrain\n\n\n1\nFalse\n71.2833\n38.0\n1\n1\ntrain\n\n\n2\nFalse\n7.9250\n26.0\n3\n0\ntrain\n\n\n3\nFalse\n53.1000\n35.0\n1\n1\ntrain\n\n\n4\nTrue\n8.0500\n35.0\n3\n0\ntrain",
    "crumbs": [
      "Classification",
      "Random Forest with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/6.2 Random Forest.html#split-data",
    "href": "classification/6.2 Random Forest.html#split-data",
    "title": "Random Forest with pipeline and hyperparameter tuning",
    "section": "2.4 split data",
    "text": "2.4 split data\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\nX_train = X_train.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 712 entries, 350 to 456\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Sex_male  712 non-null    bool   \n 1   Fare      712 non-null    float64\n 2   Age       712 non-null    float64\n 3   Pclass    712 non-null    int64  \n 4   SibSp     712 non-null    int64  \ndtypes: bool(1), float64(2), int64(2)\nmemory usage: 28.5 KB",
    "crumbs": [
      "Classification",
      "Random Forest with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/6.2 Random Forest.html#define-model",
    "href": "classification/6.2 Random Forest.html#define-model",
    "title": "Random Forest with pipeline and hyperparameter tuning",
    "section": "3.1 define model",
    "text": "3.1 define model\nThe solvers implemented in the class Logistic Regression are “liblinear”, “newton-cg”, “lbfgs”, “sag” and “saga”. According to Scikit Documentation: The “liblinear” solver was the one used by default for historical reasons before version 0.22. Since then, default use is lbfgs Algorithm.\n\n\nCode\nfrom sklearn.ensemble import RandomForestClassifier\nml_model = RandomForestClassifier()\nml_model\n\n\nRandomForestClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriNot fittedRandomForestClassifier()",
    "crumbs": [
      "Classification",
      "Random Forest with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/6.2 Random Forest.html#train-model",
    "href": "classification/6.2 Random Forest.html#train-model",
    "title": "Random Forest with pipeline and hyperparameter tuning",
    "section": "3.4 train model",
    "text": "3.4 train model\n\n\nCode\nstart_time = time.time()\n\nGridCV.fit(X_train, Y_train)\n\nend_time = time.time()\nduration = end_time - start_time\nduration\n\n\nFitting 10 folds for each of 27 candidates, totalling 270 fits\n\n\n8.21796202659607\n\n\n\n\nCode\n# get the parameter names\ncolumn_results = [f\"param_{name}\" for name in parameters.keys()]\ncolumn_results += [\"mean_test_score\", \"std_test_score\", \"rank_test_score\"]\n\ncv_results = pd.DataFrame(GridCV.cv_results_)\ncv_results = cv_results[column_results].sort_values(\n    \"mean_test_score\", ascending=False\n)\n\n\ndef shorten_param(param_name):\n    if \"__\" in param_name:\n        return param_name.rsplit(\"__\", 1)[1]\n    return param_name\n\n\ncv_results = cv_results.rename(shorten_param, axis=1)\ncv_results.head()\n\n\n\n\n\n\n\n\n\n\nmax_depth\nn_estimators\nmin_samples_leaf\nmean_test_score\nstd_test_score\nrank_test_score\n\n\n\n\n16\n30\n250\n3\n0.724570\n0.070025\n1\n\n\n17\n30\n300\n3\n0.723200\n0.073436\n2\n\n\n15\n30\n200\n3\n0.723181\n0.071370\n3\n\n\n22\n40\n250\n2\n0.723181\n0.077401\n4\n\n\n4\n20\n250\n2\n0.721811\n0.067437\n5\n\n\n\n\n\n\n\n\n\n3.4.0.1 tunning best parameters\n\n\nCode\nGridCV.best_params_\n\n\n{'model__max_depth': 30,\n 'model__min_samples_leaf': 3,\n 'model__n_estimators': 250}\n\n\n\n\n3.4.0.2 tunning best model\n\n\nCode\nml_model = GridCV.best_estimator_",
    "crumbs": [
      "Classification",
      "Random Forest with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/6.2 Random Forest.html#preformance",
    "href": "classification/6.2 Random Forest.html#preformance",
    "title": "Random Forest with pipeline and hyperparameter tuning",
    "section": "3.5 Preformance",
    "text": "3.5 Preformance\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = ml_model.predict(X_test) #always gets x and retuns y\nY_pred_dt\n\n\narray([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n       0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n       0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n       1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,\n       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n       0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n       1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0])\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_test,Y_pred_dt)  \naccuracy\n\n\n0.7039106145251397\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_test,Y_pred_dt)  \nprecision_dt\n\n\n0.64\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_test,Y_pred_dt)  \nrecall_dt\n\n\n0.47761194029850745\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_test,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[94, 18],\n       [35, 32]])\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_test, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.6584488272921108",
    "crumbs": [
      "Classification",
      "Random Forest with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/6.2 Random Forest.html#k-fold-cross-validation",
    "href": "classification/6.2 Random Forest.html#k-fold-cross-validation",
    "title": "Random Forest with pipeline and hyperparameter tuning",
    "section": "3.6 k-Fold Cross-Validation",
    "text": "3.6 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(ml_model, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.7176696542893726",
    "crumbs": [
      "Classification",
      "Random Forest with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "data manipulation/2 siuba.html",
    "href": "data manipulation/2 siuba.html",
    "title": "Data manipulation with siuba",
    "section": "",
    "text": "siuba (小巴) is a port of dplyr and other R libraries with seamless support for pandas and SQL",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html#darkgrid",
    "href": "plot/1 seaborn.html#darkgrid",
    "title": "seaborn chart",
    "section": "10.1 darkgrid",
    "text": "10.1 darkgrid\n\n\nCode\nimport seaborn as sns\n\ndf = sns.load_dataset(\"tips\")\n\nsns.set_theme()\n# Equivalent to:\n# sns.set_style(\"darkgrid\")\n\nsns.boxplot(x = \"day\", y = \"total_bill\", data = df)",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html#darkgrid-1",
    "href": "plot/1 seaborn.html#darkgrid-1",
    "title": "seaborn chart",
    "section": "10.2 darkgrid",
    "text": "10.2 darkgrid\n\n\nCode\nimport seaborn as sns\n\ndf = sns.load_dataset(\"tips\")\nsns.set_style(\"whitegrid\")\n\nsns.boxplot(x = \"day\", y = \"total_bill\", data = df)",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html#darkgrid-themes",
    "href": "plot/1 seaborn.html#darkgrid-themes",
    "title": "seaborn chart",
    "section": "9.1 darkgrid themes",
    "text": "9.1 darkgrid themes\nIf you set the set_style function without any arguments the “darkgrid” theme will be used by default, which adds a gray background and white grid lines.\n\n\nCode\nimport seaborn as sns\n\ndf = sns.load_dataset(\"tips\")\n\nsns.set_theme()\n# Equivalent to:\n# sns.set_style(\"darkgrid\")\n\nsns.boxplot(x = \"day\", y = \"total_bill\", data = df)",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html#whitegrid-themes",
    "href": "plot/1 seaborn.html#whitegrid-themes",
    "title": "seaborn chart",
    "section": "9.2 whitegrid themes",
    "text": "9.2 whitegrid themes\nIf you want to add gray grid lines but with a white background set this theme.\n\n\nCode\nimport seaborn as sns\n\ndf = sns.load_dataset(\"tips\")\nsns.set_style(\"whitegrid\")\n\nsns.boxplot(x = \"day\", y = \"total_bill\", data = df)",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html#dark-themes",
    "href": "plot/1 seaborn.html#dark-themes",
    "title": "seaborn chart",
    "section": "9.3 dark themes",
    "text": "9.3 dark themes\nThe “dark” theme is the same as “darkgrid” but without the grid lines.\n\n\nCode\nimport seaborn as sns\n\ndf = sns.load_dataset(\"tips\")\n\nsns.set_style(\"dark\")\n\nsns.boxplot(x = \"day\", y = \"total_bill\", data = df)",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html#white-themes",
    "href": "plot/1 seaborn.html#white-themes",
    "title": "seaborn chart",
    "section": "9.4 white themes",
    "text": "9.4 white themes\nThe “white” theme is the same as “whitegrid” but without the gray grid lines.\n\n\nCode\nimport seaborn as sns\n\ndf = sns.load_dataset(\"tips\")\n\nsns.set_style(\"white\")\n\nsns.boxplot(x = \"day\", y = \"total_bill\", data = df)",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html#ticks-themes",
    "href": "plot/1 seaborn.html#ticks-themes",
    "title": "seaborn chart",
    "section": "9.5 ticks themes",
    "text": "9.5 ticks themes\nThe “ticks” theme is the same as the “white” theme but this theme adds ticks to the axes.\n\n\nCode\nimport seaborn as sns\n\ndf = sns.load_dataset(\"tips\")\n\nsns.set_style(\"ticks\")\n\nsns.boxplot(x = \"day\", y = \"total_bill\", data = df)",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html#add-title",
    "href": "plot/1 seaborn.html#add-title",
    "title": "seaborn chart",
    "section": "8.1 add title",
    "text": "8.1 add title\n\n\nCode\ndf = sns.load_dataset(\"tips\")\n\nax=sns.boxplot(x = \"day\", y = \"total_bill\", data = df)\n\nax.set_title(\"tips box plot \")\n\n\nText(0.5, 1.0, 'tips box plot ')",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html#adjust-size",
    "href": "plot/1 seaborn.html#adjust-size",
    "title": "seaborn chart",
    "section": "8.2 adjust size",
    "text": "8.2 adjust size\n\n\nCode\nplt.clf()\n\nplt.figure(figsize=(10, 6))\n\nax=sns.boxplot(x = \"day\", y = \"total_bill\", data = df)\nax.set_title(\"tips box plot \")\n\nplt.show()\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html#fivethirtyeight-themes",
    "href": "plot/1 seaborn.html#fivethirtyeight-themes",
    "title": "seaborn chart",
    "section": "9.6 fivethirtyeight themes",
    "text": "9.6 fivethirtyeight themes\n\n\nCode\nplt.clf()\n\nplt.style.use('fivethirtyeight')\n\nsns.boxplot(x = \"day\", y = \"total_bill\", data = df)\n\nplt.show()",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html#dark_background",
    "href": "plot/1 seaborn.html#dark_background",
    "title": "seaborn chart",
    "section": "9.9 dark_background",
    "text": "9.9 dark_background\n\n\nCode\nplt.clf()\nplt.style.use('dark_background')\n\nsns.boxplot(x = \"day\", y = \"total_bill\", data = df)\nfig.tight_layout()\nplt.show()",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html#tableau-colorblind1",
    "href": "plot/1 seaborn.html#tableau-colorblind1",
    "title": "seaborn chart",
    "section": "11.8 tableau-colorblind1",
    "text": "11.8 tableau-colorblind1\n\n\nCode\nmatplotlib.style.use('tableau-colorblind10')\n\nsns.boxplot(x = \"day\", y = \"total_bill\", data = df)\nfig.tight_layout()\nplt.show()",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html#tableau-colorblind10",
    "href": "plot/1 seaborn.html#tableau-colorblind10",
    "title": "seaborn chart",
    "section": "9.8 tableau-colorblind10",
    "text": "9.8 tableau-colorblind10\n\n\nCode\nplt.clf()\nplt.style.use('tableau-colorblind10')\n\nsns.boxplot(x = \"day\", y = \"total_bill\", data = df)\nfig.tight_layout()\nplt.show()",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html#ggplot",
    "href": "plot/1 seaborn.html#ggplot",
    "title": "seaborn chart",
    "section": "9.7 ggplot",
    "text": "9.7 ggplot\n\n\nCode\nplt.clf()\n\nplt.style.use('ggplot')\n\nsns.boxplot(x = \"day\", y = \"total_bill\", data = df)\nfig.tight_layout()\nplt.show()",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html#add-legend",
    "href": "plot/1 seaborn.html#add-legend",
    "title": "seaborn chart",
    "section": "10.3 add legend",
    "text": "10.3 add legend\n\n\nCode\ng = sns.FacetGrid(data=tips, col=\"day\",col_wrap=2, hue=\"sex\")\n\ng.map_dataframe(sns.scatterplot, x=\"total_bill\", y=\"tip\")\n\ng.add_legend()",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "plot/2 plotnine.html#xkcd",
    "href": "plot/2 plotnine.html#xkcd",
    "title": "plotnine chart",
    "section": "9.1 xkcd",
    "text": "9.1 xkcd\n\n\nCode\np=(\n    ggplot(data=tips)+aes(x=\"tip\",fill = 'sex')+ geom_histogram(position = 'dodge')+ theme_xkcd()\n)\n\np",
    "crumbs": [
      "Plot",
      "plotnine chart"
    ]
  },
  {
    "objectID": "plot/2 plotnine.html#theme_538",
    "href": "plot/2 plotnine.html#theme_538",
    "title": "plotnine chart",
    "section": "9.2 theme_538",
    "text": "9.2 theme_538\n\n\nCode\np=(\n    ggplot(data=tips)+aes(x=\"tip\",fill = 'sex')+ geom_histogram(position = 'dodge')+ theme_538()\n)\n\np",
    "crumbs": [
      "Plot",
      "plotnine chart"
    ]
  },
  {
    "objectID": "plot/2 plotnine.html#theme_dark",
    "href": "plot/2 plotnine.html#theme_dark",
    "title": "plotnine chart",
    "section": "9.3 theme_dark",
    "text": "9.3 theme_dark\n\n\nCode\np=(\n    ggplot(data=tips)+aes(x=\"tip\",fill = 'sex')+ geom_histogram(position = 'dodge')+ theme_dark()\n)\n\np",
    "crumbs": [
      "Plot",
      "plotnine chart"
    ]
  },
  {
    "objectID": "plot/2 plotnine.html#add-title",
    "href": "plot/2 plotnine.html#add-title",
    "title": "plotnine chart",
    "section": "8.1 add title",
    "text": "8.1 add title\n\n\nCode\np=(\n    ggplot(data=tips)+aes(x=\"tip\",fill = 'sex')+ geom_histogram(position = 'dodge')+ ggtitle(\"tip by sex\")+ scale_x_continuous(name=\"new x name\")+ scale_y_continuous(name=\"new y name\")\n)\n\np",
    "crumbs": [
      "Plot",
      "plotnine chart"
    ]
  },
  {
    "objectID": "plot/3 plotly.html#add-title",
    "href": "plot/3 plotly.html#add-title",
    "title": "Plotly chart",
    "section": "8.1 add title",
    "text": "8.1 add title\n\n\nCode\nfig = px.scatter(tips,x=\"tip\", y=\"total_bill\", title=\"total_bill title\").update_layout(title_x=0.5)\nfig.show()",
    "crumbs": [
      "Plot",
      "Plotly chart"
    ]
  },
  {
    "objectID": "plot/3 plotly.html#adjust-size",
    "href": "plot/3 plotly.html#adjust-size",
    "title": "Plotly chart",
    "section": "8.2 adjust size",
    "text": "8.2 adjust size\n\n\nCode\nfig = px.scatter(tips,x=\"tip\", y=\"total_bill\")\n\nfig.update_layout(\n    autosize=False\n    ,width=200\n    ,height=200\n    )\n    \nfig.show()",
    "crumbs": [
      "Plot",
      "Plotly chart"
    ]
  },
  {
    "objectID": "plot/3 plotly.html#ggplot2-themes",
    "href": "plot/3 plotly.html#ggplot2-themes",
    "title": "Plotly chart",
    "section": "9.1 ggplot2 themes",
    "text": "9.1 ggplot2 themes\n\n\nCode\nfig = px.scatter(tips,x=\"tip\", y=\"total_bill\",template=\"ggplot2\")\nfig.show()",
    "crumbs": [
      "Plot",
      "Plotly chart"
    ]
  },
  {
    "objectID": "plot/3 plotly.html#seaborn-themes",
    "href": "plot/3 plotly.html#seaborn-themes",
    "title": "Plotly chart",
    "section": "9.2 seaborn themes",
    "text": "9.2 seaborn themes\n\n\nCode\nfig = px.scatter(tips,x=\"tip\", y=\"total_bill\",template=\"seaborn\")\nfig.show()",
    "crumbs": [
      "Plot",
      "Plotly chart"
    ]
  },
  {
    "objectID": "plot/3 plotly.html#plotly_dark-themes",
    "href": "plot/3 plotly.html#plotly_dark-themes",
    "title": "Plotly chart",
    "section": "9.3 plotly_dark themes",
    "text": "9.3 plotly_dark themes\n\n\nCode\nfig = px.scatter(tips,x=\"tip\", y=\"total_bill\",template=\"plotly_dark\")\nfig.show()",
    "crumbs": [
      "Plot",
      "Plotly chart"
    ]
  },
  {
    "objectID": "plot/2 plotnine.html#adjust-size",
    "href": "plot/2 plotnine.html#adjust-size",
    "title": "plotnine chart",
    "section": "8.2 adjust size",
    "text": "8.2 adjust size\n\n\nCode\np=(\n    ggplot(data=tips)+aes(x=\"tip\",fill = 'sex')+ geom_histogram(position = 'dodge')+ ggtitle(\"tip by sex\")+ theme(figure_size=(4, 3)) \n)\n\np",
    "crumbs": [
      "Plot",
      "plotnine chart"
    ]
  },
  {
    "objectID": "classification/1.2 decision tree and hyperparameter tuning.html#save-model",
    "href": "classification/1.2 decision tree and hyperparameter tuning.html#save-model",
    "title": "Decision tree and hyperparameter tuning",
    "section": "3.7 save model",
    "text": "3.7 save model\n\n\nCode\nfrom joblib import dump, load\ndump(model_dt, 'trained_model.joblib') \n\n\n['trained_model.joblib']",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/1.2 decision tree and hyperparameter tuning.html#load-model",
    "href": "classification/1.2 decision tree and hyperparameter tuning.html#load-model",
    "title": "Decision tree and hyperparameter tuning",
    "section": "3.8 load model",
    "text": "3.8 load model\n\n\nCode\nmodel_dt_reload = load('trained_model.joblib') \n\n\n\n\nCode\nY_pred_dt = model_dt_reload.predict(X_test) #always gets x and retuns y\n\nY_pred_dt[0:5]\n\n\narray([0, 0, 0, 1, 0])",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/1.1 decision tree on titanic data.html#save-model",
    "href": "classification/1.1 decision tree on titanic data.html#save-model",
    "title": "Decision tree",
    "section": "3.5 save model",
    "text": "3.5 save model\n\n\nCode\nfrom joblib import dump, load\ndump(model_dt, 'trained_model.joblib') \n\n\n['trained_model.joblib']",
    "crumbs": [
      "Classification",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification/1.1 decision tree on titanic data.html#load-model",
    "href": "classification/1.1 decision tree on titanic data.html#load-model",
    "title": "Decision tree",
    "section": "3.6 load model",
    "text": "3.6 load model\n\n\nCode\nmodel_dt_reload = load('trained_model.joblib') \n\n\n\n\nCode\nY_pred_dt = model_dt_reload.predict(X_test) #always gets x and retuns y\n\nY_pred_dt[0:5]\n\n\narray([1, 0, 0, 1, 1])",
    "crumbs": [
      "Classification",
      "Decision tree"
    ]
  },
  {
    "objectID": "data manipulation/2 siuba.html#drop-column",
    "href": "data manipulation/2 siuba.html#drop-column",
    "title": "Data manipulation with siuba",
    "section": "5 drop column",
    "text": "5 drop column\n\n\nCode\nsmall_mtcars &gt;&gt; select(~_.cyl)\n\n\n\n\n\n\n\n\n\n\nmpg\nhp\n\n\n\n\n0\n21.0\n110\n\n\n1\n21.0\n110\n\n\n2\n22.8\n93\n\n\n3\n21.4\n110\n\n\n4\n18.7\n175",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "classification/6.2 Random Forest.html#categorical_cols-and-numerical_cols",
    "href": "classification/6.2 Random Forest.html#categorical_cols-and-numerical_cols",
    "title": "Random Forest with pipeline and hyperparameter tuning",
    "section": "2.5 categorical_cols and numerical_cols",
    "text": "2.5 categorical_cols and numerical_cols\n\n\nCode\ncategorical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].nunique() &lt; 10 and X_train[cname].dtype == \"object\"]\n                    \n                    \nnumerical_cols = numerical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].dtype in ['int64', 'float64']]\n\n\n\n\nCode\nprint(\"The total number of categorical columns:\", len(categorical_cols))\nprint(\"The total number of numerical columns:\", len(numerical_cols))\n\n\nThe total number of categorical columns: 0\nThe total number of numerical columns: 4\n\n\n\n\nCode\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train[my_cols].copy()\n#X_val = X_val[my_cols].copy()\nX_test= X_test[my_cols].copy()\n\n\n#X_final = df_test[my_cols].copy()",
    "crumbs": [
      "Classification",
      "Random Forest with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/6.2 Random Forest.html#pipelines-for-data-preprocessing",
    "href": "classification/6.2 Random Forest.html#pipelines-for-data-preprocessing",
    "title": "Random Forest with pipeline and hyperparameter tuning",
    "section": "2.6 Pipelines for Data Preprocessing",
    "text": "2.6 Pipelines for Data Preprocessing\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\nCode\nnumerical_transformer = Pipeline(steps=[\n    ('imputer_num', SimpleImputer(strategy='median')), \n    ('scaler', StandardScaler())\n])\n\n\n\n\nCode\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n\n\nCode\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)])",
    "crumbs": [
      "Classification",
      "Random Forest with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/6.2 Random Forest.html#define-pipline",
    "href": "classification/6.2 Random Forest.html#define-pipline",
    "title": "Random Forest with pipeline and hyperparameter tuning",
    "section": "3.2 define pipline",
    "text": "3.2 define pipline\n\n\nCode\npipeline = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', ml_model)\n         ]\n)",
    "crumbs": [
      "Classification",
      "Random Forest with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/6.2 Random Forest.html#hyperparameter-tuning-set",
    "href": "classification/6.2 Random Forest.html#hyperparameter-tuning-set",
    "title": "Random Forest with pipeline and hyperparameter tuning",
    "section": "3.3 hyperparameter tuning set",
    "text": "3.3 hyperparameter tuning set\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\nparameters = {'model__max_depth':[20,30,40],\n                 'model__n_estimators':[200,250,300],\n                 'model__min_samples_leaf':[1,2,3]\n                \n                 }\n                 \n              \nGridCV = GridSearchCV(pipeline, parameters\n                    ,scoring='accuracy'\n                    ,cv=10\n                    ,n_jobs= -1, verbose=1)",
    "crumbs": [
      "Classification",
      "Random Forest with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/6.2 Random Forest.html#save-model",
    "href": "classification/6.2 Random Forest.html#save-model",
    "title": "Random Forest with pipeline and hyperparameter tuning",
    "section": "3.7 save model",
    "text": "3.7 save model\n\n\nCode\nfrom joblib import dump, load\ndump(ml_model, 'trained_model_6_2.joblib') \n\n\n['trained_model_6_2.joblib']\n\n\nsave trained pipeline\n\n\nCode\nfrom joblib import dump, load\ndump(GridCV, 'trained_pipeline_6_2.pkl', compress=True)  \n\n\n['trained_pipeline_6_2.pkl']",
    "crumbs": [
      "Classification",
      "Random Forest with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/6.2 Random Forest.html#load-model",
    "href": "classification/6.2 Random Forest.html#load-model",
    "title": "Random Forest with pipeline and hyperparameter tuning",
    "section": "3.8 load model",
    "text": "3.8 load model\n\n\nCode\nmodel_dt_reload = load('trained_model_6_2.joblib') \n\n\n\n\nCode\nY_pred_dt = model_dt_reload.predict(X_test) #always gets x and retuns y\n\nY_pred_dt[0:5]\n\n\narray([0, 0, 0, 1, 1])",
    "crumbs": [
      "Classification",
      "Random Forest with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/6.2 Random Forest.html#final-prediction",
    "href": "classification/6.2 Random Forest.html#final-prediction",
    "title": "Random Forest with pipeline and hyperparameter tuning",
    "section": "3.9 final prediction",
    "text": "3.9 final prediction",
    "crumbs": [
      "Classification",
      "Random Forest with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/3 Random forest on house price data.html#categorical_cols-and-numerical_cols",
    "href": "regression/3 Random forest on house price data.html#categorical_cols-and-numerical_cols",
    "title": "Random forest and pipeline",
    "section": "2.5 categorical_cols and numerical_cols",
    "text": "2.5 categorical_cols and numerical_cols\n\n\nCode\ncategorical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].nunique() &lt; 10 and X_train[cname].dtype == \"object\"]\n                    \n                    \nnumerical_cols = numerical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].dtype in ['int64', 'float64']]\n\n\n\n\nCode\nprint(\"The total number of categorical columns:\", len(categorical_cols))\nprint(\"The total number of numerical columns:\", len(numerical_cols))\n\n\nThe total number of categorical columns: 40\nThe total number of numerical columns: 37\n\n\n\n\nCode\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train[my_cols].copy()\nX_test= X_test[my_cols].copy()\n\n\n#X_final = df_test[my_cols].copy()",
    "crumbs": [
      "Regression",
      "Random forest and pipeline"
    ]
  },
  {
    "objectID": "classification/7 XGboost.html",
    "href": "classification/7 XGboost.html",
    "title": "XGboost with pipeline",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nimport time",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7 XGboost.html#input-data",
    "href": "classification/7 XGboost.html#input-data",
    "title": "XGboost with pipeline",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7 XGboost.html#data-eda",
    "href": "classification/7 XGboost.html#data-eda",
    "title": "XGboost with pipeline",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n\n\n0.7420382165605095\n0.18890814558058924",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7 XGboost.html#data-wrangling",
    "href": "classification/7 XGboost.html#data-wrangling",
    "title": "XGboost with pipeline",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1046 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1308 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Dealing with missing numerical variables\ndata['Age'] = data.Age.fillna(data.Age.median())\ndata['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1309 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1309 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\ndata = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\nSex_male\n\n\n\n\n0\n1\n3\nBraund, Mr. Owen Harris\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\nTrue\n\n\n1\n2\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\nFalse\n\n\n2\n3\n3\nHeikkinen, Miss. Laina\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\nFalse\n\n\n3\n4\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\nFalse\n\n\n4\n5\n3\nAllen, Mr. William Henry\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\nTrue\n\n\n\n\n\n\n\n\n\n\nCode\n# Select features columns\ndata = data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()\n\n\n\n\n\n\n\n\n\n\nSex_male\nFare\nAge\nPclass\nSibSp\nrole\n\n\n\n\n0\nTrue\n7.2500\n22.0\n3\n1\ntrain\n\n\n1\nFalse\n71.2833\n38.0\n1\n1\ntrain\n\n\n2\nFalse\n7.9250\n26.0\n3\n0\ntrain\n\n\n3\nFalse\n53.1000\n35.0\n1\n1\ntrain\n\n\n4\nTrue\n8.0500\n35.0\n3\n0\ntrain",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7 XGboost.html#split-data",
    "href": "classification/7 XGboost.html#split-data",
    "title": "XGboost with pipeline",
    "section": "2.4 split data",
    "text": "2.4 split data\n60% training / 30% validation/ 10% testing\n\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\n#X_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\n\ntraining_size=0.6\nvalidation_size=0.3\ntesting_size=0.1\n\n\nX_train, X_val, Y_train, Y_val= train_test_split(X, Y, test_size=validation_size, random_state=1)\n\n\nX_train, X_test, Y_train, Y_test= train_test_split(X_train, Y_train, test_size=testing_size/training_size, random_state=1) \n    \n\nX_train = X_train.drop('role', axis=1)\nX_val = X_val.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 519 entries, 129 to 818\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Sex_male  519 non-null    bool   \n 1   Fare      519 non-null    float64\n 2   Age       519 non-null    float64\n 3   Pclass    519 non-null    int64  \n 4   SibSp     519 non-null    int64  \ndtypes: bool(1), float64(2), int64(2)\nmemory usage: 20.8 KB\n\n\n\n\nCode\nlen(X_train)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.5824915824915825\n\n\n\n\nCode\nlen(X_val)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.30078563411896747\n\n\n\n\nCode\nlen(X_test)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.11672278338945005",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7 XGboost.html#categorical_cols-and-numerical_cols",
    "href": "classification/7 XGboost.html#categorical_cols-and-numerical_cols",
    "title": "XGboost with pipeline",
    "section": "2.5 categorical_cols and numerical_cols",
    "text": "2.5 categorical_cols and numerical_cols\n\n\nCode\ncategorical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].nunique() &lt; 10 and X_train[cname].dtype == \"object\"]\n                    \n                    \nnumerical_cols = numerical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].dtype in ['int64', 'float64']]\n\n\n\n\nCode\nprint(\"The total number of categorical columns:\", len(categorical_cols))\nprint(\"The total number of numerical columns:\", len(numerical_cols))\n\n\nThe total number of categorical columns: 0\nThe total number of numerical columns: 4\n\n\n\n\nCode\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train[my_cols].copy()\nX_val = X_val[my_cols].copy()\nX_test= X_test[my_cols].copy()\n\n\n#X_final = df_test[my_cols].copy()",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7 XGboost.html#pipelines-for-data-preprocessing",
    "href": "classification/7 XGboost.html#pipelines-for-data-preprocessing",
    "title": "XGboost with pipeline",
    "section": "2.6 Pipelines for Data Preprocessing",
    "text": "2.6 Pipelines for Data Preprocessing\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\nCode\nnumerical_transformer = Pipeline(steps=[\n    ('imputer_num', SimpleImputer(strategy='median')), \n    ('scaler', StandardScaler())\n])\n\n\n\n\nCode\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n\n\nCode\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)])",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7 XGboost.html#define-model",
    "href": "classification/7 XGboost.html#define-model",
    "title": "XGboost with pipeline",
    "section": "3.1 define model",
    "text": "3.1 define model\n\n\nCode\nimport xgboost\nprint(xgboost.__version__)\n\n\n2.0.3\n\n\n\n\nCode\nfrom xgboost import XGBClassifier\nml_model = XGBClassifier()\nml_model\n\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. XGBClassifieriNot fittedXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...)",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7 XGboost.html#define-pipline",
    "href": "classification/7 XGboost.html#define-pipline",
    "title": "XGboost with pipeline",
    "section": "3.2 define pipline",
    "text": "3.2 define pipline\n\n\nCode\npipeline = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', ml_model)\n         ]\n)",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7 XGboost.html#train-model",
    "href": "classification/7 XGboost.html#train-model",
    "title": "XGboost with pipeline",
    "section": "3.3 train model",
    "text": "3.3 train model\n\n\nCode\nstart_time = time.time()\n\npipeline.fit(X_train, Y_train)\n\nend_time = time.time()\nduration = end_time - start_time\nduration\n\n\n0.04808187484741211",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7 XGboost.html#preformance",
    "href": "classification/7 XGboost.html#preformance",
    "title": "XGboost with pipeline",
    "section": "3.4 Preformance",
    "text": "3.4 Preformance\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = pipeline.predict(X_val) #always gets x and retuns y\nY_pred_dt\n\n\narray([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,\n       1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n       1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n       0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n       1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n       1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,\n       0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,\n       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n       1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n       0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n       1, 1, 1, 1])\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_val,Y_pred_dt)  \naccuracy\n\n\n0.6865671641791045\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_val,Y_pred_dt)  \nprecision_dt\n\n\n0.6631578947368421\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_val,Y_pred_dt)  \nrecall_dt\n\n\n0.5478260869565217\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_val,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[121,  32],\n       [ 52,  63]])\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_val, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.6693378800795681",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7 XGboost.html#k-fold-cross-validation",
    "href": "classification/7 XGboost.html#k-fold-cross-validation",
    "title": "XGboost with pipeline",
    "section": "3.5 k-Fold Cross-Validation",
    "text": "3.5 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(ml_model, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.6589245705750562",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7 XGboost.html#save-model",
    "href": "classification/7 XGboost.html#save-model",
    "title": "XGboost with pipeline",
    "section": "3.6 save model",
    "text": "3.6 save model\n\n\nCode\nfrom joblib import dump, load\ndump(ml_model, 'trained_model_7.joblib') \n\n\n['trained_model_7.joblib']\n\n\nsave trained pipeline\n\n\nCode\nfrom joblib import dump, load\ndump(pipeline, 'trained_pipeline_7.pkl', compress=True)  \n\n\n['trained_pipeline_7.pkl']",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7 XGboost.html#load-model",
    "href": "classification/7 XGboost.html#load-model",
    "title": "XGboost with pipeline",
    "section": "3.7 load model",
    "text": "3.7 load model\n\n\nCode\nmodel_dt_reload = load('trained_model_7.joblib')",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7 XGboost.html#final-prediction",
    "href": "classification/7 XGboost.html#final-prediction",
    "title": "XGboost with pipeline",
    "section": "3.8 final prediction",
    "text": "3.8 final prediction\n\n\nCode\nY_pred_dt_final = model_dt_reload.predict(X_test) #always gets x and retuns y\n\nY_pred_dt_final[0:5]\n\n\narray([0, 0, 0, 0, 0])",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost.html",
    "href": "classification/7.1 XGboost.html",
    "title": "XGboost with pipeline",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nimport time",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost.html#input-data",
    "href": "classification/7.1 XGboost.html#input-data",
    "title": "XGboost with pipeline",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost.html#data-eda",
    "href": "classification/7.1 XGboost.html#data-eda",
    "title": "XGboost with pipeline",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n\n\n0.7420382165605095\n0.18890814558058924",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost.html#data-wrangling",
    "href": "classification/7.1 XGboost.html#data-wrangling",
    "title": "XGboost with pipeline",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1046 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1308 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Dealing with missing numerical variables\ndata['Age'] = data.Age.fillna(data.Age.median())\ndata['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1309 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1309 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\ndata = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\nSex_male\n\n\n\n\n0\n1\n3\nBraund, Mr. Owen Harris\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\nTrue\n\n\n1\n2\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\nFalse\n\n\n2\n3\n3\nHeikkinen, Miss. Laina\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\nFalse\n\n\n3\n4\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\nFalse\n\n\n4\n5\n3\nAllen, Mr. William Henry\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\nTrue\n\n\n\n\n\n\n\n\n\n\nCode\n# Select features columns\ndata = data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()\n\n\n\n\n\n\n\n\n\n\nSex_male\nFare\nAge\nPclass\nSibSp\nrole\n\n\n\n\n0\nTrue\n7.2500\n22.0\n3\n1\ntrain\n\n\n1\nFalse\n71.2833\n38.0\n1\n1\ntrain\n\n\n2\nFalse\n7.9250\n26.0\n3\n0\ntrain\n\n\n3\nFalse\n53.1000\n35.0\n1\n1\ntrain\n\n\n4\nTrue\n8.0500\n35.0\n3\n0\ntrain",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost.html#split-data",
    "href": "classification/7.1 XGboost.html#split-data",
    "title": "XGboost with pipeline",
    "section": "2.4 split data",
    "text": "2.4 split data\n60% training / 30% validation/ 10% testing\n\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\n#X_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\n\ntraining_size=0.6\nvalidation_size=0.3\ntesting_size=0.1\n\n\nX_train, X_val, Y_train, Y_val= train_test_split(X, Y, test_size=validation_size, random_state=1)\n\n\nX_train, X_test, Y_train, Y_test= train_test_split(X_train, Y_train, test_size=testing_size/training_size, random_state=1) \n    \n\nX_train = X_train.drop('role', axis=1)\nX_val = X_val.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 519 entries, 129 to 818\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Sex_male  519 non-null    bool   \n 1   Fare      519 non-null    float64\n 2   Age       519 non-null    float64\n 3   Pclass    519 non-null    int64  \n 4   SibSp     519 non-null    int64  \ndtypes: bool(1), float64(2), int64(2)\nmemory usage: 20.8 KB\n\n\n\n\nCode\nlen(X_train)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.5824915824915825\n\n\n\n\nCode\nlen(X_val)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.30078563411896747\n\n\n\n\nCode\nlen(X_test)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.11672278338945005",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost.html#categorical_cols-and-numerical_cols",
    "href": "classification/7.1 XGboost.html#categorical_cols-and-numerical_cols",
    "title": "XGboost with pipeline",
    "section": "2.5 categorical_cols and numerical_cols",
    "text": "2.5 categorical_cols and numerical_cols\n\n\nCode\ncategorical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].nunique() &lt; 10 and X_train[cname].dtype == \"object\"]\n                    \n                    \nnumerical_cols = numerical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].dtype in ['int64', 'float64']]\n\n\n\n\nCode\nprint(\"The total number of categorical columns:\", len(categorical_cols))\nprint(\"The total number of numerical columns:\", len(numerical_cols))\n\n\nThe total number of categorical columns: 0\nThe total number of numerical columns: 4\n\n\n\n\nCode\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train[my_cols].copy()\nX_val = X_val[my_cols].copy()\nX_test= X_test[my_cols].copy()\n\n\n#X_final = df_test[my_cols].copy()",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost.html#pipelines-for-data-preprocessing",
    "href": "classification/7.1 XGboost.html#pipelines-for-data-preprocessing",
    "title": "XGboost with pipeline",
    "section": "2.6 Pipelines for Data Preprocessing",
    "text": "2.6 Pipelines for Data Preprocessing\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\nCode\nnumerical_transformer = Pipeline(steps=[\n    ('imputer_num', SimpleImputer(strategy='median')), \n    ('scaler', StandardScaler())\n])\n\n\n\n\nCode\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n\n\nCode\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)])",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost.html#define-model",
    "href": "classification/7.1 XGboost.html#define-model",
    "title": "XGboost with pipeline",
    "section": "3.1 define model",
    "text": "3.1 define model\n\n\nCode\nimport xgboost\nprint(xgboost.__version__)\n\n\n2.0.3\n\n\n\n\nCode\nfrom xgboost import XGBClassifier\nml_model = XGBClassifier()\nml_model\n\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. XGBClassifieriNot fittedXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...)",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost.html#define-pipline",
    "href": "classification/7.1 XGboost.html#define-pipline",
    "title": "XGboost with pipeline",
    "section": "3.2 define pipline",
    "text": "3.2 define pipline\n\n\nCode\npipeline = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', ml_model)\n         ]\n)",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost.html#train-model",
    "href": "classification/7.1 XGboost.html#train-model",
    "title": "XGboost with pipeline",
    "section": "3.3 train model",
    "text": "3.3 train model\n\n\nCode\nstart_time = time.time()\n\npipeline.fit(X_train, Y_train)\n\nend_time = time.time()\nduration = end_time - start_time\nduration\n\n\n0.06467294692993164",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost.html#preformance",
    "href": "classification/7.1 XGboost.html#preformance",
    "title": "XGboost with pipeline",
    "section": "3.4 Preformance",
    "text": "3.4 Preformance\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = pipeline.predict(X_val) #always gets x and retuns y\nY_pred_dt\n\n\narray([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,\n       1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n       1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n       0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n       1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n       1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,\n       0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,\n       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n       1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n       0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n       1, 1, 1, 1])\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_val,Y_pred_dt)  \naccuracy\n\n\n0.6865671641791045\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_val,Y_pred_dt)  \nprecision_dt\n\n\n0.6631578947368421\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_val,Y_pred_dt)  \nrecall_dt\n\n\n0.5478260869565217\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_val,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[121,  32],\n       [ 52,  63]])\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_val, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.6693378800795681",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost.html#k-fold-cross-validation",
    "href": "classification/7.1 XGboost.html#k-fold-cross-validation",
    "title": "XGboost with pipeline",
    "section": "3.5 k-Fold Cross-Validation",
    "text": "3.5 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(ml_model, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.6974794622852876",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost.html#save-model",
    "href": "classification/7.1 XGboost.html#save-model",
    "title": "XGboost with pipeline",
    "section": "3.6 save model",
    "text": "3.6 save model\n\n\nCode\nfrom joblib import dump, load\ndump(ml_model, 'trained_model_7.joblib') \n\n\n['trained_model_7.joblib']\n\n\nsave trained pipeline\n\n\nCode\nfrom joblib import dump, load\ndump(pipeline, 'trained_pipeline_7.pkl', compress=True)  \n\n\n['trained_pipeline_7.pkl']",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost.html#load-model",
    "href": "classification/7.1 XGboost.html#load-model",
    "title": "XGboost with pipeline",
    "section": "3.7 load model",
    "text": "3.7 load model\n\n\nCode\nmodel_dt_reload = load('trained_model_7.joblib')",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost.html#final-prediction",
    "href": "classification/7.1 XGboost.html#final-prediction",
    "title": "XGboost with pipeline",
    "section": "3.8 final prediction",
    "text": "3.8 final prediction\n\n\nCode\nY_pred_dt_final = model_dt_reload.predict(X_test) #always gets x and retuns y\n\nY_pred_dt_final[0:5]\n\n\narray([0, 0, 0, 0, 0])",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost.html",
    "href": "classification/7.2 XGboost.html",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nimport time",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost.html#input-data",
    "href": "classification/7.2 XGboost.html#input-data",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\n\n\n# Showing overview of the train dataset\ndf_train.head()",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost.html#data-eda",
    "href": "classification/7.2 XGboost.html#data-eda",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\nCode\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost.html#data-wrangling",
    "href": "classification/7.2 XGboost.html#data-wrangling",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n\n\nCode\n# Dealing with missing numerical variables\ndata['Age'] = data.Age.fillna(data.Age.median())\ndata['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\ndata = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\nCode\n# Select features columns\ndata = data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost.html#split-data",
    "href": "classification/7.2 XGboost.html#split-data",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "2.4 split data",
    "text": "2.4 split data\n60% training / 30% validation/ 10% testing\n\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\n#X_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\n\ntraining_size=0.6\nvalidation_size=0.3\ntesting_size=0.1\n\n\nX_train, X_val, Y_train, Y_val= train_test_split(X, Y, test_size=validation_size, random_state=1)\n\n\nX_train, X_test, Y_train, Y_test= train_test_split(X_train, Y_train, test_size=testing_size/training_size, random_state=1) \n    \n\nX_train = X_train.drop('role', axis=1)\nX_val = X_val.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n\n\nCode\nlen(X_train)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n\n\nCode\nlen(X_val)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n\n\nCode\nlen(X_test)/(len(X_train) +len(X_val) +len(X_test) )",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost.html#categorical_cols-and-numerical_cols",
    "href": "classification/7.2 XGboost.html#categorical_cols-and-numerical_cols",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "2.5 categorical_cols and numerical_cols",
    "text": "2.5 categorical_cols and numerical_cols\n\n\nCode\ncategorical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].nunique() &lt; 10 and X_train[cname].dtype == \"object\"]\n                    \n                    \nnumerical_cols = numerical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].dtype in ['int64', 'float64']]\n\n\n\n\nCode\nprint(\"The total number of categorical columns:\", len(categorical_cols))\nprint(\"The total number of numerical columns:\", len(numerical_cols))\n\n\n\n\nCode\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train[my_cols].copy()\nX_val = X_val[my_cols].copy()\nX_test= X_test[my_cols].copy()\n\n\n#X_final = df_test[my_cols].copy()",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost.html#pipelines-for-data-preprocessing",
    "href": "classification/7.2 XGboost.html#pipelines-for-data-preprocessing",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "2.6 Pipelines for Data Preprocessing",
    "text": "2.6 Pipelines for Data Preprocessing\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\nCode\nnumerical_transformer = Pipeline(steps=[\n    ('imputer_num', SimpleImputer(strategy='median')), \n    ('scaler', StandardScaler())\n])\n\n\n\n\nCode\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n\n\nCode\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)])",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost.html#define-model",
    "href": "classification/7.2 XGboost.html#define-model",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "3.1 define model",
    "text": "3.1 define model\n\n\nCode\nimport xgboost\nprint(xgboost.__version__)\n\n\n\n\nCode\nfrom xgboost import XGBClassifier\nml_model = XGBClassifier()\nml_model",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost.html#define-pipline",
    "href": "classification/7.2 XGboost.html#define-pipline",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "3.2 define pipline",
    "text": "3.2 define pipline\n\n\nCode\npipeline = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', ml_model)\n         ]\n)",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost.html#define-hyperparameter-tuning-set",
    "href": "classification/7.2 XGboost.html#define-hyperparameter-tuning-set",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "3.3 define hyperparameter tuning set",
    "text": "3.3 define hyperparameter tuning set\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\n\n\nparameters = {\n        'model__learning_rate': [0.01, 0.02,0.08,0.1],\n        'model__max_depth': [3, 5, 7,8,9,10,20],\n        'model__min_child_weight': [1, 3,5,8],\n        'model__subsample': [0.5, 0.7,0.9],\n        \n       # 'model__colsample__bytree': [0.5, 0.7],\n       \n        'model__n_estimators' : [100, 200, 500],\n        'model__objective': ['reg:squarederror']\n    }\n\n\nall parameters combinations\n\n\nCode\nimport itertools\na = parameters.values()\ncombinations = list(itertools.product(*a))\n\n\n1440 combinations\n\n\nCode\nlen(combinations)\n\n\n\n\nCode\ncombinations[0:5]\n\n\n\n\nCode\nGridCV = GridSearchCV(pipeline\n                ,parameters\n                ,scoring='accuracy'\n                , cv=10, n_jobs=-1)",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost.html#train-model",
    "href": "classification/7.2 XGboost.html#train-model",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "3.4 train model",
    "text": "3.4 train model\n\n\nCode\nstart_time = time.time()\n\nGridCV.fit(X_train, Y_train)\n\nend_time = time.time()\nduration = end_time - start_time\nduration\n\n\n\n3.4.0.1 tunning result\n\n3.4.0.1.1 GridSearchCV\n\n\nCode\n# get the parameter names\ncolumn_results = [f\"param_{name}\" for name in parameters.keys()]\ncolumn_results += [\"mean_test_score\", \"std_test_score\", \"rank_test_score\"]\n\ncv_results = pd.DataFrame(GridCV.cv_results_)\ncv_results = cv_results[column_results].sort_values(\n    \"mean_test_score\", ascending=False\n)\n\n\ndef shorten_param(param_name):\n    if \"__\" in param_name:\n        return param_name.rsplit(\"__\", 1)[1]\n    return param_name\n\n\ncv_results = cv_results.rename(shorten_param, axis=1)\ncv_results.head()\n\n\n\n\n\n3.4.0.2 tunning best parameters\n\n\nCode\nGridCV.best_params_\n\n\n\n\n3.4.0.3 tunning best model\n\n\nCode\nmodel_ml = GridCV.best_estimator_",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost.html#preformance",
    "href": "classification/7.2 XGboost.html#preformance",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "3.5 Preformance",
    "text": "3.5 Preformance\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = model_ml.predict(X_val) #always gets x and retuns y\nY_pred_dt\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_val,Y_pred_dt)  \naccuracy\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_val,Y_pred_dt)  \nprecision_dt\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_val,Y_pred_dt)  \nrecall_dt\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_val,Y_pred_dt)\nconfusion_matrix_dt\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_val, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost.html#k-fold-cross-validation",
    "href": "classification/7.2 XGboost.html#k-fold-cross-validation",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "3.6 k-Fold Cross-Validation",
    "text": "3.6 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(ml_model, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost.html#save-model",
    "href": "classification/7.2 XGboost.html#save-model",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "3.7 save model",
    "text": "3.7 save model\n\n\nCode\nfrom joblib import dump, load\ndump(model_ml, 'trained_model_7_2.joblib') \n\n\nsave trained pipeline\n\n\nCode\nfrom joblib import dump, load\ndump(GridCV, 'trained_GridCV_7_2.joblib', compress=True)",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost.html#load-model",
    "href": "classification/7.2 XGboost.html#load-model",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "3.8 load model",
    "text": "3.8 load model\n\n\nCode\nmodel_dt_reload = load('trained_model_7_2.joblib') \n\n\nload grid\n\n\nCode\nGrid_reload = load('trained_GridCV_7_2.joblib')",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost.html#final-prediction",
    "href": "classification/7.2 XGboost.html#final-prediction",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "3.9 final prediction",
    "text": "3.9 final prediction\n\n\nCode\nY_pred_dt_final = model_dt_reload.predict(X_test) #always gets x and retuns y\n\n#Y_pred_dt_final[0:5]",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost copy.html",
    "href": "classification/7.2 XGboost copy.html",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nimport time",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost copy.html#input-data",
    "href": "classification/7.2 XGboost copy.html#input-data",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\n\n\n# Showing overview of the train dataset\ndf_train.head()",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost copy.html#data-eda",
    "href": "classification/7.2 XGboost copy.html#data-eda",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\nCode\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost copy.html#data-wrangling",
    "href": "classification/7.2 XGboost copy.html#data-wrangling",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n\n\nCode\n# Dealing with missing numerical variables\ndata['Age'] = data.Age.fillna(data.Age.median())\ndata['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\ndata = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\nCode\n# Select features columns\ndata = data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost copy.html#split-data",
    "href": "classification/7.2 XGboost copy.html#split-data",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "2.4 split data",
    "text": "2.4 split data\n60% training / 30% validation/ 10% testing\n\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\n#X_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\n\ntraining_size=0.6\nvalidation_size=0.3\ntesting_size=0.1\n\n\nX_train, X_val, Y_train, Y_val= train_test_split(X, Y, test_size=validation_size, random_state=1)\n\n\nX_train, X_test, Y_train, Y_test= train_test_split(X_train, Y_train, test_size=testing_size/training_size, random_state=1) \n    \n\nX_train = X_train.drop('role', axis=1)\nX_val = X_val.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n\n\nCode\nlen(X_train)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n\n\nCode\nlen(X_val)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n\n\nCode\nlen(X_test)/(len(X_train) +len(X_val) +len(X_test) )",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost copy.html#categorical_cols-and-numerical_cols",
    "href": "classification/7.2 XGboost copy.html#categorical_cols-and-numerical_cols",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "2.5 categorical_cols and numerical_cols",
    "text": "2.5 categorical_cols and numerical_cols\n\n\nCode\ncategorical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].nunique() &lt; 10 and X_train[cname].dtype == \"object\"]\n                    \n                    \nnumerical_cols = numerical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].dtype in ['int64', 'float64']]\n\n\n\n\nCode\nprint(\"The total number of categorical columns:\", len(categorical_cols))\nprint(\"The total number of numerical columns:\", len(numerical_cols))\n\n\n\n\nCode\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train[my_cols].copy()\nX_val = X_val[my_cols].copy()\nX_test= X_test[my_cols].copy()\n\n\n#X_final = df_test[my_cols].copy()",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost copy.html#pipelines-for-data-preprocessing",
    "href": "classification/7.2 XGboost copy.html#pipelines-for-data-preprocessing",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "2.6 Pipelines for Data Preprocessing",
    "text": "2.6 Pipelines for Data Preprocessing\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\nCode\nnumerical_transformer = Pipeline(steps=[\n    ('imputer_num', SimpleImputer(strategy='median')), \n    ('scaler', StandardScaler())\n])\n\n\n\n\nCode\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n\n\nCode\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)])",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost copy.html#define-model",
    "href": "classification/7.2 XGboost copy.html#define-model",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "3.1 define model",
    "text": "3.1 define model\n\n\nCode\nimport xgboost\nprint(xgboost.__version__)\n\n\n\n\nCode\nfrom xgboost import XGBClassifier\nml_model = XGBClassifier()\nml_model",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost copy.html#define-pipline",
    "href": "classification/7.2 XGboost copy.html#define-pipline",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "3.2 define pipline",
    "text": "3.2 define pipline\n\n\nCode\npipeline = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', ml_model)\n         ]\n)",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost copy.html#define-hyperparameter-tuning-set",
    "href": "classification/7.2 XGboost copy.html#define-hyperparameter-tuning-set",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "3.3 define hyperparameter tuning set",
    "text": "3.3 define hyperparameter tuning set\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\n\n\nparameters = {\n        'model__learning_rate': [0.01, 0.02,0.08,0.1],\n        'model__max_depth': [3, 5, 7,8,9,10,15,20],\n        'model__min_child_weight': [1, 3,5,8,10],\n        'model__subsample': [0.5, 0.7,0.9],\n       # 'model__colsample__bytree': [0.5, 0.7],\n        'model__n_estimators' : [100, 200, 500],\n        'model__objective': ['reg:squarederror']\n    }\n\n\nall parameters combinations\n\n\nCode\nimport itertools\na = parameters.values()\ncombinations = list(itertools.product(*a))\n\n\n1440 combinations\n\n\nCode\nlen(combinations)\n\n\n\n\nCode\ncombinations[0:5]\n\n\n\n\nCode\nGridCV = GridSearchCV(pipeline\n                ,parameters\n                ,scoring='accuracy'\n                , cv=10, n_jobs=-1)",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost copy.html#train-model",
    "href": "classification/7.2 XGboost copy.html#train-model",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "3.4 train model",
    "text": "3.4 train model\n\n\nCode\nstart_time = time.time()\n\nGridCV.fit(X_train, Y_train)\n\nend_time = time.time()\nduration = end_time - start_time\nduration\n\n\n\n3.4.0.1 tunning result\n\n3.4.0.1.1 GridSearchCV\n\n\nCode\n# get the parameter names\ncolumn_results = [f\"param_{name}\" for name in parameters.keys()]\ncolumn_results += [\"mean_test_score\", \"std_test_score\", \"rank_test_score\"]\n\ncv_results = pd.DataFrame(GridCV.cv_results_)\ncv_results = cv_results[column_results].sort_values(\n    \"mean_test_score\", ascending=False\n)\n\n\ndef shorten_param(param_name):\n    if \"__\" in param_name:\n        return param_name.rsplit(\"__\", 1)[1]\n    return param_name\n\n\ncv_results = cv_results.rename(shorten_param, axis=1)\ncv_results.head()\n\n\n\n\n\n3.4.0.2 tunning best parameters\n\n\nCode\nGridCV.best_params_\n\n\n\n\n3.4.0.3 tunning best model\n\n\nCode\nmodel_ml = GridCV.best_estimator_",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost copy.html#preformance",
    "href": "classification/7.2 XGboost copy.html#preformance",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "3.5 Preformance",
    "text": "3.5 Preformance\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = model_ml.predict(X_val) #always gets x and retuns y\nY_pred_dt\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_val,Y_pred_dt)  \naccuracy\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_val,Y_pred_dt)  \nprecision_dt\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_val,Y_pred_dt)  \nrecall_dt\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_val,Y_pred_dt)\nconfusion_matrix_dt\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_val, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost copy.html#k-fold-cross-validation",
    "href": "classification/7.2 XGboost copy.html#k-fold-cross-validation",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "3.6 k-Fold Cross-Validation",
    "text": "3.6 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(ml_model, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost copy.html#save-model",
    "href": "classification/7.2 XGboost copy.html#save-model",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "3.7 save model",
    "text": "3.7 save model\n\n\nCode\nfrom joblib import dump, load\ndump(ml_model, 'trained_model_7_2.joblib') \n\n\nsave trained pipeline\n\n\nCode\nfrom joblib import dump, load\ndump(GridCV, 'trained_GridCV_7_2.pkl', compress=True)",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost copy.html#load-model",
    "href": "classification/7.2 XGboost copy.html#load-model",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "3.8 load model",
    "text": "3.8 load model\n\n\nCode\nmodel_dt_reload = load('trained_model_7_2.joblib')",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost copy.html#final-prediction",
    "href": "classification/7.2 XGboost copy.html#final-prediction",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "3.9 final prediction",
    "text": "3.9 final prediction\n\n\nCode\nY_pred_dt_final = model_dt_reload.predict(X_test) #always gets x and retuns y\n\nY_pred_dt_final[0:5]",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost.html",
    "href": "classification/7.3 XGboost.html",
    "title": "XGboost with pipeline and fast tuning",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nimport time",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost.html#input-data",
    "href": "classification/7.3 XGboost.html#input-data",
    "title": "XGboost with pipeline and fast tuning",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost.html#data-eda",
    "href": "classification/7.3 XGboost.html#data-eda",
    "title": "XGboost with pipeline and fast tuning",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n\n\n0.7420382165605095\n0.18890814558058924",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost.html#data-wrangling",
    "href": "classification/7.3 XGboost.html#data-wrangling",
    "title": "XGboost with pipeline and fast tuning",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1046 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1308 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Dealing with missing numerical variables\ndata['Age'] = data.Age.fillna(data.Age.median())\ndata['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1309 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1309 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\ndata = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\nSex_male\n\n\n\n\n0\n1\n3\nBraund, Mr. Owen Harris\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\nTrue\n\n\n1\n2\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\nFalse\n\n\n2\n3\n3\nHeikkinen, Miss. Laina\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\nFalse\n\n\n3\n4\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\nFalse\n\n\n4\n5\n3\nAllen, Mr. William Henry\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\nTrue\n\n\n\n\n\n\n\n\n\n\nCode\n# Select features columns\ndata = data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()\n\n\n\n\n\n\n\n\n\n\nSex_male\nFare\nAge\nPclass\nSibSp\nrole\n\n\n\n\n0\nTrue\n7.2500\n22.0\n3\n1\ntrain\n\n\n1\nFalse\n71.2833\n38.0\n1\n1\ntrain\n\n\n2\nFalse\n7.9250\n26.0\n3\n0\ntrain\n\n\n3\nFalse\n53.1000\n35.0\n1\n1\ntrain\n\n\n4\nTrue\n8.0500\n35.0\n3\n0\ntrain",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost.html#split-data",
    "href": "classification/7.3 XGboost.html#split-data",
    "title": "XGboost with pipeline and fast tuning",
    "section": "2.4 split data",
    "text": "2.4 split data\n60% training / 30% validation/ 10% testing\n\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\n#X_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\n\ntraining_size=0.6\nvalidation_size=0.3\ntesting_size=0.1\n\n\nX_train, X_val, Y_train, Y_val= train_test_split(X, Y, test_size=validation_size, random_state=1)\n\n\nX_train, X_test, Y_train, Y_test= train_test_split(X_train, Y_train, test_size=testing_size/training_size, random_state=1) \n    \n\nX_train = X_train.drop('role', axis=1)\nX_val = X_val.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 519 entries, 129 to 818\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Sex_male  519 non-null    bool   \n 1   Fare      519 non-null    float64\n 2   Age       519 non-null    float64\n 3   Pclass    519 non-null    int64  \n 4   SibSp     519 non-null    int64  \ndtypes: bool(1), float64(2), int64(2)\nmemory usage: 20.8 KB\n\n\n\n\nCode\nlen(X_train)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.5824915824915825\n\n\n\n\nCode\nlen(X_val)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.30078563411896747\n\n\n\n\nCode\nlen(X_test)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.11672278338945005",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost.html#categorical_cols-and-numerical_cols",
    "href": "classification/7.3 XGboost.html#categorical_cols-and-numerical_cols",
    "title": "XGboost with pipeline and fast tuning",
    "section": "2.5 categorical_cols and numerical_cols",
    "text": "2.5 categorical_cols and numerical_cols\n\n\nCode\ncategorical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].nunique() &lt; 10 and X_train[cname].dtype == \"object\"]\n                    \n                    \nnumerical_cols = numerical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].dtype in ['int64', 'float64']]\n\n\n\n\nCode\nprint(\"The total number of categorical columns:\", len(categorical_cols))\nprint(\"The total number of numerical columns:\", len(numerical_cols))\n\n\nThe total number of categorical columns: 0\nThe total number of numerical columns: 4\n\n\n\n\nCode\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train[my_cols].copy()\nX_val = X_val[my_cols].copy()\nX_test= X_test[my_cols].copy()\n\n\n#X_final = df_test[my_cols].copy()",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost.html#pipelines-for-data-preprocessing",
    "href": "classification/7.3 XGboost.html#pipelines-for-data-preprocessing",
    "title": "XGboost with pipeline and fast tuning",
    "section": "2.6 Pipelines for Data Preprocessing",
    "text": "2.6 Pipelines for Data Preprocessing\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\nCode\nnumerical_transformer = Pipeline(steps=[\n    ('imputer_num', SimpleImputer(strategy='median')), \n    ('scaler', StandardScaler())\n])\n\n\n\n\nCode\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n\n\nCode\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)])",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost.html#define-model",
    "href": "classification/7.3 XGboost.html#define-model",
    "title": "XGboost with pipeline and fast tuning",
    "section": "3.1 define model",
    "text": "3.1 define model\n\n\nCode\nimport xgboost\nprint(xgboost.__version__)\n\n\n2.0.3\n\n\n\n\nCode\nfrom xgboost import XGBClassifier\nml_model = XGBClassifier()\nml_model\n\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. XGBClassifieriNot fittedXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...)",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost.html#define-pipline",
    "href": "classification/7.3 XGboost.html#define-pipline",
    "title": "XGboost with pipeline and fast tuning",
    "section": "3.2 define pipline",
    "text": "3.2 define pipline\n\n\nCode\npipeline = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', ml_model)\n         ]\n)",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost.html#define-hyperparameter-tuning-set",
    "href": "classification/7.3 XGboost.html#define-hyperparameter-tuning-set",
    "title": "XGboost with pipeline and fast tuning",
    "section": "3.3 define hyperparameter tuning set",
    "text": "3.3 define hyperparameter tuning set\n\n\nCode\n# explicitly require this experimental feature\nfrom sklearn.experimental import enable_halving_search_cv  # noqa\n\nfrom sklearn.model_selection import HalvingGridSearchCV\n\nparameters = {\n        'model__learning_rate': [0.01, 0.02,0.08,0.1],\n        'model__max_depth': [3, 5, 7,8,9,10,20],\n        'model__min_child_weight': [1, 3,5,8],\n        'model__subsample': [0.5, 0.7,0.9],\n        \n       # 'model__colsample__bytree': [0.5, 0.7],\n       \n        'model__n_estimators' : [100, 200, 500],\n        'model__objective': ['reg:squarederror']\n    }\n\n\nall parameters combinations\n\n\nCode\nfrom sklearn.model_selection import ParameterGrid\ncombinations=list(ParameterGrid(parameters))\n\n\n1440 combinations\n\n\nCode\nlen(combinations)\n\n\n1008\n\n\n\n\nCode\ncombinations[0:5]\n\n\n[{'model__learning_rate': 0.01,\n  'model__max_depth': 3,\n  'model__min_child_weight': 1,\n  'model__n_estimators': 100,\n  'model__objective': 'reg:squarederror',\n  'model__subsample': 0.5},\n {'model__learning_rate': 0.01,\n  'model__max_depth': 3,\n  'model__min_child_weight': 1,\n  'model__n_estimators': 100,\n  'model__objective': 'reg:squarederror',\n  'model__subsample': 0.7},\n {'model__learning_rate': 0.01,\n  'model__max_depth': 3,\n  'model__min_child_weight': 1,\n  'model__n_estimators': 100,\n  'model__objective': 'reg:squarederror',\n  'model__subsample': 0.9},\n {'model__learning_rate': 0.01,\n  'model__max_depth': 3,\n  'model__min_child_weight': 1,\n  'model__n_estimators': 200,\n  'model__objective': 'reg:squarederror',\n  'model__subsample': 0.5},\n {'model__learning_rate': 0.01,\n  'model__max_depth': 3,\n  'model__min_child_weight': 1,\n  'model__n_estimators': 200,\n  'model__objective': 'reg:squarederror',\n  'model__subsample': 0.7}]\n\n\nSearch over specified parameter values with successive halving.\nThe search strategy starts evaluating all the candidates with a small amount of resources and iteratively selects the best candidates, using more and more resources.\n\n\nCode\nGridCV = HalvingGridSearchCV(pipeline\n                ,parameters\n                ,scoring='accuracy'\n                ,max_resources=100\n                , cv=10, n_jobs=-1)",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost.html#train-model",
    "href": "classification/7.3 XGboost.html#train-model",
    "title": "XGboost with pipeline and fast tuning",
    "section": "3.4 train model",
    "text": "3.4 train model\n\n\nCode\nstart_time = time.time()\n\nRAY_AIR_NEW_OUTPUT=0\nGridCV.fit(X_train, Y_train)\n\nend_time = time.time()\nduration = end_time - start_time\nduration\n\n\n39.03060698509216\n\n\n\n3.4.0.1 tunning result\n\n3.4.0.1.1 GridSearchCV\n\n\nCode\n# get the parameter names\ncolumn_results = [f\"param_{name}\" for name in parameters.keys()]\ncolumn_results += [\"mean_test_score\", \"std_test_score\", \"rank_test_score\"]\n\ncv_results = pd.DataFrame(GridCV.cv_results_)\ncv_results = cv_results[column_results].sort_values(\n    \"mean_test_score\", ascending=False\n)\n\n\ndef shorten_param(param_name):\n    if \"__\" in param_name:\n        return param_name.rsplit(\"__\", 1)[1]\n    return param_name\n\n\ncv_results = cv_results.rename(shorten_param, axis=1)\ncv_results.head()\n\n\n\n\n\n\n\n\n\n\nlearning_rate\nmax_depth\nmin_child_weight\nsubsample\nn_estimators\nobjective\nmean_test_score\nstd_test_score\nrank_test_score\n\n\n\n\n927\n0.1\n9\n8\n0.5\n100\nreg:squarederror\n0.816667\n0.165831\n1\n\n\n783\n0.1\n3\n8\n0.5\n100\nreg:squarederror\n0.816667\n0.165831\n1\n\n\n963\n0.1\n10\n8\n0.5\n100\nreg:squarederror\n0.816667\n0.165831\n1\n\n\n999\n0.1\n20\n8\n0.5\n100\nreg:squarederror\n0.816667\n0.165831\n1\n\n\n855\n0.1\n7\n8\n0.5\n100\nreg:squarederror\n0.816667\n0.165831\n1\n\n\n\n\n\n\n\n\n\n\n\n3.4.0.2 tunning best parameters\n\n\nCode\nGridCV.best_params_\n\n\n{'model__learning_rate': 0.1,\n 'model__max_depth': 3,\n 'model__min_child_weight': 8,\n 'model__n_estimators': 100,\n 'model__objective': 'reg:squarederror',\n 'model__subsample': 0.5}\n\n\n\n\n3.4.0.3 tunning best model\n\n\nCode\nmodel_ml = GridCV.best_estimator_",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost.html#preformance",
    "href": "classification/7.3 XGboost.html#preformance",
    "title": "XGboost with pipeline and fast tuning",
    "section": "3.5 Preformance",
    "text": "3.5 Preformance\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = model_ml.predict(X_val) #always gets x and retuns y\nY_pred_dt\n\n\narray([1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n       1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n       1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n       0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n       1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n       1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,\n       0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,\n       0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n       1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n       0, 0, 1, 1])\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_val,Y_pred_dt)  \naccuracy\n\n\n0.6865671641791045\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_val,Y_pred_dt)  \nprecision_dt\n\n\n0.6867469879518072\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_val,Y_pred_dt)  \nrecall_dt\n\n\n0.4956521739130435\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_val,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[127,  26],\n       [ 58,  57]])\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_val, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.6628587666950838",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost.html#k-fold-cross-validation",
    "href": "classification/7.3 XGboost.html#k-fold-cross-validation",
    "title": "XGboost with pipeline and fast tuning",
    "section": "3.6 k-Fold Cross-Validation",
    "text": "3.6 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(ml_model, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.6897871545929799",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost.html#save-model",
    "href": "classification/7.3 XGboost.html#save-model",
    "title": "XGboost with pipeline and fast tuning",
    "section": "3.7 save model",
    "text": "3.7 save model\n\n\nCode\nfrom joblib import dump, load\ndump(model_ml, 'trained_model_7_2.joblib') \n\n\n['trained_model_7_2.joblib']\n\n\nsave trained pipeline\n\n\nCode\nfrom joblib import dump, load\ndump(GridCV, 'trained_GridCV_7_2.joblib', compress=True)  \n\n\n['trained_GridCV_7_2.joblib']",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost.html#load-model",
    "href": "classification/7.3 XGboost.html#load-model",
    "title": "XGboost with pipeline and fast tuning",
    "section": "3.8 load model",
    "text": "3.8 load model\n\n\nCode\nmodel_dt_reload = load('trained_model_7_2.joblib') \n\n\nload grid\n\n\nCode\nGrid_reload = load('trained_GridCV_7_2.joblib')",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost.html#final-prediction",
    "href": "classification/7.3 XGboost.html#final-prediction",
    "title": "XGboost with pipeline and fast tuning",
    "section": "3.9 final prediction",
    "text": "3.9 final prediction\n\n\nCode\nY_pred_dt_final = model_dt_reload.predict(X_test) #always gets x and retuns y\n\n#Y_pred_dt_final[0:5]",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html",
    "href": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "",
    "text": "import os\n#os.system('pip install xgboost')\n\n\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nimport time\nfrom siuba import *",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#input-data",
    "href": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#input-data",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "input data",
    "text": "input data\n\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\nPassengerId_test = df_test['PassengerId']\n\ndf_train = df_train &gt;&gt; select(~_.PassengerId)\ndf_test = df_test &gt;&gt; select(~_.PassengerId)\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#data-eda",
    "href": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#data-eda",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "data EDA",
    "text": "data EDA\n\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\n\n\n\n\n\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\n\n\n\n\n\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n\n0.7420382165605095\n0.18890814558058924",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#data-wrangling",
    "href": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#data-wrangling",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 11 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Pclass    1309 non-null   int64  \n 1   Name      1309 non-null   object \n 2   Sex       1309 non-null   object \n 3   Age       1046 non-null   float64\n 4   SibSp     1309 non-null   int64  \n 5   Parch     1309 non-null   int64  \n 6   Ticket    1309 non-null   object \n 7   Fare      1308 non-null   float64\n 8   Cabin     295 non-null    object \n 9   Embarked  1307 non-null   object \n 10  role      1309 non-null   object \ndtypes: float64(2), int64(3), object(6)\nmemory usage: 122.7+ KB\n\n\n\n# Dealing with missing numerical variables\n#data['Age'] = data.Age.fillna(data.Age.median())\n#data['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 11 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Pclass    1309 non-null   int64  \n 1   Name      1309 non-null   object \n 2   Sex       1309 non-null   object \n 3   Age       1046 non-null   float64\n 4   SibSp     1309 non-null   int64  \n 5   Parch     1309 non-null   int64  \n 6   Ticket    1309 non-null   object \n 7   Fare      1308 non-null   float64\n 8   Cabin     295 non-null    object \n 9   Embarked  1307 non-null   object \n 10  role      1309 non-null   object \ndtypes: float64(2), int64(3), object(6)\nmemory usage: 122.7+ KB\n\n\n\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\n#data = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\n\n\n\n\n\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\n\n\n\n\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\n\n\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\n\n\n2\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\n\n\n3\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\n\n\n4\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\n\n\n\n\n\n\n\n\n\n# Select features columns\n#data = data[['Sex', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()\n\n\n\n\n\n\n\n\n\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\n\n\n\n\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\n\n\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\n\n\n2\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\n\n\n3\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\n\n\n4\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#split-data",
    "href": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#split-data",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "split data",
    "text": "split data\n60% training / 30% validation/ 10% testing\n\n\nY=df_train['Survived']\nX=data[data.role =='train']\n\n#X_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\n\ntraining_size=0.8\nvalidation_size=0.1\ntesting_size=0.1\n\n\nX_train, X_val, Y_train, Y_val= train_test_split(X, Y, test_size=validation_size, random_state=1)\n\n\nX_train, X_test, Y_train, Y_test= train_test_split(X_train, Y_train, test_size=testing_size/training_size, random_state=1) \n    \n\nX_train = X_train.drop('role', axis=1)\nX_val = X_val.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\nX_train.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 700 entries, 658 to 554\nData columns (total 10 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Pclass    700 non-null    int64  \n 1   Name      700 non-null    object \n 2   Sex       700 non-null    object \n 3   Age       559 non-null    float64\n 4   SibSp     700 non-null    int64  \n 5   Parch     700 non-null    int64  \n 6   Ticket    700 non-null    object \n 7   Fare      700 non-null    float64\n 8   Cabin     160 non-null    object \n 9   Embarked  698 non-null    object \ndtypes: float64(2), int64(3), object(5)\nmemory usage: 60.2+ KB\n\n\n\nlen(X_train)/(len(X_train) +len(X_val) +len(X_test) )\n\n0.7856341189674523\n\n\n\nlen(X_val)/(len(X_train) +len(X_val) +len(X_test) )\n\n0.10101010101010101\n\n\n\nlen(X_test)/(len(X_train) +len(X_val) +len(X_test) )\n\n0.11335578002244669",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#categorical_cols-and-numerical_cols",
    "href": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#categorical_cols-and-numerical_cols",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "categorical_cols and numerical_cols",
    "text": "categorical_cols and numerical_cols\n\ncategorical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].nunique() &lt; 10 and X_train[cname].dtype == \"object\"]\n                    \n                    \nnumerical_cols = numerical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].dtype in ['int64', 'float64']]\n\n\nprint(\"The total number of categorical columns:\", len(categorical_cols))\nprint(\"The total number of numerical columns:\", len(numerical_cols))\n\nThe total number of categorical columns: 2\nThe total number of numerical columns: 5\n\n\n\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train[my_cols].copy()\nX_val = X_val[my_cols].copy()\nX_test= X_test[my_cols].copy()\n\nmy_cols\n\n#X_final = df_test[my_cols].copy()\n\n['Sex', 'Embarked', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#pipelines-for-data-preprocessing",
    "href": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#pipelines-for-data-preprocessing",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "Pipelines for Data Preprocessing",
    "text": "Pipelines for Data Preprocessing\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n\nnumerical_transformer = Pipeline(steps=[\n    ('imputer_num', SimpleImputer(strategy='median')), \n    ('scaler', StandardScaler())\n])\n\n\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)])",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#define-model",
    "href": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#define-model",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "define model",
    "text": "define model\n\nimport xgboost\nprint(xgboost.__version__)\n\n2.0.3\n\n\n\nfrom xgboost import XGBClassifier\nml_model = XGBClassifier()\nml_model\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. XGBClassifieriNot fittedXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...)",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#define-pipline",
    "href": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#define-pipline",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "define pipline",
    "text": "define pipline\n\npipeline = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', ml_model)\n         ]\n)\n\npipeline\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer_num',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['Pclass', 'Age', 'SibSp',\n                                                   'Parch', 'Fare']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('imputer_cat',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('onehot',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['Sex', 'Emba...\n                               feature_types=None, gamma=None, grow_policy=None,\n                               importance_type=None,\n                               interaction_constraints=None, learning_rate=None,\n                               max_bin=None, max_cat_threshold=None,\n                               max_cat_to_onehot=None, max_delta_step=None,\n                               max_depth=None, max_leaves=None,\n                               min_child_weight=None, missing=nan,\n                               monotone_constraints=None, multi_strategy=None,\n                               n_estimators=None, n_jobs=None,\n                               num_parallel_tree=None, random_state=None, ...))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiNot fittedPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer_num',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['Pclass', 'Age', 'SibSp',\n                                                   'Parch', 'Fare']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('imputer_cat',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('onehot',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['Sex', 'Emba...\n                               feature_types=None, gamma=None, grow_policy=None,\n                               importance_type=None,\n                               interaction_constraints=None, learning_rate=None,\n                               max_bin=None, max_cat_threshold=None,\n                               max_cat_to_onehot=None, max_delta_step=None,\n                               max_depth=None, max_leaves=None,\n                               min_child_weight=None, missing=nan,\n                               monotone_constraints=None, multi_strategy=None,\n                               n_estimators=None, n_jobs=None,\n                               num_parallel_tree=None, random_state=None, ...))])  preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('imputer_num',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('scaler', StandardScaler())]),\n                                 ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']),\n                                ('cat',\n                                 Pipeline(steps=[('imputer_cat',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('onehot',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 ['Sex', 'Embarked'])]) num['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='median')  StandardScaler?Documentation for StandardScalerStandardScaler() cat['Sex', 'Embarked']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='most_frequent')  OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore') XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...)",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#define-hyperparameter-tuning-set",
    "href": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#define-hyperparameter-tuning-set",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "define hyperparameter tuning set",
    "text": "define hyperparameter tuning set\n\nfrom sklearn.model_selection import GridSearchCV\n\n\nparameters = {\n        'model__learning_rate': [0.01, 0.02,0.08,0.1],\n        'model__max_depth': [3, 5, 7,8,9,10,20],\n        'model__min_child_weight': [1, 3,5,8],\n        'model__subsample': [0.5, 0.7,0.9],\n        \n       # 'model__colsample__bytree': [0.5, 0.7],\n       \n        'model__n_estimators' : [100, 200, 500],\n        'model__objective': ['reg:squarederror']\n    }\n\nall parameters combinations\n\nimport itertools\na = parameters.values()\ncombinations = list(itertools.product(*a))\n\n1440 combinations\n\nlen(combinations)\n\n1008\n\n\n\ncombinations[0:5]\n\n[(0.01, 3, 1, 0.5, 100, 'reg:squarederror'),\n (0.01, 3, 1, 0.5, 200, 'reg:squarederror'),\n (0.01, 3, 1, 0.5, 500, 'reg:squarederror'),\n (0.01, 3, 1, 0.7, 100, 'reg:squarederror'),\n (0.01, 3, 1, 0.7, 200, 'reg:squarederror')]\n\n\n\nGridCV = GridSearchCV(pipeline\n                ,parameters\n                ,scoring='accuracy'\n                , cv=10, n_jobs=-1)",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#train-model",
    "href": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#train-model",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "train model",
    "text": "train model\n\nstart_time = time.time()\n\nGridCV.fit(X_train, Y_train)\n\nend_time = time.time()\nduration = end_time - start_time\nduration\n\n156.45451498031616\n\n\n\ntunning result\n\nGridSearchCV\n\n# get the parameter names\ncolumn_results = [f\"param_{name}\" for name in parameters.keys()]\ncolumn_results += [\"mean_test_score\", \"std_test_score\", \"rank_test_score\"]\n\ncv_results = pd.DataFrame(GridCV.cv_results_)\ncv_results = cv_results[column_results].sort_values(\n    \"mean_test_score\", ascending=False\n)\n\n\ndef shorten_param(param_name):\n    if \"__\" in param_name:\n        return param_name.rsplit(\"__\", 1)[1]\n    return param_name\n\n\ncv_results = cv_results.rename(shorten_param, axis=1)\ncv_results.head()\n\n\n\n\n\n\n\n\n\nlearning_rate\nmax_depth\nmin_child_weight\nsubsample\nn_estimators\nobjective\nmean_test_score\nstd_test_score\nrank_test_score\n\n\n\n\n778\n0.1\n3\n5\n0.7\n200\nreg:squarederror\n0.842857\n0.053069\n1\n\n\n215\n0.01\n10\n8\n0.9\n500\nreg:squarederror\n0.838571\n0.034434\n2\n\n\n228\n0.01\n20\n3\n0.5\n200\nreg:squarederror\n0.838571\n0.046533\n2\n\n\n168\n0.01\n9\n5\n0.5\n500\nreg:squarederror\n0.838571\n0.046092\n4\n\n\n777\n0.1\n3\n5\n0.5\n200\nreg:squarederror\n0.838571\n0.046092\n4\n\n\n\n\n\n\n\n\n\n\n\ntunning best parameters\n\nGridCV.best_params_\n\n{'model__learning_rate': 0.1,\n 'model__max_depth': 3,\n 'model__min_child_weight': 5,\n 'model__n_estimators': 200,\n 'model__objective': 'reg:squarederror',\n 'model__subsample': 0.7}\n\n\n\n\ntunning best model\n\nmodel_ml = GridCV.best_estimator_",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#preformance",
    "href": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#preformance",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "Preformance",
    "text": "Preformance\n\n#Using predict method to test the model\nY_pred_dt = model_ml.predict(X_val) #always gets x and retuns y\nY_pred_dt\n\narray([1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n       1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n       1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n       0, 1])\n\n\n\nAccuracy\n\n\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_val,Y_pred_dt)  \naccuracy\n\n0.8\n\n\n\nPrecision\n\n\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_val,Y_pred_dt)  \nprecision_dt\n\n0.8275862068965517\n\n\n\nRecall\n\n\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_val,Y_pred_dt)  \nrecall_dt\n\n0.6486486486486487\n\n\n\nConfusion matrix\n\n\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_val,Y_pred_dt)\nconfusion_matrix_dt\n\narray([[48,  5],\n       [13, 24]])\n\n\n\nAUC - ROC Curve\n\n\nauc_dt = metrics.roc_auc_score(Y_val, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n0.7771545130035696",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#k-fold-cross-validation",
    "href": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#k-fold-cross-validation",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "k-Fold Cross-Validation",
    "text": "k-Fold Cross-Validation\n\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(pipeline, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n0.8114285714285714",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#save-model",
    "href": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#save-model",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "save model",
    "text": "save model\n\nfrom joblib import dump, load\ndump(model_ml, 'trained_model_7_2.joblib') \n\n['trained_model_7_2.joblib']\n\n\nsave trained pipeline\n\nfrom joblib import dump, load\ndump(GridCV, 'trained_GridCV_7_2.joblib', compress=True)  \n\n['trained_GridCV_7_2.joblib']",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#load-model",
    "href": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#load-model",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "load model",
    "text": "load model\n\nmodel_dt_reload = load('trained_model_7_2.joblib') \n\nload grid\n\nGrid_reload = load('trained_GridCV_7_2.joblib')",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#final-prediction",
    "href": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#final-prediction",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "final prediction",
    "text": "final prediction\n\nY_pred_dt_final = model_dt_reload.predict(X_test) #always gets x and retuns y\n\n#Y_pred_dt_final[0:5]",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost with pipeline and fast tuning.html",
    "href": "classification/7.3 XGboost with pipeline and fast tuning.html",
    "title": "XGboost with pipeline and fast tuning",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nimport time\nfrom siuba import *",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost with pipeline and fast tuning.html#input-data",
    "href": "classification/7.3 XGboost with pipeline and fast tuning.html#input-data",
    "title": "XGboost with pipeline and fast tuning",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\nPassengerId_test = df_test['PassengerId']\n\ndf_train = df_train &gt;&gt; select(~_.PassengerId)\ndf_test = df_test &gt;&gt; select(~_.PassengerId)\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost with pipeline and fast tuning.html#data-eda",
    "href": "classification/7.3 XGboost with pipeline and fast tuning.html#data-eda",
    "title": "XGboost with pipeline and fast tuning",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n\n\n0.7420382165605095\n0.18890814558058924",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost with pipeline and fast tuning.html#data-wrangling",
    "href": "classification/7.3 XGboost with pipeline and fast tuning.html#data-wrangling",
    "title": "XGboost with pipeline and fast tuning",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 11 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Pclass    1309 non-null   int64  \n 1   Name      1309 non-null   object \n 2   Sex       1309 non-null   object \n 3   Age       1046 non-null   float64\n 4   SibSp     1309 non-null   int64  \n 5   Parch     1309 non-null   int64  \n 6   Ticket    1309 non-null   object \n 7   Fare      1308 non-null   float64\n 8   Cabin     295 non-null    object \n 9   Embarked  1307 non-null   object \n 10  role      1309 non-null   object \ndtypes: float64(2), int64(3), object(6)\nmemory usage: 122.7+ KB\n\n\n\n\nCode\n# Dealing with missing numerical variables\n#data['Age'] = data.Age.fillna(data.Age.median())\n#data['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 11 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Pclass    1309 non-null   int64  \n 1   Name      1309 non-null   object \n 2   Sex       1309 non-null   object \n 3   Age       1046 non-null   float64\n 4   SibSp     1309 non-null   int64  \n 5   Parch     1309 non-null   int64  \n 6   Ticket    1309 non-null   object \n 7   Fare      1308 non-null   float64\n 8   Cabin     295 non-null    object \n 9   Embarked  1307 non-null   object \n 10  role      1309 non-null   object \ndtypes: float64(2), int64(3), object(6)\nmemory usage: 122.7+ KB\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\n#data = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\n\n\n\n\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\n\n\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\n\n\n2\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\n\n\n3\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\n\n\n4\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\n\n\n\n\n\n\n\n\n\n\nCode\n# Select features columns\n#data = data[['Sex', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\n\n\n\n\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\n\n\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\n\n\n2\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\n\n\n3\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\n\n\n4\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost with pipeline and fast tuning.html#split-data",
    "href": "classification/7.3 XGboost with pipeline and fast tuning.html#split-data",
    "title": "XGboost with pipeline and fast tuning",
    "section": "2.4 split data",
    "text": "2.4 split data\n60% training / 30% validation/ 10% testing\n\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\n#X_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\n\ntraining_size=0.8\nvalidation_size=0.1\ntesting_size=0.1\n\n\nX_train, X_val, Y_train, Y_val= train_test_split(X, Y, test_size=validation_size, random_state=1)\n\n\nX_train, X_test, Y_train, Y_test= train_test_split(X_train, Y_train, test_size=testing_size/training_size, random_state=1) \n    \n\nX_train = X_train.drop('role', axis=1)\nX_val = X_val.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 700 entries, 658 to 554\nData columns (total 10 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Pclass    700 non-null    int64  \n 1   Name      700 non-null    object \n 2   Sex       700 non-null    object \n 3   Age       559 non-null    float64\n 4   SibSp     700 non-null    int64  \n 5   Parch     700 non-null    int64  \n 6   Ticket    700 non-null    object \n 7   Fare      700 non-null    float64\n 8   Cabin     160 non-null    object \n 9   Embarked  698 non-null    object \ndtypes: float64(2), int64(3), object(5)\nmemory usage: 60.2+ KB\n\n\n\n\nCode\nlen(X_train)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.7856341189674523\n\n\n\n\nCode\nlen(X_val)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.10101010101010101\n\n\n\n\nCode\nlen(X_test)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.11335578002244669",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost with pipeline and fast tuning.html#categorical_cols-and-numerical_cols",
    "href": "classification/7.3 XGboost with pipeline and fast tuning.html#categorical_cols-and-numerical_cols",
    "title": "XGboost with pipeline and fast tuning",
    "section": "2.5 categorical_cols and numerical_cols",
    "text": "2.5 categorical_cols and numerical_cols\n\n\nCode\ncategorical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].nunique() &lt; 10 and X_train[cname].dtype == \"object\"]\n                    \n                    \nnumerical_cols = numerical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].dtype in ['int64', 'float64']]\n\n\n\n\nCode\nprint(\"The total number of categorical columns:\", len(categorical_cols))\nprint(\"The total number of numerical columns:\", len(numerical_cols))\n\n\nThe total number of categorical columns: 2\nThe total number of numerical columns: 5\n\n\n\n\nCode\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train[my_cols].copy()\nX_val = X_val[my_cols].copy()\nX_test= X_test[my_cols].copy()\n\nmy_cols\n#X_final = df_test[my_cols].copy()\n\n\n['Sex', 'Embarked', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost with pipeline and fast tuning.html#pipelines-for-data-preprocessing",
    "href": "classification/7.3 XGboost with pipeline and fast tuning.html#pipelines-for-data-preprocessing",
    "title": "XGboost with pipeline and fast tuning",
    "section": "2.6 Pipelines for Data Preprocessing",
    "text": "2.6 Pipelines for Data Preprocessing\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\nCode\nnumerical_transformer = Pipeline(steps=[\n    ('imputer_num', SimpleImputer(strategy='median')), \n    ('scaler', StandardScaler())\n])\n\n\n\n\nCode\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n\n\nCode\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)])",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost with pipeline and fast tuning.html#define-model",
    "href": "classification/7.3 XGboost with pipeline and fast tuning.html#define-model",
    "title": "XGboost with pipeline and fast tuning",
    "section": "3.1 define model",
    "text": "3.1 define model\n\n\nCode\nimport xgboost\nprint(xgboost.__version__)\n\n\n2.0.3\n\n\n\n\nCode\nfrom xgboost import XGBClassifier\nml_model = XGBClassifier()\nml_model\n\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. XGBClassifieriNot fittedXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...)",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost with pipeline and fast tuning.html#define-pipline",
    "href": "classification/7.3 XGboost with pipeline and fast tuning.html#define-pipline",
    "title": "XGboost with pipeline and fast tuning",
    "section": "3.2 define pipline",
    "text": "3.2 define pipline\n\n\nCode\npipeline = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', ml_model)\n         ]\n)\n\npipeline\n\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer_num',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['Pclass', 'Age', 'SibSp',\n                                                   'Parch', 'Fare']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('imputer_cat',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('onehot',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['Sex', 'Emba...\n                               feature_types=None, gamma=None, grow_policy=None,\n                               importance_type=None,\n                               interaction_constraints=None, learning_rate=None,\n                               max_bin=None, max_cat_threshold=None,\n                               max_cat_to_onehot=None, max_delta_step=None,\n                               max_depth=None, max_leaves=None,\n                               min_child_weight=None, missing=nan,\n                               monotone_constraints=None, multi_strategy=None,\n                               n_estimators=None, n_jobs=None,\n                               num_parallel_tree=None, random_state=None, ...))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiNot fittedPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer_num',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['Pclass', 'Age', 'SibSp',\n                                                   'Parch', 'Fare']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('imputer_cat',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('onehot',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['Sex', 'Emba...\n                               feature_types=None, gamma=None, grow_policy=None,\n                               importance_type=None,\n                               interaction_constraints=None, learning_rate=None,\n                               max_bin=None, max_cat_threshold=None,\n                               max_cat_to_onehot=None, max_delta_step=None,\n                               max_depth=None, max_leaves=None,\n                               min_child_weight=None, missing=nan,\n                               monotone_constraints=None, multi_strategy=None,\n                               n_estimators=None, n_jobs=None,\n                               num_parallel_tree=None, random_state=None, ...))])  preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('imputer_num',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('scaler', StandardScaler())]),\n                                 ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']),\n                                ('cat',\n                                 Pipeline(steps=[('imputer_cat',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('onehot',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 ['Sex', 'Embarked'])]) num['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='median')  StandardScaler?Documentation for StandardScalerStandardScaler() cat['Sex', 'Embarked']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='most_frequent')  OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore') XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...)",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost with pipeline and fast tuning.html#define-hyperparameter-tuning-set",
    "href": "classification/7.3 XGboost with pipeline and fast tuning.html#define-hyperparameter-tuning-set",
    "title": "XGboost with pipeline and fast tuning",
    "section": "3.3 define hyperparameter tuning set",
    "text": "3.3 define hyperparameter tuning set\n\n\nCode\n# explicitly require this experimental feature\nfrom sklearn.experimental import enable_halving_search_cv  # noqa\n\nfrom sklearn.model_selection import HalvingGridSearchCV\n\nparameters = {\n        'model__learning_rate': [0.01, 0.02,0.08,0.1],\n        'model__max_depth': [3, 5, 7,8,9,10,20],\n        'model__min_child_weight': [1, 3,5,8],\n        'model__subsample': [0.5, 0.7,0.9],\n        \n       # 'model__colsample__bytree': [0.5, 0.7],\n       \n        'model__n_estimators' : [100, 200, 500],\n        'model__objective': ['reg:squarederror']\n    }\n\n\nall parameters combinations\n\n\nCode\nfrom sklearn.model_selection import ParameterGrid\ncombinations=list(ParameterGrid(parameters))\n\n\n1440 combinations\n\n\nCode\nlen(combinations)\n\n\n1008\n\n\n\n\nCode\ncombinations[0:5]\n\n\n[{'model__learning_rate': 0.01,\n  'model__max_depth': 3,\n  'model__min_child_weight': 1,\n  'model__n_estimators': 100,\n  'model__objective': 'reg:squarederror',\n  'model__subsample': 0.5},\n {'model__learning_rate': 0.01,\n  'model__max_depth': 3,\n  'model__min_child_weight': 1,\n  'model__n_estimators': 100,\n  'model__objective': 'reg:squarederror',\n  'model__subsample': 0.7},\n {'model__learning_rate': 0.01,\n  'model__max_depth': 3,\n  'model__min_child_weight': 1,\n  'model__n_estimators': 100,\n  'model__objective': 'reg:squarederror',\n  'model__subsample': 0.9},\n {'model__learning_rate': 0.01,\n  'model__max_depth': 3,\n  'model__min_child_weight': 1,\n  'model__n_estimators': 200,\n  'model__objective': 'reg:squarederror',\n  'model__subsample': 0.5},\n {'model__learning_rate': 0.01,\n  'model__max_depth': 3,\n  'model__min_child_weight': 1,\n  'model__n_estimators': 200,\n  'model__objective': 'reg:squarederror',\n  'model__subsample': 0.7}]\n\n\nSearch over specified parameter values with successive halving.\nThe search strategy starts evaluating all the candidates with a small amount of resources and iteratively selects the best candidates, using more and more resources.\n\n\nCode\nGridCV = HalvingGridSearchCV(pipeline\n                ,parameters\n                ,scoring='accuracy'\n                ,max_resources=100\n                , cv=10, n_jobs=-1)",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost with pipeline and fast tuning.html#train-model",
    "href": "classification/7.3 XGboost with pipeline and fast tuning.html#train-model",
    "title": "XGboost with pipeline and fast tuning",
    "section": "3.4 train model",
    "text": "3.4 train model\n\n\nCode\nstart_time = time.time()\n\nRAY_AIR_NEW_OUTPUT=0\nGridCV.fit(X_train, Y_train)\n\nend_time = time.time()\nduration = end_time - start_time\nduration\n\n\n46.2377769947052\n\n\n\n3.4.0.1 tunning result\n\n3.4.0.1.1 GridSearchCV\n\n\nCode\n# get the parameter names\ncolumn_results = [f\"param_{name}\" for name in parameters.keys()]\ncolumn_results += [\"mean_test_score\", \"std_test_score\", \"rank_test_score\"]\n\ncv_results = pd.DataFrame(GridCV.cv_results_)\ncv_results = cv_results[column_results].sort_values(\n    \"mean_test_score\", ascending=False\n)\n\n\ndef shorten_param(param_name):\n    if \"__\" in param_name:\n        return param_name.rsplit(\"__\", 1)[1]\n    return param_name\n\n\ncv_results = cv_results.rename(shorten_param, axis=1)\ncv_results.head()\n\n\n\n\n\n\n\n\n\n\nlearning_rate\nmax_depth\nmin_child_weight\nsubsample\nn_estimators\nobjective\nmean_test_score\nstd_test_score\nrank_test_score\n\n\n\n\n892\n0.1\n8\n8\n0.7\n100\nreg:squarederror\n0.8\n0.244949\n1\n\n\n199\n0.01\n10\n5\n0.7\n100\nreg:squarederror\n0.8\n0.244949\n1\n\n\n489\n0.02\n20\n5\n0.5\n200\nreg:squarederror\n0.8\n0.244949\n1\n\n\n356\n0.02\n7\n8\n0.9\n200\nreg:squarederror\n0.8\n0.244949\n1\n\n\n954\n0.1\n10\n5\n0.5\n100\nreg:squarederror\n0.8\n0.244949\n1\n\n\n\n\n\n\n\n\n\n\n\n3.4.0.2 tunning best parameters\n\n\nCode\nGridCV.best_params_\n\n\n{'model__learning_rate': 0.01,\n 'model__max_depth': 3,\n 'model__min_child_weight': 3,\n 'model__n_estimators': 100,\n 'model__objective': 'reg:squarederror',\n 'model__subsample': 0.7}\n\n\n\n\n3.4.0.3 tunning best model\n\n\nCode\nmodel_ml = GridCV.best_estimator_",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost with pipeline and fast tuning.html#preformance",
    "href": "classification/7.3 XGboost with pipeline and fast tuning.html#preformance",
    "title": "XGboost with pipeline and fast tuning",
    "section": "3.5 Preformance",
    "text": "3.5 Preformance\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = model_ml.predict(X_val) #always gets x and retuns y\nY_pred_dt\n\n\narray([1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n       1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n       1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n       0, 1])\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_val,Y_pred_dt)  \naccuracy\n\n\n0.7555555555555555\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_val,Y_pred_dt)  \nprecision_dt\n\n\n0.8260869565217391\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_val,Y_pred_dt)  \nrecall_dt\n\n\n0.5135135135135135\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_val,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[49,  4],\n       [18, 19]])\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_val, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.719020907700153",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost with pipeline and fast tuning.html#k-fold-cross-validation",
    "href": "classification/7.3 XGboost with pipeline and fast tuning.html#k-fold-cross-validation",
    "title": "XGboost with pipeline and fast tuning",
    "section": "3.6 k-Fold Cross-Validation",
    "text": "3.6 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(model_ml, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.8242857142857144",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost with pipeline and fast tuning.html#save-model",
    "href": "classification/7.3 XGboost with pipeline and fast tuning.html#save-model",
    "title": "XGboost with pipeline and fast tuning",
    "section": "3.7 save model",
    "text": "3.7 save model\n\n\nCode\nfrom joblib import dump, load\ndump(model_ml, 'trained_model_7_2.joblib') \n\n\n['trained_model_7_2.joblib']\n\n\nsave trained pipeline\n\n\nCode\nfrom joblib import dump, load\ndump(GridCV, 'trained_GridCV_7_2.joblib', compress=True)  \n\n\n['trained_GridCV_7_2.joblib']",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost with pipeline and fast tuning.html#load-model",
    "href": "classification/7.3 XGboost with pipeline and fast tuning.html#load-model",
    "title": "XGboost with pipeline and fast tuning",
    "section": "3.8 load model",
    "text": "3.8 load model\n\n\nCode\nmodel_dt_reload = load('trained_model_7_2.joblib') \n\n\nload grid\n\n\nCode\nGrid_reload = load('trained_GridCV_7_2.joblib')",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost with pipeline and fast tuning.html#final-prediction",
    "href": "classification/7.3 XGboost with pipeline and fast tuning.html#final-prediction",
    "title": "XGboost with pipeline and fast tuning",
    "section": "3.9 final prediction",
    "text": "3.9 final prediction\n\n\nCode\nY_pred_dt_final = model_dt_reload.predict(X_test) #always gets x and retuns y\n\n#Y_pred_dt_final[0:5]",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost with pipeline.html",
    "href": "classification/7.1 XGboost with pipeline.html",
    "title": "XGboost with pipeline",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nimport time\nfrom siuba import *",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost with pipeline.html#input-data",
    "href": "classification/7.1 XGboost with pipeline.html#input-data",
    "title": "XGboost with pipeline",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\nPassengerId_test = df_test['PassengerId']\n\ndf_train = df_train &gt;&gt; select(~_.PassengerId)\ndf_test = df_test &gt;&gt; select(~_.PassengerId)\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost with pipeline.html#data-eda",
    "href": "classification/7.1 XGboost with pipeline.html#data-eda",
    "title": "XGboost with pipeline",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n\n\n0.7420382165605095\n0.18890814558058924",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost with pipeline.html#data-wrangling",
    "href": "classification/7.1 XGboost with pipeline.html#data-wrangling",
    "title": "XGboost with pipeline",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 11 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Pclass    1309 non-null   int64  \n 1   Name      1309 non-null   object \n 2   Sex       1309 non-null   object \n 3   Age       1046 non-null   float64\n 4   SibSp     1309 non-null   int64  \n 5   Parch     1309 non-null   int64  \n 6   Ticket    1309 non-null   object \n 7   Fare      1308 non-null   float64\n 8   Cabin     295 non-null    object \n 9   Embarked  1307 non-null   object \n 10  role      1309 non-null   object \ndtypes: float64(2), int64(3), object(6)\nmemory usage: 122.7+ KB\n\n\n\n\nCode\n# Dealing with missing numerical variables\n#data['Age'] = data.Age.fillna(data.Age.median())\n#data['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 11 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Pclass    1309 non-null   int64  \n 1   Name      1309 non-null   object \n 2   Sex       1309 non-null   object \n 3   Age       1046 non-null   float64\n 4   SibSp     1309 non-null   int64  \n 5   Parch     1309 non-null   int64  \n 6   Ticket    1309 non-null   object \n 7   Fare      1308 non-null   float64\n 8   Cabin     295 non-null    object \n 9   Embarked  1307 non-null   object \n 10  role      1309 non-null   object \ndtypes: float64(2), int64(3), object(6)\nmemory usage: 122.7+ KB\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\n#data = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\n\n\n\n\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\n\n\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\n\n\n2\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\n\n\n3\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\n\n\n4\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\n\n\n\n\n\n\n\n\n\n\nCode\n# Select features columns\n\n#data = data[['Sex', 'Fare', 'Age','Pclass', 'SibSp','role']]\n\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\n\n\n\n\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\n\n\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\n\n\n2\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\n\n\n3\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\n\n\n4\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\n\n\n\n\n\n\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 11 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Pclass    1309 non-null   int64  \n 1   Name      1309 non-null   object \n 2   Sex       1309 non-null   object \n 3   Age       1046 non-null   float64\n 4   SibSp     1309 non-null   int64  \n 5   Parch     1309 non-null   int64  \n 6   Ticket    1309 non-null   object \n 7   Fare      1308 non-null   float64\n 8   Cabin     295 non-null    object \n 9   Embarked  1307 non-null   object \n 10  role      1309 non-null   object \ndtypes: float64(2), int64(3), object(6)\nmemory usage: 122.7+ KB",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost with pipeline.html#split-data",
    "href": "classification/7.1 XGboost with pipeline.html#split-data",
    "title": "XGboost with pipeline",
    "section": "2.4 split data",
    "text": "2.4 split data\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\n#X_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\n\ntraining_size=0.8\nvalidation_size=0.1\ntesting_size=0.1\n\n\nX_train, X_val, Y_train, Y_val= train_test_split(X, Y, test_size=validation_size, random_state=1)\n\n\nX_train, X_test, Y_train, Y_test= train_test_split(X_train, Y_train, test_size=testing_size/training_size, random_state=1) \n    \n\nX_train = X_train.drop('role', axis=1)\nX_val = X_val.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 700 entries, 658 to 554\nData columns (total 10 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Pclass    700 non-null    int64  \n 1   Name      700 non-null    object \n 2   Sex       700 non-null    object \n 3   Age       559 non-null    float64\n 4   SibSp     700 non-null    int64  \n 5   Parch     700 non-null    int64  \n 6   Ticket    700 non-null    object \n 7   Fare      700 non-null    float64\n 8   Cabin     160 non-null    object \n 9   Embarked  698 non-null    object \ndtypes: float64(2), int64(3), object(5)\nmemory usage: 60.2+ KB\n\n\n\n\nCode\nlen(X_train)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.7856341189674523\n\n\n\n\nCode\nlen(X_val)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.10101010101010101\n\n\n\n\nCode\nlen(X_test)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.11335578002244669",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost with pipeline.html#categorical_cols-and-numerical_cols",
    "href": "classification/7.1 XGboost with pipeline.html#categorical_cols-and-numerical_cols",
    "title": "XGboost with pipeline",
    "section": "2.5 categorical_cols and numerical_cols",
    "text": "2.5 categorical_cols and numerical_cols\n\n\nCode\ncategorical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].nunique() &lt; 10 and X_train[cname].dtype == \"object\"]\n                    \n                    \nnumerical_cols = numerical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].dtype in ['int64', 'float64']]\n\n\n\n\nCode\nprint(\"The total number of categorical columns:\", len(categorical_cols))\nprint(\"The total number of numerical columns:\", len(numerical_cols))\n\n\nThe total number of categorical columns: 2\nThe total number of numerical columns: 5\n\n\n\n\nCode\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train[my_cols].copy()\nX_val = X_val[my_cols].copy()\nX_test= X_test[my_cols].copy()\n\n\n#X_final = df_test[my_cols].copy()\n\n\n\n\nCode\nmy_cols\n\n\n['Sex', 'Embarked', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost with pipeline.html#pipelines-for-data-preprocessing",
    "href": "classification/7.1 XGboost with pipeline.html#pipelines-for-data-preprocessing",
    "title": "XGboost with pipeline",
    "section": "2.6 Pipelines for Data Preprocessing",
    "text": "2.6 Pipelines for Data Preprocessing\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\nCode\nnumerical_transformer = Pipeline(steps=[\n    ('imputer_num', SimpleImputer(strategy='median')), \n    ('scaler', StandardScaler())\n])\n\n\n\n\nCode\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n\n\nCode\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)])",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost with pipeline.html#define-model",
    "href": "classification/7.1 XGboost with pipeline.html#define-model",
    "title": "XGboost with pipeline",
    "section": "3.1 define model",
    "text": "3.1 define model\n\n\nCode\nimport xgboost\nprint(xgboost.__version__)\n\n\n2.0.3\n\n\n\n\nCode\nfrom xgboost import XGBClassifier\nml_model = XGBClassifier()\nml_model\n\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. XGBClassifieriNot fittedXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...)",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost with pipeline.html#define-pipline",
    "href": "classification/7.1 XGboost with pipeline.html#define-pipline",
    "title": "XGboost with pipeline",
    "section": "3.2 define pipline",
    "text": "3.2 define pipline\n\n\nCode\npipeline = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', ml_model)\n         ]\n)\n\npipeline\n\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer_num',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['Pclass', 'Age', 'SibSp',\n                                                   'Parch', 'Fare']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('imputer_cat',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('onehot',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['Sex', 'Emba...\n                               feature_types=None, gamma=None, grow_policy=None,\n                               importance_type=None,\n                               interaction_constraints=None, learning_rate=None,\n                               max_bin=None, max_cat_threshold=None,\n                               max_cat_to_onehot=None, max_delta_step=None,\n                               max_depth=None, max_leaves=None,\n                               min_child_weight=None, missing=nan,\n                               monotone_constraints=None, multi_strategy=None,\n                               n_estimators=None, n_jobs=None,\n                               num_parallel_tree=None, random_state=None, ...))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiNot fittedPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer_num',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['Pclass', 'Age', 'SibSp',\n                                                   'Parch', 'Fare']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('imputer_cat',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('onehot',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['Sex', 'Emba...\n                               feature_types=None, gamma=None, grow_policy=None,\n                               importance_type=None,\n                               interaction_constraints=None, learning_rate=None,\n                               max_bin=None, max_cat_threshold=None,\n                               max_cat_to_onehot=None, max_delta_step=None,\n                               max_depth=None, max_leaves=None,\n                               min_child_weight=None, missing=nan,\n                               monotone_constraints=None, multi_strategy=None,\n                               n_estimators=None, n_jobs=None,\n                               num_parallel_tree=None, random_state=None, ...))])  preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('imputer_num',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('scaler', StandardScaler())]),\n                                 ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']),\n                                ('cat',\n                                 Pipeline(steps=[('imputer_cat',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('onehot',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 ['Sex', 'Embarked'])]) num['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='median')  StandardScaler?Documentation for StandardScalerStandardScaler() cat['Sex', 'Embarked']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='most_frequent')  OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore') XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...)",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost with pipeline.html#train-model",
    "href": "classification/7.1 XGboost with pipeline.html#train-model",
    "title": "XGboost with pipeline",
    "section": "3.3 train model",
    "text": "3.3 train model\n\n\nCode\nstart_time = time.time()\n\npipeline.fit(X_train, Y_train)\n\nend_time = time.time()\nduration = end_time - start_time\nduration\n\n\n0.03803300857543945",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost with pipeline.html#preformance",
    "href": "classification/7.1 XGboost with pipeline.html#preformance",
    "title": "XGboost with pipeline",
    "section": "3.4 Preformance",
    "text": "3.4 Preformance\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = pipeline.predict(X_val) #always gets x and retuns y\nY_pred_dt\n\n\narray([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n       1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,\n       1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n       0, 1])\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_val,Y_pred_dt)  \naccuracy\n\n\n0.7666666666666667\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_val,Y_pred_dt)  \nprecision_dt\n\n\n0.75\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_val,Y_pred_dt)  \nrecall_dt\n\n\n0.6486486486486487\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_val,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[45,  8],\n       [13, 24]])\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_val, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.7488526262111168",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost with pipeline.html#k-fold-cross-validation",
    "href": "classification/7.1 XGboost with pipeline.html#k-fold-cross-validation",
    "title": "XGboost with pipeline",
    "section": "3.5 k-Fold Cross-Validation",
    "text": "3.5 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(pipeline, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.8057142857142857",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost with pipeline.html#save-model",
    "href": "classification/7.1 XGboost with pipeline.html#save-model",
    "title": "XGboost with pipeline",
    "section": "3.6 save model",
    "text": "3.6 save model\n\n\nCode\nfrom joblib import dump, load\ndump(ml_model, 'trained_model_7.joblib') \n\n\n['trained_model_7.joblib']\n\n\nsave trained pipeline\n\n\nCode\nfrom joblib import dump, load\ndump(pipeline, 'trained_pipeline_7.joblib', compress=True)  \n\n\n['trained_pipeline_7.joblib']",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost with pipeline.html#load-model",
    "href": "classification/7.1 XGboost with pipeline.html#load-model",
    "title": "XGboost with pipeline",
    "section": "3.7 load model",
    "text": "3.7 load model\n\n\nCode\nmodel_dt_reload = load('trained_model_7.joblib') \n\n\nload pipeline\n\n\nCode\npipeline_reload = load('trained_pipeline_7.joblib')",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost with pipeline.html#final-prediction",
    "href": "classification/7.1 XGboost with pipeline.html#final-prediction",
    "title": "XGboost with pipeline",
    "section": "3.8 final prediction",
    "text": "3.8 final prediction\n\n\nCode\nY_pred_dt_final = pipeline_reload.predict(X_test) #always gets x and retuns y\n\nY_pred_dt_final[0:5]\n\n\narray([0, 0, 0, 1, 0])",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/8 Multiple Models using Pipeline.html",
    "href": "classification/8 Multiple Models using Pipeline.html",
    "title": "Multiple models using pipeline",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nimport time\nfrom siuba import *\n\nfrom sklearn.experimental import enable_halving_search_cv  # noqa\nfrom sklearn.model_selection import HalvingGridSearchCV",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline"
    ]
  },
  {
    "objectID": "classification/8 Multiple Models using Pipeline.html#input-data",
    "href": "classification/8 Multiple Models using Pipeline.html#input-data",
    "title": "Multiple models using pipeline",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\nPassengerId_test = df_test['PassengerId']\n\ndf_train = df_train &gt;&gt; select(~_.PassengerId)\ndf_test = df_test &gt;&gt; select(~_.PassengerId)\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline"
    ]
  },
  {
    "objectID": "classification/8 Multiple Models using Pipeline.html#data-eda",
    "href": "classification/8 Multiple Models using Pipeline.html#data-eda",
    "title": "Multiple models using pipeline",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n\n\n0.7420382165605095\n0.18890814558058924",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline"
    ]
  },
  {
    "objectID": "classification/8 Multiple Models using Pipeline.html#data-wrangling",
    "href": "classification/8 Multiple Models using Pipeline.html#data-wrangling",
    "title": "Multiple models using pipeline",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 11 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Pclass    1309 non-null   int64  \n 1   Name      1309 non-null   object \n 2   Sex       1309 non-null   object \n 3   Age       1046 non-null   float64\n 4   SibSp     1309 non-null   int64  \n 5   Parch     1309 non-null   int64  \n 6   Ticket    1309 non-null   object \n 7   Fare      1308 non-null   float64\n 8   Cabin     295 non-null    object \n 9   Embarked  1307 non-null   object \n 10  role      1309 non-null   object \ndtypes: float64(2), int64(3), object(6)\nmemory usage: 122.7+ KB\n\n\n\n\nCode\n# Dealing with missing numerical variables\n#data['Age'] = data.Age.fillna(data.Age.median())\n#data['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 11 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Pclass    1309 non-null   int64  \n 1   Name      1309 non-null   object \n 2   Sex       1309 non-null   object \n 3   Age       1046 non-null   float64\n 4   SibSp     1309 non-null   int64  \n 5   Parch     1309 non-null   int64  \n 6   Ticket    1309 non-null   object \n 7   Fare      1308 non-null   float64\n 8   Cabin     295 non-null    object \n 9   Embarked  1307 non-null   object \n 10  role      1309 non-null   object \ndtypes: float64(2), int64(3), object(6)\nmemory usage: 122.7+ KB\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\n#data = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\n\n\n\n\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\n\n\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\n\n\n2\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\n\n\n3\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\n\n\n4\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\n\n\n\n\n\n\n\n\n\n\nCode\n# Select features columns\n#data = data[['Sex', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\n\n\n\n\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\n\n\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\n\n\n2\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\n\n\n3\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\n\n\n4\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline"
    ]
  },
  {
    "objectID": "classification/8 Multiple Models using Pipeline.html#split-data",
    "href": "classification/8 Multiple Models using Pipeline.html#split-data",
    "title": "Multiple models using pipeline",
    "section": "2.4 split data",
    "text": "2.4 split data\n60% training / 30% validation/ 10% testing\n\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\n#X_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\n\ntraining_size=0.8\nvalidation_size=0.1\ntesting_size=0.1\n\n\nX_train, X_val, Y_train, Y_val= train_test_split(X, Y, test_size=validation_size, random_state=1)\n\n\nX_train, X_test, Y_train, Y_test= train_test_split(X_train, Y_train, test_size=testing_size/training_size, random_state=1) \n    \n\nX_train = X_train.drop('role', axis=1)\nX_val = X_val.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 700 entries, 658 to 554\nData columns (total 10 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Pclass    700 non-null    int64  \n 1   Name      700 non-null    object \n 2   Sex       700 non-null    object \n 3   Age       559 non-null    float64\n 4   SibSp     700 non-null    int64  \n 5   Parch     700 non-null    int64  \n 6   Ticket    700 non-null    object \n 7   Fare      700 non-null    float64\n 8   Cabin     160 non-null    object \n 9   Embarked  698 non-null    object \ndtypes: float64(2), int64(3), object(5)\nmemory usage: 60.2+ KB\n\n\n\n\nCode\nlen(X_train)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.7856341189674523\n\n\n\n\nCode\nlen(X_val)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.10101010101010101\n\n\n\n\nCode\nlen(X_test)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.11335578002244669",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline"
    ]
  },
  {
    "objectID": "classification/8 Multiple Models using Pipeline.html#categorical_cols-and-numerical_cols",
    "href": "classification/8 Multiple Models using Pipeline.html#categorical_cols-and-numerical_cols",
    "title": "Multiple models using pipeline",
    "section": "2.5 categorical_cols and numerical_cols",
    "text": "2.5 categorical_cols and numerical_cols\n\n\nCode\ncategorical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].nunique() &lt; 10 and X_train[cname].dtype == \"object\"]\n                    \n                    \nnumerical_cols = numerical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].dtype in ['int64', 'float64']]\n\n\n\n\nCode\nprint(\"The total number of categorical columns:\", len(categorical_cols))\nprint(\"The total number of numerical columns:\", len(numerical_cols))\n\n\nThe total number of categorical columns: 2\nThe total number of numerical columns: 5\n\n\n\n\nCode\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train[my_cols].copy()\nX_val = X_val[my_cols].copy()\nX_test= X_test[my_cols].copy()\n\nmy_cols\n#X_final = df_test[my_cols].copy()\n\n\n['Sex', 'Embarked', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline"
    ]
  },
  {
    "objectID": "classification/8 Multiple Models using Pipeline.html#pipelines-for-data-preprocessing",
    "href": "classification/8 Multiple Models using Pipeline.html#pipelines-for-data-preprocessing",
    "title": "Multiple models using pipeline",
    "section": "2.6 Pipelines for Data Preprocessing",
    "text": "2.6 Pipelines for Data Preprocessing\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\nCode\nnumerical_transformer = Pipeline(steps=[\n    ('imputer_num', SimpleImputer(strategy='median')), \n    ('scaler', StandardScaler())\n])\n\n\n\n\nCode\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n\n\nCode\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)])",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline"
    ]
  },
  {
    "objectID": "classification/8 Multiple Models using Pipeline.html#define-model",
    "href": "classification/8 Multiple Models using Pipeline.html#define-model",
    "title": "Multiple models using pipeline",
    "section": "3.1 define model",
    "text": "3.1 define model\n\n\nCode\nimport xgboost\nprint(xgboost.__version__)\n\n\n2.0.3\n\n\n\n3.1.1 XGB model\n\n\nCode\nfrom xgboost import XGBClassifier\nxgb_model = XGBClassifier()\nxgb_model\n\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. XGBClassifieriNot fittedXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...) \n\n\n\n\n3.1.2 Random Forest model\n\n\nCode\nfrom sklearn.ensemble import RandomForestClassifier\nrandom_forest_model = RandomForestClassifier()\nrandom_forest_model\n\n\nRandomForestClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriNot fittedRandomForestClassifier() \n\n\n\n\n3.1.3 Logistic Regression model\n\n\nCode\nfrom sklearn.linear_model import LogisticRegression\nLogisticRegression_model = LogisticRegression(solver='liblinear')\nLogisticRegression_model\n\n\nLogisticRegression(solver='liblinear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniNot fittedLogisticRegression(solver='liblinear')",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline"
    ]
  },
  {
    "objectID": "classification/8 Multiple Models using Pipeline.html#define-pipline",
    "href": "classification/8 Multiple Models using Pipeline.html#define-pipline",
    "title": "Multiple models using pipeline",
    "section": "3.2 define pipline",
    "text": "3.2 define pipline\n\n\nCode\npipeline_xgb = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', xgb_model)\n         ]\n)\n\npipeline_rf = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', random_forest_model)\n         ]\n)\n\npipeline_lr = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', LogisticRegression_model)\n         ]\n)",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline"
    ]
  },
  {
    "objectID": "classification/8 Multiple Models using Pipeline.html#define-gridsearch",
    "href": "classification/8 Multiple Models using Pipeline.html#define-gridsearch",
    "title": "Multiple models using pipeline",
    "section": "3.3 define GridSearch",
    "text": "3.3 define GridSearch\n\n\nCode\n# Grid_xgb = HalvingGridSearchCV(pipeline_xgb\n#                 ,parameters\n#                 ,scoring='accuracy'\n#                 ,max_resources=100\n#                 , cv=10, n_jobs=-1)\n#                 \n# Grid_rf = HalvingGridSearchCV(pipeline_rf\n#                 ,parameters\n#                 ,scoring='accuracy'\n#                 ,max_resources=100\n#                 , cv=10, n_jobs=-1)\n#                 \n# Grid_lr = HalvingGridSearchCV(pipeline_lr\n#                 ,parameters\n#                 ,scoring='accuracy'\n#                 ,max_resources=100\n#                 , cv=10, n_jobs=-1)",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline"
    ]
  },
  {
    "objectID": "classification/8 Multiple Models using Pipeline.html#train-model",
    "href": "classification/8 Multiple Models using Pipeline.html#train-model",
    "title": "Multiple models using pipeline",
    "section": "3.4 train model",
    "text": "3.4 train model\n\n\nCode\nstart_time = time.time()\n\n\npipelines = [pipeline_xgb, pipeline_rf, pipeline_lr, ]\nfor pipe in pipelines:\n    pipe.fit(X_train,Y_train)\n\n\nend_time = time.time()\nduration = end_time - start_time\nduration\n\n\n0.10587692260742188",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline"
    ]
  },
  {
    "objectID": "classification/8 Multiple Models using Pipeline.html#preformance",
    "href": "classification/8 Multiple Models using Pipeline.html#preformance",
    "title": "Multiple models using pipeline",
    "section": "3.5 Preformance",
    "text": "3.5 Preformance\n\n\nCode\ngrid_dict = {0: 'XGB', 1: 'random forest', 2: 'linear regression'}\n\nfor i, model in enumerate(pipelines):\n    print('{} Test Accuracy: {}'.format(grid_dict[i],\n    model.score(X_test,Y_test)))\n    #print('{} Best Params: {}'.format(grid_dict[i],          model.best_params_))\n\n\nXGB Test Accuracy: 0.8514851485148515\nrandom forest Test Accuracy: 0.801980198019802\nlinear regression Test Accuracy: 0.7821782178217822\n\n\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = pipeline_xgb.predict(X_test) #always gets x and retuns y\nY_pred_dt\n\n\narray([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n       1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,\n       0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,\n       1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0])\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_test,Y_pred_dt)  \naccuracy\n\n\n0.8514851485148515\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_test,Y_pred_dt)  \nprecision_dt\n\n\n0.8064516129032258\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_test,Y_pred_dt)  \nrecall_dt\n\n\n0.7352941176470589\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_test,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[61,  6],\n       [ 9, 25]])\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_test, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.8228709394205443",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline"
    ]
  },
  {
    "objectID": "classification/8 Multiple Models using Pipeline.html#k-fold-cross-validation",
    "href": "classification/8 Multiple Models using Pipeline.html#k-fold-cross-validation",
    "title": "Multiple models using pipeline",
    "section": "3.6 k-Fold Cross-Validation",
    "text": "3.6 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(pipeline_xgb, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.8042857142857143",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline"
    ]
  },
  {
    "objectID": "classification/8 Multiple Models using Pipeline.html#save-model",
    "href": "classification/8 Multiple Models using Pipeline.html#save-model",
    "title": "Multiple models using pipeline",
    "section": "3.7 save model",
    "text": "3.7 save model\n\n\nCode\nfrom joblib import dump, load\ndump(pipeline_xgb, 'trained_pipeline_8.joblib', compress=True)  \n\n\n['trained_pipeline_8.joblib']",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline"
    ]
  },
  {
    "objectID": "classification/8 Multiple Models using Pipeline.html#load-model",
    "href": "classification/8 Multiple Models using Pipeline.html#load-model",
    "title": "Multiple models using pipeline",
    "section": "3.8 load model",
    "text": "3.8 load model\n\n\nCode\nmodel_reload = load('trained_pipeline_8.joblib')",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline"
    ]
  },
  {
    "objectID": "classification/8 Multiple Models using Pipeline.html#final-prediction",
    "href": "classification/8 Multiple Models using Pipeline.html#final-prediction",
    "title": "Multiple models using pipeline",
    "section": "3.9 final prediction",
    "text": "3.9 final prediction\n\n\nCode\nY_pred_dt_final = model_reload.predict(X_val) #always gets x and retuns y\n\nY_pred_dt_final[0:5]\n\n\narray([1, 0, 1, 1, 1])",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline"
    ]
  },
  {
    "objectID": "classification/8.2 Multiple Models using Pipeline and tuning.html",
    "href": "classification/8.2 Multiple Models using Pipeline and tuning.html",
    "title": "Multiple models using pipeline and tuning",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nimport time\nfrom siuba import *\n\nfrom sklearn.experimental import enable_halving_search_cv  # noqa\nfrom sklearn.model_selection import HalvingGridSearchCV",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline and tuning"
    ]
  },
  {
    "objectID": "classification/8.2 Multiple Models using Pipeline and tuning.html#input-data",
    "href": "classification/8.2 Multiple Models using Pipeline and tuning.html#input-data",
    "title": "Multiple models using pipeline and tuning",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\nPassengerId_test = df_test['PassengerId']\n\ndf_train = df_train &gt;&gt; select(~_.PassengerId)\ndf_test = df_test &gt;&gt; select(~_.PassengerId)\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline and tuning"
    ]
  },
  {
    "objectID": "classification/8.2 Multiple Models using Pipeline and tuning.html#data-eda",
    "href": "classification/8.2 Multiple Models using Pipeline and tuning.html#data-eda",
    "title": "Multiple models using pipeline and tuning",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n\n\n0.7420382165605095\n0.18890814558058924",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline and tuning"
    ]
  },
  {
    "objectID": "classification/8.2 Multiple Models using Pipeline and tuning.html#data-wrangling",
    "href": "classification/8.2 Multiple Models using Pipeline and tuning.html#data-wrangling",
    "title": "Multiple models using pipeline and tuning",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 11 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Pclass    1309 non-null   int64  \n 1   Name      1309 non-null   object \n 2   Sex       1309 non-null   object \n 3   Age       1046 non-null   float64\n 4   SibSp     1309 non-null   int64  \n 5   Parch     1309 non-null   int64  \n 6   Ticket    1309 non-null   object \n 7   Fare      1308 non-null   float64\n 8   Cabin     295 non-null    object \n 9   Embarked  1307 non-null   object \n 10  role      1309 non-null   object \ndtypes: float64(2), int64(3), object(6)\nmemory usage: 122.7+ KB\n\n\n\n\nCode\n# Dealing with missing numerical variables\n#data['Age'] = data.Age.fillna(data.Age.median())\n#data['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 11 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Pclass    1309 non-null   int64  \n 1   Name      1309 non-null   object \n 2   Sex       1309 non-null   object \n 3   Age       1046 non-null   float64\n 4   SibSp     1309 non-null   int64  \n 5   Parch     1309 non-null   int64  \n 6   Ticket    1309 non-null   object \n 7   Fare      1308 non-null   float64\n 8   Cabin     295 non-null    object \n 9   Embarked  1307 non-null   object \n 10  role      1309 non-null   object \ndtypes: float64(2), int64(3), object(6)\nmemory usage: 122.7+ KB\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\n#data = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\n\n\n\n\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\n\n\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\n\n\n2\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\n\n\n3\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\n\n\n4\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\n\n\n\n\n\n\n\n\n\n\nCode\n# Select features columns\n#data = data[['Sex', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\n\n\n\n\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\n\n\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\n\n\n2\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\n\n\n3\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\n\n\n4\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline and tuning"
    ]
  },
  {
    "objectID": "classification/8.2 Multiple Models using Pipeline and tuning.html#split-data",
    "href": "classification/8.2 Multiple Models using Pipeline and tuning.html#split-data",
    "title": "Multiple models using pipeline and tuning",
    "section": "2.4 split data",
    "text": "2.4 split data\n60% training / 30% validation/ 10% testing\n\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\n#X_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\n\ntraining_size=0.8\nvalidation_size=0.1\ntesting_size=0.1\n\n\nX_train, X_val, Y_train, Y_val= train_test_split(X, Y, test_size=validation_size, random_state=1)\n\n\nX_train, X_test, Y_train, Y_test= train_test_split(X_train, Y_train, test_size=testing_size/training_size, random_state=1) \n    \n\nX_train = X_train.drop('role', axis=1)\nX_val = X_val.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 700 entries, 658 to 554\nData columns (total 10 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Pclass    700 non-null    int64  \n 1   Name      700 non-null    object \n 2   Sex       700 non-null    object \n 3   Age       559 non-null    float64\n 4   SibSp     700 non-null    int64  \n 5   Parch     700 non-null    int64  \n 6   Ticket    700 non-null    object \n 7   Fare      700 non-null    float64\n 8   Cabin     160 non-null    object \n 9   Embarked  698 non-null    object \ndtypes: float64(2), int64(3), object(5)\nmemory usage: 60.2+ KB\n\n\n\n\nCode\nlen(X_train)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.7856341189674523\n\n\n\n\nCode\nlen(X_val)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.10101010101010101\n\n\n\n\nCode\nlen(X_test)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.11335578002244669",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline and tuning"
    ]
  },
  {
    "objectID": "classification/8.2 Multiple Models using Pipeline and tuning.html#categorical_cols-and-numerical_cols",
    "href": "classification/8.2 Multiple Models using Pipeline and tuning.html#categorical_cols-and-numerical_cols",
    "title": "Multiple models using pipeline and tuning",
    "section": "2.5 categorical_cols and numerical_cols",
    "text": "2.5 categorical_cols and numerical_cols\n\n\nCode\ncategorical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].nunique() &lt; 10 and X_train[cname].dtype == \"object\"]\n                    \n                    \nnumerical_cols = numerical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].dtype in ['int64', 'float64']]\n\n\n\n\nCode\nprint(\"The total number of categorical columns:\", len(categorical_cols))\nprint(\"The total number of numerical columns:\", len(numerical_cols))\n\n\nThe total number of categorical columns: 2\nThe total number of numerical columns: 5\n\n\n\n\nCode\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train[my_cols].copy()\nX_val = X_val[my_cols].copy()\nX_test= X_test[my_cols].copy()\n\nmy_cols\n#X_final = df_test[my_cols].copy()\n\n\n['Sex', 'Embarked', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline and tuning"
    ]
  },
  {
    "objectID": "classification/8.2 Multiple Models using Pipeline and tuning.html#pipelines-for-data-preprocessing",
    "href": "classification/8.2 Multiple Models using Pipeline and tuning.html#pipelines-for-data-preprocessing",
    "title": "Multiple models using pipeline and tuning",
    "section": "2.6 Pipelines for Data Preprocessing",
    "text": "2.6 Pipelines for Data Preprocessing\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\nCode\nnumerical_transformer = Pipeline(steps=[\n    ('imputer_num', SimpleImputer(strategy='median')), \n    ('scaler', StandardScaler())\n])\n\n\n\n\nCode\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n\n\nCode\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)])",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline and tuning"
    ]
  },
  {
    "objectID": "classification/8.2 Multiple Models using Pipeline and tuning.html#define-model",
    "href": "classification/8.2 Multiple Models using Pipeline and tuning.html#define-model",
    "title": "Multiple models using pipeline and tuning",
    "section": "3.1 define model",
    "text": "3.1 define model\n\n\nCode\nimport xgboost\nprint(xgboost.__version__)\n\n\n2.0.3\n\n\n\n3.1.1 XGB model\n\n\nCode\nfrom xgboost import XGBClassifier\nxgb_model = XGBClassifier()\nxgb_model\n\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. XGBClassifieriNot fittedXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...) \n\n\n\n\n3.1.2 Random Forest model\n\n\nCode\nfrom sklearn.ensemble import RandomForestClassifier\nrandom_forest_model = RandomForestClassifier()\nrandom_forest_model\n\n\nRandomForestClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriNot fittedRandomForestClassifier() \n\n\n\n\n3.1.3 Logistic Regression model\n\n\nCode\nfrom sklearn.linear_model import LogisticRegression\nLogisticRegression_model = LogisticRegression(solver='liblinear')\nLogisticRegression_model\n\n\nLogisticRegression(solver='liblinear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniNot fittedLogisticRegression(solver='liblinear')",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline and tuning"
    ]
  },
  {
    "objectID": "classification/8.2 Multiple Models using Pipeline and tuning.html#define-pipline",
    "href": "classification/8.2 Multiple Models using Pipeline and tuning.html#define-pipline",
    "title": "Multiple models using pipeline and tuning",
    "section": "3.2 define pipline",
    "text": "3.2 define pipline\n\n\nCode\npipeline_xgb = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', xgb_model)\n         ]\n)\n\npipeline_rf = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', random_forest_model)\n         ]\n)\n\npipeline_lr = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', LogisticRegression_model)\n         ]\n)",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline and tuning"
    ]
  },
  {
    "objectID": "classification/8.2 Multiple Models using Pipeline and tuning.html#define-gridsearch",
    "href": "classification/8.2 Multiple Models using Pipeline and tuning.html#define-gridsearch",
    "title": "Multiple models using pipeline and tuning",
    "section": "3.3 define GridSearch",
    "text": "3.3 define GridSearch\n\n\nCode\nparameters_xgb= {\n        'model__learning_rate': [0.01, 0.02,0.08,0.1],\n        'model__max_depth': [3, 5, 7,8,9,10,20],\n        'model__min_child_weight': [1, 3,5,8],\n        'model__subsample': [0.5, 0.7,0.9],\n        \n       # 'model__colsample__bytree': [0.5, 0.7],\n       \n        'model__n_estimators' : [100, 200, 500],\n        'model__objective': ['reg:squarederror']\n    }\n\n\nGrid_xgb = HalvingGridSearchCV(pipeline_xgb\n                ,parameters_xgb \n                ,scoring='accuracy'\n                ,max_resources=100\n                , cv=10, n_jobs=-1)\n                \n                \nparameters_rf = {'model__max_depth':[20,30,40],\n                 'model__n_estimators':[200,250,300],\n                 'model__min_samples_leaf':[1,2,3]\n                 }                \n                \n\nGrid_rf = HalvingGridSearchCV(pipeline_rf\n                ,parameters_rf\n                ,scoring='accuracy'\n                ,max_resources=100\n                , cv=10, n_jobs=-1)",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline and tuning"
    ]
  },
  {
    "objectID": "classification/8.2 Multiple Models using Pipeline and tuning.html#train-model",
    "href": "classification/8.2 Multiple Models using Pipeline and tuning.html#train-model",
    "title": "Multiple models using pipeline and tuning",
    "section": "3.4 train model",
    "text": "3.4 train model\n\n\nCode\nstart_time = time.time()\n\n\nGrids = [Grid_xgb, Grid_rf,pipeline_xgb,pipeline_rf,pipeline_lr]\nfor Grid in Grids:\n    Grid.fit(X_train,Y_train)\n\n\nend_time = time.time()\nduration = end_time - start_time\nduration\n\n\n45.8913140296936",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline and tuning"
    ]
  },
  {
    "objectID": "classification/8.2 Multiple Models using Pipeline and tuning.html#preformance",
    "href": "classification/8.2 Multiple Models using Pipeline and tuning.html#preformance",
    "title": "Multiple models using pipeline and tuning",
    "section": "3.5 Preformance",
    "text": "3.5 Preformance\n\n\nCode\ngrid_dict = {0: 'XGB', 1: 'random forest', 2: 'XGB non tune',3: 'ramdon forest non tune',4:'linear regression non tune' }\n\nfor i, model in enumerate(Grids):\n    print('{} Test Accuracy: {}'.format(grid_dict[i],\n    model.score(X_test,Y_test)))\n    #print('{} Best Params: {}'.format(grid_dict[i], model.best_params_))\n\n\nXGB Test Accuracy: 0.8712871287128713\nrandom forest Test Accuracy: 0.8217821782178217\nXGB non tune Test Accuracy: 0.8514851485148515\nramdon forest non tune Test Accuracy: 0.8217821782178217\nlinear regression non tune Test Accuracy: 0.7821782178217822\n\n\n\n\nCode\nbest_ml=Grid_xgb.best_estimator_\n\n\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = best_ml.predict(X_test) #always gets x and retuns y\nY_pred_dt\n\n\narray([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n       0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n       0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_test,Y_pred_dt)  \naccuracy\n\n\n0.8712871287128713\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_test,Y_pred_dt)  \nprecision_dt\n\n\n0.9565217391304348\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_test,Y_pred_dt)  \nrecall_dt\n\n\n0.6470588235294118\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_test,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[66,  1],\n       [12, 22]])\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_test, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.8160667251975416",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline and tuning"
    ]
  },
  {
    "objectID": "classification/8.2 Multiple Models using Pipeline and tuning.html#k-fold-cross-validation",
    "href": "classification/8.2 Multiple Models using Pipeline and tuning.html#k-fold-cross-validation",
    "title": "Multiple models using pipeline and tuning",
    "section": "3.6 k-Fold Cross-Validation",
    "text": "3.6 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(pipeline_xgb, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.7985714285714286",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline and tuning"
    ]
  },
  {
    "objectID": "classification/8.2 Multiple Models using Pipeline and tuning.html#save-model",
    "href": "classification/8.2 Multiple Models using Pipeline and tuning.html#save-model",
    "title": "Multiple models using pipeline and tuning",
    "section": "3.7 save model",
    "text": "3.7 save model\n\n\nCode\nfrom joblib import dump, load\ndump(Grid_xgb, 'trained_grid_8_2.joblib', compress=True)  \n\n\n['trained_grid_8_2.joblib']",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline and tuning"
    ]
  },
  {
    "objectID": "classification/8.2 Multiple Models using Pipeline and tuning.html#load-model",
    "href": "classification/8.2 Multiple Models using Pipeline and tuning.html#load-model",
    "title": "Multiple models using pipeline and tuning",
    "section": "3.8 load model",
    "text": "3.8 load model\n\n\nCode\nmodel_reload = load('trained_grid_8_2.joblib') \n\n\n\n\nCode\nbest_ml=model_reload.best_estimator_",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline and tuning"
    ]
  },
  {
    "objectID": "classification/8.2 Multiple Models using Pipeline and tuning.html#final-prediction",
    "href": "classification/8.2 Multiple Models using Pipeline and tuning.html#final-prediction",
    "title": "Multiple models using pipeline and tuning",
    "section": "3.9 final prediction",
    "text": "3.9 final prediction\n\n\nCode\nY_pred_dt_final =best_ml.predict(X_val) #always gets x and retuns y\n\nY_pred_dt_final[0:5]\n\n\narray([1, 0, 1, 1, 1])",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline and tuning"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html#change-name",
    "href": "plot/1 seaborn.html#change-name",
    "title": "seaborn chart",
    "section": "8.3 change name",
    "text": "8.3 change name\n\n\nCode\nax=sns.boxplot(x = \"day\", y = \"total_bill\", data = df)\nax.set_title(\"tips box plot \")\nax.set(xlabel='x-axis label', ylabel='y-axis label')",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "plot/3 plotly.html#change-x-y-name",
    "href": "plot/3 plotly.html#change-x-y-name",
    "title": "Plotly chart",
    "section": "8.3 change x y name",
    "text": "8.3 change x y name\n\n\nCode\nfig = px.scatter(tips,x=\"tip\", y=\"total_bill\"\n                ,labels={\n                     \"tip\": \"new x label)\",\n                     \"total_bill\": \"new y label\"\n                 }\n\n)\n\n    \nfig.show()",
    "crumbs": [
      "Plot",
      "Plotly chart"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html#change-x-y-name",
    "href": "plot/1 seaborn.html#change-x-y-name",
    "title": "seaborn chart",
    "section": "8.3 change x y name",
    "text": "8.3 change x y name\n\n\nCode\nax=sns.boxplot(x = \"day\", y = \"total_bill\", data = df)\nax.set_title(\"tips box plot \")\nax.set(xlabel='x-axis label', ylabel='y-axis label')",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "plot/2 plotnine.html#change-x-y-name",
    "href": "plot/2 plotnine.html#change-x-y-name",
    "title": "plotnine chart",
    "section": "8.3 change x y name",
    "text": "8.3 change x y name\n\n\nCode\np=(\n    ggplot(data=tips)+aes(x=\"tip\",y=\"total_bill\")+ geom_point()\n)\n\np",
    "crumbs": [
      "Plot",
      "plotnine chart"
    ]
  }
]