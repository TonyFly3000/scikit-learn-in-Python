[
  {
    "objectID": "model type/2 random forest.html",
    "href": "model type/2 random forest.html",
    "title": "Random forest",
    "section": "",
    "text": "1 Pros\n\nEasier to interpret than Neural Network\nFast training and making inference\n\n\n\n2 Cons\n\nThe size & inference speed of the random forest can sometimes be an issue\nRandom forests cannot learn and reuse internal representations\n\n\n\n3 reference:\nhttps://www.youtube.com/watch?v=w5gB8zyLx-8",
    "crumbs": [
      "model type",
      "Random forest"
    ]
  },
  {
    "objectID": "model type/3 gradient boosted trees.html",
    "href": "model type/3 gradient boosted trees.html",
    "title": "Gradient boosted trees",
    "section": "",
    "text": "1 Pros\n\nNative support for numerical and categorical features, and no necessary need for feature pre-processing.\nGBT are fast and light-weight and with great performance.\n\n\n\n2 Cons\n\nGBT can overfit.\nDecision tree trained sequentially -&gt; slower training.\nGBT cannot learn & reuse internal representations.Poor performance on image and long text.\n\n\n\n3 reference:\nhttps://www.youtube.com/watch?v=w5gB8zyLx-8",
    "crumbs": [
      "model type",
      "Gradient boosted trees"
    ]
  },
  {
    "objectID": "data/2 siuba.html",
    "href": "data/2 siuba.html",
    "title": "Data manipulation with siuba",
    "section": "",
    "text": "1 reference:\nhttps://siuba.org/",
    "crumbs": [
      "Data",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "regression/3 Random forest on house price data.html",
    "href": "regression/3 Random forest on house price data.html",
    "title": "Random forest and pipeline",
    "section": "",
    "text": "with pipeline",
    "crumbs": [
      "Regression",
      "Random forest and pipeline"
    ]
  },
  {
    "objectID": "regression/3 Random forest on house price data.html#input-data",
    "href": "regression/3 Random forest on house price data.html#input-data",
    "title": "Random forest and pipeline",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nId = df_train['Id']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nId\nMSSubClass\nMSZoning\nLotFrontage\nLotArea\nStreet\nAlley\nLotShape\nLandContour\nUtilities\n...\nPoolArea\nPoolQC\nFence\nMiscFeature\nMiscVal\nMoSold\nYrSold\nSaleType\nSaleCondition\nSalePrice\n\n\n\n\n0\n1\n60\nRL\n65.0\n8450\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n2\n2008\nWD\nNormal\n208500\n\n\n1\n2\n20\nRL\n80.0\n9600\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2007\nWD\nNormal\n181500\n\n\n2\n3\n60\nRL\n68.0\n11250\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n9\n2008\nWD\nNormal\n223500\n\n\n3\n4\n70\nRL\n60.0\n9550\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n2\n2006\nWD\nAbnorml\n140000\n\n\n4\n5\n60\nRL\n84.0\n14260\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n12\n2008\nWD\nNormal\n250000\n\n\n\n\n5 rows × 81 columns\n\n\n\n\n\n\nCode\n#df_train.info()\n\n\n\n\nCode\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['SalePrice'], axis=1), df_test])",
    "crumbs": [
      "Regression",
      "Random forest and pipeline"
    ]
  },
  {
    "objectID": "regression/3 Random forest on house price data.html#data-eda",
    "href": "regression/3 Random forest on house price data.html#data-eda",
    "title": "Random forest and pipeline",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\nin step 1",
    "crumbs": [
      "Regression",
      "Random forest and pipeline"
    ]
  },
  {
    "objectID": "regression/3 Random forest on house price data.html#data-wrangling",
    "href": "regression/3 Random forest on house price data.html#data-wrangling",
    "title": "Random forest and pipeline",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling",
    "crumbs": [
      "Regression",
      "Random forest and pipeline"
    ]
  },
  {
    "objectID": "regression/3 Random forest on house price data.html#split-data",
    "href": "regression/3 Random forest on house price data.html#split-data",
    "title": "Random forest and pipeline",
    "section": "2.4 split data",
    "text": "2.4 split data\n\n\nCode\nY = df_train.SalePrice\nX = df_train.drop(['SalePrice'], axis=1)\n\n\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\nX_train = X_train.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nprint(X_train.shape)\nprint(X_test.shape)\n\n\n(1168, 80)\n(292, 80)\n\n\n\n\nCode\nprint(Y_train.shape)\nprint(Y_test.shape)\n\n\n(1168,)\n(292,)",
    "crumbs": [
      "Regression",
      "Random forest and pipeline"
    ]
  },
  {
    "objectID": "regression/3 Random forest on house price data.html#pipelines-for-data-preprocessing",
    "href": "regression/3 Random forest on house price data.html#pipelines-for-data-preprocessing",
    "title": "Random forest and pipeline",
    "section": "2.6 Pipelines for Data Preprocessing",
    "text": "2.6 Pipelines for Data Preprocessing\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\nCode\nnumerical_transformer = Pipeline(steps=[\n    ('imputer_num', SimpleImputer(strategy='median')), \n    ('scaler', StandardScaler())\n])\n\n\n\n\nCode\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n\n\nCode\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)])",
    "crumbs": [
      "Regression",
      "Random forest and pipeline"
    ]
  },
  {
    "objectID": "regression/3 Random forest on house price data.html#define-model",
    "href": "regression/3 Random forest on house price data.html#define-model",
    "title": "Random forest and pipeline",
    "section": "3.1 define model",
    "text": "3.1 define model\nrandom forest with hyper parameter tuning\n\n\nCode\nfrom sklearn.ensemble import RandomForestRegressor\n   \n\nml_model = RandomForestRegressor(random_state=0)\nml_model\n\n\nRandomForestRegressor(random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestRegressor?Documentation for RandomForestRegressoriNot fittedRandomForestRegressor(random_state=0)",
    "crumbs": [
      "Regression",
      "Random forest and pipeline"
    ]
  },
  {
    "objectID": "regression/3 Random forest on house price data.html#define-pipline",
    "href": "regression/3 Random forest on house price data.html#define-pipline",
    "title": "Random forest and pipeline",
    "section": "3.2 define pipline",
    "text": "3.2 define pipline\n\n\nCode\npipeline = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', ml_model)\n         ]\n)",
    "crumbs": [
      "Regression",
      "Random forest and pipeline"
    ]
  },
  {
    "objectID": "regression/3 Random forest on house price data.html#train-model",
    "href": "regression/3 Random forest on house price data.html#train-model",
    "title": "Random forest and pipeline",
    "section": "3.3 train model",
    "text": "3.3 train model\n\n\nCode\npipeline.fit(X_train, Y_train)\n\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer_num',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['Id', 'MSSubClass',\n                                                   'LotFrontage', 'LotArea',\n                                                   'OverallQual', 'OverallCond',\n                                                   'YearBuilt', 'YearRemodAdd',\n                                                   'MasVnrArea', 'BsmtFinSF1',\n                                                   'BsmtFinSF2', 'BsmtUnfSF',\n                                                   'TotalBsmtSF', '1stFlrSF',\n                                                   '2ndFlrSF'...\n                                                   'LotConfig', 'LandSlope',\n                                                   'Condition1', 'Condition2',\n                                                   'BldgType', 'HouseStyle',\n                                                   'RoofStyle', 'RoofMatl',\n                                                   'MasVnrType', 'ExterQual',\n                                                   'ExterCond', 'Foundation',\n                                                   'BsmtQual', 'BsmtCond',\n                                                   'BsmtExposure',\n                                                   'BsmtFinType1',\n                                                   'BsmtFinType2', 'Heating',\n                                                   'HeatingQC', 'CentralAir',\n                                                   'Electrical', 'KitchenQual',\n                                                   'Functional', 'FireplaceQu', ...])])),\n                ('model', RandomForestRegressor(random_state=0))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiFittedPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer_num',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['Id', 'MSSubClass',\n                                                   'LotFrontage', 'LotArea',\n                                                   'OverallQual', 'OverallCond',\n                                                   'YearBuilt', 'YearRemodAdd',\n                                                   'MasVnrArea', 'BsmtFinSF1',\n                                                   'BsmtFinSF2', 'BsmtUnfSF',\n                                                   'TotalBsmtSF', '1stFlrSF',\n                                                   '2ndFlrSF'...\n                                                   'LotConfig', 'LandSlope',\n                                                   'Condition1', 'Condition2',\n                                                   'BldgType', 'HouseStyle',\n                                                   'RoofStyle', 'RoofMatl',\n                                                   'MasVnrType', 'ExterQual',\n                                                   'ExterCond', 'Foundation',\n                                                   'BsmtQual', 'BsmtCond',\n                                                   'BsmtExposure',\n                                                   'BsmtFinType1',\n                                                   'BsmtFinType2', 'Heating',\n                                                   'HeatingQC', 'CentralAir',\n                                                   'Electrical', 'KitchenQual',\n                                                   'Functional', 'FireplaceQu', ...])])),\n                ('model', RandomForestRegressor(random_state=0))])  preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('imputer_num',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('scaler', StandardScaler())]),\n                                 ['Id', 'MSSubClass', 'LotFrontage', 'LotArea',\n                                  'OverallQual', 'OverallCond', 'YearBuilt',\n                                  'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1',\n                                  'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF',\n                                  '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n                                  'GrLivArea', 'Bsm...\n                                 ['MSZoning', 'Street', 'Alley', 'LotShape',\n                                  'LandContour', 'Utilities', 'LotConfig',\n                                  'LandSlope', 'Condition1', 'Condition2',\n                                  'BldgType', 'HouseStyle', 'RoofStyle',\n                                  'RoofMatl', 'MasVnrType', 'ExterQual',\n                                  'ExterCond', 'Foundation', 'BsmtQual',\n                                  'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n                                  'BsmtFinType2', 'Heating', 'HeatingQC',\n                                  'CentralAir', 'Electrical', 'KitchenQual',\n                                  'Functional', 'FireplaceQu', ...])]) num['Id', 'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='median')  StandardScaler?Documentation for StandardScalerStandardScaler() cat['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'SaleType', 'SaleCondition']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='most_frequent')  OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore')  RandomForestRegressor?Documentation for RandomForestRegressorRandomForestRegressor(random_state=0) \n\n\n\n\nCode\nvar=pipeline[:-1].get_feature_names_out()\nvar\n\n\narray(['num__Id', 'num__MSSubClass', 'num__LotFrontage', 'num__LotArea',\n       'num__OverallQual', 'num__OverallCond', 'num__YearBuilt',\n       'num__YearRemodAdd', 'num__MasVnrArea', 'num__BsmtFinSF1',\n       'num__BsmtFinSF2', 'num__BsmtUnfSF', 'num__TotalBsmtSF',\n       'num__1stFlrSF', 'num__2ndFlrSF', 'num__LowQualFinSF',\n       'num__GrLivArea', 'num__BsmtFullBath', 'num__BsmtHalfBath',\n       'num__FullBath', 'num__HalfBath', 'num__BedroomAbvGr',\n       'num__KitchenAbvGr', 'num__TotRmsAbvGrd', 'num__Fireplaces',\n       'num__GarageYrBlt', 'num__GarageCars', 'num__GarageArea',\n       'num__WoodDeckSF', 'num__OpenPorchSF', 'num__EnclosedPorch',\n       'num__3SsnPorch', 'num__ScreenPorch', 'num__PoolArea',\n       'num__MiscVal', 'num__MoSold', 'num__YrSold',\n       'cat__MSZoning_C (all)', 'cat__MSZoning_FV', 'cat__MSZoning_RH',\n       'cat__MSZoning_RL', 'cat__MSZoning_RM', 'cat__Street_Grvl',\n       'cat__Street_Pave', 'cat__Alley_Grvl', 'cat__Alley_Pave',\n       'cat__LotShape_IR1', 'cat__LotShape_IR2', 'cat__LotShape_IR3',\n       'cat__LotShape_Reg', 'cat__LandContour_Bnk',\n       'cat__LandContour_HLS', 'cat__LandContour_Low',\n       'cat__LandContour_Lvl', 'cat__Utilities_AllPub',\n       'cat__Utilities_NoSeWa', 'cat__LotConfig_Corner',\n       'cat__LotConfig_CulDSac', 'cat__LotConfig_FR2',\n       'cat__LotConfig_FR3', 'cat__LotConfig_Inside',\n       'cat__LandSlope_Gtl', 'cat__LandSlope_Mod', 'cat__LandSlope_Sev',\n       'cat__Condition1_Artery', 'cat__Condition1_Feedr',\n       'cat__Condition1_Norm', 'cat__Condition1_PosA',\n       'cat__Condition1_PosN', 'cat__Condition1_RRAe',\n       'cat__Condition1_RRAn', 'cat__Condition1_RRNe',\n       'cat__Condition1_RRNn', 'cat__Condition2_Artery',\n       'cat__Condition2_Feedr', 'cat__Condition2_Norm',\n       'cat__Condition2_PosA', 'cat__Condition2_PosN',\n       'cat__Condition2_RRAe', 'cat__Condition2_RRAn',\n       'cat__Condition2_RRNn', 'cat__BldgType_1Fam',\n       'cat__BldgType_2fmCon', 'cat__BldgType_Duplex',\n       'cat__BldgType_Twnhs', 'cat__BldgType_TwnhsE',\n       'cat__HouseStyle_1.5Fin', 'cat__HouseStyle_1.5Unf',\n       'cat__HouseStyle_1Story', 'cat__HouseStyle_2.5Fin',\n       'cat__HouseStyle_2.5Unf', 'cat__HouseStyle_2Story',\n       'cat__HouseStyle_SFoyer', 'cat__HouseStyle_SLvl',\n       'cat__RoofStyle_Flat', 'cat__RoofStyle_Gable',\n       'cat__RoofStyle_Gambrel', 'cat__RoofStyle_Hip',\n       'cat__RoofStyle_Mansard', 'cat__RoofStyle_Shed',\n       'cat__RoofMatl_ClyTile', 'cat__RoofMatl_CompShg',\n       'cat__RoofMatl_Metal', 'cat__RoofMatl_Roll',\n       'cat__RoofMatl_Tar&Grv', 'cat__RoofMatl_WdShake',\n       'cat__RoofMatl_WdShngl', 'cat__MasVnrType_BrkCmn',\n       'cat__MasVnrType_BrkFace', 'cat__MasVnrType_Stone',\n       'cat__ExterQual_Ex', 'cat__ExterQual_Fa', 'cat__ExterQual_Gd',\n       'cat__ExterQual_TA', 'cat__ExterCond_Ex', 'cat__ExterCond_Fa',\n       'cat__ExterCond_Gd', 'cat__ExterCond_Po', 'cat__ExterCond_TA',\n       'cat__Foundation_BrkTil', 'cat__Foundation_CBlock',\n       'cat__Foundation_PConc', 'cat__Foundation_Slab',\n       'cat__Foundation_Stone', 'cat__Foundation_Wood',\n       'cat__BsmtQual_Ex', 'cat__BsmtQual_Fa', 'cat__BsmtQual_Gd',\n       'cat__BsmtQual_TA', 'cat__BsmtCond_Fa', 'cat__BsmtCond_Gd',\n       'cat__BsmtCond_Po', 'cat__BsmtCond_TA', 'cat__BsmtExposure_Av',\n       'cat__BsmtExposure_Gd', 'cat__BsmtExposure_Mn',\n       'cat__BsmtExposure_No', 'cat__BsmtFinType1_ALQ',\n       'cat__BsmtFinType1_BLQ', 'cat__BsmtFinType1_GLQ',\n       'cat__BsmtFinType1_LwQ', 'cat__BsmtFinType1_Rec',\n       'cat__BsmtFinType1_Unf', 'cat__BsmtFinType2_ALQ',\n       'cat__BsmtFinType2_BLQ', 'cat__BsmtFinType2_GLQ',\n       'cat__BsmtFinType2_LwQ', 'cat__BsmtFinType2_Rec',\n       'cat__BsmtFinType2_Unf', 'cat__Heating_GasA', 'cat__Heating_GasW',\n       'cat__Heating_Grav', 'cat__Heating_OthW', 'cat__Heating_Wall',\n       'cat__HeatingQC_Ex', 'cat__HeatingQC_Fa', 'cat__HeatingQC_Gd',\n       'cat__HeatingQC_TA', 'cat__CentralAir_N', 'cat__CentralAir_Y',\n       'cat__Electrical_FuseA', 'cat__Electrical_FuseF',\n       'cat__Electrical_FuseP', 'cat__Electrical_Mix',\n       'cat__Electrical_SBrkr', 'cat__KitchenQual_Ex',\n       'cat__KitchenQual_Fa', 'cat__KitchenQual_Gd',\n       'cat__KitchenQual_TA', 'cat__Functional_Maj1',\n       'cat__Functional_Maj2', 'cat__Functional_Min1',\n       'cat__Functional_Min2', 'cat__Functional_Mod',\n       'cat__Functional_Sev', 'cat__Functional_Typ',\n       'cat__FireplaceQu_Ex', 'cat__FireplaceQu_Fa',\n       'cat__FireplaceQu_Gd', 'cat__FireplaceQu_Po',\n       'cat__FireplaceQu_TA', 'cat__GarageType_2Types',\n       'cat__GarageType_Attchd', 'cat__GarageType_Basment',\n       'cat__GarageType_BuiltIn', 'cat__GarageType_CarPort',\n       'cat__GarageType_Detchd', 'cat__GarageFinish_Fin',\n       'cat__GarageFinish_RFn', 'cat__GarageFinish_Unf',\n       'cat__GarageQual_Ex', 'cat__GarageQual_Fa', 'cat__GarageQual_Gd',\n       'cat__GarageQual_Po', 'cat__GarageQual_TA', 'cat__GarageCond_Ex',\n       'cat__GarageCond_Fa', 'cat__GarageCond_Gd', 'cat__GarageCond_Po',\n       'cat__GarageCond_TA', 'cat__PavedDrive_N', 'cat__PavedDrive_P',\n       'cat__PavedDrive_Y', 'cat__PoolQC_Ex', 'cat__PoolQC_Fa',\n       'cat__PoolQC_Gd', 'cat__Fence_GdPrv', 'cat__Fence_GdWo',\n       'cat__Fence_MnPrv', 'cat__Fence_MnWw', 'cat__MiscFeature_Gar2',\n       'cat__MiscFeature_Shed', 'cat__MiscFeature_TenC',\n       'cat__SaleType_COD', 'cat__SaleType_CWD', 'cat__SaleType_Con',\n       'cat__SaleType_ConLD', 'cat__SaleType_ConLI',\n       'cat__SaleType_ConLw', 'cat__SaleType_New', 'cat__SaleType_Oth',\n       'cat__SaleType_WD', 'cat__SaleCondition_Abnorml',\n       'cat__SaleCondition_AdjLand', 'cat__SaleCondition_Alloca',\n       'cat__SaleCondition_Family', 'cat__SaleCondition_Normal',\n       'cat__SaleCondition_Partial'], dtype=object)\n\n\n\n\nCode\nfitted_model=pipeline.steps[1][1]\n\n\nvariable importance\n\n\nCode\nimportances = fitted_model.feature_importances_\nvi=pd.DataFrame({\"variable\":var,\"importances\":importances})\nvi=vi.sort_values('importances',ascending=False)\nvi\n\n\n\n\n\n\n\n\n\n\nvariable\nimportances\n\n\n\n\n4\nnum__OverallQual\n5.854710e-01\n\n\n16\nnum__GrLivArea\n1.060767e-01\n\n\n14\nnum__2ndFlrSF\n4.614483e-02\n\n\n12\nnum__TotalBsmtSF\n3.347513e-02\n\n\n9\nnum__BsmtFinSF1\n3.002623e-02\n\n\n...\n...\n...\n\n\n103\ncat__RoofMatl_Roll\n9.740956e-08\n\n\n102\ncat__RoofMatl_Metal\n2.400973e-08\n\n\n78\ncat__Condition2_RRAe\n7.159526e-09\n\n\n212\ncat__MiscFeature_TenC\n0.000000e+00\n\n\n152\ncat__Heating_OthW\n0.000000e+00\n\n\n\n\n228 rows × 2 columns",
    "crumbs": [
      "Regression",
      "Random forest and pipeline"
    ]
  },
  {
    "objectID": "regression/3 Random forest on house price data.html#preformance",
    "href": "regression/3 Random forest on house price data.html#preformance",
    "title": "Random forest and pipeline",
    "section": "3.4 Preformance",
    "text": "3.4 Preformance\n\n\nCode\nY_pred_dt =pipeline.predict(X_test) #always gets x and retuns y\n\n\nR 2\n\n\nCode\nfrom sklearn.metrics import r2_score\nr2_score(Y_test, Y_pred_dt)\n\n\n0.8683012817494923\n\n\nMAE\n\n\nCode\nfrom sklearn.metrics import mean_absolute_error\nmean_absolute_error(Y_test, Y_pred_dt)\n\n\n15621.411643835614\n\n\nRMSE\n\n\nCode\nfrom  math import sqrt\nfrom sklearn.metrics import mean_squared_error\nmse=mean_squared_error(Y_test, Y_pred_dt)\nrmse=sqrt(mse)\nrmse\n\n\n25142.869071094177",
    "crumbs": [
      "Regression",
      "Random forest and pipeline"
    ]
  },
  {
    "objectID": "regression/3 Random forest on house price data.html#k-fold-cross-validation",
    "href": "regression/3 Random forest on house price data.html#k-fold-cross-validation",
    "title": "Random forest and pipeline",
    "section": "3.5 k-Fold Cross-Validation",
    "text": "3.5 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \n\n\n\n\nCode\ncv_dt = cross_val_score(pipeline, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.8615932726287449\n\n\n\n\nCode\ncv_dt = cross_val_score(pipeline, X_train, Y_train, cv=kf_dt,scoring = 'neg_mean_squared_error')\nnp.mean(np.sqrt(np.abs(cv_dt)))\n\n\n30392.220992319042",
    "crumbs": [
      "Regression",
      "Random forest and pipeline"
    ]
  },
  {
    "objectID": "regression/4 Random forest on house price data.html",
    "href": "regression/4 Random forest on house price data.html",
    "title": "Random forest and pipeline, hyperparameter tuning",
    "section": "",
    "text": "with pipeline and tunning",
    "crumbs": [
      "Regression",
      "Random forest and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/4 Random forest on house price data.html#input-data",
    "href": "regression/4 Random forest on house price data.html#input-data",
    "title": "Random forest and pipeline, hyperparameter tuning",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nId = df_train['Id']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nId\nMSSubClass\nMSZoning\nLotFrontage\nLotArea\nStreet\nAlley\nLotShape\nLandContour\nUtilities\n...\nPoolArea\nPoolQC\nFence\nMiscFeature\nMiscVal\nMoSold\nYrSold\nSaleType\nSaleCondition\nSalePrice\n\n\n\n\n0\n1\n60\nRL\n65.0\n8450\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n2\n2008\nWD\nNormal\n208500\n\n\n1\n2\n20\nRL\n80.0\n9600\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2007\nWD\nNormal\n181500\n\n\n2\n3\n60\nRL\n68.0\n11250\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n9\n2008\nWD\nNormal\n223500\n\n\n3\n4\n70\nRL\n60.0\n9550\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n2\n2006\nWD\nAbnorml\n140000\n\n\n4\n5\n60\nRL\n84.0\n14260\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n12\n2008\nWD\nNormal\n250000\n\n\n\n\n5 rows × 81 columns\n\n\n\n\n\n\nCode\n#df_train.info()\n\n\n\n\nCode\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['SalePrice'], axis=1), df_test])",
    "crumbs": [
      "Regression",
      "Random forest and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/4 Random forest on house price data.html#data-eda",
    "href": "regression/4 Random forest on house price data.html#data-eda",
    "title": "Random forest and pipeline, hyperparameter tuning",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\nin step 1",
    "crumbs": [
      "Regression",
      "Random forest and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/4 Random forest on house price data.html#data-wrangling",
    "href": "regression/4 Random forest on house price data.html#data-wrangling",
    "title": "Random forest and pipeline, hyperparameter tuning",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling",
    "crumbs": [
      "Regression",
      "Random forest and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/4 Random forest on house price data.html#split-data",
    "href": "regression/4 Random forest on house price data.html#split-data",
    "title": "Random forest and pipeline, hyperparameter tuning",
    "section": "2.4 split data",
    "text": "2.4 split data\n\n\nCode\nY = df_train.SalePrice\nX = df_train.drop(['SalePrice'], axis=1)\n\n\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\nX_train = X_train.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nprint(X_train.shape)\nprint(X_test.shape)\n\n\n(1168, 80)\n(292, 80)\n\n\n\n\nCode\nprint(Y_train.shape)\nprint(Y_test.shape)\n\n\n(1168,)\n(292,)\n\n\n\n\nCode\ncategorical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].nunique() &lt; 10 and X_train[cname].dtype == \"object\"]\n                    \n                    \nnumerical_cols = numerical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].dtype in ['int64', 'float64']]\n\n\n\n\nCode\nprint(\"The total number of categorical columns:\", len(categorical_cols))\nprint(\"The total number of numerical columns:\", len(numerical_cols))\n\n\nThe total number of categorical columns: 40\nThe total number of numerical columns: 37\n\n\n\n\nCode\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train[my_cols].copy()\nX_test= X_test[my_cols].copy()\n\n\nX_final = df_test[my_cols].copy()",
    "crumbs": [
      "Regression",
      "Random forest and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/4 Random forest on house price data.html#pipelines-for-data-preprocessing",
    "href": "regression/4 Random forest on house price data.html#pipelines-for-data-preprocessing",
    "title": "Random forest and pipeline, hyperparameter tuning",
    "section": "2.5 Pipelines for Data Preprocessing",
    "text": "2.5 Pipelines for Data Preprocessing\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\nCode\nnumerical_transformer = Pipeline(steps=[\n    ('imputer_num', SimpleImputer(strategy='median')), \n    ('scaler', StandardScaler())\n])\n\n\n\n\nCode\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n\n\nCode\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)])",
    "crumbs": [
      "Regression",
      "Random forest and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/4 Random forest on house price data.html#define-model",
    "href": "regression/4 Random forest on house price data.html#define-model",
    "title": "Random forest and pipeline, hyperparameter tuning",
    "section": "3.1 define model",
    "text": "3.1 define model\nrandom forest with hyper parameter tuning\n\n\nCode\nfrom sklearn.ensemble import RandomForestRegressor\n   \n\nml_model = RandomForestRegressor(random_state=0)\nml_model\n\n\nRandomForestRegressor(random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestRegressor?Documentation for RandomForestRegressoriNot fittedRandomForestRegressor(random_state=0)",
    "crumbs": [
      "Regression",
      "Random forest and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/4 Random forest on house price data.html#define-pipline",
    "href": "regression/4 Random forest on house price data.html#define-pipline",
    "title": "Random forest and pipeline, hyperparameter tuning",
    "section": "3.2 define pipline",
    "text": "3.2 define pipline\n\n\nCode\npipeline = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', ml_model)\n         ]\n)",
    "crumbs": [
      "Regression",
      "Random forest and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/4 Random forest on house price data.html#hyperparameter-tuning-set",
    "href": "regression/4 Random forest on house price data.html#hyperparameter-tuning-set",
    "title": "Random forest and pipeline, hyperparameter tuning",
    "section": "3.3 hyperparameter tuning set",
    "text": "3.3 hyperparameter tuning set\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {'model__max_depth':[20,30,40],\n                 'model__n_estimators':[200,250,300],\n                 'model__min_samples_leaf':[1,2,3]\n                 }\n                 \n              \nGridCV = GridSearchCV(pipeline, param_grid, n_jobs= -1, verbose=1)",
    "crumbs": [
      "Regression",
      "Random forest and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/4 Random forest on house price data.html#train-model",
    "href": "regression/4 Random forest on house price data.html#train-model",
    "title": "Random forest and pipeline, hyperparameter tuning",
    "section": "3.4 train model",
    "text": "3.4 train model\n\n\nCode\nGridCV.fit(X_train, Y_train)\n\n\nFitting 5 folds for each of 27 candidates, totalling 135 fits\n\n\nGridSearchCV(estimator=Pipeline(steps=[('preprocessor',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         Pipeline(steps=[('imputer_num',\n                                                                                          SimpleImputer(strategy='median')),\n                                                                                         ('scaler',\n                                                                                          StandardScaler())]),\n                                                                         ['Id',\n                                                                          'MSSubClass',\n                                                                          'LotFrontage',\n                                                                          'LotArea',\n                                                                          'OverallQual',\n                                                                          'OverallCond',\n                                                                          'YearBuilt',\n                                                                          'YearRemodAdd',\n                                                                          'MasVnrArea',\n                                                                          'BsmtFinSF1',\n                                                                          'BsmtFinSF2',\n                                                                          'BsmtUnfSF',\n                                                                          'TotalBsmtSF...\n                                                                          'Foundation',\n                                                                          'BsmtQual',\n                                                                          'BsmtCond',\n                                                                          'BsmtExposure',\n                                                                          'BsmtFinType1',\n                                                                          'BsmtFinType2',\n                                                                          'Heating',\n                                                                          'HeatingQC',\n                                                                          'CentralAir',\n                                                                          'Electrical',\n                                                                          'KitchenQual',\n                                                                          'Functional',\n                                                                          'FireplaceQu', ...])])),\n                                       ('model',\n                                        RandomForestRegressor(random_state=0))]),\n             n_jobs=-1,\n             param_grid={'model__max_depth': [20, 30, 40],\n                         'model__min_samples_leaf': [1, 2, 3],\n                         'model__n_estimators': [200, 250, 300]},\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(estimator=Pipeline(steps=[('preprocessor',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         Pipeline(steps=[('imputer_num',\n                                                                                          SimpleImputer(strategy='median')),\n                                                                                         ('scaler',\n                                                                                          StandardScaler())]),\n                                                                         ['Id',\n                                                                          'MSSubClass',\n                                                                          'LotFrontage',\n                                                                          'LotArea',\n                                                                          'OverallQual',\n                                                                          'OverallCond',\n                                                                          'YearBuilt',\n                                                                          'YearRemodAdd',\n                                                                          'MasVnrArea',\n                                                                          'BsmtFinSF1',\n                                                                          'BsmtFinSF2',\n                                                                          'BsmtUnfSF',\n                                                                          'TotalBsmtSF...\n                                                                          'Foundation',\n                                                                          'BsmtQual',\n                                                                          'BsmtCond',\n                                                                          'BsmtExposure',\n                                                                          'BsmtFinType1',\n                                                                          'BsmtFinType2',\n                                                                          'Heating',\n                                                                          'HeatingQC',\n                                                                          'CentralAir',\n                                                                          'Electrical',\n                                                                          'KitchenQual',\n                                                                          'Functional',\n                                                                          'FireplaceQu', ...])])),\n                                       ('model',\n                                        RandomForestRegressor(random_state=0))]),\n             n_jobs=-1,\n             param_grid={'model__max_depth': [20, 30, 40],\n                         'model__min_samples_leaf': [1, 2, 3],\n                         'model__n_estimators': [200, 250, 300]},\n             verbose=1) estimator: PipelinePipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer_num',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['Id', 'MSSubClass',\n                                                   'LotFrontage', 'LotArea',\n                                                   'OverallQual', 'OverallCond',\n                                                   'YearBuilt', 'YearRemodAdd',\n                                                   'MasVnrArea', 'BsmtFinSF1',\n                                                   'BsmtFinSF2', 'BsmtUnfSF',\n                                                   'TotalBsmtSF', '1stFlrSF',\n                                                   '2ndFlrSF'...\n                                                   'LotConfig', 'LandSlope',\n                                                   'Condition1', 'Condition2',\n                                                   'BldgType', 'HouseStyle',\n                                                   'RoofStyle', 'RoofMatl',\n                                                   'MasVnrType', 'ExterQual',\n                                                   'ExterCond', 'Foundation',\n                                                   'BsmtQual', 'BsmtCond',\n                                                   'BsmtExposure',\n                                                   'BsmtFinType1',\n                                                   'BsmtFinType2', 'Heating',\n                                                   'HeatingQC', 'CentralAir',\n                                                   'Electrical', 'KitchenQual',\n                                                   'Functional', 'FireplaceQu', ...])])),\n                ('model', RandomForestRegressor(random_state=0))])  preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('imputer_num',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('scaler', StandardScaler())]),\n                                 ['Id', 'MSSubClass', 'LotFrontage', 'LotArea',\n                                  'OverallQual', 'OverallCond', 'YearBuilt',\n                                  'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1',\n                                  'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF',\n                                  '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n                                  'GrLivArea', 'Bsm...\n                                 ['MSZoning', 'Street', 'Alley', 'LotShape',\n                                  'LandContour', 'Utilities', 'LotConfig',\n                                  'LandSlope', 'Condition1', 'Condition2',\n                                  'BldgType', 'HouseStyle', 'RoofStyle',\n                                  'RoofMatl', 'MasVnrType', 'ExterQual',\n                                  'ExterCond', 'Foundation', 'BsmtQual',\n                                  'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n                                  'BsmtFinType2', 'Heating', 'HeatingQC',\n                                  'CentralAir', 'Electrical', 'KitchenQual',\n                                  'Functional', 'FireplaceQu', ...])]) num['Id', 'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='median')  StandardScaler?Documentation for StandardScalerStandardScaler() cat['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'SaleType', 'SaleCondition']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='most_frequent')  OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore')  RandomForestRegressor?Documentation for RandomForestRegressorRandomForestRegressor(random_state=0) \n\n\n\n\nCode\nGridCV.best_params_\n\n\n{'model__max_depth': 20,\n 'model__min_samples_leaf': 1,\n 'model__n_estimators': 300}\n\n\n\n\nCode\nGridCV.best_score_\n\n\n0.8356691049766422\n\n\nbest model as pipeline\n\n\nCode\noptimised_model_pipeline = GridCV.best_estimator_\n\n\n\n\nCode\nvar=optimised_model_pipeline[:-1].get_feature_names_out()\n#var\n\n\n\n\nCode\nfitted_model=optimised_model_pipeline.steps[1][1]\n\n\nvariable importance\n\n\nCode\nimportances = fitted_model.feature_importances_\nvi=pd.DataFrame({\"variable\":var,\"importances\":importances})\nvi=vi.sort_values('importances',ascending=False)\nvi\n\n\n\n\n\n\n\n\n\n\nvariable\nimportances\n\n\n\n\n4\nnum__OverallQual\n5.549448e-01\n\n\n16\nnum__GrLivArea\n1.112510e-01\n\n\n9\nnum__BsmtFinSF1\n3.698242e-02\n\n\n12\nnum__TotalBsmtSF\n3.172121e-02\n\n\n14\nnum__2ndFlrSF\n2.840322e-02\n\n\n...\n...\n...\n\n\n116\ncat__ExterCond_Po\n2.580107e-08\n\n\n196\ncat__GarageCond_Ex\n1.632168e-08\n\n\n212\ncat__MiscFeature_Othr\n7.883686e-09\n\n\n54\ncat__Utilities_AllPub\n5.100970e-09\n\n\n102\ncat__RoofMatl_Roll\n1.013719e-11\n\n\n\n\n230 rows × 2 columns",
    "crumbs": [
      "Regression",
      "Random forest and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/4 Random forest on house price data.html#preformance",
    "href": "regression/4 Random forest on house price data.html#preformance",
    "title": "Random forest and pipeline, hyperparameter tuning",
    "section": "3.5 Preformance",
    "text": "3.5 Preformance\n\n\nCode\nY_pred_dt =optimised_model_pipeline.predict(X_test) #always gets x and retuns y\n\n\nR 2\n\n\nCode\nfrom sklearn.metrics import r2_score\nr2_score(Y_test, Y_pred_dt)\n\n\n0.9060762300832943\n\n\nMAE\n\n\nCode\nfrom sklearn.metrics import mean_absolute_error\nmean_absolute_error(Y_test, Y_pred_dt)\n\n\n16590.44897687198\n\n\nRMSE\n\n\nCode\nfrom  math import sqrt\nfrom sklearn.metrics import mean_squared_error\nmse=mean_squared_error(Y_test, Y_pred_dt)\nrmse=sqrt(mse)\nrmse\n\n\n25706.585324016956",
    "crumbs": [
      "Regression",
      "Random forest and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/4 Random forest on house price data.html#k-fold-cross-validation",
    "href": "regression/4 Random forest on house price data.html#k-fold-cross-validation",
    "title": "Random forest and pipeline, hyperparameter tuning",
    "section": "3.6 k-Fold Cross-Validation",
    "text": "3.6 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \n\n\n\n\nCode\ncv_dt = cross_val_score(optimised_model_pipeline, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.8401868489597248\n\n\n\n\nCode\ncv_dt = cross_val_score(optimised_model_pipeline, X_train, Y_train, cv=kf_dt,scoring = 'neg_mean_squared_error')\nnp.mean(np.sqrt(np.abs(cv_dt)))\n\n\n30709.02967606353",
    "crumbs": [
      "Regression",
      "Random forest and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\nhttps://tonyfly3000.github.io/scikit-learn-in-Python/\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "intro/3 install package.html",
    "href": "intro/3 install package.html",
    "title": "install pacakge",
    "section": "",
    "text": "1 install package\n\n\nCode\nimport os\nos.system('pip install -U scikit-learn')\n\n\n\n\n2 check one package version\n\n\nCode\nimport os\nos.system('pip show scikit-learn')\n\n\nName: scikit-learn\nVersion: 1.4.1.post1\nSummary: A set of python modules for machine learning and data mining\nHome-page: https://scikit-learn.org\nAuthor: \nAuthor-email: \nLicense: new BSD\nLocation: /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages\nRequires: joblib, numpy, scipy, threadpoolctl\nRequired-by: librosa\n\n\n0\n\n\n\n\n3 check all package version\n\n\nCode\nimport os\nos.system('pip list')\n\n\nPackage                      Version\n---------------------------- ------------\nabsl-py                      1.4.0\naioquic-mitmproxy            0.9.21.1\nannotated-types              0.6.0\nanyio                        4.0.0\nappdirs                      1.4.4\nappnope                      0.1.3\nargon2-cffi                  23.1.0\nargon2-cffi-bindings         21.2.0\narrow                        1.3.0\nasgiref                      3.7.2\nasttokens                    2.4.0\nastunparse                   1.6.3\nasync-lru                    2.0.4\nattrs                        23.1.0\naudioread                    3.0.1\nBabel                        2.13.0\nbackcall                     0.2.0\nbeautifulsoup4               4.12.2\nbleach                       6.1.0\nblinker                      1.7.0\nBrotli                       1.1.0\ncachetools                   5.3.1\ncertifi                      2023.7.22\ncffi                         1.16.0\ncharset-normalizer           3.3.0\nclick                        8.1.7\ncomm                         0.1.4\ncontourpy                    1.1.1\ncryptography                 41.0.7\ncycler                       0.12.1\ndacite                       1.8.1\ndebugpy                      1.8.0\ndecorator                    5.1.1\ndefusedxml                   0.7.1\ndill                         0.3.8\ndm-tree                      0.1.8\netils                        1.8.0\nexecuting                    2.0.0\nfastjsonschema               2.18.1\nfilelock                     3.12.4\nFlask                        3.0.0\nflatbuffers                  23.5.26\nfonttools                    4.43.1\nfqdn                         1.5.1\nfsspec                       2023.9.2\nfuture                       1.0.0\ngast                         0.5.4\ngoogle-auth                  2.23.3\ngoogle-auth-oauthlib         1.0.0\ngoogle-pasta                 0.2.0\ngoogleapis-common-protos     1.63.0\ngrpcio                       1.59.0\nh11                          0.14.0\nh2                           4.1.0\nh5py                         3.10.0\nhpack                        4.0.0\nhtmlmin                      0.1.12\nhtmltools                    0.5.1\nhuggingface-hub              0.17.3\nhyperframe                   6.0.1\nidna                         3.4\nImageHash                    4.3.1\nimportlib_resources          6.4.0\ninstall                      1.3.5\nipykernel                    6.25.2\nipython                      8.16.1\nipython-genutils             0.2.0\nipywidgets                   8.1.1\nisoduration                  20.11.0\nitsdangerous                 2.1.2\njedi                         0.19.1\nJinja2                       3.1.2\njoblib                       1.3.2\njson5                        0.9.14\njsonpointer                  2.4\njsonschema                   4.19.1\njsonschema-specifications    2023.7.1\njupyter                      1.0.0\njupyter_client               8.3.1\njupyter-console              6.6.3\njupyter_core                 5.3.2\njupyter-events               0.7.0\njupyter-lsp                  2.2.0\njupyter_server               2.7.3\njupyter_server_terminals     0.4.4\njupyterlab                   4.0.6\njupyterlab-pygments          0.2.2\njupyterlab_server            2.25.0\njupyterlab-widgets           3.0.9\nkaggle                       1.5.16\nkaitaistruct                 0.10\nkeras                        3.1.1\nKeras-Preprocessing          1.1.2\nkiwisolver                   1.4.5\nlazy_loader                  0.3\nldap3                        2.9.1\nlibclang                     16.0.6\nlibrosa                      0.10.1\nlinkify-it-py                2.0.3\nllvmlite                     0.41.0\nlxml                         4.9.3\nlzstring                     1.0.4\nMarkdown                     3.5\nmarkdown-it-py               3.0.0\nMarkupSafe                   2.1.3\nmatplotlib                   3.8.3\nmatplotlib-inline            0.1.6\nmdit-py-plugins              0.4.0\nmdurl                        0.1.2\nmissingno                    0.5.2\nmistune                      3.0.2\nmitmproxy                    10.1.5\nmitmproxy-macos              0.4.1\nmitmproxy_rs                 0.4.1\nmizani                       0.11.1\nml-dtypes                    0.3.2\nmpmath                       1.3.0\nmsgpack                      1.0.7\nmultimethod                  1.10\nmutagen                      1.47.0\nnamex                        0.0.7\nnbclient                     0.8.0\nnbconvert                    7.9.2\nnbformat                     5.9.2\nnest-asyncio                 1.5.8\nnetworkx                     3.1\nnltk                         3.8.1\nnotebook                     7.0.4\nnotebook_shim                0.2.3\nnumba                        0.58.0\nnumpy                        1.26.4\noauthlib                     3.2.2\nopencv-python                4.8.1.78\nopendatasets                 0.1.22\nopt-einsum                   3.3.0\noptree                       0.10.0\noutcome                      1.3.0.post0\noverrides                    7.4.0\npackaging                    23.2\npandas                       2.2.1\npandas-profiling             3.2.0\npandocfilters                1.5.0\nparso                        0.8.3\npasslib                      1.7.4\npatchworklib                 0.6.4\npatsy                        0.5.3\npexpect                      4.8.0\nphik                         0.12.3\npickleshare                  0.7.5\nPillow                       10.0.1\npip                          24.0\nplatformdirs                 3.11.0\nplotly                       5.20.0\nplotnine                     0.13.3\npooch                        1.7.0\nprometheus-client            0.17.1\npromise                      2.3\nprompt-toolkit               3.0.36\nprotobuf                     3.20.3\npsutil                       5.9.5\nptyprocess                   0.7.0\npublicsuffix2                2.20191221\npure-eval                    0.2.2\npyarrow                      15.0.2\npyasn1                       0.5.0\npyasn1-modules               0.3.0\npycparser                    2.21\npycryptodomex                3.20.0\npydantic                     1.10.13\npydantic_core                2.10.1\npydantic-settings            2.0.3\nPygments                     2.16.1\npylsqpack                    0.3.18\npyOpenSSL                    23.3.0\npyparsing                    3.1.1\npyperclip                    1.8.2\npypi-latest                  0.1.2\nPySocks                      1.7.1\npython-dateutil              2.8.2\npython-dotenv                1.0.0\npython-json-logger           2.0.7\npython-multipart             0.0.9\npython-slugify               8.0.1\npytz                         2023.3.post1\nPyWavelets                   1.4.1\nPyYAML                       6.0.1\npyzmq                        25.1.1\nqtconsole                    5.4.4\nQtPy                         2.4.0\nquestionary                  2.0.1\nreferencing                  0.30.2\nregex                        2023.10.3\nrequests                     2.31.0\nrequests-oauthlib            1.3.1\nrfc3339-validator            0.1.4\nrfc3986-validator            0.1.1\nrich                         13.7.1\nrpds-py                      0.10.4\nrsa                          4.9\nruamel.yaml                  0.18.5\nruamel.yaml.clib             0.2.8\nsafetensors                  0.4.0\nscikit-learn                 1.4.1.post1\nscipy                        1.12.0\nseaborn                      0.12.2\nselenium                     4.15.2\nSend2Trash                   1.8.2\nservice-identity             23.1.0\nsetuptools                   65.5.0\nshiny                        0.7.1\nshinylive                    0.2.2\nsiuba                        0.4.4\nsix                          1.16.0\nsniffio                      1.3.0\nsortedcontainers             2.4.0\nsoundfile                    0.12.1\nsoupsieve                    2.5\nsoxr                         0.3.7\nSQLAlchemy                   2.0.22\nstack-data                   0.6.3\nstarlette                    0.34.0\nstatsmodels                  0.14.0\nsweetviz                     2.3.1\nsympy                        1.12\ntangled-up-in-unicode        0.2.0\ntenacity                     8.2.3\ntensorboard                  2.16.2\ntensorboard-data-server      0.7.1\ntensorflow                   2.16.1\ntensorflow-datasets          4.9.4\ntensorflow_decision_forests  1.9.0\ntensorflow-estimator         2.14.0\ntensorflow-io-gcs-filesystem 0.34.0\ntensorflow-macos             2.14.0\ntensorflow-metadata          1.14.0\ntermcolor                    2.3.0\nterminado                    0.17.1\ntext-unidecode               1.3\ntf_keras                     2.16.0\nthreadpoolctl                3.2.0\ntinycss2                     1.2.1\ntokenizers                   0.14.1\ntoml                         0.10.2\ntorch                        2.1.0\ntorchvision                  0.16.0\ntornado                      6.3.3\ntqdm                         4.66.1\ntraitlets                    5.11.2\ntransformers                 4.34.0\ntrio                         0.23.1\ntrio-websocket               0.11.1\ntypeguard                    4.1.5\ntypes-python-dateutil        2.8.19.14\ntyping_extensions            4.8.0\ntzdata                       2023.3\nuc-micro-py                  1.0.3\nuri-template                 1.3.0\nurllib3                      2.0.6\nurwid-mitmproxy              2.1.2.1\nuvicorn                      0.27.1\nvisions                      0.7.5\nwatchfiles                   0.21.0\nwcwidth                      0.2.8\nwebcolors                    1.13\nwebencodings                 0.5.1\nwebsocket-client             1.6.4\nwebsockets                   12.0\nWerkzeug                     3.0.0\nwheel                        0.41.2\nwidgetsnbextension           4.0.9\nwordcloud                    1.9.2\nwrapt                        1.14.1\nwsproto                      1.2.0\nwurlitzer                    3.0.3\nxgboost                      2.0.3\nydata-profiling              4.6.0\nyt-dlp                       2023.12.30\nzipp                         3.18.1\nzstandard                    0.22.0\n\n\n0",
    "crumbs": [
      "Intro",
      "install pacakge"
    ]
  },
  {
    "objectID": "plot/2 plotnine.html",
    "href": "plot/2 plotnine.html",
    "title": "plotnine chart",
    "section": "",
    "text": "plotnine is an implementation of a grammar of graphics in Python based on ggplot2.\nCode\nimport plotnine\nprint(plotnine.__version__)\n\n\n0.13.3\nCode\nfrom plotnine import *\n\nprint(plotnine.__version__)\n\n\n0.13.3\nCode\nfrom plotnine import *\nimport seaborn as sns\nimport pandas as pd\n\n\n# Apply the default theme\n\n\n# Load an example dataset\ntips = sns.load_dataset(\"tips\")\ntips.head()\n\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4",
    "crumbs": [
      "Plot",
      "plotnine chart"
    ]
  },
  {
    "objectID": "plot/2 plotnine.html#color-by-group",
    "href": "plot/2 plotnine.html#color-by-group",
    "title": "plotnine chart",
    "section": "1.1 color by group",
    "text": "1.1 color by group\n\n\nCode\np=(\n    ggplot(data=tips)+aes(x=\"tip\",y=\"total_bill\")+ geom_point(aes(color=\"sex\"))\n)\n\np",
    "crumbs": [
      "Plot",
      "plotnine chart"
    ]
  },
  {
    "objectID": "plot/2 plotnine.html#size-by-group",
    "href": "plot/2 plotnine.html#size-by-group",
    "title": "plotnine chart",
    "section": "1.2 size by group",
    "text": "1.2 size by group\n\n\nCode\np=(\n    ggplot(data=tips)+aes(x=\"tip\",y=\"total_bill\",size=\"size\")+ geom_point()\n)\n\np",
    "crumbs": [
      "Plot",
      "plotnine chart"
    ]
  },
  {
    "objectID": "plot/2 plotnine.html#color-by-group-1",
    "href": "plot/2 plotnine.html#color-by-group-1",
    "title": "plotnine chart",
    "section": "2.1 color by group",
    "text": "2.1 color by group\n\n\nCode\np=(\n    ggplot(data=dowjones4)+aes(x=\"Date\",y=\"Price\")+ geom_line(aes(color=\"type\"))\n)\n\np",
    "crumbs": [
      "Plot",
      "plotnine chart"
    ]
  },
  {
    "objectID": "plot/2 plotnine.html#color-by-group-2",
    "href": "plot/2 plotnine.html#color-by-group-2",
    "title": "plotnine chart",
    "section": "3.1 color by group",
    "text": "3.1 color by group\n\n\nCode\np=(\n    ggplot(data=tips)+aes(x=\"tip\",fill = 'sex')+ geom_histogram(position = 'dodge')\n)\n\np",
    "crumbs": [
      "Plot",
      "plotnine chart"
    ]
  },
  {
    "objectID": "plot/2 plotnine.html#color-by-group-3",
    "href": "plot/2 plotnine.html#color-by-group-3",
    "title": "plotnine chart",
    "section": "5.1 color by group",
    "text": "5.1 color by group\n\n\nCode\np=(\n    ggplot(data=tips)+aes(x='day',y='tip',fill=\"sex\")+geom_boxplot()\n)\n\np",
    "crumbs": [
      "Plot",
      "plotnine chart"
    ]
  },
  {
    "objectID": "plot/2 plotnine.html#color-by-group-4",
    "href": "plot/2 plotnine.html#color-by-group-4",
    "title": "plotnine chart",
    "section": "6.1 color by group",
    "text": "6.1 color by group\n\n\nCode\np=(\n    ggplot(data=tips)+aes(x='day',y='tip',fill=\"sex\")+geom_jitter(position=position_jitterdodge())\n)\n\np",
    "crumbs": [
      "Plot",
      "plotnine chart"
    ]
  },
  {
    "objectID": "classification/1 decision tree on titanic data.html",
    "href": "classification/1 decision tree on titanic data.html",
    "title": "Decision tree",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split",
    "crumbs": [
      "Classification",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification/1 decision tree on titanic data.html#input-data",
    "href": "classification/1 decision tree on titanic data.html#input-data",
    "title": "Decision tree",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification/1 decision tree on titanic data.html#data-eda",
    "href": "classification/1 decision tree on titanic data.html#data-eda",
    "title": "Decision tree",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n\n\n0.7420382165605095\n0.18890814558058924",
    "crumbs": [
      "Classification",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification/1 decision tree on titanic data.html#data-wrangling",
    "href": "classification/1 decision tree on titanic data.html#data-wrangling",
    "title": "Decision tree",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1046 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1308 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Dealing with missing numerical variables\ndata['Age'] = data.Age.fillna(data.Age.median())\ndata['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1309 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1309 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\ndata = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\nSex_male\n\n\n\n\n0\n1\n3\nBraund, Mr. Owen Harris\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\nTrue\n\n\n1\n2\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\nFalse\n\n\n2\n3\n3\nHeikkinen, Miss. Laina\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\nFalse\n\n\n3\n4\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\nFalse\n\n\n4\n5\n3\nAllen, Mr. William Henry\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\nTrue\n\n\n\n\n\n\n\n\n\n\nCode\n# Select features columns\ndata = data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()\n\n\n\n\n\n\n\n\n\n\nSex_male\nFare\nAge\nPclass\nSibSp\nrole\n\n\n\n\n0\nTrue\n7.2500\n22.0\n3\n1\ntrain\n\n\n1\nFalse\n71.2833\n38.0\n1\n1\ntrain\n\n\n2\nFalse\n7.9250\n26.0\n3\n0\ntrain\n\n\n3\nFalse\n53.1000\n35.0\n1\n1\ntrain\n\n\n4\nTrue\n8.0500\n35.0\n3\n0\ntrain",
    "crumbs": [
      "Classification",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification/1 decision tree on titanic data.html#split-data",
    "href": "classification/1 decision tree on titanic data.html#split-data",
    "title": "Decision tree",
    "section": "2.4 split data",
    "text": "2.4 split data\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\nX_train = X_train.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 712 entries, 694 to 823\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Sex_male  712 non-null    bool   \n 1   Fare      712 non-null    float64\n 2   Age       712 non-null    float64\n 3   Pclass    712 non-null    int64  \n 4   SibSp     712 non-null    int64  \ndtypes: bool(1), float64(2), int64(2)\nmemory usage: 28.5 KB",
    "crumbs": [
      "Classification",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification/1 decision tree on titanic data.html#define-model",
    "href": "classification/1 decision tree on titanic data.html#define-model",
    "title": "Decision tree",
    "section": "3.1 define model",
    "text": "3.1 define model\n\n\nCode\nmodel_dt = tree.DecisionTreeClassifier(max_depth=3)   #model with deph 3\nmodel_dt\n\n\nDecisionTreeClassifier(max_depth=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriNot fittedDecisionTreeClassifier(max_depth=3)",
    "crumbs": [
      "Classification",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification/1 decision tree on titanic data.html#train-model",
    "href": "classification/1 decision tree on titanic data.html#train-model",
    "title": "Decision tree",
    "section": "3.2 train model",
    "text": "3.2 train model\n\n\nCode\nmodel_dt.fit(X_train,Y_train)\n\n\nDecisionTreeClassifier(max_depth=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier(max_depth=3) \n\n\nvariable importance\n\n\nCode\nimportances = model_dt.feature_importances_\nvi=pd.DataFrame({\"variable\":X_train.columns,\"importances\":importances})\nvi=vi.sort_values('importances',ascending=False)\nvi\n\n\n\n\n\n\n\n\n\n\nvariable\nimportances\n\n\n\n\n0\nSex_male\n0.659607\n\n\n3\nPclass\n0.200035\n\n\n1\nFare\n0.075849\n\n\n2\nAge\n0.064509\n\n\n4\nSibSp\n0.000000",
    "crumbs": [
      "Classification",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification/1 decision tree on titanic data.html#preformance",
    "href": "classification/1 decision tree on titanic data.html#preformance",
    "title": "Decision tree",
    "section": "3.3 Preformance",
    "text": "3.3 Preformance\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = model_dt.predict(X_test) #always gets x and retuns y\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_test,Y_pred_dt)  \naccuracy\n\n\n0.776536312849162\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_test,Y_pred_dt)  \nprecision_dt\n\n\n0.8148148148148148\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_test,Y_pred_dt)  \nrecall_dt\n\n\n0.5945945945945946\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_test,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[95, 10],\n       [30, 44]])\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_test, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.7496782496782497",
    "crumbs": [
      "Classification",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification/1 decision tree on titanic data.html#k-fold-cross-validation",
    "href": "classification/1 decision tree on titanic data.html#k-fold-cross-validation",
    "title": "Decision tree",
    "section": "3.4 k-Fold Cross-Validation",
    "text": "3.4 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(model_dt, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.8229291834925638",
    "crumbs": [
      "Classification",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification/0 titanic data.html#download-data",
    "href": "classification/0 titanic data.html#download-data",
    "title": "Titanic Dataset",
    "section": "2.1 download data",
    "text": "2.1 download data\nhttps://www.kaggle.com/c/titanic/data",
    "crumbs": [
      "Classification",
      "Titanic Dataset"
    ]
  },
  {
    "objectID": "classification/0 titanic data.html#input-data",
    "href": "classification/0 titanic data.html#input-data",
    "title": "Titanic Dataset",
    "section": "2.2 input data",
    "text": "2.2 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "Titanic Dataset"
    ]
  },
  {
    "objectID": "classification/0 titanic data.html#data-eda",
    "href": "classification/0 titanic data.html#data-eda",
    "title": "Titanic Dataset",
    "section": "2.3 data EDA",
    "text": "2.3 data EDA\n\n\nCode\ndf_train.describe()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\n\n\n\n\ncount\n891.000000\n891.000000\n891.000000\n714.000000\n891.000000\n891.000000\n891.000000\n\n\nmean\n446.000000\n0.383838\n2.308642\n29.699118\n0.523008\n0.381594\n32.204208\n\n\nstd\n257.353842\n0.486592\n0.836071\n14.526497\n1.102743\n0.806057\n49.693429\n\n\nmin\n1.000000\n0.000000\n1.000000\n0.420000\n0.000000\n0.000000\n0.000000\n\n\n25%\n223.500000\n0.000000\n2.000000\n20.125000\n0.000000\n0.000000\n7.910400\n\n\n50%\n446.000000\n0.000000\n3.000000\n28.000000\n0.000000\n0.000000\n14.454200\n\n\n75%\n668.500000\n1.000000\n3.000000\n38.000000\n1.000000\n0.000000\n31.000000\n\n\nmax\n891.000000\n1.000000\n3.000000\n80.000000\n8.000000\n6.000000\n512.329200\n\n\n\n\n\n\n\n\n\n\nCode\ndf_train.describe(include=[object])\n\n\n\n\n\n\n\n\n\n\nName\nSex\nTicket\nCabin\nEmbarked\n\n\n\n\ncount\n891\n891\n891\n204\n889\n\n\nunique\n891\n2\n681\n147\n3\n\n\ntop\nBraund, Mr. Owen Harris\nmale\n347082\nB96 B98\nS\n\n\nfreq\n1\n577\n7\n4\n644\n\n\n\n\n\n\n\n\nMissing Data\n\n\nCode\ndf_train.isnull().sum()\n\n\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\n\n\n\n\nCode\nimport seaborn as sns\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport sweetviz as sv\nmy_report = sv.analyze(df_train)\n\n\n\n\n\n\n\nCode\nmy_report.show_notebook()",
    "crumbs": [
      "Classification",
      "Titanic Dataset"
    ]
  },
  {
    "objectID": "classification/0 titanic data.html#feature-vs-target",
    "href": "classification/0 titanic data.html#feature-vs-target",
    "title": "Titanic Dataset",
    "section": "2.4 feature vs target",
    "text": "2.4 feature vs target\n\n\nCode\nmy_report2 = sv.analyze(df_train,target_feat='Survived')\n\n\n\n\n\n\n\nCode\nmy_report2.show_notebook()",
    "crumbs": [
      "Classification",
      "Titanic Dataset"
    ]
  },
  {
    "objectID": "classification/0 titanic data.html#compare-train-data-and-test-data",
    "href": "classification/0 titanic data.html#compare-train-data-and-test-data",
    "title": "Titanic Dataset",
    "section": "2.5 compare train data and test data",
    "text": "2.5 compare train data and test data\n\n\nCode\ncompare = sv.compare(source=df_train, compare=df_test)\n\n\n\n\n\n\n\nCode\ncompare.show_notebook()",
    "crumbs": [
      "Classification",
      "Titanic Dataset"
    ]
  },
  {
    "objectID": "classification/0 titanic data.html#data-dictionary",
    "href": "classification/0 titanic data.html#data-dictionary",
    "title": "Titanic Dataset",
    "section": "2.6 data dictionary",
    "text": "2.6 data dictionary\npclass: A proxy for socio-economic status (SES) 1st = Upper 2nd = Middle 3rd = Lower\nage: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\nsibsp: The dataset defines family relations in this way…\nSibling = brother, sister, stepbrother, stepsister\nSpouse = husband, wife (mistresses and fiancés were ignored)\nparch: The dataset defines family relations in this way…\nParent = mother, father\nChild = daughter, son, stepdaughter, stepson Some children travelled only with a nanny, therefore parch=0 for them.",
    "crumbs": [
      "Classification",
      "Titanic Dataset"
    ]
  },
  {
    "objectID": "classification/2 Logistic Regression on titanic data.html",
    "href": "classification/2 Logistic Regression on titanic data.html",
    "title": "Classification with Logistic Regression",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split",
    "crumbs": [
      "Classification",
      "Classification with Logistic Regression"
    ]
  },
  {
    "objectID": "classification/2 Logistic Regression on titanic data.html#input-data",
    "href": "classification/2 Logistic Regression on titanic data.html#input-data",
    "title": "Classification with Logistic Regression",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "Classification with Logistic Regression"
    ]
  },
  {
    "objectID": "classification/2 Logistic Regression on titanic data.html#data-eda",
    "href": "classification/2 Logistic Regression on titanic data.html#data-eda",
    "title": "Classification with Logistic Regression",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n\n\n0.7420382165605095\n0.18890814558058924",
    "crumbs": [
      "Classification",
      "Classification with Logistic Regression"
    ]
  },
  {
    "objectID": "classification/2 Logistic Regression on titanic data.html#data-wrangling",
    "href": "classification/2 Logistic Regression on titanic data.html#data-wrangling",
    "title": "Classification with Logistic Regression",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1046 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1308 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Dealing with missing numerical variables\ndata['Age'] = data.Age.fillna(data.Age.median())\ndata['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1309 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1309 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\ndata = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\nSex_male\n\n\n\n\n0\n1\n3\nBraund, Mr. Owen Harris\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\nTrue\n\n\n1\n2\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\nFalse\n\n\n2\n3\n3\nHeikkinen, Miss. Laina\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\nFalse\n\n\n3\n4\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\nFalse\n\n\n4\n5\n3\nAllen, Mr. William Henry\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\nTrue\n\n\n\n\n\n\n\n\n\n\nCode\n# Select features columns\ndata = data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()\n\n\n\n\n\n\n\n\n\n\nSex_male\nFare\nAge\nPclass\nSibSp\nrole\n\n\n\n\n0\nTrue\n7.2500\n22.0\n3\n1\ntrain\n\n\n1\nFalse\n71.2833\n38.0\n1\n1\ntrain\n\n\n2\nFalse\n7.9250\n26.0\n3\n0\ntrain\n\n\n3\nFalse\n53.1000\n35.0\n1\n1\ntrain\n\n\n4\nTrue\n8.0500\n35.0\n3\n0\ntrain",
    "crumbs": [
      "Classification",
      "Classification with Logistic Regression"
    ]
  },
  {
    "objectID": "classification/2 Logistic Regression on titanic data.html#split-data",
    "href": "classification/2 Logistic Regression on titanic data.html#split-data",
    "title": "Classification with Logistic Regression",
    "section": "2.4 split data",
    "text": "2.4 split data\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\nX_train = X_train.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 712 entries, 47 to 317\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Sex_male  712 non-null    bool   \n 1   Fare      712 non-null    float64\n 2   Age       712 non-null    float64\n 3   Pclass    712 non-null    int64  \n 4   SibSp     712 non-null    int64  \ndtypes: bool(1), float64(2), int64(2)\nmemory usage: 28.5 KB",
    "crumbs": [
      "Classification",
      "Classification with Logistic Regression"
    ]
  },
  {
    "objectID": "classification/2 Logistic Regression on titanic data.html#define-model",
    "href": "classification/2 Logistic Regression on titanic data.html#define-model",
    "title": "Classification with Logistic Regression",
    "section": "3.1 define model",
    "text": "3.1 define model\nThe solvers implemented in the class Logistic Regression are “liblinear”, “newton-cg”, “lbfgs”, “sag” and “saga”. According to Scikit Documentation: The “liblinear” solver was the one used by default for historical reasons before version 0.22. Since then, default use is lbfgs Algorithm.\n\n\nCode\nfrom sklearn.linear_model import LogisticRegression\nml_model = LogisticRegression(solver='liblinear')\nml_model\n\n\nLogisticRegression(solver='liblinear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniNot fittedLogisticRegression(solver='liblinear')",
    "crumbs": [
      "Classification",
      "Classification with Logistic Regression"
    ]
  },
  {
    "objectID": "classification/2 Logistic Regression on titanic data.html#train-model",
    "href": "classification/2 Logistic Regression on titanic data.html#train-model",
    "title": "Classification with Logistic Regression",
    "section": "3.2 train model",
    "text": "3.2 train model\n\n\nCode\nml_model.fit(X_train,Y_train)\n\n\nLogisticRegression(solver='liblinear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression(solver='liblinear') \n\n\nvariable importance\n\n\nCode\ncoefficients = ml_model.coef_[0]\n\nfeature_importance = pd.DataFrame({'Feature': X_train.columns, 'Importance': np.abs(coefficients)})\nfeature_importance = feature_importance.sort_values('Importance', ascending=False)\nfeature_importance\n\n\n\n\n\n\n\n\n\n\nFeature\nImportance\n\n\n\n\n0\nSex_male\n2.527450\n\n\n3\nPclass\n0.722334\n\n\n4\nSibSp\n0.325418\n\n\n2\nAge\n0.025424\n\n\n1\nFare\n0.004535",
    "crumbs": [
      "Classification",
      "Classification with Logistic Regression"
    ]
  },
  {
    "objectID": "classification/2 Logistic Regression on titanic data.html#preformance",
    "href": "classification/2 Logistic Regression on titanic data.html#preformance",
    "title": "Classification with Logistic Regression",
    "section": "3.3 Preformance",
    "text": "3.3 Preformance\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = ml_model.predict(X_test) #always gets x and retuns y\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_test,Y_pred_dt)  \naccuracy\n\n\n0.7932960893854749\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_test,Y_pred_dt)  \nprecision_dt\n\n\n0.6842105263157895\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_test,Y_pred_dt)  \nrecall_dt\n\n\n0.6724137931034483\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_test,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[103,  18],\n       [ 19,  39]])\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_test, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.761826731262468",
    "crumbs": [
      "Classification",
      "Classification with Logistic Regression"
    ]
  },
  {
    "objectID": "classification/2 Logistic Regression on titanic data.html#k-fold-cross-validation",
    "href": "classification/2 Logistic Regression on titanic data.html#k-fold-cross-validation",
    "title": "Classification with Logistic Regression",
    "section": "3.4 k-Fold Cross-Validation",
    "text": "3.4 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(ml_model, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.7921205555008373",
    "crumbs": [
      "Classification",
      "Classification with Logistic Regression"
    ]
  },
  {
    "objectID": "classification/3 Support Vector Machines on titanic data.html",
    "href": "classification/3 Support Vector Machines on titanic data.html",
    "title": "Classification with Support Vector Machines",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split",
    "crumbs": [
      "Classification",
      "Classification with Support Vector Machines"
    ]
  },
  {
    "objectID": "classification/3 Support Vector Machines on titanic data.html#input-data",
    "href": "classification/3 Support Vector Machines on titanic data.html#input-data",
    "title": "Classification with Support Vector Machines",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "Classification with Support Vector Machines"
    ]
  },
  {
    "objectID": "classification/3 Support Vector Machines on titanic data.html#data-eda",
    "href": "classification/3 Support Vector Machines on titanic data.html#data-eda",
    "title": "Classification with Support Vector Machines",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n\n\n0.7420382165605095\n0.18890814558058924",
    "crumbs": [
      "Classification",
      "Classification with Support Vector Machines"
    ]
  },
  {
    "objectID": "classification/3 Support Vector Machines on titanic data.html#data-wrangling",
    "href": "classification/3 Support Vector Machines on titanic data.html#data-wrangling",
    "title": "Classification with Support Vector Machines",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1046 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1308 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Dealing with missing numerical variables\ndata['Age'] = data.Age.fillna(data.Age.median())\ndata['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1309 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1309 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\ndata = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\nSex_male\n\n\n\n\n0\n1\n3\nBraund, Mr. Owen Harris\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\nTrue\n\n\n1\n2\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\nFalse\n\n\n2\n3\n3\nHeikkinen, Miss. Laina\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\nFalse\n\n\n3\n4\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\nFalse\n\n\n4\n5\n3\nAllen, Mr. William Henry\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\nTrue\n\n\n\n\n\n\n\n\n\n\nCode\n# Select features columns\ndata = data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()\n\n\n\n\n\n\n\n\n\n\nSex_male\nFare\nAge\nPclass\nSibSp\nrole\n\n\n\n\n0\nTrue\n7.2500\n22.0\n3\n1\ntrain\n\n\n1\nFalse\n71.2833\n38.0\n1\n1\ntrain\n\n\n2\nFalse\n7.9250\n26.0\n3\n0\ntrain\n\n\n3\nFalse\n53.1000\n35.0\n1\n1\ntrain\n\n\n4\nTrue\n8.0500\n35.0\n3\n0\ntrain",
    "crumbs": [
      "Classification",
      "Classification with Support Vector Machines"
    ]
  },
  {
    "objectID": "classification/3 Support Vector Machines on titanic data.html#split-data",
    "href": "classification/3 Support Vector Machines on titanic data.html#split-data",
    "title": "Classification with Support Vector Machines",
    "section": "2.4 split data",
    "text": "2.4 split data\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\nX_train = X_train.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 712 entries, 280 to 57\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Sex_male  712 non-null    bool   \n 1   Fare      712 non-null    float64\n 2   Age       712 non-null    float64\n 3   Pclass    712 non-null    int64  \n 4   SibSp     712 non-null    int64  \ndtypes: bool(1), float64(2), int64(2)\nmemory usage: 28.5 KB",
    "crumbs": [
      "Classification",
      "Classification with Support Vector Machines"
    ]
  },
  {
    "objectID": "classification/3 Support Vector Machines on titanic data.html#define-model",
    "href": "classification/3 Support Vector Machines on titanic data.html#define-model",
    "title": "Classification with Support Vector Machines",
    "section": "3.1 define model",
    "text": "3.1 define model\nThe solvers implemented in the class Logistic Regression are “liblinear”, “newton-cg”, “lbfgs”, “sag” and “saga”. According to Scikit Documentation: The “liblinear” solver was the one used by default for historical reasons before version 0.22. Since then, default use is lbfgs Algorithm.\n\n\nCode\nfrom sklearn import svm\nml_model = svm.SVC(kernel=\"linear\")\nml_model\n\n\nSVC(kernel='linear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  SVC?Documentation for SVCiNot fittedSVC(kernel='linear')",
    "crumbs": [
      "Classification",
      "Classification with Support Vector Machines"
    ]
  },
  {
    "objectID": "classification/3 Support Vector Machines on titanic data.html#train-model",
    "href": "classification/3 Support Vector Machines on titanic data.html#train-model",
    "title": "Classification with Support Vector Machines",
    "section": "3.2 train model",
    "text": "3.2 train model\n\n\nCode\nml_model.fit(X_train,Y_train)\n\n\nSVC(kernel='linear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  SVC?Documentation for SVCiFittedSVC(kernel='linear')",
    "crumbs": [
      "Classification",
      "Classification with Support Vector Machines"
    ]
  },
  {
    "objectID": "classification/3 Support Vector Machines on titanic data.html#preformance",
    "href": "classification/3 Support Vector Machines on titanic data.html#preformance",
    "title": "Classification with Support Vector Machines",
    "section": "3.3 Preformance",
    "text": "3.3 Preformance\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = ml_model.predict(X_test) #always gets x and retuns y\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_test,Y_pred_dt)  \naccuracy\n\n\n0.7430167597765364\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_test,Y_pred_dt)  \nprecision_dt\n\n\n0.7575757575757576\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_test,Y_pred_dt)  \nrecall_dt\n\n\n0.625\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_test,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[83, 16],\n       [30, 50]])\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_test, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.7316919191919191",
    "crumbs": [
      "Classification",
      "Classification with Support Vector Machines"
    ]
  },
  {
    "objectID": "classification/3 Support Vector Machines on titanic data.html#k-fold-cross-validation",
    "href": "classification/3 Support Vector Machines on titanic data.html#k-fold-cross-validation",
    "title": "Classification with Support Vector Machines",
    "section": "3.4 k-Fold Cross-Validation",
    "text": "3.4 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(ml_model, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.8005515611149413",
    "crumbs": [
      "Classification",
      "Classification with Support Vector Machines"
    ]
  },
  {
    "objectID": "classification/5 Random Forest on titanic dat.html",
    "href": "classification/5 Random Forest on titanic dat.html",
    "title": "Classification with Random Forest",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split",
    "crumbs": [
      "Classification",
      "Classification with Random Forest"
    ]
  },
  {
    "objectID": "classification/5 Random Forest on titanic dat.html#input-data",
    "href": "classification/5 Random Forest on titanic dat.html#input-data",
    "title": "Classification with Random Forest",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "Classification with Random Forest"
    ]
  },
  {
    "objectID": "classification/5 Random Forest on titanic dat.html#data-eda",
    "href": "classification/5 Random Forest on titanic dat.html#data-eda",
    "title": "Classification with Random Forest",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n\n\n0.7420382165605095\n0.18890814558058924",
    "crumbs": [
      "Classification",
      "Classification with Random Forest"
    ]
  },
  {
    "objectID": "classification/5 Random Forest on titanic dat.html#data-wrangling",
    "href": "classification/5 Random Forest on titanic dat.html#data-wrangling",
    "title": "Classification with Random Forest",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1046 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1308 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Dealing with missing numerical variables\ndata['Age'] = data.Age.fillna(data.Age.median())\ndata['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1309 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1309 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\ndata = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\nSex_male\n\n\n\n\n0\n1\n3\nBraund, Mr. Owen Harris\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\nTrue\n\n\n1\n2\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\nFalse\n\n\n2\n3\n3\nHeikkinen, Miss. Laina\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\nFalse\n\n\n3\n4\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\nFalse\n\n\n4\n5\n3\nAllen, Mr. William Henry\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\nTrue\n\n\n\n\n\n\n\n\n\n\nCode\n# Select features columns\ndata = data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()\n\n\n\n\n\n\n\n\n\n\nSex_male\nFare\nAge\nPclass\nSibSp\nrole\n\n\n\n\n0\nTrue\n7.2500\n22.0\n3\n1\ntrain\n\n\n1\nFalse\n71.2833\n38.0\n1\n1\ntrain\n\n\n2\nFalse\n7.9250\n26.0\n3\n0\ntrain\n\n\n3\nFalse\n53.1000\n35.0\n1\n1\ntrain\n\n\n4\nTrue\n8.0500\n35.0\n3\n0\ntrain",
    "crumbs": [
      "Classification",
      "Classification with Random Forest"
    ]
  },
  {
    "objectID": "classification/5 Random Forest on titanic dat.html#split-data",
    "href": "classification/5 Random Forest on titanic dat.html#split-data",
    "title": "Classification with Random Forest",
    "section": "2.4 split data",
    "text": "2.4 split data\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\nX_train = X_train.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 712 entries, 802 to 189\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Sex_male  712 non-null    bool   \n 1   Fare      712 non-null    float64\n 2   Age       712 non-null    float64\n 3   Pclass    712 non-null    int64  \n 4   SibSp     712 non-null    int64  \ndtypes: bool(1), float64(2), int64(2)\nmemory usage: 28.5 KB",
    "crumbs": [
      "Classification",
      "Classification with Random Forest"
    ]
  },
  {
    "objectID": "classification/5 Random Forest on titanic dat.html#define-model",
    "href": "classification/5 Random Forest on titanic dat.html#define-model",
    "title": "Classification with Random Forest",
    "section": "3.1 define model",
    "text": "3.1 define model\nThe solvers implemented in the class Logistic Regression are “liblinear”, “newton-cg”, “lbfgs”, “sag” and “saga”. According to Scikit Documentation: The “liblinear” solver was the one used by default for historical reasons before version 0.22. Since then, default use is lbfgs Algorithm.\n\n\nCode\nfrom sklearn.ensemble import RandomForestClassifier\nml_model = RandomForestClassifier()\nml_model\n\n\nRandomForestClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriNot fittedRandomForestClassifier()",
    "crumbs": [
      "Classification",
      "Classification with Random Forest"
    ]
  },
  {
    "objectID": "classification/5 Random Forest on titanic dat.html#train-model",
    "href": "classification/5 Random Forest on titanic dat.html#train-model",
    "title": "Classification with Random Forest",
    "section": "3.2 train model",
    "text": "3.2 train model\n\n\nCode\nml_model.fit(X_train,Y_train)\n\n\nRandomForestClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriFittedRandomForestClassifier()",
    "crumbs": [
      "Classification",
      "Classification with Random Forest"
    ]
  },
  {
    "objectID": "classification/5 Random Forest on titanic dat.html#preformance",
    "href": "classification/5 Random Forest on titanic dat.html#preformance",
    "title": "Classification with Random Forest",
    "section": "3.3 Preformance",
    "text": "3.3 Preformance\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = ml_model.predict(X_test) #always gets x and retuns y\nY_pred_dt\n\n\narray([0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n       1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,\n       0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n       1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,\n       0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,\n       0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n       0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,\n       1, 0, 0])\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_test,Y_pred_dt)  \naccuracy\n\n\n0.8435754189944135\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_test,Y_pred_dt)  \nprecision_dt\n\n\n0.7727272727272727\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_test,Y_pred_dt)  \nrecall_dt\n\n\n0.796875\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_test,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[100,  15],\n       [ 13,  51]])\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_test, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.8332201086956522",
    "crumbs": [
      "Classification",
      "Classification with Random Forest"
    ]
  },
  {
    "objectID": "classification/5 Random Forest on titanic dat.html#k-fold-cross-validation",
    "href": "classification/5 Random Forest on titanic dat.html#k-fold-cross-validation",
    "title": "Classification with Random Forest",
    "section": "3.4 k-Fold Cross-Validation",
    "text": "3.4 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(ml_model, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.7978233034571063",
    "crumbs": [
      "Classification",
      "Classification with Random Forest"
    ]
  },
  {
    "objectID": "classification/4 KNN on titanic data.html",
    "href": "classification/4 KNN on titanic data.html",
    "title": "Classification with K-Nearest Neighbors",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split",
    "crumbs": [
      "Classification",
      "Classification with K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "classification/4 KNN on titanic data.html#input-data",
    "href": "classification/4 KNN on titanic data.html#input-data",
    "title": "Classification with K-Nearest Neighbors",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "Classification with K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "classification/4 KNN on titanic data.html#data-eda",
    "href": "classification/4 KNN on titanic data.html#data-eda",
    "title": "Classification with K-Nearest Neighbors",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n\n\n0.7420382165605095\n0.18890814558058924",
    "crumbs": [
      "Classification",
      "Classification with K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "classification/4 KNN on titanic data.html#data-wrangling",
    "href": "classification/4 KNN on titanic data.html#data-wrangling",
    "title": "Classification with K-Nearest Neighbors",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1046 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1308 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Dealing with missing numerical variables\ndata['Age'] = data.Age.fillna(data.Age.median())\ndata['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1309 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1309 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\ndata = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\nSex_male\n\n\n\n\n0\n1\n3\nBraund, Mr. Owen Harris\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\nTrue\n\n\n1\n2\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\nFalse\n\n\n2\n3\n3\nHeikkinen, Miss. Laina\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\nFalse\n\n\n3\n4\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\nFalse\n\n\n4\n5\n3\nAllen, Mr. William Henry\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\nTrue\n\n\n\n\n\n\n\n\n\n\nCode\n# Select features columns\ndata = data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()\n\n\n\n\n\n\n\n\n\n\nSex_male\nFare\nAge\nPclass\nSibSp\nrole\n\n\n\n\n0\nTrue\n7.2500\n22.0\n3\n1\ntrain\n\n\n1\nFalse\n71.2833\n38.0\n1\n1\ntrain\n\n\n2\nFalse\n7.9250\n26.0\n3\n0\ntrain\n\n\n3\nFalse\n53.1000\n35.0\n1\n1\ntrain\n\n\n4\nTrue\n8.0500\n35.0\n3\n0\ntrain",
    "crumbs": [
      "Classification",
      "Classification with K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "classification/4 KNN on titanic data.html#split-data",
    "href": "classification/4 KNN on titanic data.html#split-data",
    "title": "Classification with K-Nearest Neighbors",
    "section": "2.4 split data",
    "text": "2.4 split data\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\nX_train = X_train.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 712 entries, 192 to 506\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Sex_male  712 non-null    bool   \n 1   Fare      712 non-null    float64\n 2   Age       712 non-null    float64\n 3   Pclass    712 non-null    int64  \n 4   SibSp     712 non-null    int64  \ndtypes: bool(1), float64(2), int64(2)\nmemory usage: 28.5 KB",
    "crumbs": [
      "Classification",
      "Classification with K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "classification/4 KNN on titanic data.html#define-model",
    "href": "classification/4 KNN on titanic data.html#define-model",
    "title": "Classification with K-Nearest Neighbors",
    "section": "3.1 define model",
    "text": "3.1 define model\nThe solvers implemented in the class Logistic Regression are “liblinear”, “newton-cg”, “lbfgs”, “sag” and “saga”. According to Scikit Documentation: The “liblinear” solver was the one used by default for historical reasons before version 0.22. Since then, default use is lbfgs Algorithm.\n\n\nCode\nfrom sklearn.neighbors import KNeighborsRegressor \nml_model = KNeighborsRegressor()\nml_model\n\n\nKNeighborsRegressor()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KNeighborsRegressor?Documentation for KNeighborsRegressoriNot fittedKNeighborsRegressor()",
    "crumbs": [
      "Classification",
      "Classification with K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "classification/4 KNN on titanic data.html#train-model",
    "href": "classification/4 KNN on titanic data.html#train-model",
    "title": "Classification with K-Nearest Neighbors",
    "section": "3.2 train model",
    "text": "3.2 train model\n\n\nCode\nml_model.fit(X_train,Y_train)\n\n\nKNeighborsRegressor()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KNeighborsRegressor?Documentation for KNeighborsRegressoriFittedKNeighborsRegressor()",
    "crumbs": [
      "Classification",
      "Classification with K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "classification/4 KNN on titanic data.html#preformance",
    "href": "classification/4 KNN on titanic data.html#preformance",
    "title": "Classification with K-Nearest Neighbors",
    "section": "3.3 Preformance",
    "text": "3.3 Preformance\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = ml_model.predict(X_test) #always gets x and retuns y\nY_pred_dt\n\n\narray([0.2, 0.2, 0.6, 0.4, 0.2, 0.8, 0.2, 0.4, 0. , 0.6, 0.4, 0.6, 0. ,\n       0.6, 0. , 0.6, 0. , 0.6, 1. , 0. , 0. , 0.8, 0.4, 0.8, 0.2, 0.2,\n       0.4, 0.2, 0. , 0.6, 0.4, 0. , 0.4, 0.6, 0. , 0.4, 0.8, 0. , 0.6,\n       0.6, 0.8, 0.4, 0.6, 0.6, 0.6, 0.2, 1. , 0.2, 0.8, 0.4, 0.4, 0.6,\n       0. , 0.2, 0.8, 0.6, 0. , 0.4, 1. , 0. , 0. , 0.4, 0.8, 0. , 0.4,\n       0.4, 0.4, 0.8, 0. , 0.4, 0.8, 0.2, 0.4, 1. , 0.8, 0.2, 0.2, 0.4,\n       0.4, 0. , 0.8, 0. , 0.2, 0.2, 0. , 0.4, 0. , 0.4, 0.4, 0.6, 0.2,\n       0. , 0.4, 0. , 0. , 0. , 0. , 0.8, 0. , 0.6, 0. , 0.4, 0.4, 1. ,\n       0.8, 0. , 0.2, 0.6, 0.6, 0.2, 0.2, 0.2, 0. , 0.2, 0. , 0. , 0. ,\n       0.2, 0.6, 0.2, 0.4, 0.8, 0.2, 0.4, 0.8, 0. , 0.4, 0.2, 0. , 0.2,\n       1. , 0.2, 0.2, 0.4, 0.6, 0.4, 0.6, 0.2, 0.4, 0.4, 0. , 0.2, 0.4,\n       0. , 0.8, 0.8, 0.6, 0.6, 0.2, 0.4, 0.2, 0.4, 0. , 0.4, 0.8, 0.2,\n       1. , 0. , 0.2, 0. , 0.8, 1. , 0.2, 0.2, 0.2, 0.8, 0.4, 0.4, 0. ,\n       0.2, 0.6, 0.2, 0.2, 0.8, 0.6, 0.2, 0.8, 0.2, 0.4])\n\n\n\n\nCode\n# its criteria is to round to 1 when higher than 0.5\nY_pred_dt = np.round(Y_pred_dt)  \n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_test,Y_pred_dt)  \naccuracy\n\n\n0.6983240223463687\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_test,Y_pred_dt)  \nprecision_dt\n\n\n0.6491228070175439\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_test,Y_pred_dt)  \nrecall_dt\n\n\n0.5211267605633803\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_test,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[88, 20],\n       [34, 37]])\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_test, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.6679707876890976",
    "crumbs": [
      "Classification",
      "Classification with K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "classification/4 KNN on titanic data.html#k-fold-cross-validation",
    "href": "classification/4 KNN on titanic data.html#k-fold-cross-validation",
    "title": "Classification with K-Nearest Neighbors",
    "section": "3.4 k-Fold Cross-Validation",
    "text": "3.4 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(ml_model, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.12900438323146735",
    "crumbs": [
      "Classification",
      "Classification with K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html",
    "href": "plot/1 seaborn.html",
    "title": "seaborn chart",
    "section": "",
    "text": "Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.\nCode\nimport seaborn as sns\nprint(sns.__version__)\n\n\n0.12.2\nCode\n# Import seaborn\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib\n\n# Apply the default theme\n#sns.set_theme()\n\n# Load an example dataset\ntips = sns.load_dataset(\"tips\")\ntips.head()\n\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html#color-by-group",
    "href": "plot/1 seaborn.html#color-by-group",
    "title": "seaborn chart",
    "section": "1.1 color by group",
    "text": "1.1 color by group\n\n\nCode\nsns.scatterplot(data=tips,x='tip',y='total_bill',hue='sex')",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html#size-by-group",
    "href": "plot/1 seaborn.html#size-by-group",
    "title": "seaborn chart",
    "section": "1.2 size by group",
    "text": "1.2 size by group\n\n\nCode\nsns.scatterplot(data=tips,x='tip',y='total_bill',size='size')",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html#color-by-group-1",
    "href": "plot/1 seaborn.html#color-by-group-1",
    "title": "seaborn chart",
    "section": "2.1 color by group",
    "text": "2.1 color by group\n\n\nCode\nimport random\nfrom siuba import _, mutate, filter, group_by, summarize,show_query\nfrom siuba import *\n\ndowjones2=dowjones&gt;&gt;mutate(type='old')\n\ndowjones3=dowjones&gt;&gt;mutate(Price=_.Price+random.random()*200,type='new')\n\ndowjones4=pd.concat([dowjones2, dowjones3], ignore_index = True)&gt;&gt; arrange(_.Date)\n\n\n\n\nCode\ndowjones4.head()\n\n\n\n\n\n\n\n\n\n\nDate\nPrice\ntype\n\n\n\n\n0\n1914-12-01\n55.000000\nold\n\n\n649\n1914-12-01\n140.834679\nnew\n\n\n1\n1915-01-01\n56.550000\nold\n\n\n650\n1915-01-01\n142.384679\nnew\n\n\n2\n1915-02-01\n56.000000\nold\n\n\n\n\n\n\n\n\n\n\nCode\nsns.lineplot(data=dowjones4,x='Date',y='Price',hue='type')",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html#color-by-group-2",
    "href": "plot/1 seaborn.html#color-by-group-2",
    "title": "seaborn chart",
    "section": "3.1 color by group",
    "text": "3.1 color by group\n\n\nCode\nsns.histplot(data=tips,x='tip',hue='sex',multiple=\"dodge\")",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html#color-by-group-3",
    "href": "plot/1 seaborn.html#color-by-group-3",
    "title": "seaborn chart",
    "section": "5.1 color by group",
    "text": "5.1 color by group\n\n\nCode\nsns.boxplot(data=tips,x='day',y='tip',hue='sex')",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html#color-by-group-4",
    "href": "plot/1 seaborn.html#color-by-group-4",
    "title": "seaborn chart",
    "section": "6.1 color by group",
    "text": "6.1 color by group\n\n\nCode\nsns.stripplot(data=tips,x='day',y='tip',hue='sex',dodge=True)\n\n\n\n\n\n\n\n\n\njoin plot\n\n\nCode\nsns.jointplot(data=tips,x='total_bill',y='tip',kind='reg')",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "intro/4 data structure in Python .html#singular",
    "href": "intro/4 data structure in Python .html#singular",
    "title": "data structure in Python",
    "section": "1.1 singular",
    "text": "1.1 singular\n\n\nCode\na=1\ntype(a)\n\n\nint\n\n\n\n\nCode\na=1.3\ntype(a)\n\n\nfloat\n\n\n\n\nCode\na='hell'\ntype(a)\n\n\nstr",
    "crumbs": [
      "Intro",
      "data structure in Python"
    ]
  },
  {
    "objectID": "intro/4 data structure in Python .html#list",
    "href": "intro/4 data structure in Python .html#list",
    "title": "data structure in Python",
    "section": "1.2 list",
    "text": "1.2 list\n\n\nCode\na=[1,2,3]\n\na\n\n\n[1, 2, 3]\n\n\n\n\nCode\ntype(a) \n\n\nlist\n\n\n\n\nCode\nfruits = ['orange', 'apple', 'pear', 'banana', 'kiwi', 'apple', 'banana','apple']\n\n\n\n1.2.1 find length of the list with len()\n\n\nCode\nlen(fruits)\n\n\n8\n\n\n\n\n1.2.2 find how many time in the list with count()\n\n\nCode\nfruits.count('apple')\n\n\n3\n\n\n\n\n1.2.3 find locaiton of on the list with index()\nshow the first ‘apple’ index. python list start at 0\n\n\nCode\nfruits.index('apple')\n\n\n1\n\n\nall ‘apple’ in the list\n\n\nCode\n[index for index, value in enumerate(fruits) if value == 'apple']\n\n\n[1, 5, 7]",
    "crumbs": [
      "Intro",
      "data structure in Python"
    ]
  },
  {
    "objectID": "intro/4 data structure in Python .html#reverse-the-list",
    "href": "intro/4 data structure in Python .html#reverse-the-list",
    "title": "data structure in Python",
    "section": "1.3 reverse the list",
    "text": "1.3 reverse the list\n\n\nCode\nfruits.reverse()\nfruits\n\n\n['apple', 'banana', 'apple', 'kiwi', 'banana', 'pear', 'apple', 'orange']\n\n\n\n1.3.1 sort the list\n\n\nCode\nfruits.sort()\nfruits\n\n\n['apple', 'apple', 'apple', 'banana', 'banana', 'kiwi', 'orange', 'pear']\n\n\n\n\n1.3.2 add element on the list\n\n\nCode\nfruits.append('grape')\nfruits\n\n\n['apple',\n 'apple',\n 'apple',\n 'banana',\n 'banana',\n 'kiwi',\n 'orange',\n 'pear',\n 'grape']",
    "crumbs": [
      "Intro",
      "data structure in Python"
    ]
  },
  {
    "objectID": "intro/4 data structure in Python .html#drop-last-element",
    "href": "intro/4 data structure in Python .html#drop-last-element",
    "title": "data structure in Python",
    "section": "1.4 drop last element",
    "text": "1.4 drop last element\n\n\nCode\nfruits.pop()\n\nfruits\n\n\n['apple', 'apple', 'apple', 'banana', 'banana', 'kiwi', 'orange', 'pear']\n\n\n\n1.4.1 List Comprehensions\nusing loop:\n\n\nCode\nsquares = []\nfor x in range(10):\n  squares.append(x**2)\n  \nsquares\n\n\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n\n\nusing List Comprehensions\n\n\nCode\nsquares = [x**2 for x in range(10)]\nsquares\n\n\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]",
    "crumbs": [
      "Intro",
      "data structure in Python"
    ]
  },
  {
    "objectID": "intro/4 data structure in Python .html#tuples",
    "href": "intro/4 data structure in Python .html#tuples",
    "title": "data structure in Python",
    "section": "1.5 Tuples",
    "text": "1.5 Tuples\n\n\nCode\nfruits = ('orange', 'apple', 'pear', 'banana', 'kiwi', 'apple', 'banana','apple')\n\nfruits\n\n\n('orange', 'apple', 'pear', 'banana', 'kiwi', 'apple', 'banana', 'apple')\n\n\n\n\nCode\ntype(fruits)\n\n\ntuple\n\n\ntuple can not be modified.",
    "crumbs": [
      "Intro",
      "data structure in Python"
    ]
  },
  {
    "objectID": "intro/4 data structure in Python .html#sets",
    "href": "intro/4 data structure in Python .html#sets",
    "title": "data structure in Python",
    "section": "1.6 Sets",
    "text": "1.6 Sets\nA set is an unordered collection with no duplicate elements.\n\n\nCode\nbasket = {'apple', 'orange', 'apple', 'pear', 'orange', 'banana'}\n\nbasket\n\n\n{'apple', 'banana', 'orange', 'pear'}\n\n\n\n\nCode\ntype(basket)\n\n\nset",
    "crumbs": [
      "Intro",
      "data structure in Python"
    ]
  },
  {
    "objectID": "intro/4 data structure in Python .html#dictionaries",
    "href": "intro/4 data structure in Python .html#dictionaries",
    "title": "data structure in Python",
    "section": "1.7 Dictionaries",
    "text": "1.7 Dictionaries\n\n\nCode\ntel = {'jack': 4098, 'sape': 4139}\n\ntel\n\n\n{'jack': 4098, 'sape': 4139}\n\n\n\n\nCode\ntype(tel)\n\n\ndict\n\n\n\n\nCode\ntel['jack']\n\n\n4098",
    "crumbs": [
      "Intro",
      "data structure in Python"
    ]
  },
  {
    "objectID": "regression/0 house price data.html#download-data",
    "href": "regression/0 house price data.html#download-data",
    "title": "Housing Prices Dataset",
    "section": "2.1 download data",
    "text": "2.1 download data\nhttps://www.kaggle.com/competitions/house-prices-advanced-regression-techniques",
    "crumbs": [
      "Regression",
      "Housing Prices Dataset"
    ]
  },
  {
    "objectID": "regression/0 house price data.html#input-data",
    "href": "regression/0 house price data.html#input-data",
    "title": "Housing Prices Dataset",
    "section": "2.2 input data",
    "text": "2.2 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nId = df_train['Id']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nId\nMSSubClass\nMSZoning\nLotFrontage\nLotArea\nStreet\nAlley\nLotShape\nLandContour\nUtilities\n...\nPoolArea\nPoolQC\nFence\nMiscFeature\nMiscVal\nMoSold\nYrSold\nSaleType\nSaleCondition\nSalePrice\n\n\n\n\n0\n1\n60\nRL\n65.0\n8450\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n2\n2008\nWD\nNormal\n208500\n\n\n1\n2\n20\nRL\n80.0\n9600\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2007\nWD\nNormal\n181500\n\n\n2\n3\n60\nRL\n68.0\n11250\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n9\n2008\nWD\nNormal\n223500\n\n\n3\n4\n70\nRL\n60.0\n9550\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n2\n2006\nWD\nAbnorml\n140000\n\n\n4\n5\n60\nRL\n84.0\n14260\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n12\n2008\nWD\nNormal\n250000\n\n\n\n\n5 rows × 81 columns\n\n\n\n\n\n\nCode\n#df_train.info()\n\n\n\n\nCode\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['SalePrice'], axis=1), df_test])",
    "crumbs": [
      "Regression",
      "Housing Prices Dataset"
    ]
  },
  {
    "objectID": "regression/0 house price data.html#data-eda",
    "href": "regression/0 house price data.html#data-eda",
    "title": "Housing Prices Dataset",
    "section": "2.3 data EDA",
    "text": "2.3 data EDA\n\n\nCode\ndf_train.describe()\n\n\n\n\n\n\n\n\n\n\nId\nMSSubClass\nLotFrontage\nLotArea\nOverallQual\nOverallCond\nYearBuilt\nYearRemodAdd\nMasVnrArea\nBsmtFinSF1\n...\nWoodDeckSF\nOpenPorchSF\nEnclosedPorch\n3SsnPorch\nScreenPorch\nPoolArea\nMiscVal\nMoSold\nYrSold\nSalePrice\n\n\n\n\ncount\n1460.000000\n1460.000000\n1201.000000\n1460.000000\n1460.000000\n1460.000000\n1460.000000\n1460.000000\n1452.000000\n1460.000000\n...\n1460.000000\n1460.000000\n1460.000000\n1460.000000\n1460.000000\n1460.000000\n1460.000000\n1460.000000\n1460.000000\n1460.000000\n\n\nmean\n730.500000\n56.897260\n70.049958\n10516.828082\n6.099315\n5.575342\n1971.267808\n1984.865753\n103.685262\n443.639726\n...\n94.244521\n46.660274\n21.954110\n3.409589\n15.060959\n2.758904\n43.489041\n6.321918\n2007.815753\n180921.195890\n\n\nstd\n421.610009\n42.300571\n24.284752\n9981.264932\n1.382997\n1.112799\n30.202904\n20.645407\n181.066207\n456.098091\n...\n125.338794\n66.256028\n61.119149\n29.317331\n55.757415\n40.177307\n496.123024\n2.703626\n1.328095\n79442.502883\n\n\nmin\n1.000000\n20.000000\n21.000000\n1300.000000\n1.000000\n1.000000\n1872.000000\n1950.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n2006.000000\n34900.000000\n\n\n25%\n365.750000\n20.000000\n59.000000\n7553.500000\n5.000000\n5.000000\n1954.000000\n1967.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n5.000000\n2007.000000\n129975.000000\n\n\n50%\n730.500000\n50.000000\n69.000000\n9478.500000\n6.000000\n5.000000\n1973.000000\n1994.000000\n0.000000\n383.500000\n...\n0.000000\n25.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n6.000000\n2008.000000\n163000.000000\n\n\n75%\n1095.250000\n70.000000\n80.000000\n11601.500000\n7.000000\n6.000000\n2000.000000\n2004.000000\n166.000000\n712.250000\n...\n168.000000\n68.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n8.000000\n2009.000000\n214000.000000\n\n\nmax\n1460.000000\n190.000000\n313.000000\n215245.000000\n10.000000\n9.000000\n2010.000000\n2010.000000\n1600.000000\n5644.000000\n...\n857.000000\n547.000000\n552.000000\n508.000000\n480.000000\n738.000000\n15500.000000\n12.000000\n2010.000000\n755000.000000\n\n\n\n\n8 rows × 38 columns\n\n\n\n\n\n\nCode\ndf_train.describe(include=[object])\n\n\n\n\n\n\n\n\n\n\nMSZoning\nStreet\nAlley\nLotShape\nLandContour\nUtilities\nLotConfig\nLandSlope\nNeighborhood\nCondition1\n...\nGarageFinish\nGarageQual\nGarageCond\nPavedDrive\nPoolQC\nFence\nMiscFeature\nSaleType\nSaleCondition\nrole\n\n\n\n\ncount\n1460\n1460\n91\n1460\n1460\n1460\n1460\n1460\n1460\n1460\n...\n1379\n1379\n1379\n1460\n7\n281\n54\n1460\n1460\n1460\n\n\nunique\n5\n2\n2\n4\n4\n2\n5\n3\n25\n9\n...\n3\n5\n5\n3\n3\n4\n4\n9\n6\n1\n\n\ntop\nRL\nPave\nGrvl\nReg\nLvl\nAllPub\nInside\nGtl\nNAmes\nNorm\n...\nUnf\nTA\nTA\nY\nGd\nMnPrv\nShed\nWD\nNormal\ntrain\n\n\nfreq\n1151\n1454\n50\n925\n1311\n1459\n1052\n1382\n225\n1260\n...\n605\n1311\n1326\n1340\n3\n157\n49\n1267\n1198\n1460\n\n\n\n\n4 rows × 44 columns\n\n\n\n\nMissing Data\n\n\nCode\ndf_train.isnull().sum()\n\n\nId                 0\nMSSubClass         0\nMSZoning           0\nLotFrontage      259\nLotArea            0\n                ... \nYrSold             0\nSaleType           0\nSaleCondition      0\nSalePrice          0\nrole               0\nLength: 82, dtype: int64\n\n\n\n\nCode\nimport seaborn as sns\nsns.histplot(data=df_train,x='SalePrice')",
    "crumbs": [
      "Regression",
      "Housing Prices Dataset"
    ]
  },
  {
    "objectID": "regression/0 house price data.html#feature-vs-target",
    "href": "regression/0 house price data.html#feature-vs-target",
    "title": "Housing Prices Dataset",
    "section": "2.5 feature vs target",
    "text": "2.5 feature vs target\n\n\nCode\nmy_report2 = sv.analyze(df_train,target_feat='SalePrice')\n\n\n\n\n\n\n\nCode\nmy_report2.show_notebook()",
    "crumbs": [
      "Regression",
      "Housing Prices Dataset"
    ]
  },
  {
    "objectID": "regression/0 house price data.html#compare-train-data-and-test-data",
    "href": "regression/0 house price data.html#compare-train-data-and-test-data",
    "title": "Housing Prices Dataset",
    "section": "2.6 compare train data and test data",
    "text": "2.6 compare train data and test data\n\n\nCode\ncompare = sv.compare(source=df_train, compare=df_test)\n\n\n\n\n\n\n\nCode\ncompare.show_notebook()",
    "crumbs": [
      "Regression",
      "Housing Prices Dataset"
    ]
  },
  {
    "objectID": "regression/0 house price data.html#data-dictionary",
    "href": "regression/0 house price data.html#data-dictionary",
    "title": "Housing Prices Dataset",
    "section": "2.7 data dictionary",
    "text": "2.7 data dictionary\nSalePrice - the property’s sale price in dollars. This is the target variable that you’re trying to predict.\nMSSubClass: The building class\nMSZoning: The general zoning classification\nLotFrontage: Linear feet of street connected to property\nLotArea: Lot size in square feet\nStreet: Type of road access\nAlley: Type of alley access\nLotShape: General shape of property\nLandContour: Flatness of the property\nUtilities: Type of utilities available\nLotConfig: Lot configuration\nLandSlope: Slope of property\nNeighborhood: Physical locations within Ames city limits\nCondition1: Proximity to main road or railroad\nCondition2: Proximity to main road or railroad (if a second is present)\nBldgType: Type of dwelling\nHouseStyle: Style of dwelling\nOverallQual: Overall material and finish quality\nOverallCond: Overall condition rating\nYearBuilt: Original construction date\nYearRemodAdd: Remodel date\nRoofStyle: Type of roof\nRoofMatl: Roof material\nExterior1st: Exterior covering on house\nExterior2nd: Exterior covering on house (if more than one material)\nMasVnrType: Masonry veneer type\nMasVnrArea: Masonry veneer area in square feet\nExterQual: Exterior material quality\nExterCond: Present condition of the material on the exterior\nFoundation: Type of foundation\nBsmtQual: Height of the basement\nBsmtCond: General condition of the basement\nBsmtExposure: Walkout or garden level basement walls\nBsmtFinType1: Quality of basement finished area\nBsmtFinSF1: Type 1 finished square feet\nBsmtFinType2: Quality of second finished area (if present)\nBsmtFinSF2: Type 2 finished square feet\nBsmtUnfSF: Unfinished square feet of basement area\nTotalBsmtSF: Total square feet of basement area\nHeating: Type of heating\nHeatingQC: Heating quality and condition\nCentralAir: Central air conditioning\nElectrical: Electrical system\n1stFlrSF: First Floor square feet\n2ndFlrSF: Second floor square feet\nLowQualFinSF: Low quality finished square feet (all floors)\nGrLivArea: Above grade (ground) living area square feet\nBsmtFullBath: Basement full bathrooms\nBsmtHalfBath: Basement half bathrooms\nFullBath: Full bathrooms above grade\nHalfBath: Half baths above grade\nBedroom: Number of bedrooms above basement level\nKitchen: Number of kitchens\nKitchenQual: Kitchen quality\nTotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\nFunctional: Home functionality rating\nFireplaces: Number of fireplaces\nFireplaceQu: Fireplace quality\nGarageType: Garage location\nGarageYrBlt: Year garage was built\nGarageFinish: Interior finish of the garage\nGarageCars: Size of garage in car capacity\nGarageArea: Size of garage in square feet\nGarageQual: Garage quality\nGarageCond: Garage condition\nPavedDrive: Paved driveway\nWoodDeckSF: Wood deck area in square feet\nOpenPorchSF: Open porch area in square feet\nEnclosedPorch: Enclosed porch area in square feet\n3SsnPorch: Three season porch area in square feet\nScreenPorch: Screen porch area in square feet\nPoolArea: Pool area in square feet\nPoolQC: Pool quality\nFence: Fence quality\nMiscFeature: Miscellaneous feature not covered in other categories\nMiscVal: $Value of miscellaneous feature\nMoSold: Month Sold\nYrSold: Year Sold\nSaleType: Type of sale\nSaleCondition: Condition of sale",
    "crumbs": [
      "Regression",
      "Housing Prices Dataset"
    ]
  },
  {
    "objectID": "regression/5 XGboost on house price data.html",
    "href": "regression/5 XGboost on house price data.html",
    "title": "XGboost and pipeline, hyperparameter tuning",
    "section": "",
    "text": "with pipeline and tunning",
    "crumbs": [
      "Regression",
      "XGboost and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/5 XGboost on house price data.html#input-data",
    "href": "regression/5 XGboost on house price data.html#input-data",
    "title": "XGboost and pipeline, hyperparameter tuning",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nId = df_train['Id']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nId\nMSSubClass\nMSZoning\nLotFrontage\nLotArea\nStreet\nAlley\nLotShape\nLandContour\nUtilities\n...\nPoolArea\nPoolQC\nFence\nMiscFeature\nMiscVal\nMoSold\nYrSold\nSaleType\nSaleCondition\nSalePrice\n\n\n\n\n0\n1\n60\nRL\n65.0\n8450\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n2\n2008\nWD\nNormal\n208500\n\n\n1\n2\n20\nRL\n80.0\n9600\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2007\nWD\nNormal\n181500\n\n\n2\n3\n60\nRL\n68.0\n11250\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n9\n2008\nWD\nNormal\n223500\n\n\n3\n4\n70\nRL\n60.0\n9550\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n2\n2006\nWD\nAbnorml\n140000\n\n\n4\n5\n60\nRL\n84.0\n14260\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n12\n2008\nWD\nNormal\n250000\n\n\n\n\n5 rows × 81 columns\n\n\n\n\n\n\nCode\n#df_train.info()\n\n\n\n\nCode\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['SalePrice'], axis=1), df_test])",
    "crumbs": [
      "Regression",
      "XGboost and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/5 XGboost on house price data.html#data-eda",
    "href": "regression/5 XGboost on house price data.html#data-eda",
    "title": "XGboost and pipeline, hyperparameter tuning",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\nin step 1",
    "crumbs": [
      "Regression",
      "XGboost and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/5 XGboost on house price data.html#data-wrangling",
    "href": "regression/5 XGboost on house price data.html#data-wrangling",
    "title": "XGboost and pipeline, hyperparameter tuning",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling",
    "crumbs": [
      "Regression",
      "XGboost and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/5 XGboost on house price data.html#split-data",
    "href": "regression/5 XGboost on house price data.html#split-data",
    "title": "XGboost and pipeline, hyperparameter tuning",
    "section": "2.4 split data",
    "text": "2.4 split data\n\n\nCode\nY = df_train.SalePrice\nX = df_train.drop(['SalePrice'], axis=1)\n\n\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\nX_train = X_train.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nprint(X_train.shape)\nprint(X_test.shape)\n\n\n(1168, 80)\n(292, 80)\n\n\n\n\nCode\nprint(Y_train.shape)\nprint(Y_test.shape)\n\n\n(1168,)\n(292,)\n\n\n\n\nCode\ncategorical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].nunique() &lt; 10 and X_train[cname].dtype == \"object\"]\n                    \n                    \nnumerical_cols = numerical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].dtype in ['int64', 'float64']]\n\n\n\n\nCode\nprint(\"The total number of categorical columns:\", len(categorical_cols))\nprint(\"The total number of numerical columns:\", len(numerical_cols))\n\n\nThe total number of categorical columns: 40\nThe total number of numerical columns: 37\n\n\n\n\nCode\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train[my_cols].copy()\nX_test= X_test[my_cols].copy()\n\n\nX_final = df_test[my_cols].copy()",
    "crumbs": [
      "Regression",
      "XGboost and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/5 XGboost on house price data.html#pipelines-for-data-preprocessing",
    "href": "regression/5 XGboost on house price data.html#pipelines-for-data-preprocessing",
    "title": "XGboost and pipeline, hyperparameter tuning",
    "section": "2.5 Pipelines for Data Preprocessing",
    "text": "2.5 Pipelines for Data Preprocessing\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\nCode\nnumerical_transformer = Pipeline(steps=[\n    ('imputer_num', SimpleImputer(strategy='median')), \n    ('scaler', StandardScaler())\n])\n\n\n\n\nCode\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n\n\nCode\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)])",
    "crumbs": [
      "Regression",
      "XGboost and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/5 XGboost on house price data.html#define-model",
    "href": "regression/5 XGboost on house price data.html#define-model",
    "title": "XGboost and pipeline, hyperparameter tuning",
    "section": "3.1 define model",
    "text": "3.1 define model\nrandom forest with hyper parameter tuning\n\n\nCode\nimport xgboost\nprint(xgboost.__version__)\n\n\n2.0.3\n\n\nrandom forest with hyper parameter tuning\n\n\nCode\nfrom xgboost import XGBRegressor\n\nml_model = XGBRegressor()\nml_model\n\n\nXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=None, n_jobs=None,\n             num_parallel_tree=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. XGBRegressoriNot fittedXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=None, n_jobs=None,\n             num_parallel_tree=None, random_state=None, ...)",
    "crumbs": [
      "Regression",
      "XGboost and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/5 XGboost on house price data.html#define-pipline",
    "href": "regression/5 XGboost on house price data.html#define-pipline",
    "title": "XGboost and pipeline, hyperparameter tuning",
    "section": "3.2 define pipline",
    "text": "3.2 define pipline\n\n\nCode\npipeline = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', ml_model)\n         ]\n)",
    "crumbs": [
      "Regression",
      "XGboost and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/5 XGboost on house price data.html#hyperparameter-tuning-set",
    "href": "regression/5 XGboost on house price data.html#hyperparameter-tuning-set",
    "title": "XGboost and pipeline, hyperparameter tuning",
    "section": "3.3 hyperparameter tuning set",
    "text": "3.3 hyperparameter tuning set\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\n\n\nparam_grid = {\n        'model__learning_rate': [0.01, 0.1],\n        'model__max_depth': [3, 5, 7, 10],\n        'model__min_child_weight': [1, 3, 5],\n        'model__subsample': [0.5, 0.7],\n       # 'model__colsample__bytree': [0.5, 0.7],\n        'model__n_estimators' : [100, 200, 500],\n        'model__objective': ['reg:squarederror']\n    }\n\n\n\n\nCode\nGridCV = GridSearchCV(pipeline, param_grid, n_jobs= -1, verbose=1)",
    "crumbs": [
      "Regression",
      "XGboost and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/5 XGboost on house price data.html#train-model",
    "href": "regression/5 XGboost on house price data.html#train-model",
    "title": "XGboost and pipeline, hyperparameter tuning",
    "section": "3.4 train model",
    "text": "3.4 train model\n\n\nCode\nGridCV.fit(X_train, Y_train)\n\n\nFitting 5 folds for each of 144 candidates, totalling 720 fits\n\n\nGridSearchCV(estimator=Pipeline(steps=[('preprocessor',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         Pipeline(steps=[('imputer_num',\n                                                                                          SimpleImputer(strategy='median')),\n                                                                                         ('scaler',\n                                                                                          StandardScaler())]),\n                                                                         ['Id',\n                                                                          'MSSubClass',\n                                                                          'LotFrontage',\n                                                                          'LotArea',\n                                                                          'OverallQual',\n                                                                          'OverallCond',\n                                                                          'YearBuilt',\n                                                                          'YearRemodAdd',\n                                                                          'MasVnrArea',\n                                                                          'BsmtFinSF1',\n                                                                          'BsmtFinSF2',\n                                                                          'BsmtUnfSF',\n                                                                          'TotalBsmtSF...\n                                                     monotone_constraints=None,\n                                                     multi_strategy=None,\n                                                     n_estimators=None,\n                                                     n_jobs=None,\n                                                     num_parallel_tree=None,\n                                                     random_state=None, ...))]),\n             n_jobs=-1,\n             param_grid={'model__learning_rate': [0.01, 0.1],\n                         'model__max_depth': [3, 5, 7, 10],\n                         'model__min_child_weight': [1, 3, 5],\n                         'model__n_estimators': [100, 200, 500],\n                         'model__objective': ['reg:squarederror'],\n                         'model__subsample': [0.5, 0.7]},\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(estimator=Pipeline(steps=[('preprocessor',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         Pipeline(steps=[('imputer_num',\n                                                                                          SimpleImputer(strategy='median')),\n                                                                                         ('scaler',\n                                                                                          StandardScaler())]),\n                                                                         ['Id',\n                                                                          'MSSubClass',\n                                                                          'LotFrontage',\n                                                                          'LotArea',\n                                                                          'OverallQual',\n                                                                          'OverallCond',\n                                                                          'YearBuilt',\n                                                                          'YearRemodAdd',\n                                                                          'MasVnrArea',\n                                                                          'BsmtFinSF1',\n                                                                          'BsmtFinSF2',\n                                                                          'BsmtUnfSF',\n                                                                          'TotalBsmtSF...\n                                                     monotone_constraints=None,\n                                                     multi_strategy=None,\n                                                     n_estimators=None,\n                                                     n_jobs=None,\n                                                     num_parallel_tree=None,\n                                                     random_state=None, ...))]),\n             n_jobs=-1,\n             param_grid={'model__learning_rate': [0.01, 0.1],\n                         'model__max_depth': [3, 5, 7, 10],\n                         'model__min_child_weight': [1, 3, 5],\n                         'model__n_estimators': [100, 200, 500],\n                         'model__objective': ['reg:squarederror'],\n                         'model__subsample': [0.5, 0.7]},\n             verbose=1) estimator: PipelinePipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer_num',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['Id', 'MSSubClass',\n                                                   'LotFrontage', 'LotArea',\n                                                   'OverallQual', 'OverallCond',\n                                                   'YearBuilt', 'YearRemodAdd',\n                                                   'MasVnrArea', 'BsmtFinSF1',\n                                                   'BsmtFinSF2', 'BsmtUnfSF',\n                                                   'TotalBsmtSF', '1stFlrSF',\n                                                   '2ndFlrSF'...\n                              feature_types=None, gamma=None, grow_policy=None,\n                              importance_type=None,\n                              interaction_constraints=None, learning_rate=None,\n                              max_bin=None, max_cat_threshold=None,\n                              max_cat_to_onehot=None, max_delta_step=None,\n                              max_depth=None, max_leaves=None,\n                              min_child_weight=None, missing=nan,\n                              monotone_constraints=None, multi_strategy=None,\n                              n_estimators=None, n_jobs=None,\n                              num_parallel_tree=None, random_state=None, ...))])  preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('imputer_num',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('scaler', StandardScaler())]),\n                                 ['Id', 'MSSubClass', 'LotFrontage', 'LotArea',\n                                  'OverallQual', 'OverallCond', 'YearBuilt',\n                                  'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1',\n                                  'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF',\n                                  '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n                                  'GrLivArea', 'Bsm...\n                                 ['MSZoning', 'Street', 'Alley', 'LotShape',\n                                  'LandContour', 'Utilities', 'LotConfig',\n                                  'LandSlope', 'Condition1', 'Condition2',\n                                  'BldgType', 'HouseStyle', 'RoofStyle',\n                                  'RoofMatl', 'MasVnrType', 'ExterQual',\n                                  'ExterCond', 'Foundation', 'BsmtQual',\n                                  'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n                                  'BsmtFinType2', 'Heating', 'HeatingQC',\n                                  'CentralAir', 'Electrical', 'KitchenQual',\n                                  'Functional', 'FireplaceQu', ...])]) num['Id', 'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='median')  StandardScaler?Documentation for StandardScalerStandardScaler() cat['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'SaleType', 'SaleCondition']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='most_frequent')  OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore') XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=None, n_jobs=None,\n             num_parallel_tree=None, random_state=None, ...) \n\n\n\n\nCode\nGridCV.best_params_\n\n\n{'model__learning_rate': 0.01,\n 'model__max_depth': 5,\n 'model__min_child_weight': 5,\n 'model__n_estimators': 500,\n 'model__objective': 'reg:squarederror',\n 'model__subsample': 0.5}\n\n\n\n\nCode\nGridCV.best_score_\n\n\n0.8682297036551574\n\n\nbest model as pipeline\n\n\nCode\noptimised_model_pipeline = GridCV.best_estimator_\n\n\n\n\nCode\nvar=optimised_model_pipeline[:-1].get_feature_names_out()\n#var\n\n\n\n\nCode\nfitted_model=optimised_model_pipeline.steps[1][1]\n\n\nvariable importance\n\n\nCode\nimportances = fitted_model.feature_importances_\nvi=pd.DataFrame({\"variable\":var,\"importances\":importances})\nvi=vi.sort_values('importances',ascending=False)\nvi\n\n\n\n\n\n\n\n\n\n\nvariable\nimportances\n\n\n\n\n4\nnum__OverallQual\n0.189310\n\n\n26\nnum__GarageCars\n0.069107\n\n\n126\ncat__BsmtQual_Ex\n0.038735\n\n\n19\nnum__FullBath\n0.036401\n\n\n114\ncat__ExterQual_TA\n0.031176\n\n\n...\n...\n...\n\n\n112\ncat__ExterQual_Fa\n0.000000\n\n\n43\ncat__Street_Pave\n0.000000\n\n\n45\ncat__Alley_Pave\n0.000000\n\n\n106\ncat__RoofMatl_WdShake\n0.000000\n\n\n115\ncat__ExterCond_Ex\n0.000000\n\n\n\n\n230 rows × 2 columns",
    "crumbs": [
      "Regression",
      "XGboost and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/5 XGboost on house price data.html#preformance",
    "href": "regression/5 XGboost on house price data.html#preformance",
    "title": "XGboost and pipeline, hyperparameter tuning",
    "section": "3.5 Preformance",
    "text": "3.5 Preformance\n\n\nCode\nY_pred_dt =optimised_model_pipeline.predict(X_test) #always gets x and retuns y\n\n\nR 2\n\n\nCode\nfrom sklearn.metrics import r2_score\nr2_score(Y_test, Y_pred_dt)\n\n\n0.9095407797423984\n\n\nMAE\n\n\nCode\nfrom sklearn.metrics import mean_absolute_error\nmean_absolute_error(Y_test, Y_pred_dt)\n\n\n15227.14426369863\n\n\nRMSE\n\n\nCode\nfrom  math import sqrt\nfrom sklearn.metrics import mean_squared_error\nmse=mean_squared_error(Y_test, Y_pred_dt)\nrmse=sqrt(mse)\nrmse\n\n\n22960.975221159788",
    "crumbs": [
      "Regression",
      "XGboost and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/5 XGboost on house price data.html#k-fold-cross-validation",
    "href": "regression/5 XGboost on house price data.html#k-fold-cross-validation",
    "title": "XGboost and pipeline, hyperparameter tuning",
    "section": "3.6 k-Fold Cross-Validation",
    "text": "3.6 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \n\n\n\n\nCode\ncv_dt = cross_val_score(optimised_model_pipeline, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.8683986008609523\n\n\n\n\nCode\ncv_dt = cross_val_score(optimised_model_pipeline, X_train, Y_train, cv=kf_dt,scoring = 'neg_mean_squared_error')\nnp.mean(np.sqrt(np.abs(cv_dt)))\n\n\n30095.130277299217",
    "crumbs": [
      "Regression",
      "XGboost and pipeline, hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/2 Decision Tree on house price data copy.html",
    "href": "regression/2 Decision Tree on house price data copy.html",
    "title": "Decision tree",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split",
    "crumbs": [
      "Regression",
      "Decision tree"
    ]
  },
  {
    "objectID": "regression/2 Decision Tree on house price data copy.html#input-data",
    "href": "regression/2 Decision Tree on house price data copy.html#input-data",
    "title": "Decision tree",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nId = df_train['Id']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nId\nMSSubClass\nMSZoning\nLotFrontage\nLotArea\nStreet\nAlley\nLotShape\nLandContour\nUtilities\n...\nPoolArea\nPoolQC\nFence\nMiscFeature\nMiscVal\nMoSold\nYrSold\nSaleType\nSaleCondition\nSalePrice\n\n\n\n\n0\n1\n60\nRL\n65.0\n8450\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n2\n2008\nWD\nNormal\n208500\n\n\n1\n2\n20\nRL\n80.0\n9600\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2007\nWD\nNormal\n181500\n\n\n2\n3\n60\nRL\n68.0\n11250\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n9\n2008\nWD\nNormal\n223500\n\n\n3\n4\n70\nRL\n60.0\n9550\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n2\n2006\nWD\nAbnorml\n140000\n\n\n4\n5\n60\nRL\n84.0\n14260\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n12\n2008\nWD\nNormal\n250000\n\n\n\n\n5 rows × 81 columns\n\n\n\n\n\n\nCode\n#df_train.info()\n\n\n\n\nCode\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['SalePrice'], axis=1), df_test])",
    "crumbs": [
      "Regression",
      "Decision tree"
    ]
  },
  {
    "objectID": "regression/2 Decision Tree on house price data copy.html#data-eda",
    "href": "regression/2 Decision Tree on house price data copy.html#data-eda",
    "title": "Decision tree",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\nin step 1",
    "crumbs": [
      "Regression",
      "Decision tree"
    ]
  },
  {
    "objectID": "regression/2 Decision Tree on house price data copy.html#data-wrangling",
    "href": "regression/2 Decision Tree on house price data copy.html#data-wrangling",
    "title": "Decision tree",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling",
    "crumbs": [
      "Regression",
      "Decision tree"
    ]
  },
  {
    "objectID": "regression/2 Decision Tree on house price data copy.html#split-data",
    "href": "regression/2 Decision Tree on house price data copy.html#split-data",
    "title": "Decision tree",
    "section": "2.4 split data",
    "text": "2.4 split data\n\n\nCode\nY = df_train.SalePrice\nX = df_train.drop(['SalePrice'], axis=1)\n\n\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\nX_train = X_train.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nprint(X_train.shape)\nprint(X_test.shape)\n\n\n(1168, 80)\n(292, 80)\n\n\n\n\nCode\nprint(Y_train.shape)\nprint(Y_test.shape)\n\n\n(1168,)\n(292,)\n\n\n\n\nCode\ncategorical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].nunique() &lt; 10 and X_train[cname].dtype == \"object\"]\n                    \n                    \nnumerical_cols = numerical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].dtype in ['int64', 'float64']]\n\n\n\n\nCode\nprint(\"The total number of categorical columns:\", len(categorical_cols))\nprint(\"The total number of numerical columns:\", len(numerical_cols))\n\n\nThe total number of categorical columns: 40\nThe total number of numerical columns: 37\n\n\n\n\nCode\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train[my_cols].copy()\nX_test= X_test[my_cols].copy()\n\n\nX_final = df_test[my_cols].copy()",
    "crumbs": [
      "Regression",
      "Decision tree"
    ]
  },
  {
    "objectID": "regression/2 Decision Tree on house price data copy.html#pipelines-for-data-preprocessing",
    "href": "regression/2 Decision Tree on house price data copy.html#pipelines-for-data-preprocessing",
    "title": "Decision tree",
    "section": "2.5 Pipelines for Data Preprocessing",
    "text": "2.5 Pipelines for Data Preprocessing\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\nCode\nnumerical_transformer = Pipeline(steps=[\n    ('imputer_num', SimpleImputer(strategy='median')), \n    ('scaler', StandardScaler())\n])\n\n\n\n\nCode\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n\n\nCode\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)])",
    "crumbs": [
      "Regression",
      "Decision tree"
    ]
  },
  {
    "objectID": "regression/2 Decision Tree on house price data copy.html#define-model",
    "href": "regression/2 Decision Tree on house price data copy.html#define-model",
    "title": "Decision tree",
    "section": "3.1 define model",
    "text": "3.1 define model\n\n\nCode\nml_model = DecisionTreeRegressor(random_state=0)\nml_model\n\n\nDecisionTreeRegressor(random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeRegressor?Documentation for DecisionTreeRegressoriNot fittedDecisionTreeRegressor(random_state=0)",
    "crumbs": [
      "Regression",
      "Decision tree"
    ]
  },
  {
    "objectID": "regression/2 Decision Tree on house price data copy.html#define-pipline",
    "href": "regression/2 Decision Tree on house price data copy.html#define-pipline",
    "title": "Decision tree",
    "section": "3.2 define pipline",
    "text": "3.2 define pipline\n\n\nCode\npipeline = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model_dt', ml_model)\n         ]\n)",
    "crumbs": [
      "Regression",
      "Decision tree"
    ]
  },
  {
    "objectID": "regression/2 Decision Tree on house price data copy.html#train-model",
    "href": "regression/2 Decision Tree on house price data copy.html#train-model",
    "title": "Decision tree",
    "section": "3.3 train model",
    "text": "3.3 train model\n\n\nCode\npipeline.fit(X_train, Y_train)\n\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer_num',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['Id', 'MSSubClass',\n                                                   'LotFrontage', 'LotArea',\n                                                   'OverallQual', 'OverallCond',\n                                                   'YearBuilt', 'YearRemodAdd',\n                                                   'MasVnrArea', 'BsmtFinSF1',\n                                                   'BsmtFinSF2', 'BsmtUnfSF',\n                                                   'TotalBsmtSF', '1stFlrSF',\n                                                   '2ndFlrSF'...\n                                                   'LotConfig', 'LandSlope',\n                                                   'Condition1', 'Condition2',\n                                                   'BldgType', 'HouseStyle',\n                                                   'RoofStyle', 'RoofMatl',\n                                                   'MasVnrType', 'ExterQual',\n                                                   'ExterCond', 'Foundation',\n                                                   'BsmtQual', 'BsmtCond',\n                                                   'BsmtExposure',\n                                                   'BsmtFinType1',\n                                                   'BsmtFinType2', 'Heating',\n                                                   'HeatingQC', 'CentralAir',\n                                                   'Electrical', 'KitchenQual',\n                                                   'Functional', 'FireplaceQu', ...])])),\n                ('model_dt', DecisionTreeRegressor(random_state=0))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiFittedPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer_num',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['Id', 'MSSubClass',\n                                                   'LotFrontage', 'LotArea',\n                                                   'OverallQual', 'OverallCond',\n                                                   'YearBuilt', 'YearRemodAdd',\n                                                   'MasVnrArea', 'BsmtFinSF1',\n                                                   'BsmtFinSF2', 'BsmtUnfSF',\n                                                   'TotalBsmtSF', '1stFlrSF',\n                                                   '2ndFlrSF'...\n                                                   'LotConfig', 'LandSlope',\n                                                   'Condition1', 'Condition2',\n                                                   'BldgType', 'HouseStyle',\n                                                   'RoofStyle', 'RoofMatl',\n                                                   'MasVnrType', 'ExterQual',\n                                                   'ExterCond', 'Foundation',\n                                                   'BsmtQual', 'BsmtCond',\n                                                   'BsmtExposure',\n                                                   'BsmtFinType1',\n                                                   'BsmtFinType2', 'Heating',\n                                                   'HeatingQC', 'CentralAir',\n                                                   'Electrical', 'KitchenQual',\n                                                   'Functional', 'FireplaceQu', ...])])),\n                ('model_dt', DecisionTreeRegressor(random_state=0))])  preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('imputer_num',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('scaler', StandardScaler())]),\n                                 ['Id', 'MSSubClass', 'LotFrontage', 'LotArea',\n                                  'OverallQual', 'OverallCond', 'YearBuilt',\n                                  'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1',\n                                  'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF',\n                                  '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n                                  'GrLivArea', 'Bsm...\n                                 ['MSZoning', 'Street', 'Alley', 'LotShape',\n                                  'LandContour', 'Utilities', 'LotConfig',\n                                  'LandSlope', 'Condition1', 'Condition2',\n                                  'BldgType', 'HouseStyle', 'RoofStyle',\n                                  'RoofMatl', 'MasVnrType', 'ExterQual',\n                                  'ExterCond', 'Foundation', 'BsmtQual',\n                                  'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n                                  'BsmtFinType2', 'Heating', 'HeatingQC',\n                                  'CentralAir', 'Electrical', 'KitchenQual',\n                                  'Functional', 'FireplaceQu', ...])]) num['Id', 'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='median')  StandardScaler?Documentation for StandardScalerStandardScaler() cat['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'SaleType', 'SaleCondition']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='most_frequent')  OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore')  DecisionTreeRegressor?Documentation for DecisionTreeRegressorDecisionTreeRegressor(random_state=0) \n\n\n\n\nCode\nfitted_model=pipeline.steps[1][1]\n\n\n\n\nCode\nvar=pipeline[:-1].get_feature_names_out()\nvar\n\n\narray(['num__Id', 'num__MSSubClass', 'num__LotFrontage', 'num__LotArea',\n       'num__OverallQual', 'num__OverallCond', 'num__YearBuilt',\n       'num__YearRemodAdd', 'num__MasVnrArea', 'num__BsmtFinSF1',\n       'num__BsmtFinSF2', 'num__BsmtUnfSF', 'num__TotalBsmtSF',\n       'num__1stFlrSF', 'num__2ndFlrSF', 'num__LowQualFinSF',\n       'num__GrLivArea', 'num__BsmtFullBath', 'num__BsmtHalfBath',\n       'num__FullBath', 'num__HalfBath', 'num__BedroomAbvGr',\n       'num__KitchenAbvGr', 'num__TotRmsAbvGrd', 'num__Fireplaces',\n       'num__GarageYrBlt', 'num__GarageCars', 'num__GarageArea',\n       'num__WoodDeckSF', 'num__OpenPorchSF', 'num__EnclosedPorch',\n       'num__3SsnPorch', 'num__ScreenPorch', 'num__PoolArea',\n       'num__MiscVal', 'num__MoSold', 'num__YrSold',\n       'cat__MSZoning_C (all)', 'cat__MSZoning_FV', 'cat__MSZoning_RH',\n       'cat__MSZoning_RL', 'cat__MSZoning_RM', 'cat__Street_Grvl',\n       'cat__Street_Pave', 'cat__Alley_Grvl', 'cat__Alley_Pave',\n       'cat__LotShape_IR1', 'cat__LotShape_IR2', 'cat__LotShape_IR3',\n       'cat__LotShape_Reg', 'cat__LandContour_Bnk',\n       'cat__LandContour_HLS', 'cat__LandContour_Low',\n       'cat__LandContour_Lvl', 'cat__Utilities_AllPub',\n       'cat__Utilities_NoSeWa', 'cat__LotConfig_Corner',\n       'cat__LotConfig_CulDSac', 'cat__LotConfig_FR2',\n       'cat__LotConfig_FR3', 'cat__LotConfig_Inside',\n       'cat__LandSlope_Gtl', 'cat__LandSlope_Mod', 'cat__LandSlope_Sev',\n       'cat__Condition1_Artery', 'cat__Condition1_Feedr',\n       'cat__Condition1_Norm', 'cat__Condition1_PosA',\n       'cat__Condition1_PosN', 'cat__Condition1_RRAe',\n       'cat__Condition1_RRAn', 'cat__Condition1_RRNe',\n       'cat__Condition1_RRNn', 'cat__Condition2_Feedr',\n       'cat__Condition2_Norm', 'cat__Condition2_PosA',\n       'cat__Condition2_PosN', 'cat__Condition2_RRAe',\n       'cat__Condition2_RRAn', 'cat__Condition2_RRNn',\n       'cat__BldgType_1Fam', 'cat__BldgType_2fmCon',\n       'cat__BldgType_Duplex', 'cat__BldgType_Twnhs',\n       'cat__BldgType_TwnhsE', 'cat__HouseStyle_1.5Fin',\n       'cat__HouseStyle_1.5Unf', 'cat__HouseStyle_1Story',\n       'cat__HouseStyle_2.5Fin', 'cat__HouseStyle_2.5Unf',\n       'cat__HouseStyle_2Story', 'cat__HouseStyle_SFoyer',\n       'cat__HouseStyle_SLvl', 'cat__RoofStyle_Flat',\n       'cat__RoofStyle_Gable', 'cat__RoofStyle_Gambrel',\n       'cat__RoofStyle_Hip', 'cat__RoofStyle_Mansard',\n       'cat__RoofStyle_Shed', 'cat__RoofMatl_ClyTile',\n       'cat__RoofMatl_CompShg', 'cat__RoofMatl_Membran',\n       'cat__RoofMatl_Metal', 'cat__RoofMatl_Roll',\n       'cat__RoofMatl_Tar&Grv', 'cat__RoofMatl_WdShake',\n       'cat__RoofMatl_WdShngl', 'cat__MasVnrType_BrkCmn',\n       'cat__MasVnrType_BrkFace', 'cat__MasVnrType_Stone',\n       'cat__ExterQual_Ex', 'cat__ExterQual_Fa', 'cat__ExterQual_Gd',\n       'cat__ExterQual_TA', 'cat__ExterCond_Ex', 'cat__ExterCond_Fa',\n       'cat__ExterCond_Gd', 'cat__ExterCond_Po', 'cat__ExterCond_TA',\n       'cat__Foundation_BrkTil', 'cat__Foundation_CBlock',\n       'cat__Foundation_PConc', 'cat__Foundation_Slab',\n       'cat__Foundation_Stone', 'cat__Foundation_Wood',\n       'cat__BsmtQual_Ex', 'cat__BsmtQual_Fa', 'cat__BsmtQual_Gd',\n       'cat__BsmtQual_TA', 'cat__BsmtCond_Fa', 'cat__BsmtCond_Gd',\n       'cat__BsmtCond_Po', 'cat__BsmtCond_TA', 'cat__BsmtExposure_Av',\n       'cat__BsmtExposure_Gd', 'cat__BsmtExposure_Mn',\n       'cat__BsmtExposure_No', 'cat__BsmtFinType1_ALQ',\n       'cat__BsmtFinType1_BLQ', 'cat__BsmtFinType1_GLQ',\n       'cat__BsmtFinType1_LwQ', 'cat__BsmtFinType1_Rec',\n       'cat__BsmtFinType1_Unf', 'cat__BsmtFinType2_ALQ',\n       'cat__BsmtFinType2_BLQ', 'cat__BsmtFinType2_GLQ',\n       'cat__BsmtFinType2_LwQ', 'cat__BsmtFinType2_Rec',\n       'cat__BsmtFinType2_Unf', 'cat__Heating_Floor', 'cat__Heating_GasA',\n       'cat__Heating_GasW', 'cat__Heating_Grav', 'cat__Heating_OthW',\n       'cat__Heating_Wall', 'cat__HeatingQC_Ex', 'cat__HeatingQC_Fa',\n       'cat__HeatingQC_Gd', 'cat__HeatingQC_Po', 'cat__HeatingQC_TA',\n       'cat__CentralAir_N', 'cat__CentralAir_Y', 'cat__Electrical_FuseA',\n       'cat__Electrical_FuseF', 'cat__Electrical_FuseP',\n       'cat__Electrical_Mix', 'cat__Electrical_SBrkr',\n       'cat__KitchenQual_Ex', 'cat__KitchenQual_Fa',\n       'cat__KitchenQual_Gd', 'cat__KitchenQual_TA',\n       'cat__Functional_Maj1', 'cat__Functional_Maj2',\n       'cat__Functional_Min1', 'cat__Functional_Min2',\n       'cat__Functional_Mod', 'cat__Functional_Sev',\n       'cat__Functional_Typ', 'cat__FireplaceQu_Ex',\n       'cat__FireplaceQu_Fa', 'cat__FireplaceQu_Gd',\n       'cat__FireplaceQu_Po', 'cat__FireplaceQu_TA',\n       'cat__GarageType_2Types', 'cat__GarageType_Attchd',\n       'cat__GarageType_Basment', 'cat__GarageType_BuiltIn',\n       'cat__GarageType_CarPort', 'cat__GarageType_Detchd',\n       'cat__GarageFinish_Fin', 'cat__GarageFinish_RFn',\n       'cat__GarageFinish_Unf', 'cat__GarageQual_Ex',\n       'cat__GarageQual_Fa', 'cat__GarageQual_Gd', 'cat__GarageQual_Po',\n       'cat__GarageQual_TA', 'cat__GarageCond_Ex', 'cat__GarageCond_Fa',\n       'cat__GarageCond_Gd', 'cat__GarageCond_Po', 'cat__GarageCond_TA',\n       'cat__PavedDrive_N', 'cat__PavedDrive_P', 'cat__PavedDrive_Y',\n       'cat__PoolQC_Ex', 'cat__PoolQC_Fa', 'cat__PoolQC_Gd',\n       'cat__Fence_GdPrv', 'cat__Fence_GdWo', 'cat__Fence_MnPrv',\n       'cat__Fence_MnWw', 'cat__MiscFeature_Gar2',\n       'cat__MiscFeature_Othr', 'cat__MiscFeature_Shed',\n       'cat__SaleType_COD', 'cat__SaleType_CWD', 'cat__SaleType_Con',\n       'cat__SaleType_ConLD', 'cat__SaleType_ConLI',\n       'cat__SaleType_ConLw', 'cat__SaleType_New', 'cat__SaleType_Oth',\n       'cat__SaleType_WD', 'cat__SaleCondition_Abnorml',\n       'cat__SaleCondition_AdjLand', 'cat__SaleCondition_Alloca',\n       'cat__SaleCondition_Family', 'cat__SaleCondition_Normal',\n       'cat__SaleCondition_Partial'], dtype=object)\n\n\nvariable importance\n\n\nCode\nimportances = fitted_model.feature_importances_\nvi=pd.DataFrame({\"variable\":var,\"importances\":importances})\nvi=vi.sort_values('importances',ascending=False)\nvi\n\n\n\n\n\n\n\n\n\n\nvariable\nimportances\n\n\n\n\n4\nnum__OverallQual\n0.618841\n\n\n16\nnum__GrLivArea\n0.086857\n\n\n14\nnum__2ndFlrSF\n0.072518\n\n\n12\nnum__TotalBsmtSF\n0.035454\n\n\n26\nnum__GarageCars\n0.023574\n\n\n...\n...\n...\n\n\n110\ncat__ExterQual_Ex\n0.000000\n\n\n168\ncat__KitchenQual_Fa\n0.000000\n\n\n70\ncat__Condition1_RRAn\n0.000000\n\n\n71\ncat__Condition1_RRNe\n0.000000\n\n\n68\ncat__Condition1_PosN\n0.000000\n\n\n\n\n230 rows × 2 columns",
    "crumbs": [
      "Regression",
      "Decision tree"
    ]
  },
  {
    "objectID": "regression/2 Decision Tree on house price data copy.html#preformance",
    "href": "regression/2 Decision Tree on house price data copy.html#preformance",
    "title": "Decision tree",
    "section": "3.4 Preformance",
    "text": "3.4 Preformance\n\n\nCode\nY_pred_dt =pipeline.predict(X_test) #always gets x and retuns y\n\n\nR 2\n\n\nCode\nfrom sklearn.metrics import r2_score\nr2_score(Y_test, Y_pred_dt)\n\n\n0.786080833023575\n\n\nMAE\n\n\nCode\nfrom sklearn.metrics import mean_absolute_error\nmean_absolute_error(Y_test, Y_pred_dt)\n\n\n24463.89383561644\n\n\nRMSE\n\n\nCode\nfrom  math import sqrt\nfrom sklearn.metrics import mean_squared_error\nmse=mean_squared_error(Y_test, Y_pred_dt)\nrmse=sqrt(mse)\nrmse\n\n\n33873.177831444824",
    "crumbs": [
      "Regression",
      "Decision tree"
    ]
  },
  {
    "objectID": "regression/2 Decision Tree on house price data copy.html#k-fold-cross-validation",
    "href": "regression/2 Decision Tree on house price data copy.html#k-fold-cross-validation",
    "title": "Decision tree",
    "section": "3.5 k-Fold Cross-Validation",
    "text": "3.5 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \n\n\n\n\nCode\ncv_dt = cross_val_score(pipeline, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.6440603651748669\n\n\n\n\nCode\ncv_dt = cross_val_score(pipeline, X_train, Y_train, cv=kf_dt,scoring = 'neg_mean_squared_error')\nnp.mean(np.sqrt(np.abs(cv_dt)))\n\n\n46489.563013617146",
    "crumbs": [
      "Regression",
      "Decision tree"
    ]
  },
  {
    "objectID": "data/1 Pandas.html",
    "href": "data/1 Pandas.html",
    "title": "Data manipulation with Pandas",
    "section": "",
    "text": "Code\nimport sys\nprint(sys.version)\n\n\n3.11.4 (v3.11.4:d2340ef257, Jun  6 2023, 19:15:51) [Clang 13.0.0 (clang-1300.0.29.30)]\nCode\nimport pandas as pd\nprint('pandas version', pd.__version__)\n\n\npandas version 2.2.1",
    "crumbs": [
      "Data",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data/1 Pandas.html#reading-from-parquet",
    "href": "data/1 Pandas.html#reading-from-parquet",
    "title": "Data manipulation with Pandas",
    "section": "0.1 Reading from parquet",
    "text": "0.1 Reading from parquet\n\n\nCode\ndf = pd.read_parquet(\"data/Combined_Flights_2022.parquet\")",
    "crumbs": [
      "Data",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data/1 Pandas.html#get-first-3",
    "href": "data/1 Pandas.html#get-first-3",
    "title": "Data manipulation with Pandas",
    "section": "0.2 get first 3",
    "text": "0.2 get first 3\n\n\nCode\ndf.head(3)\n\n\n\n\n\n\n\n\n\n\nFlightDate\nAirline\nOrigin\nDest\nCancelled\nDiverted\nCRSDepTime\nDepTime\nDepDelayMinutes\nDepDelay\n...\nWheelsOff\nWheelsOn\nTaxiIn\nCRSArrTime\nArrDelay\nArrDel15\nArrivalDelayGroups\nArrTimeBlk\nDistanceGroup\nDivAirportLandings\n\n\n\n\n0\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nGJT\nDEN\nFalse\nFalse\n1133\n1123.0\n0.0\n-10.0\n...\n1140.0\n1220.0\n8.0\n1245\n-17.0\n0.0\n-2.0\n1200-1259\n1\n0\n\n\n1\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nHRL\nIAH\nFalse\nFalse\n732\n728.0\n0.0\n-4.0\n...\n744.0\n839.0\n9.0\n849\n-1.0\n0.0\n-1.0\n0800-0859\n2\n0\n\n\n2\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nDRO\nDEN\nFalse\nFalse\n1529\n1514.0\n0.0\n-15.0\n...\n1535.0\n1622.0\n14.0\n1639\n-3.0\n0.0\n-1.0\n1600-1659\n2\n0\n\n\n\n\n3 rows × 61 columns",
    "crumbs": [
      "Data",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data/1 Pandas.html#get-last-3",
    "href": "data/1 Pandas.html#get-last-3",
    "title": "Data manipulation with Pandas",
    "section": "0.3 get last 3",
    "text": "0.3 get last 3\n\n\nCode\ndf.tail(3)\n\n\n\n\n\n\n\n\n\n\nFlightDate\nAirline\nOrigin\nDest\nCancelled\nDiverted\nCRSDepTime\nDepTime\nDepDelayMinutes\nDepDelay\n...\nWheelsOff\nWheelsOn\nTaxiIn\nCRSArrTime\nArrDelay\nArrDel15\nArrivalDelayGroups\nArrTimeBlk\nDistanceGroup\nDivAirportLandings\n\n\n\n\n590539\n2022-03-08\nRepublic Airlines\nALB\nORD\nFalse\nFalse\n1700\n2318.0\n378.0\n378.0\n...\n2337.0\n52.0\n7.0\n1838\n381.0\n1.0\n12.0\n1800-1859\n3\n0\n\n\n590540\n2022-03-25\nRepublic Airlines\nEWR\nPIT\nFalse\nTrue\n2129\n2322.0\n113.0\n113.0\n...\n2347.0\n933.0\n6.0\n2255\nNaN\nNaN\nNaN\n2200-2259\n2\n1\n\n\n590541\n2022-03-07\nRepublic Airlines\nEWR\nRDU\nFalse\nTrue\n1154\n1148.0\n0.0\n-6.0\n...\n1201.0\n1552.0\n4.0\n1333\nNaN\nNaN\nNaN\n1300-1359\n2\n1\n\n\n\n\n3 rows × 61 columns",
    "crumbs": [
      "Data",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data/1 Pandas.html#get-ramdon-5",
    "href": "data/1 Pandas.html#get-ramdon-5",
    "title": "Data manipulation with Pandas",
    "section": "0.4 get ramdon 5",
    "text": "0.4 get ramdon 5\n\n\nCode\ndf.sample(5, random_state=42)\n\n\n\n\n\n\n\n\n\n\nFlightDate\nAirline\nOrigin\nDest\nCancelled\nDiverted\nCRSDepTime\nDepTime\nDepDelayMinutes\nDepDelay\n...\nWheelsOff\nWheelsOn\nTaxiIn\nCRSArrTime\nArrDelay\nArrDel15\nArrivalDelayGroups\nArrTimeBlk\nDistanceGroup\nDivAirportLandings\n\n\n\n\n324021\n2022-03-19\nSkyWest Airlines Inc.\nASE\nDEN\nFalse\nFalse\n1831\n1826.0\n0.0\n-5.0\n...\n1845.0\n1916.0\n7.0\n1933\n-10.0\n0.0\n-1.0\n1900-1959\n1\n0\n\n\n34739\n2022-02-16\nSkyWest Airlines Inc.\nTYS\nDEN\nFalse\nFalse\n1605\n1605.0\n0.0\n0.0\n...\n1614.0\n1728.0\n44.0\n1737\n35.0\n1.0\n2.0\n1700-1759\n5\n0\n\n\n304494\n2022-01-18\nAmerican Airlines Inc.\nLAX\nOGG\nFalse\nFalse\n1719\n1714.0\n0.0\n-5.0\n...\n1728.0\n2042.0\n10.0\n2049\n3.0\n0.0\n0.0\n2000-2059\n10\n0\n\n\n205451\n2022-01-23\nSouthwest Airlines Co.\nLAS\nBWI\nFalse\nFalse\n1515\n1533.0\n18.0\n18.0\n...\n1548.0\n2259.0\n3.0\n2235\n27.0\n1.0\n1.0\n2200-2259\n9\n0\n\n\n173176\n2022-07-01\nDelta Air Lines Inc.\nDTW\nRDU\nFalse\nFalse\n715\n709.0\n0.0\n-6.0\n...\n721.0\n834.0\n3.0\n853\n-16.0\n0.0\n-2.0\n0800-0859\n3\n0\n\n\n\n\n5 rows × 61 columns",
    "crumbs": [
      "Data",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data/1 Pandas.html#get-columns-names",
    "href": "data/1 Pandas.html#get-columns-names",
    "title": "Data manipulation with Pandas",
    "section": "0.5 get columns names",
    "text": "0.5 get columns names\n\n\nCode\ndf.columns\n\n\nIndex(['FlightDate', 'Airline', 'Origin', 'Dest', 'Cancelled', 'Diverted',\n       'CRSDepTime', 'DepTime', 'DepDelayMinutes', 'DepDelay', 'ArrTime',\n       'ArrDelayMinutes', 'AirTime', 'CRSElapsedTime', 'ActualElapsedTime',\n       'Distance', 'Year', 'Quarter', 'Month', 'DayofMonth', 'DayOfWeek',\n       'Marketing_Airline_Network', 'Operated_or_Branded_Code_Share_Partners',\n       'DOT_ID_Marketing_Airline', 'IATA_Code_Marketing_Airline',\n       'Flight_Number_Marketing_Airline', 'Operating_Airline',\n       'DOT_ID_Operating_Airline', 'IATA_Code_Operating_Airline',\n       'Tail_Number', 'Flight_Number_Operating_Airline', 'OriginAirportID',\n       'OriginAirportSeqID', 'OriginCityMarketID', 'OriginCityName',\n       'OriginState', 'OriginStateFips', 'OriginStateName', 'OriginWac',\n       'DestAirportID', 'DestAirportSeqID', 'DestCityMarketID', 'DestCityName',\n       'DestState', 'DestStateFips', 'DestStateName', 'DestWac', 'DepDel15',\n       'DepartureDelayGroups', 'DepTimeBlk', 'TaxiOut', 'WheelsOff',\n       'WheelsOn', 'TaxiIn', 'CRSArrTime', 'ArrDelay', 'ArrDel15',\n       'ArrivalDelayGroups', 'ArrTimeBlk', 'DistanceGroup',\n       'DivAirportLandings'],\n      dtype='object')",
    "crumbs": [
      "Data",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data/1 Pandas.html#get-row-index",
    "href": "data/1 Pandas.html#get-row-index",
    "title": "Data manipulation with Pandas",
    "section": "0.6 get row index",
    "text": "0.6 get row index\n\n\nCode\ndf.index\n\n\nIndex([     0,      1,      2,      3,      4,      5,      6,      7,      8,\n            9,\n       ...\n       590532, 590533, 590534, 590535, 590536, 590537, 590538, 590539, 590540,\n       590541],\n      dtype='int64', length=4078318)",
    "crumbs": [
      "Data",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data/1 Pandas.html#get-long-info",
    "href": "data/1 Pandas.html#get-long-info",
    "title": "Data manipulation with Pandas",
    "section": "0.7 get long info",
    "text": "0.7 get long info\n\n\nCode\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 4078318 entries, 0 to 590541\nData columns (total 61 columns):\n #   Column                                   Dtype         \n---  ------                                   -----         \n 0   FlightDate                               datetime64[us]\n 1   Airline                                  object        \n 2   Origin                                   object        \n 3   Dest                                     object        \n 4   Cancelled                                bool          \n 5   Diverted                                 bool          \n 6   CRSDepTime                               int64         \n 7   DepTime                                  float64       \n 8   DepDelayMinutes                          float64       \n 9   DepDelay                                 float64       \n 10  ArrTime                                  float64       \n 11  ArrDelayMinutes                          float64       \n 12  AirTime                                  float64       \n 13  CRSElapsedTime                           float64       \n 14  ActualElapsedTime                        float64       \n 15  Distance                                 float64       \n 16  Year                                     int64         \n 17  Quarter                                  int64         \n 18  Month                                    int64         \n 19  DayofMonth                               int64         \n 20  DayOfWeek                                int64         \n 21  Marketing_Airline_Network                object        \n 22  Operated_or_Branded_Code_Share_Partners  object        \n 23  DOT_ID_Marketing_Airline                 int64         \n 24  IATA_Code_Marketing_Airline              object        \n 25  Flight_Number_Marketing_Airline          int64         \n 26  Operating_Airline                        object        \n 27  DOT_ID_Operating_Airline                 int64         \n 28  IATA_Code_Operating_Airline              object        \n 29  Tail_Number                              object        \n 30  Flight_Number_Operating_Airline          int64         \n 31  OriginAirportID                          int64         \n 32  OriginAirportSeqID                       int64         \n 33  OriginCityMarketID                       int64         \n 34  OriginCityName                           object        \n 35  OriginState                              object        \n 36  OriginStateFips                          int64         \n 37  OriginStateName                          object        \n 38  OriginWac                                int64         \n 39  DestAirportID                            int64         \n 40  DestAirportSeqID                         int64         \n 41  DestCityMarketID                         int64         \n 42  DestCityName                             object        \n 43  DestState                                object        \n 44  DestStateFips                            int64         \n 45  DestStateName                            object        \n 46  DestWac                                  int64         \n 47  DepDel15                                 float64       \n 48  DepartureDelayGroups                     float64       \n 49  DepTimeBlk                               object        \n 50  TaxiOut                                  float64       \n 51  WheelsOff                                float64       \n 52  WheelsOn                                 float64       \n 53  TaxiIn                                   float64       \n 54  CRSArrTime                               int64         \n 55  ArrDelay                                 float64       \n 56  ArrDel15                                 float64       \n 57  ArrivalDelayGroups                       float64       \n 58  ArrTimeBlk                               object        \n 59  DistanceGroup                            int64         \n 60  DivAirportLandings                       int64         \ndtypes: bool(2), datetime64[us](1), float64(18), int64(23), object(17)\nmemory usage: 1.8+ GB",
    "crumbs": [
      "Data",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data/1 Pandas.html#get-short-info",
    "href": "data/1 Pandas.html#get-short-info",
    "title": "Data manipulation with Pandas",
    "section": "0.8 get short info",
    "text": "0.8 get short info\n\n\nCode\ndf.info(verbose=False)\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 4078318 entries, 0 to 590541\nColumns: 61 entries, FlightDate to DivAirportLandings\ndtypes: bool(2), datetime64[us](1), float64(18), int64(23), object(17)\nmemory usage: 1.8+ GB",
    "crumbs": [
      "Data",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data/1 Pandas.html#summary-of-numeric-column",
    "href": "data/1 Pandas.html#summary-of-numeric-column",
    "title": "Data manipulation with Pandas",
    "section": "0.9 summary of numeric column",
    "text": "0.9 summary of numeric column\n\n\nCode\ndf.describe()\n\n\n\n\n\n\n\n\n\n\nFlightDate\nCRSDepTime\nDepTime\nDepDelayMinutes\nDepDelay\nArrTime\nArrDelayMinutes\nAirTime\nCRSElapsedTime\nActualElapsedTime\n...\nTaxiOut\nWheelsOff\nWheelsOn\nTaxiIn\nCRSArrTime\nArrDelay\nArrDel15\nArrivalDelayGroups\nDistanceGroup\nDivAirportLandings\n\n\n\n\ncount\n4078318\n4.078318e+06\n3.957885e+06\n3.957823e+06\n3.957823e+06\n3.954079e+06\n3.944916e+06\n3.944916e+06\n4.078318e+06\n3.944916e+06\n...\n3.955652e+06\n3.955652e+06\n3.954076e+06\n3.954076e+06\n4.078318e+06\n3.944916e+06\n3.944916e+06\n3.944916e+06\n4.078318e+06\n4.078318e+06\n\n\nmean\n2022-04-18 12:10:43.903101\n1.329587e+03\n1.334374e+03\n1.601494e+01\n1.309049e+01\n1.457886e+03\n1.578307e+01\n1.110075e+02\n1.413211e+02\n1.358624e+02\n...\n1.697375e+01\n1.356576e+03\n1.455073e+03\n7.894387e+00\n1.486058e+03\n7.528486e+00\n2.164715e-01\n-6.256103e-02\n3.663516e+00\n3.685098e-03\n\n\nmin\n2022-01-01 00:00:00\n1.000000e+00\n1.000000e+00\n0.000000e+00\n-7.800000e+01\n1.000000e+00\n0.000000e+00\n8.000000e+00\n-4.800000e+01\n1.400000e+01\n...\n1.000000e+00\n1.000000e+00\n1.000000e+00\n1.000000e+00\n1.000000e+00\n-1.000000e+02\n0.000000e+00\n-2.000000e+00\n1.000000e+00\n0.000000e+00\n\n\n25%\n2022-02-25 00:00:00\n9.140000e+02\n9.170000e+02\n0.000000e+00\n-5.000000e+00\n1.046000e+03\n0.000000e+00\n6.000000e+01\n8.900000e+01\n8.300000e+01\n...\n1.100000e+01\n9.320000e+02\n1.044000e+03\n4.000000e+00\n1.103000e+03\n-1.400000e+01\n0.000000e+00\n-1.000000e+00\n2.000000e+00\n0.000000e+00\n\n\n50%\n2022-04-19 00:00:00\n1.320000e+03\n1.325000e+03\n0.000000e+00\n-2.000000e+00\n1.500000e+03\n0.000000e+00\n9.400000e+01\n1.240000e+02\n1.190000e+02\n...\n1.500000e+01\n1.338000e+03\n1.456000e+03\n6.000000e+00\n1.513000e+03\n-5.000000e+00\n0.000000e+00\n-1.000000e+00\n3.000000e+00\n0.000000e+00\n\n\n75%\n2022-06-11 00:00:00\n1.735000e+03\n1.744000e+03\n1.100000e+01\n1.100000e+01\n1.914000e+03\n1.000000e+01\n1.410000e+02\n1.710000e+02\n1.670000e+02\n...\n1.900000e+01\n1.758000e+03\n1.909000e+03\n9.000000e+00\n1.920000e+03\n1.000000e+01\n0.000000e+00\n0.000000e+00\n5.000000e+00\n0.000000e+00\n\n\nmax\n2022-07-31 00:00:00\n2.359000e+03\n2.400000e+03\n7.223000e+03\n7.223000e+03\n2.400000e+03\n7.232000e+03\n7.270000e+02\n6.900000e+02\n7.640000e+02\n...\n2.210000e+02\n2.400000e+03\n2.400000e+03\n2.900000e+02\n2.359000e+03\n7.232000e+03\n1.000000e+00\n1.200000e+01\n1.100000e+01\n9.000000e+00\n\n\nstd\nNaN\n4.904801e+02\n5.056219e+02\n5.231498e+01\n5.332016e+01\n5.431841e+02\n5.198424e+01\n6.996246e+01\n7.179635e+01\n7.185501e+01\n...\n9.495407e+00\n5.075580e+02\n5.378428e+02\n6.663118e+00\n5.185078e+02\n5.524625e+01\n4.118393e-01\n2.487442e+00\n2.320848e+00\n1.141331e-01\n\n\n\n\n8 rows × 42 columns",
    "crumbs": [
      "Data",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data/1 Pandas.html#summary-of-categorical-column",
    "href": "data/1 Pandas.html#summary-of-categorical-column",
    "title": "Data manipulation with Pandas",
    "section": "0.10 summary of categorical column",
    "text": "0.10 summary of categorical column\n\n\nCode\ndf[[\"Airline\"]].describe()\n\n\n\n\n\n\n\n\n\n\nAirline\n\n\n\n\ncount\n4078318\n\n\nunique\n21\n\n\ntop\nSouthwest Airlines Co.\n\n\nfreq\n731925",
    "crumbs": [
      "Data",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "model type/1 decision tree.html",
    "href": "model type/1 decision tree.html",
    "title": "Decision tree",
    "section": "",
    "text": "1 Pros\n\nEasier to interpret than Neural Network\nFast training and making inference\n\n\n\n2 Cons\n\nProne to overfitting\n\n\n\n3 reference:\nhttps://www.youtube.com/watch?v=6DlWndLbk90\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "model type",
      "Decision tree"
    ]
  },
  {
    "objectID": "data/2 siuba.html#comparison",
    "href": "data/2 siuba.html#comparison",
    "title": "Data manipulation with siuba",
    "section": "1 Comparison",
    "text": "1 Comparison",
    "crumbs": [
      "Data",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data/2 siuba.html#package-download",
    "href": "data/2 siuba.html#package-download",
    "title": "Data manipulation with siuba",
    "section": "2 Package download",
    "text": "2 Package download\n\n\nCode\nimport os\nos.system('pip install siuba')",
    "crumbs": [
      "Data",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data/2 siuba.html#load-package",
    "href": "data/2 siuba.html#load-package",
    "title": "Data manipulation with siuba",
    "section": "3 load package",
    "text": "3 load package\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nimport seaborn as sns\n\nfrom siuba import _, mutate, filter, group_by, summarize,show_query\nfrom siuba import *\nfrom siuba.data import mtcars,penguins\n\n\n\n\nCode\nimport os\nos.system('pip show siuba')\n\n\nName: siuba\nVersion: 0.4.4\nSummary: A package for quick, scrappy analyses with pandas and SQL\nHome-page: https://github.com/machow/siuba\nAuthor: Michael Chow\nAuthor-email: mc_al_gh_siuba@fastmail.com\nLicense: MIT\nLocation: /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages\nRequires: numpy, pandas, PyYAML, SQLAlchemy\nRequired-by: \n\n\n0\n\n\n\n\nCode\nsmall_mtcars = mtcars &gt;&gt; select(_.cyl, _.mpg, _.hp)&gt;&gt; head(5)\nsmall_penguins=penguins&gt;&gt; head(5)",
    "crumbs": [
      "Data",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data/2 siuba.html#select",
    "href": "data/2 siuba.html#select",
    "title": "Data manipulation with siuba",
    "section": "4 select",
    "text": "4 select\n\n\nCode\nsmall_mtcars &gt;&gt; select(_.cyl, _.mpg)\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\n\n\n\n\n0\n6\n21.0\n\n\n1\n6\n21.0\n\n\n2\n4\n22.8\n\n\n3\n6\n21.4\n\n\n4\n8\n18.7\n\n\n\n\n\n\n\n\nexclude\n\n\nCode\nsmall_mtcars &gt;&gt; select(~_.cyl)\n\n\n\n\n\n\n\n\n\n\nmpg\nhp\n\n\n\n\n0\n21.0\n110\n\n\n1\n21.0\n110\n\n\n2\n22.8\n93\n\n\n3\n21.4\n110\n\n\n4\n18.7\n175",
    "crumbs": [
      "Data",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data/2 siuba.html#renaming",
    "href": "data/2 siuba.html#renaming",
    "title": "Data manipulation with siuba",
    "section": "5 Renaming",
    "text": "5 Renaming\n\n\nCode\nsmall_mtcars &gt;&gt; rename(new_name_mpg = _.mpg)\n\n\n\n\n\n\n\n\n\n\ncyl\nnew_name_mpg\nhp\n\n\n\n\n0\n6\n21.0\n110\n\n\n1\n6\n21.0\n110\n\n\n2\n4\n22.8\n93\n\n\n3\n6\n21.4\n110\n\n\n4\n8\n18.7\n175",
    "crumbs": [
      "Data",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data/2 siuba.html#mutate",
    "href": "data/2 siuba.html#mutate",
    "title": "Data manipulation with siuba",
    "section": "6 Mutate",
    "text": "6 Mutate\n\n\nCode\nmtcars.head()&gt;&gt; mutate(gear2 = _.gear+1)\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\ngear2\n\n\n\n\n0\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n5\n\n\n1\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n5\n\n\n2\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n5\n\n\n3\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n4\n\n\n4\n18.7\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n4",
    "crumbs": [
      "Data",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data/2 siuba.html#filter",
    "href": "data/2 siuba.html#filter",
    "title": "Data manipulation with siuba",
    "section": "7 Filter",
    "text": "7 Filter\n\n\nCode\nmtcars&gt;&gt; filter(_.gear ==4)\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n7\n24.4\n4\n146.7\n62\n3.69\n3.190\n20.00\n1\n0\n4\n2\n\n\n8\n22.8\n4\n140.8\n95\n3.92\n3.150\n22.90\n1\n0\n4\n2\n\n\n9\n19.2\n6\n167.6\n123\n3.92\n3.440\n18.30\n1\n0\n4\n4\n\n\n10\n17.8\n6\n167.6\n123\n3.92\n3.440\n18.90\n1\n0\n4\n4\n\n\n17\n32.4\n4\n78.7\n66\n4.08\n2.200\n19.47\n1\n1\n4\n1\n\n\n18\n30.4\n4\n75.7\n52\n4.93\n1.615\n18.52\n1\n1\n4\n2\n\n\n19\n33.9\n4\n71.1\n65\n4.22\n1.835\n19.90\n1\n1\n4\n1\n\n\n25\n27.3\n4\n79.0\n66\n4.08\n1.935\n18.90\n1\n1\n4\n1\n\n\n31\n21.4\n4\n121.0\n109\n4.11\n2.780\n18.60\n1\n1\n4\n2\n\n\n\n\n\n\n\n\nFilters with OR conditions\n\n\nCode\nmtcars &gt;&gt; filter((_.cyl == 4) | (_.gear == 5))\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n2\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n7\n24.4\n4\n146.7\n62\n3.69\n3.190\n20.00\n1\n0\n4\n2\n\n\n8\n22.8\n4\n140.8\n95\n3.92\n3.150\n22.90\n1\n0\n4\n2\n\n\n17\n32.4\n4\n78.7\n66\n4.08\n2.200\n19.47\n1\n1\n4\n1\n\n\n18\n30.4\n4\n75.7\n52\n4.93\n1.615\n18.52\n1\n1\n4\n2\n\n\n19\n33.9\n4\n71.1\n65\n4.22\n1.835\n19.90\n1\n1\n4\n1\n\n\n20\n21.5\n4\n120.1\n97\n3.70\n2.465\n20.01\n1\n0\n3\n1\n\n\n25\n27.3\n4\n79.0\n66\n4.08\n1.935\n18.90\n1\n1\n4\n1\n\n\n26\n26.0\n4\n120.3\n91\n4.43\n2.140\n16.70\n0\n1\n5\n2\n\n\n27\n30.4\n4\n95.1\n113\n3.77\n1.513\n16.90\n1\n1\n5\n2\n\n\n28\n15.8\n8\n351.0\n264\n4.22\n3.170\n14.50\n0\n1\n5\n4\n\n\n29\n19.7\n6\n145.0\n175\n3.62\n2.770\n15.50\n0\n1\n5\n6\n\n\n30\n15.0\n8\n301.0\n335\n3.54\n3.570\n14.60\n0\n1\n5\n8\n\n\n31\n21.4\n4\n121.0\n109\n4.11\n2.780\n18.60\n1\n1\n4\n2\n\n\n\n\n\n\n\n\nDropping NA values",
    "crumbs": [
      "Data",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data/2 siuba.html#group-by",
    "href": "data/2 siuba.html#group-by",
    "title": "Data manipulation with siuba",
    "section": "8 group by",
    "text": "8 group by\n\n8.1 average,min,max,sum\n\n\nCode\ntbl_query = (mtcars\n  &gt;&gt; group_by(_.cyl)\n  &gt;&gt; summarize(avg_hp = _.hp.mean()\n              ,min_hp=_.hp.min()\n              ,max_hp=_.hp.max()\n              ,totol_disp=_.disp.sum()\n  )\n  )\n\ntbl_query\n\n\n\n\n\n\n\n\n\n\ncyl\navg_hp\nmin_hp\nmax_hp\ntotol_disp\n\n\n\n\n0\n4\n82.636364\n52\n113\n1156.5\n\n\n1\n6\n122.285714\n105\n175\n1283.2\n\n\n2\n8\n209.214286\n150\n335\n4943.4\n\n\n\n\n\n\n\n\n\n\n8.2 count\n\n\nCode\nmtcars &gt;&gt; group_by(_.cyl)  &gt;&gt; summarize(n = _.shape[0])\n\n\n\n\n\n\n\n\n\n\ncyl\nn\n\n\n\n\n0\n4\n11\n\n\n1\n6\n7\n\n\n2\n8\n14",
    "crumbs": [
      "Data",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data/2 siuba.html#order",
    "href": "data/2 siuba.html#order",
    "title": "Data manipulation with siuba",
    "section": "9 order",
    "text": "9 order\n\n\nCode\nsmall_mtcars &gt;&gt; arrange(_.hp)\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\nhp\n\n\n\n\n2\n4\n22.8\n93\n\n\n0\n6\n21.0\n110\n\n\n1\n6\n21.0\n110\n\n\n3\n6\n21.4\n110\n\n\n4\n8\n18.7\n175\n\n\n\n\n\n\n\n\nSort in descending order\n\n\nCode\nsmall_mtcars &gt;&gt; arrange(-_.hp)\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\nhp\n\n\n\n\n4\n8\n18.7\n175\n\n\n0\n6\n21.0\n110\n\n\n1\n6\n21.0\n110\n\n\n3\n6\n21.4\n110\n\n\n2\n4\n22.8\n93\n\n\n\n\n\n\n\n\nArrange by multiple variables\n\n\nCode\nsmall_mtcars &gt;&gt; arrange(_.cyl, -_.mpg)\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\nhp\n\n\n\n\n2\n4\n22.8\n93\n\n\n3\n6\n21.4\n110\n\n\n0\n6\n21.0\n110\n\n\n1\n6\n21.0\n110\n\n\n4\n8\n18.7\n175",
    "crumbs": [
      "Data",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data/2 siuba.html#using-siuba-with-database",
    "href": "data/2 siuba.html#using-siuba-with-database",
    "title": "Data manipulation with siuba",
    "section": "10 using siuba with database",
    "text": "10 using siuba with database\n\n10.1 set up a sqlite database, with an mtcars table.\n\n\nCode\nfrom sqlalchemy import create_engine\nfrom siuba.sql import LazyTbl\nfrom siuba import _, group_by, summarize, show_query, collect \nfrom siuba.data import mtcars\n\n# copy in to sqlite, using the pandas .to_sql() method\nengine = create_engine(\"sqlite:///:memory:\")\nmtcars.to_sql(\"mtcars\", engine, if_exists = \"replace\")\n\n\n32\n\n\n\n\n10.2 create table\n\n\nCode\n# Create a lazy SQL DataFrame\ntbl_mtcars = LazyTbl(engine, \"mtcars\")\ntbl_mtcars\n\n\n\n# Source: lazy query\n# DB Conn: Engine(sqlite:///:memory:)\n# Preview:\n\n\n\n\n\n\n\nindex\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\n0\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\n1\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\n2\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n3\n3\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n4\n4\n18.7\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\n\n\n# .. may have more rows\n\n\n\n\n\n10.3 create query\n\n\nCode\n# connect with siuba\n\ntbl_query = (tbl_mtcars\n  &gt;&gt; group_by(_.mpg)\n  &gt;&gt; summarize(avg_hp = _.hp.mean())\n  )\n\ntbl_query\n\n\n\n# Source: lazy query\n# DB Conn: Engine(sqlite:///:memory:)\n# Preview:\n\n\n\n\n\n\n\nmpg\navg_hp\n\n\n\n\n0\n10.4\n210.0\n\n\n1\n13.3\n245.0\n\n\n2\n14.3\n245.0\n\n\n3\n14.7\n230.0\n\n\n4\n15.0\n335.0\n\n\n\n\n# .. may have more rows\n\n\n\n\n\n10.4 show query\n\n\nCode\n tbl_query &gt;&gt; show_query()\n\n\nSELECT mtcars.mpg, avg(mtcars.hp) AS avg_hp \nFROM mtcars GROUP BY mtcars.mpg\n\n\n\n# Source: lazy query\n# DB Conn: Engine(sqlite:///:memory:)\n# Preview:\n\n\n\n\n\n\n\nmpg\navg_hp\n\n\n\n\n0\n10.4\n210.0\n\n\n1\n13.3\n245.0\n\n\n2\n14.3\n245.0\n\n\n3\n14.7\n230.0\n\n\n4\n15.0\n335.0\n\n\n\n\n# .. may have more rows\n\n\n\n\n\n10.5 Collect to DataFrame\nbecause lazy expressions,the collect function is actually running the sql.\n\n\nCode\ndata=tbl_query &gt;&gt; collect()\nprint(data)\n\n\n     mpg  avg_hp\n0   10.4   210.0\n1   13.3   245.0\n2   14.3   245.0\n3   14.7   230.0\n4   15.0   335.0\n5   15.2   165.0\n6   15.5   150.0\n7   15.8   264.0\n8   16.4   180.0\n9   17.3   180.0\n10  17.8   123.0\n11  18.1   105.0\n12  18.7   175.0\n13  19.2   149.0\n14  19.7   175.0\n15  21.0   110.0\n16  21.4   109.5\n17  21.5    97.0\n18  22.8    94.0\n19  24.4    62.0\n20  26.0    91.0\n21  27.3    66.0\n22  30.4    82.5\n23  32.4    66.0\n24  33.9    65.0",
    "crumbs": [
      "Data",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data/2 siuba.html#reference",
    "href": "data/2 siuba.html#reference",
    "title": "Data manipulation with siuba",
    "section": "11 reference:",
    "text": "11 reference:\nhttps://siuba.org/",
    "crumbs": [
      "Data",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data/2 siuba.html#select-column",
    "href": "data/2 siuba.html#select-column",
    "title": "Data manipulation with siuba",
    "section": "4 select column",
    "text": "4 select column\n\n\nCode\nsmall_mtcars &gt;&gt; select(_.cyl, _.mpg)\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\n\n\n\n\n0\n6\n21.0\n\n\n1\n6\n21.0\n\n\n2\n4\n22.8\n\n\n3\n6\n21.4\n\n\n4\n8\n18.7\n\n\n\n\n\n\n\n\nexclude\n\n\nCode\nsmall_mtcars &gt;&gt; select(~_.cyl)\n\n\n\n\n\n\n\n\n\n\nmpg\nhp\n\n\n\n\n0\n21.0\n110\n\n\n1\n21.0\n110\n\n\n2\n22.8\n93\n\n\n3\n21.4\n110\n\n\n4\n18.7\n175",
    "crumbs": [
      "Data",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data/2 siuba.html#renaming-column",
    "href": "data/2 siuba.html#renaming-column",
    "title": "Data manipulation with siuba",
    "section": "5 Renaming column",
    "text": "5 Renaming column\n\n\nCode\nsmall_mtcars &gt;&gt; rename(new_name_mpg = _.mpg)\n\n\n\n\n\n\n\n\n\n\ncyl\nnew_name_mpg\nhp\n\n\n\n\n0\n6\n21.0\n110\n\n\n1\n6\n21.0\n110\n\n\n2\n4\n22.8\n93\n\n\n3\n6\n21.4\n110\n\n\n4\n8\n18.7\n175",
    "crumbs": [
      "Data",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data/2 siuba.html#filter-rows",
    "href": "data/2 siuba.html#filter-rows",
    "title": "Data manipulation with siuba",
    "section": "7 Filter rows",
    "text": "7 Filter rows\n\n\nCode\nmtcars&gt;&gt; filter(_.gear ==4)\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n7\n24.4\n4\n146.7\n62\n3.69\n3.190\n20.00\n1\n0\n4\n2\n\n\n8\n22.8\n4\n140.8\n95\n3.92\n3.150\n22.90\n1\n0\n4\n2\n\n\n9\n19.2\n6\n167.6\n123\n3.92\n3.440\n18.30\n1\n0\n4\n4\n\n\n10\n17.8\n6\n167.6\n123\n3.92\n3.440\n18.90\n1\n0\n4\n4\n\n\n17\n32.4\n4\n78.7\n66\n4.08\n2.200\n19.47\n1\n1\n4\n1\n\n\n18\n30.4\n4\n75.7\n52\n4.93\n1.615\n18.52\n1\n1\n4\n2\n\n\n19\n33.9\n4\n71.1\n65\n4.22\n1.835\n19.90\n1\n1\n4\n1\n\n\n25\n27.3\n4\n79.0\n66\n4.08\n1.935\n18.90\n1\n1\n4\n1\n\n\n31\n21.4\n4\n121.0\n109\n4.11\n2.780\n18.60\n1\n1\n4\n2\n\n\n\n\n\n\n\n\n\n7.1 Filters with AND conditions\n\n\nCode\nmtcars &gt;&gt; filter((_.cyl &gt;4) & (_.gear == 5))\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n28\n15.8\n8\n351.0\n264\n4.22\n3.17\n14.5\n0\n1\n5\n4\n\n\n29\n19.7\n6\n145.0\n175\n3.62\n2.77\n15.5\n0\n1\n5\n6\n\n\n30\n15.0\n8\n301.0\n335\n3.54\n3.57\n14.6\n0\n1\n5\n8\n\n\n\n\n\n\n\n\n\n\n7.2 Filters with OR conditions\n\n\nCode\nmtcars &gt;&gt; filter((_.cyl == 6) | (_.gear == 5))\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n3\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n5\n18.1\n6\n225.0\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n\n\n9\n19.2\n6\n167.6\n123\n3.92\n3.440\n18.30\n1\n0\n4\n4\n\n\n10\n17.8\n6\n167.6\n123\n3.92\n3.440\n18.90\n1\n0\n4\n4\n\n\n26\n26.0\n4\n120.3\n91\n4.43\n2.140\n16.70\n0\n1\n5\n2\n\n\n27\n30.4\n4\n95.1\n113\n3.77\n1.513\n16.90\n1\n1\n5\n2\n\n\n28\n15.8\n8\n351.0\n264\n4.22\n3.170\n14.50\n0\n1\n5\n4\n\n\n29\n19.7\n6\n145.0\n175\n3.62\n2.770\n15.50\n0\n1\n5\n6\n\n\n30\n15.0\n8\n301.0\n335\n3.54\n3.570\n14.60\n0\n1\n5\n8\n\n\n\n\n\n\n\n\n\n\n7.3 Dropping NA values",
    "crumbs": [
      "Data",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data/2 siuba.html#order-rows",
    "href": "data/2 siuba.html#order-rows",
    "title": "Data manipulation with siuba",
    "section": "9 order rows",
    "text": "9 order rows\n\n\nCode\nsmall_mtcars &gt;&gt; arrange(_.hp)\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\nhp\n\n\n\n\n2\n4\n22.8\n93\n\n\n0\n6\n21.0\n110\n\n\n1\n6\n21.0\n110\n\n\n3\n6\n21.4\n110\n\n\n4\n8\n18.7\n175\n\n\n\n\n\n\n\n\n\n9.1 Sort in descending order\n\n\nCode\nsmall_mtcars &gt;&gt; arrange(-_.hp)\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\nhp\n\n\n\n\n4\n8\n18.7\n175\n\n\n0\n6\n21.0\n110\n\n\n1\n6\n21.0\n110\n\n\n3\n6\n21.4\n110\n\n\n2\n4\n22.8\n93\n\n\n\n\n\n\n\n\n\n\n9.2 Arrange by multiple variables\n\n\nCode\nsmall_mtcars &gt;&gt; arrange(_.cyl, -_.mpg)\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\nhp\n\n\n\n\n2\n4\n22.8\n93\n\n\n3\n6\n21.4\n110\n\n\n0\n6\n21.0\n110\n\n\n1\n6\n21.0\n110\n\n\n4\n8\n18.7\n175",
    "crumbs": [
      "Data",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/2 siuba.html#comparison",
    "href": "data manipulation/2 siuba.html#comparison",
    "title": "Data manipulation with siuba",
    "section": "1 Comparison",
    "text": "1 Comparison",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/2 siuba.html#package-download",
    "href": "data manipulation/2 siuba.html#package-download",
    "title": "Data manipulation with siuba",
    "section": "2 Package download",
    "text": "2 Package download\n\n\nCode\nimport os\nos.system('pip install siuba')",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/2 siuba.html#load-package",
    "href": "data manipulation/2 siuba.html#load-package",
    "title": "Data manipulation with siuba",
    "section": "3 load package",
    "text": "3 load package\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nimport seaborn as sns\n\n\n\nfrom siuba.siu import call\nfrom siuba import _, mutate, filter, group_by, summarize,show_query\nfrom siuba import *\n\nfrom siuba.data import mtcars,penguins\n\n\n\n\nCode\nimport os\nos.system('pip show siuba')\n\n\nName: siuba\nVersion: 0.4.4\nSummary: A package for quick, scrappy analyses with pandas and SQL\nHome-page: https://github.com/machow/siuba\nAuthor: Michael Chow\nAuthor-email: mc_al_gh_siuba@fastmail.com\nLicense: MIT\nLocation: /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages\nRequires: numpy, pandas, PyYAML, SQLAlchemy\nRequired-by: \n\n\n0\n\n\n\n\nCode\nsmall_mtcars = mtcars &gt;&gt; select(_.cyl, _.mpg, _.hp)&gt;&gt; head(5)",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/2 siuba.html#select-column",
    "href": "data manipulation/2 siuba.html#select-column",
    "title": "Data manipulation with siuba",
    "section": "4 select column",
    "text": "4 select column\n\n4.1 select columns by name\n\n\nCode\nsmall_mtcars &gt;&gt; select(_.cyl, _.mpg)\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\n\n\n\n\n0\n6\n21.0\n\n\n1\n6\n21.0\n\n\n2\n4\n22.8\n\n\n3\n6\n21.4\n\n\n4\n8\n18.7\n\n\n\n\n\n\n\n\n\n\n4.2 select columns by name match with ‘p’\n\n\nCode\nsmall_mtcars &gt;&gt; select(_.contains(\"p\"))\n\n\n\n\n\n\n\n\n\n\nmpg\nhp\n\n\n\n\n0\n21.0\n110\n\n\n1\n21.0\n110\n\n\n2\n22.8\n93\n\n\n3\n21.4\n110\n\n\n4\n18.7\n175\n\n\n\n\n\n\n\n\n\n\n4.3 select columns by index\n\n4.3.1 select first and 3rd columns\n\n\nCode\nsmall_mtcars &gt;&gt; select(0,2)\n\n\n\n\n\n\n\n\n\n\ncyl\nhp\n\n\n\n\n0\n6\n110\n\n\n1\n6\n110\n\n\n2\n4\n93\n\n\n3\n6\n110\n\n\n4\n8\n175\n\n\n\n\n\n\n\n\n\n\n4.3.2 select first to 3rd columns\n\n\nCode\nsmall_mtcars &gt;&gt; select(_[0:3])\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\nhp\n\n\n\n\n0\n6\n21.0\n110\n\n\n1\n6\n21.0\n110\n\n\n2\n4\n22.8\n93\n\n\n3\n6\n21.4\n110\n\n\n4\n8\n18.7\n175",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/2 siuba.html#renaming-column",
    "href": "data manipulation/2 siuba.html#renaming-column",
    "title": "Data manipulation with siuba",
    "section": "6 Renaming column",
    "text": "6 Renaming column\n\n\nCode\nsmall_mtcars &gt;&gt; rename(new_name_mpg = _.mpg)\n\n\n\n\n\n\n\n\n\n\ncyl\nnew_name_mpg\nhp\n\n\n\n\n0\n6\n21.0\n110\n\n\n1\n6\n21.0\n110\n\n\n2\n4\n22.8\n93\n\n\n3\n6\n21.4\n110\n\n\n4\n8\n18.7\n175",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/2 siuba.html#create-column",
    "href": "data manipulation/2 siuba.html#create-column",
    "title": "Data manipulation with siuba",
    "section": "7 Create column",
    "text": "7 Create column\n\n7.1 Mutate\n\n\nCode\nmtcars.head()&gt;&gt; mutate(gear2 = _.gear+1\n                      ,gear3=if_else(_.gear &gt; 3, \"long\", \"short\")\n                       ,qsec2=case_when({\n                                          _.qsec &lt;= 17: \"short\",\n                                          _.qsec &lt;= 18: \"Medium\",\n                                          True: \"long\"\n                                                     })\n                       )\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\ngear2\ngear3\nqsec2\n\n\n\n\n0\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n5\nlong\nshort\n\n\n1\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n5\nlong\nMedium\n\n\n2\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n5\nlong\nlong\n\n\n3\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n4\nshort\nlong\n\n\n4\n18.7\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n4\nshort\nMedium\n\n\n\n\n\n\n\n\n\n\n7.2 Transmute,create column and only keep this column\n\n\nCode\nmtcars.head()&gt;&gt; transmute(gear2 = _.gear+1)\n\n\n\n\n\n\n\n\n\n\ngear2\n\n\n\n\n0\n5\n\n\n1\n5\n\n\n2\n5\n\n\n3\n4\n\n\n4\n4",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/2 siuba.html#filter-rows",
    "href": "data manipulation/2 siuba.html#filter-rows",
    "title": "Data manipulation with siuba",
    "section": "8 Filter rows",
    "text": "8 Filter rows\n\n\nCode\nmtcars&gt;&gt; filter(_.gear ==4)\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n7\n24.4\n4\n146.7\n62\n3.69\n3.190\n20.00\n1\n0\n4\n2\n\n\n8\n22.8\n4\n140.8\n95\n3.92\n3.150\n22.90\n1\n0\n4\n2\n\n\n9\n19.2\n6\n167.6\n123\n3.92\n3.440\n18.30\n1\n0\n4\n4\n\n\n10\n17.8\n6\n167.6\n123\n3.92\n3.440\n18.90\n1\n0\n4\n4\n\n\n17\n32.4\n4\n78.7\n66\n4.08\n2.200\n19.47\n1\n1\n4\n1\n\n\n18\n30.4\n4\n75.7\n52\n4.93\n1.615\n18.52\n1\n1\n4\n2\n\n\n19\n33.9\n4\n71.1\n65\n4.22\n1.835\n19.90\n1\n1\n4\n1\n\n\n25\n27.3\n4\n79.0\n66\n4.08\n1.935\n18.90\n1\n1\n4\n1\n\n\n31\n21.4\n4\n121.0\n109\n4.11\n2.780\n18.60\n1\n1\n4\n2\n\n\n\n\n\n\n\n\n\n8.1 Filters with AND conditions\n\n\nCode\nmtcars &gt;&gt; filter((_.cyl &gt;4) & (_.gear == 5))\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n28\n15.8\n8\n351.0\n264\n4.22\n3.17\n14.5\n0\n1\n5\n4\n\n\n29\n19.7\n6\n145.0\n175\n3.62\n2.77\n15.5\n0\n1\n5\n6\n\n\n30\n15.0\n8\n301.0\n335\n3.54\n3.57\n14.6\n0\n1\n5\n8\n\n\n\n\n\n\n\n\n\n\n8.2 Filters with OR conditions\n\n\nCode\nmtcars &gt;&gt; filter((_.cyl == 6) | (_.gear == 5))\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n3\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n5\n18.1\n6\n225.0\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n\n\n9\n19.2\n6\n167.6\n123\n3.92\n3.440\n18.30\n1\n0\n4\n4\n\n\n10\n17.8\n6\n167.6\n123\n3.92\n3.440\n18.90\n1\n0\n4\n4\n\n\n26\n26.0\n4\n120.3\n91\n4.43\n2.140\n16.70\n0\n1\n5\n2\n\n\n27\n30.4\n4\n95.1\n113\n3.77\n1.513\n16.90\n1\n1\n5\n2\n\n\n28\n15.8\n8\n351.0\n264\n4.22\n3.170\n14.50\n0\n1\n5\n4\n\n\n29\n19.7\n6\n145.0\n175\n3.62\n2.770\n15.50\n0\n1\n5\n6\n\n\n30\n15.0\n8\n301.0\n335\n3.54\n3.570\n14.60\n0\n1\n5\n8\n\n\n\n\n\n\n\n\n\n\n8.3 filter row with index\n\n8.3.1 first 3\n\n\nCode\nsmall_mtcars&gt;&gt;head(3)\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\nhp\n\n\n\n\n0\n6\n21.0\n110\n\n\n1\n6\n21.0\n110\n\n\n2\n4\n22.8\n93\n\n\n\n\n\n\n\n\n\n\n8.3.2 last 3\n\n\nCode\n# not in siuba, in pandas\nsmall_mtcars.tail(3)\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\nhp\n\n\n\n\n2\n4\n22.8\n93\n\n\n3\n6\n21.4\n110\n\n\n4\n8\n18.7\n175\n\n\n\n\n\n\n\n\n\n\n8.3.3 5th rows\n\n\nCode\n# not in siuba, in pandas\nmtcars.iloc[[4]]\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n4\n18.7\n8\n360.0\n175\n3.15\n3.44\n17.02\n0\n0\n3\n2\n\n\n\n\n\n\n\n\n\n\n8.3.4 1 and 5th rows\n\n\nCode\n# not in siuba, in pandas\nmtcars.iloc[[0,4]]\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\n21.0\n6\n160.0\n110\n3.90\n2.62\n16.46\n0\n1\n4\n4\n\n\n4\n18.7\n8\n360.0\n175\n3.15\n3.44\n17.02\n0\n0\n3\n2\n\n\n\n\n\n\n\n\n\n\n8.3.5 1 to 5th rows\n\n\nCode\n# not in siuba, in pandas\nmtcars.iloc[0:4]\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n3\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n\n\n\n\n\n\n\n\n8.3.6 get ramdon 5 rows\n\n\nCode\nmtcars.sample(5, random_state=42)\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n29\n19.7\n6\n145.0\n175\n3.62\n2.770\n15.50\n0\n1\n5\n6\n\n\n15\n10.4\n8\n460.0\n215\n3.00\n5.424\n17.82\n0\n0\n3\n4\n\n\n24\n19.2\n8\n400.0\n175\n3.08\n3.845\n17.05\n0\n0\n3\n2\n\n\n17\n32.4\n4\n78.7\n66\n4.08\n2.200\n19.47\n1\n1\n4\n1\n\n\n8\n22.8\n4\n140.8\n95\n3.92\n3.150\n22.90\n1\n0\n4\n2",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/2 siuba.html#group-by",
    "href": "data manipulation/2 siuba.html#group-by",
    "title": "Data manipulation with siuba",
    "section": "10 group by",
    "text": "10 group by\n\n10.1 average,min,max,sum\n\n\nCode\ntbl_query = (mtcars\n  &gt;&gt; group_by(_.cyl)\n  &gt;&gt; summarize(avg_hp = _.hp.mean()\n              ,min_hp=_.hp.min()\n              ,max_hp=_.hp.max()\n              ,totol_disp=_.disp.sum()\n  )\n  )\n\ntbl_query\n\n\n\n\n\n\n\n\n\n\ncyl\navg_hp\nmin_hp\nmax_hp\ntotol_disp\n\n\n\n\n0\n4\n82.636364\n52\n113\n1156.5\n\n\n1\n6\n122.285714\n105\n175\n1283.2\n\n\n2\n8\n209.214286\n150\n335\n4943.4\n\n\n\n\n\n\n\n\n\n\n10.2 count\n\n\nCode\nmtcars &gt;&gt; group_by(_.cyl)  &gt;&gt; summarize(n = _.shape[0])\n\n\n\n\n\n\n\n\n\n\ncyl\nn\n\n\n\n\n0\n4\n11\n\n\n1\n6\n7\n\n\n2\n8\n14",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/2 siuba.html#order-rows",
    "href": "data manipulation/2 siuba.html#order-rows",
    "title": "Data manipulation with siuba",
    "section": "11 order rows",
    "text": "11 order rows\n\n\nCode\nsmall_mtcars &gt;&gt; arrange(_.hp)\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\nhp\n\n\n\n\n2\n4\n22.8\n93\n\n\n0\n6\n21.0\n110\n\n\n1\n6\n21.0\n110\n\n\n3\n6\n21.4\n110\n\n\n4\n8\n18.7\n175\n\n\n\n\n\n\n\n\n\n11.1 Sort in descending order\n\n\nCode\nsmall_mtcars &gt;&gt; arrange(-_.hp)\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\nhp\n\n\n\n\n4\n8\n18.7\n175\n\n\n0\n6\n21.0\n110\n\n\n1\n6\n21.0\n110\n\n\n3\n6\n21.4\n110\n\n\n2\n4\n22.8\n93\n\n\n\n\n\n\n\n\n\n\n11.2 Arrange by multiple variables\n\n\nCode\nsmall_mtcars &gt;&gt; arrange(_.cyl, -_.mpg)\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\nhp\n\n\n\n\n2\n4\n22.8\n93\n\n\n3\n6\n21.4\n110\n\n\n0\n6\n21.0\n110\n\n\n1\n6\n21.0\n110\n\n\n4\n8\n18.7\n175",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/2 siuba.html#join",
    "href": "data manipulation/2 siuba.html#join",
    "title": "Data manipulation with siuba",
    "section": "12 join",
    "text": "12 join\n\n\nCode\nlhs = pd.DataFrame({'id': [1,2,3], 'val': ['lhs.1', 'lhs.2', 'lhs.3']})\nrhs = pd.DataFrame({'id': [1,2,4], 'val': ['rhs.1', 'rhs.2', 'rhs.3']})\n\n\n\n\nCode\nlhs\n\n\n\n\n\n\n\n\n\n\nid\nval\n\n\n\n\n0\n1\nlhs.1\n\n\n1\n2\nlhs.2\n\n\n2\n3\nlhs.3\n\n\n\n\n\n\n\n\n\n\nCode\nrhs\n\n\n\n\n\n\n\n\n\n\nid\nval\n\n\n\n\n0\n1\nrhs.1\n\n\n1\n2\nrhs.2\n\n\n2\n4\nrhs.3\n\n\n\n\n\n\n\n\n\n12.1 inner_join\n\n\nCode\nresult=lhs &gt;&gt; inner_join(_, rhs, on=\"id\")\nresult\n\n\n\n\n\n\n\n\n\n\nid\nval_x\nval_y\n\n\n\n\n0\n1\nlhs.1\nrhs.1\n\n\n1\n2\nlhs.2\nrhs.2\n\n\n\n\n\n\n\n\n\n\n12.2 full join\n\n\nCode\nresult=rhs &gt;&gt; full_join(_, lhs, on=\"id\")\nresult\n\n\n\n\n\n\n\n\n\n\nid\nval_x\nval_y\n\n\n\n\n0\n1\nrhs.1\nlhs.1\n\n\n1\n2\nrhs.2\nlhs.2\n\n\n2\n3\nNaN\nlhs.3\n\n\n3\n4\nrhs.3\nNaN\n\n\n\n\n\n\n\n\n\n\n12.3 left join\n\n\nCode\nresult=lhs &gt;&gt; left_join(_, rhs, on=\"id\")\nresult\n\n\n\n\n\n\n\n\n\n\nid\nval_x\nval_y\n\n\n\n\n0\n1\nlhs.1\nrhs.1\n\n\n1\n2\nlhs.2\nrhs.2\n\n\n2\n3\nlhs.3\nNaN\n\n\n\n\n\n\n\n\n\n\n12.4 anti join\nkeep data in left which not in right\n\n\nCode\nresult=lhs &gt;&gt; anti_join(_, rhs, on=\"id\")\nresult\n\n\n\n\n\n\n\n\n\n\nid\nval\n\n\n\n\n2\n3\nlhs.3\n\n\n\n\n\n\n\n\nkeep data in right which not in left\n\n\nCode\nresult=rhs &gt;&gt; anti_join(_, lhs, on=\"id\")\nresult\n\n\n\n\n\n\n\n\n\n\nid\nval\n\n\n\n\n2\n4\nrhs.3",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/2 siuba.html#using-siuba-with-database",
    "href": "data manipulation/2 siuba.html#using-siuba-with-database",
    "title": "Data manipulation with siuba",
    "section": "16 using siuba with database",
    "text": "16 using siuba with database\n\n16.1 set up a sqlite database, with an mtcars table.\n\n\nCode\nfrom sqlalchemy import create_engine\nfrom siuba.sql import LazyTbl\nfrom siuba import _, group_by, summarize, show_query, collect \nfrom siuba.data import mtcars\n\n# copy in to sqlite, using the pandas .to_sql() method\nengine = create_engine(\"sqlite:///:memory:\")\nmtcars.to_sql(\"mtcars\", engine, if_exists = \"replace\")\n\n\n32\n\n\n\n\n16.2 create table\n\n\nCode\n# Create a lazy SQL DataFrame\ntbl_mtcars = LazyTbl(engine, \"mtcars\")\ntbl_mtcars\n\n\n\n# Source: lazy query\n# DB Conn: Engine(sqlite:///:memory:)\n# Preview:\n\n\n\n\n\n\n\nindex\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\n0\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\n1\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\n2\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n3\n3\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n4\n4\n18.7\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\n\n\n# .. may have more rows\n\n\n\n\n\n16.3 create query\n\n\nCode\n# connect with siuba\n\ntbl_query = (tbl_mtcars\n  &gt;&gt; group_by(_.mpg)\n  &gt;&gt; summarize(avg_hp = _.hp.mean())\n  )\n\ntbl_query\n\n\n\n# Source: lazy query\n# DB Conn: Engine(sqlite:///:memory:)\n# Preview:\n\n\n\n\n\n\n\nmpg\navg_hp\n\n\n\n\n0\n10.4\n210.0\n\n\n1\n13.3\n245.0\n\n\n2\n14.3\n245.0\n\n\n3\n14.7\n230.0\n\n\n4\n15.0\n335.0\n\n\n\n\n# .. may have more rows\n\n\n\n\n\n16.4 show query\n\n\nCode\n tbl_query &gt;&gt; show_query()\n\n\nSELECT mtcars.mpg, avg(mtcars.hp) AS avg_hp \nFROM mtcars GROUP BY mtcars.mpg\n\n\n\n# Source: lazy query\n# DB Conn: Engine(sqlite:///:memory:)\n# Preview:\n\n\n\n\n\n\n\nmpg\navg_hp\n\n\n\n\n0\n10.4\n210.0\n\n\n1\n13.3\n245.0\n\n\n2\n14.3\n245.0\n\n\n3\n14.7\n230.0\n\n\n4\n15.0\n335.0\n\n\n\n\n# .. may have more rows\n\n\n\n\n\n16.5 Collect to DataFrame\nbecause lazy expressions,the collect function is actually running the sql.\n\n\nCode\ndata=tbl_query &gt;&gt; collect()\nprint(data)\n\n\n     mpg  avg_hp\n0   10.4   210.0\n1   13.3   245.0\n2   14.3   245.0\n3   14.7   230.0\n4   15.0   335.0\n5   15.2   165.0\n6   15.5   150.0\n7   15.8   264.0\n8   16.4   180.0\n9   17.3   180.0\n10  17.8   123.0\n11  18.1   105.0\n12  18.7   175.0\n13  19.2   149.0\n14  19.7   175.0\n15  21.0   110.0\n16  21.4   109.5\n17  21.5    97.0\n18  22.8    94.0\n19  24.4    62.0\n20  26.0    91.0\n21  27.3    66.0\n22  30.4    82.5\n23  32.4    66.0\n24  33.9    65.0",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/2 siuba.html#reference",
    "href": "data manipulation/2 siuba.html#reference",
    "title": "Data manipulation with siuba",
    "section": "17 reference:",
    "text": "17 reference:\nhttps://siuba.org/",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html",
    "href": "data manipulation/1 Pandas.html",
    "title": "Data manipulation with Pandas",
    "section": "",
    "text": "pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.\nCode\nimport pandas as pd\nprint('pandas version', pd.__version__)\n\n\npandas version 2.2.1\nCode\nimport os\n#os.system('pip show pandas')",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#reading-from-parquet",
    "href": "data manipulation/1 Pandas.html#reading-from-parquet",
    "title": "Data manipulation with Pandas",
    "section": "0.14 Reading from parquet",
    "text": "0.14 Reading from parquet\n\n\nCode\nbig_df = pd.read_parquet(\"data/Combined_Flights_2022.parquet\")\n\ndf=big_df.head(10)",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#get-first-3",
    "href": "data manipulation/1 Pandas.html#get-first-3",
    "title": "Data manipulation with Pandas",
    "section": "0.2 get first 3",
    "text": "0.2 get first 3\n\n\nCode\ndf.head(3)\n\n\n\n\n\n\n\n\n\n\nFlightDate\nAirline\nOrigin\nDest\nCancelled\nDiverted\nCRSDepTime\nDepTime\nDepDelayMinutes\nDepDelay\n...\nWheelsOff\nWheelsOn\nTaxiIn\nCRSArrTime\nArrDelay\nArrDel15\nArrivalDelayGroups\nArrTimeBlk\nDistanceGroup\nDivAirportLandings\n\n\n\n\n0\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nGJT\nDEN\nFalse\nFalse\n1133\n1123.0\n0.0\n-10.0\n...\n1140.0\n1220.0\n8.0\n1245\n-17.0\n0.0\n-2.0\n1200-1259\n1\n0\n\n\n1\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nHRL\nIAH\nFalse\nFalse\n732\n728.0\n0.0\n-4.0\n...\n744.0\n839.0\n9.0\n849\n-1.0\n0.0\n-1.0\n0800-0859\n2\n0\n\n\n2\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nDRO\nDEN\nFalse\nFalse\n1529\n1514.0\n0.0\n-15.0\n...\n1535.0\n1622.0\n14.0\n1639\n-3.0\n0.0\n-1.0\n1600-1659\n2\n0\n\n\n\n\n3 rows × 61 columns",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#get-last-3",
    "href": "data manipulation/1 Pandas.html#get-last-3",
    "title": "Data manipulation with Pandas",
    "section": "0.3 get last 3",
    "text": "0.3 get last 3\n\n\nCode\ndf.tail(3)\n\n\n\n\n\n\n\n\n\n\nFlightDate\nAirline\nOrigin\nDest\nCancelled\nDiverted\nCRSDepTime\nDepTime\nDepDelayMinutes\nDepDelay\n...\nWheelsOff\nWheelsOn\nTaxiIn\nCRSArrTime\nArrDelay\nArrDel15\nArrivalDelayGroups\nArrTimeBlk\nDistanceGroup\nDivAirportLandings\n\n\n\n\n590539\n2022-03-08\nRepublic Airlines\nALB\nORD\nFalse\nFalse\n1700\n2318.0\n378.0\n378.0\n...\n2337.0\n52.0\n7.0\n1838\n381.0\n1.0\n12.0\n1800-1859\n3\n0\n\n\n590540\n2022-03-25\nRepublic Airlines\nEWR\nPIT\nFalse\nTrue\n2129\n2322.0\n113.0\n113.0\n...\n2347.0\n933.0\n6.0\n2255\nNaN\nNaN\nNaN\n2200-2259\n2\n1\n\n\n590541\n2022-03-07\nRepublic Airlines\nEWR\nRDU\nFalse\nTrue\n1154\n1148.0\n0.0\n-6.0\n...\n1201.0\n1552.0\n4.0\n1333\nNaN\nNaN\nNaN\n1300-1359\n2\n1\n\n\n\n\n3 rows × 61 columns",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#get-ramdon-5",
    "href": "data manipulation/1 Pandas.html#get-ramdon-5",
    "title": "Data manipulation with Pandas",
    "section": "0.4 get ramdon 5",
    "text": "0.4 get ramdon 5\n\n\nCode\ndf.sample(5, random_state=42)\n\n\n\n\n\n\n\n\n\n\nFlightDate\nAirline\nOrigin\nDest\nCancelled\nDiverted\nCRSDepTime\nDepTime\nDepDelayMinutes\nDepDelay\n...\nWheelsOff\nWheelsOn\nTaxiIn\nCRSArrTime\nArrDelay\nArrDel15\nArrivalDelayGroups\nArrTimeBlk\nDistanceGroup\nDivAirportLandings\n\n\n\n\n324021\n2022-03-19\nSkyWest Airlines Inc.\nASE\nDEN\nFalse\nFalse\n1831\n1826.0\n0.0\n-5.0\n...\n1845.0\n1916.0\n7.0\n1933\n-10.0\n0.0\n-1.0\n1900-1959\n1\n0\n\n\n34739\n2022-02-16\nSkyWest Airlines Inc.\nTYS\nDEN\nFalse\nFalse\n1605\n1605.0\n0.0\n0.0\n...\n1614.0\n1728.0\n44.0\n1737\n35.0\n1.0\n2.0\n1700-1759\n5\n0\n\n\n304494\n2022-01-18\nAmerican Airlines Inc.\nLAX\nOGG\nFalse\nFalse\n1719\n1714.0\n0.0\n-5.0\n...\n1728.0\n2042.0\n10.0\n2049\n3.0\n0.0\n0.0\n2000-2059\n10\n0\n\n\n205451\n2022-01-23\nSouthwest Airlines Co.\nLAS\nBWI\nFalse\nFalse\n1515\n1533.0\n18.0\n18.0\n...\n1548.0\n2259.0\n3.0\n2235\n27.0\n1.0\n1.0\n2200-2259\n9\n0\n\n\n173176\n2022-07-01\nDelta Air Lines Inc.\nDTW\nRDU\nFalse\nFalse\n715\n709.0\n0.0\n-6.0\n...\n721.0\n834.0\n3.0\n853\n-16.0\n0.0\n-2.0\n0800-0859\n3\n0\n\n\n\n\n5 rows × 61 columns",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#get-columns-names",
    "href": "data manipulation/1 Pandas.html#get-columns-names",
    "title": "Data manipulation with Pandas",
    "section": "0.18 get columns names",
    "text": "0.18 get columns names\n\n\nCode\ndf.columns\n\n\nIndex(['FlightDate', 'Airline', 'Origin', 'Dest', 'Cancelled', 'Diverted',\n       'CRSDepTime', 'DepTime', 'DepDelayMinutes', 'DepDelay', 'ArrTime',\n       'ArrDelayMinutes', 'AirTime', 'CRSElapsedTime', 'ActualElapsedTime',\n       'Distance', 'Year', 'Quarter', 'Month', 'DayofMonth', 'DayOfWeek',\n       'Marketing_Airline_Network', 'Operated_or_Branded_Code_Share_Partners',\n       'DOT_ID_Marketing_Airline', 'IATA_Code_Marketing_Airline',\n       'Flight_Number_Marketing_Airline', 'Operating_Airline',\n       'DOT_ID_Operating_Airline', 'IATA_Code_Operating_Airline',\n       'Tail_Number', 'Flight_Number_Operating_Airline', 'OriginAirportID',\n       'OriginAirportSeqID', 'OriginCityMarketID', 'OriginCityName',\n       'OriginState', 'OriginStateFips', 'OriginStateName', 'OriginWac',\n       'DestAirportID', 'DestAirportSeqID', 'DestCityMarketID', 'DestCityName',\n       'DestState', 'DestStateFips', 'DestStateName', 'DestWac', 'DepDel15',\n       'DepartureDelayGroups', 'DepTimeBlk', 'TaxiOut', 'WheelsOff',\n       'WheelsOn', 'TaxiIn', 'CRSArrTime', 'ArrDelay', 'ArrDel15',\n       'ArrivalDelayGroups', 'ArrTimeBlk', 'DistanceGroup',\n       'DivAirportLandings'],\n      dtype='object')",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#get-row-index",
    "href": "data manipulation/1 Pandas.html#get-row-index",
    "title": "Data manipulation with Pandas",
    "section": "0.19 get row index",
    "text": "0.19 get row index\n\n\nCode\ndf.index\n\n\nIndex([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype='int64')",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#get-long-info",
    "href": "data manipulation/1 Pandas.html#get-long-info",
    "title": "Data manipulation with Pandas",
    "section": "0.20 get long info",
    "text": "0.20 get long info\n\n\nCode\ndf.shape\n\n\n(10, 61)\n\n\n\n\nCode\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 10 entries, 0 to 9\nData columns (total 61 columns):\n #   Column                                   Non-Null Count  Dtype         \n---  ------                                   --------------  -----         \n 0   FlightDate                               10 non-null     datetime64[us]\n 1   Airline                                  10 non-null     object        \n 2   Origin                                   10 non-null     object        \n 3   Dest                                     10 non-null     object        \n 4   Cancelled                                10 non-null     bool          \n 5   Diverted                                 10 non-null     bool          \n 6   CRSDepTime                               10 non-null     int64         \n 7   DepTime                                  10 non-null     float64       \n 8   DepDelayMinutes                          10 non-null     float64       \n 9   DepDelay                                 10 non-null     float64       \n 10  ArrTime                                  10 non-null     float64       \n 11  ArrDelayMinutes                          10 non-null     float64       \n 12  AirTime                                  10 non-null     float64       \n 13  CRSElapsedTime                           10 non-null     float64       \n 14  ActualElapsedTime                        10 non-null     float64       \n 15  Distance                                 10 non-null     float64       \n 16  Year                                     10 non-null     int64         \n 17  Quarter                                  10 non-null     int64         \n 18  Month                                    10 non-null     int64         \n 19  DayofMonth                               10 non-null     int64         \n 20  DayOfWeek                                10 non-null     int64         \n 21  Marketing_Airline_Network                10 non-null     object        \n 22  Operated_or_Branded_Code_Share_Partners  10 non-null     object        \n 23  DOT_ID_Marketing_Airline                 10 non-null     int64         \n 24  IATA_Code_Marketing_Airline              10 non-null     object        \n 25  Flight_Number_Marketing_Airline          10 non-null     int64         \n 26  Operating_Airline                        10 non-null     object        \n 27  DOT_ID_Operating_Airline                 10 non-null     int64         \n 28  IATA_Code_Operating_Airline              10 non-null     object        \n 29  Tail_Number                              10 non-null     object        \n 30  Flight_Number_Operating_Airline          10 non-null     int64         \n 31  OriginAirportID                          10 non-null     int64         \n 32  OriginAirportSeqID                       10 non-null     int64         \n 33  OriginCityMarketID                       10 non-null     int64         \n 34  OriginCityName                           10 non-null     object        \n 35  OriginState                              10 non-null     object        \n 36  OriginStateFips                          10 non-null     int64         \n 37  OriginStateName                          10 non-null     object        \n 38  OriginWac                                10 non-null     int64         \n 39  DestAirportID                            10 non-null     int64         \n 40  DestAirportSeqID                         10 non-null     int64         \n 41  DestCityMarketID                         10 non-null     int64         \n 42  DestCityName                             10 non-null     object        \n 43  DestState                                10 non-null     object        \n 44  DestStateFips                            10 non-null     int64         \n 45  DestStateName                            10 non-null     object        \n 46  DestWac                                  10 non-null     int64         \n 47  DepDel15                                 10 non-null     float64       \n 48  DepartureDelayGroups                     10 non-null     float64       \n 49  DepTimeBlk                               10 non-null     object        \n 50  TaxiOut                                  10 non-null     float64       \n 51  WheelsOff                                10 non-null     float64       \n 52  WheelsOn                                 10 non-null     float64       \n 53  TaxiIn                                   10 non-null     float64       \n 54  CRSArrTime                               10 non-null     int64         \n 55  ArrDelay                                 10 non-null     float64       \n 56  ArrDel15                                 10 non-null     float64       \n 57  ArrivalDelayGroups                       10 non-null     float64       \n 58  ArrTimeBlk                               10 non-null     object        \n 59  DistanceGroup                            10 non-null     int64         \n 60  DivAirportLandings                       10 non-null     int64         \ndtypes: bool(2), datetime64[us](1), float64(18), int64(23), object(17)\nmemory usage: 4.7+ KB",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#get-short-info",
    "href": "data manipulation/1 Pandas.html#get-short-info",
    "title": "Data manipulation with Pandas",
    "section": "0.21 get short info",
    "text": "0.21 get short info\n\n\nCode\ndf.info(verbose=False)\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 10 entries, 0 to 9\nColumns: 61 entries, FlightDate to DivAirportLandings\ndtypes: bool(2), datetime64[us](1), float64(18), int64(23), object(17)\nmemory usage: 4.7+ KB",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#summary-of-numeric-column",
    "href": "data manipulation/1 Pandas.html#summary-of-numeric-column",
    "title": "Data manipulation with Pandas",
    "section": "0.22 summary of numeric column",
    "text": "0.22 summary of numeric column\n\n\nCode\ndf.describe()\n\n\n\n\n\n\n\n\n\n\nFlightDate\nCRSDepTime\nDepTime\nDepDelayMinutes\nDepDelay\nArrTime\nArrDelayMinutes\nAirTime\nCRSElapsedTime\nActualElapsedTime\n...\nTaxiOut\nWheelsOff\nWheelsOn\nTaxiIn\nCRSArrTime\nArrDelay\nArrDel15\nArrivalDelayGroups\nDistanceGroup\nDivAirportLandings\n\n\n\n\ncount\n10\n10.000000\n10.000000\n10.0\n10.00000\n10.000000\n10.000000\n10.000000\n10.00000\n10.000000\n...\n10.00000\n10.000000\n10.000000\n10.000000\n10.000000\n10.000000\n10.0\n10.000000\n10.000000\n10.0\n\n\nmean\n2022-04-04 00:00:00\n1256.500000\n1249.600000\n0.0\n-6.90000\n1390.000000\n1.100000\n58.400000\n84.00000\n84.400000\n...\n18.00000\n1275.600000\n1378.000000\n8.000000\n1400.500000\n-6.500000\n0.0\n-1.000000\n2.000000\n0.0\n\n\nmin\n2022-04-04 00:00:00\n732.000000\n728.000000\n0.0\n-15.00000\n848.000000\n0.000000\n26.000000\n52.00000\n42.000000\n...\n11.00000\n744.000000\n839.000000\n4.000000\n849.000000\n-18.000000\n0.0\n-2.000000\n1.000000\n0.0\n\n\n25%\n2022-04-04 00:00:00\n998.500000\n993.250000\n0.0\n-10.00000\n1230.500000\n0.000000\n41.750000\n70.00000\n67.750000\n...\n16.00000\n1047.500000\n1223.500000\n5.250000\n1241.250000\n-12.500000\n0.0\n-1.000000\n1.250000\n0.0\n\n\n50%\n2022-04-04 00:00:00\n1134.000000\n1129.000000\n0.0\n-6.00000\n1281.000000\n0.000000\n52.000000\n74.50000\n78.500000\n...\n17.00000\n1147.000000\n1249.000000\n7.000000\n1275.500000\n-7.000000\n0.0\n-1.000000\n2.000000\n0.0\n\n\n75%\n2022-04-04 00:00:00\n1432.250000\n1426.000000\n0.0\n-3.25000\n1538.500000\n0.000000\n59.250000\n89.25000\n82.750000\n...\n20.50000\n1442.000000\n1534.000000\n8.750000\n1584.750000\n-1.250000\n0.0\n-1.000000\n2.000000\n0.0\n\n\nmax\n2022-04-04 00:00:00\n2139.000000\n2136.000000\n0.0\n0.00000\n2218.000000\n6.000000\n136.000000\n157.00000\n174.000000\n...\n25.00000\n2147.000000\n2213.000000\n16.000000\n2231.000000\n6.000000\n0.0\n0.000000\n4.000000\n0.0\n\n\nstd\nNaN\n396.759163\n396.390156\n0.0\n4.72464\n370.127906\n2.330951\n30.638393\n29.81424\n35.693759\n...\n3.91578\n387.953949\n371.973416\n4.082483\n375.500925\n8.669871\n0.0\n0.666667\n0.942809\n0.0\n\n\n\n\n8 rows × 42 columns",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#summary-of-categorical-column",
    "href": "data manipulation/1 Pandas.html#summary-of-categorical-column",
    "title": "Data manipulation with Pandas",
    "section": "0.23 summary of categorical column",
    "text": "0.23 summary of categorical column\n\n\nCode\ndf[[\"Airline\"]].describe()\n\n\n\n\n\n\n\n\n\n\nAirline\n\n\n\n\ncount\n10\n\n\nunique\n1\n\n\ntop\nCommutair Aka Champlain Enterprises, Inc.\n\n\nfreq\n10",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/2 siuba.html#reshape-tables",
    "href": "data manipulation/2 siuba.html#reshape-tables",
    "title": "Data manipulation with siuba",
    "section": "13 Reshape tables",
    "text": "13 Reshape tables\n\n\nCode\ncosts = pd.DataFrame({\n    'id': [1,2],\n    'price_x': [.1, .2],\n    'price_y': [.4, .5],\n    'price_z': [.7, .8]\n})\n\ncosts\n\n\n\n\n\n\n\n\n\n\nid\nprice_x\nprice_y\nprice_z\n\n\n\n\n0\n1\n0.1\n0.4\n0.7\n\n\n1\n2\n0.2\n0.5\n0.8\n\n\n\n\n\n\n\n\n\n13.1 Gather data long(wide to long)\nBelow 3 method will give same result\n\n\nCode\n# selecting each variable manually\ncosts &gt;&gt; gather('measure', 'value', _.price_x, _.price_y, _.price_z)\n\n\n\n\n\n\n\n\n\n\nid\nmeasure\nvalue\n\n\n\n\n0\n1\nprice_x\n0.1\n\n\n1\n2\nprice_x\n0.2\n\n\n2\n1\nprice_y\n0.4\n\n\n3\n2\nprice_y\n0.5\n\n\n4\n1\nprice_z\n0.7\n\n\n5\n2\nprice_z\n0.8\n\n\n\n\n\n\n\n\nother way:\n\n\nCode\n# selecting variables using a slice\ncosts &gt;&gt; gather('measure', 'value', _[\"price_x\":\"price_z\"])\n\n\n\n\n\n\n\n\n\n\nid\nmeasure\nvalue\n\n\n\n\n0\n1\nprice_x\n0.1\n\n\n1\n2\nprice_x\n0.2\n\n\n2\n1\nprice_y\n0.4\n\n\n3\n2\nprice_y\n0.5\n\n\n4\n1\nprice_z\n0.7\n\n\n5\n2\nprice_z\n0.8\n\n\n\n\n\n\n\n\nother way:\n\n\nCode\n# selecting by excluding id\ncosts &gt;&gt; gather('measure', 'value', -_.id)\n\n\n\n\n\n\n\n\n\n\nid\nmeasure\nvalue\n\n\n\n\n0\n1\nprice_x\n0.1\n\n\n1\n2\nprice_x\n0.2\n\n\n2\n1\nprice_y\n0.4\n\n\n3\n2\nprice_y\n0.5\n\n\n4\n1\nprice_z\n0.7\n\n\n5\n2\nprice_z\n0.8\n\n\n\n\n\n\n\n\n\n\n13.2 Spread data wide(long to wide)\n\n\nCode\ncosts_long= costs&gt;&gt; gather('measure', 'value', -_.id)\ncosts_long\n\n\n\n\n\n\n\n\n\n\nid\nmeasure\nvalue\n\n\n\n\n0\n1\nprice_x\n0.1\n\n\n1\n2\nprice_x\n0.2\n\n\n2\n1\nprice_y\n0.4\n\n\n3\n2\nprice_y\n0.5\n\n\n4\n1\nprice_z\n0.7\n\n\n5\n2\nprice_z\n0.8\n\n\n\n\n\n\n\n\n\n\nCode\ncosts_long&gt;&gt; spread('measure', 'value')\n\n\n\n\n\n\n\n\n\n\nid\nprice_x\nprice_y\nprice_z\n\n\n\n\n0\n1\n0.1\n0.4\n0.7\n\n\n1\n2\n0.2\n0.5\n0.8",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/2 siuba.html#string",
    "href": "data manipulation/2 siuba.html#string",
    "title": "Data manipulation with siuba",
    "section": "14 string",
    "text": "14 string\n\n\nCode\ndf = pd.DataFrame({'text': ['abc', 'DDD','1243c','aeEe'], 'num': [3, 4,7,8]})\n\ndf\n\n\n\n\n\n\n\n\n\n\ntext\nnum\n\n\n\n\n0\nabc\n3\n\n\n1\nDDD\n4\n\n\n2\n1243c\n7\n\n\n3\naeEe\n8\n\n\n\n\n\n\n\n\n\n14.1 upper case\n\n\nCode\ndf&gt;&gt; mutate(text_new=_.text.str.upper())\n\n\n\n\n\n\n\n\n\n\ntext\nnum\ntext_new\n\n\n\n\n0\nabc\n3\nABC\n\n\n1\nDDD\n4\nDDD\n\n\n2\n1243c\n7\n1243C\n\n\n3\naeEe\n8\nAEEE\n\n\n\n\n\n\n\n\n\n\n14.2 lower case\n\n\nCode\ndf&gt;&gt; mutate(text_new=_.text.str.lower())\n\n\n\n\n\n\n\n\n\n\ntext\nnum\ntext_new\n\n\n\n\n0\nabc\n3\nabc\n\n\n1\nDDD\n4\nddd\n\n\n2\n1243c\n7\n1243c\n\n\n3\naeEe\n8\naeee\n\n\n\n\n\n\n\n\n\n\n14.3 match\n\n\nCode\ndf&gt;&gt; mutate(text_new1=if_else(_.text== \"abc\",'T','F')\n            ,text_new2=if_else(_.text.str.startswith(\"a\"),'T','F')\n            ,text_new3=if_else(_.text.str.endswith(\"c\"),'T','F')\n            ,text_new4=if_else(_.text.str.contains(\"4\"),'T','F')\n\n)\n\n\n\n\n\n\n\n\n\n\ntext\nnum\ntext_new1\ntext_new2\ntext_new3\ntext_new4\n\n\n\n\n0\nabc\n3\nT\nT\nT\nF\n\n\n1\nDDD\n4\nF\nF\nF\nF\n\n\n2\n1243c\n7\nF\nF\nT\nT\n\n\n3\naeEe\n8\nF\nT\nF\nF\n\n\n\n\n\n\n\n\n\n\n14.4 concatenation\n\n\nCode\ndf&gt;&gt; mutate(text_new1=_.text+' is '+_.text\n)\n\n\n\n\n\n\n\n\n\n\ntext\nnum\ntext_new1\n\n\n\n\n0\nabc\n3\nabc is abc\n\n\n1\nDDD\n4\nDDD is DDD\n\n\n2\n1243c\n7\n1243c is 1243c\n\n\n3\naeEe\n8\naeEe is aeEe\n\n\n\n\n\n\n\n\n\n\n14.5 replace\nUse .str.replace(…, regex=True) with regular expressions to replace patterns in strings.\nFor example, the code below uses “p.”, where . is called a wildcard–which matches any character.\n\n\nCode\ndf&gt;&gt; mutate(text_new1=_.text.str.replace(\"a.\", \"XX\", regex=True)\n)\n\n\n\n\n\n\n\n\n\n\ntext\nnum\ntext_new1\n\n\n\n\n0\nabc\n3\nXXc\n\n\n1\nDDD\n4\nDDD\n\n\n2\n1243c\n7\n1243c\n\n\n3\naeEe\n8\nXXEe\n\n\n\n\n\n\n\n\n\n\n14.6 extract\nUse str.extract() with a regular expression to pull out a matching piece of text.\nFor example the regular expression “^(.*) ” contains the following pieces:\n\na matches the literal letter “a”\n.* has a . which matches anything, and * which modifies it to apply 0 or more times.\n\n\n\nCode\ndf&gt;&gt; mutate(text_new1=_.text.str.extract(\"a(.*)\")\n            ,text_new2=_.text.str.extract(\"(.*)c\")\n)\n\n\n\n\n\n\n\n\n\n\ntext\nnum\ntext_new1\ntext_new2\n\n\n\n\n0\nabc\n3\nbc\nab\n\n\n1\nDDD\n4\nNaN\nNaN\n\n\n2\n1243c\n7\nNaN\n1243\n\n\n3\naeEe\n8\neEe\nNaN",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/2 siuba.html#date",
    "href": "data manipulation/2 siuba.html#date",
    "title": "Data manipulation with siuba",
    "section": "15 date",
    "text": "15 date\n\n\nCode\ndf_dates = pd.DataFrame({\n    \"dates\": pd.to_datetime([\"2021-01-02\", \"2021-02-03\"]),\n    \"raw\": [\"2023-04-05 06:07:08\", \"2024-05-06 07:08:09\"],\n})\ndf_dates\n\n\n\n\n\n\n\n\n\n\ndates\nraw\n\n\n\n\n0\n2021-01-02\n2023-04-05 06:07:08\n\n\n1\n2021-02-03\n2024-05-06 07:08:09\n\n\n\n\n\n\n\n\n\n\nCode\nfrom datetime import datetime\n\ndf_date=df_dates&gt;&gt;mutate(month=_.dates.dt.month_name()\n                  ,date_format_raw = call(pd.to_datetime, _.raw)\n                  ,date_format_raw_year=_.date_format_raw.dt.year\n\n)\n\ndf_date\n\n\n\n\n\n\n\n\n\n\ndates\nraw\nmonth\ndate_format_raw\ndate_format_raw_year\n\n\n\n\n0\n2021-01-02\n2023-04-05 06:07:08\nJanuary\n2023-04-05 06:07:08\n2023\n\n\n1\n2021-02-03\n2024-05-06 07:08:09\nFebruary\n2024-05-06 07:08:09\n2024\n\n\n\n\n\n\n\n\n\n\nCode\ndf_date.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2 entries, 0 to 1\nData columns (total 5 columns):\n #   Column                Non-Null Count  Dtype         \n---  ------                --------------  -----         \n 0   dates                 2 non-null      datetime64[ns]\n 1   raw                   2 non-null      object        \n 2   month                 2 non-null      object        \n 3   date_format_raw       2 non-null      datetime64[ns]\n 4   date_format_raw_year  2 non-null      int32         \ndtypes: datetime64[ns](2), int32(1), object(2)\nmemory usage: 204.0+ bytes",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/2 siuba.html#comparison-with-different-python-dataframe-package",
    "href": "data manipulation/2 siuba.html#comparison-with-different-python-dataframe-package",
    "title": "Data manipulation with siuba",
    "section": "1 Comparison with different python dataframe package",
    "text": "1 Comparison with different python dataframe package",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "plot/3 plotly.html",
    "href": "plot/3 plotly.html",
    "title": "Plotly chart",
    "section": "",
    "text": "Plotly’s Python graphing library makes interactive, publication-quality graphs. Examples of how to make line plots, scatter plots, area charts, bar charts, error bars, box plots, histograms, heatmaps, subplots, multiple-axes, polar charts, and bubble charts.\nCode\nimport plotly\nprint(plotly.__version__)\n\n\n5.20.0\nCode\nfrom plotnine import *\nimport seaborn as sns\nimport plotly.express as px\nimport pandas as pd\n\n# Apply the default theme\n\n\n# Load an example dataset\ntips = sns.load_dataset(\"tips\")\ntips.head()\n\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4",
    "crumbs": [
      "Plot",
      "Plotly chart"
    ]
  },
  {
    "objectID": "plot/3 plotly.html#color-by-group",
    "href": "plot/3 plotly.html#color-by-group",
    "title": "Plotly chart",
    "section": "1.1 color by group",
    "text": "1.1 color by group\n\n\nCode\nfig = px.scatter(tips,x=\"tip\", y=\"total_bill\",color=\"sex\")\nfig.show()",
    "crumbs": [
      "Plot",
      "Plotly chart"
    ]
  },
  {
    "objectID": "plot/3 plotly.html#size-by-group",
    "href": "plot/3 plotly.html#size-by-group",
    "title": "Plotly chart",
    "section": "1.2 size by group",
    "text": "1.2 size by group\n\n\nCode\nfig = px.scatter(tips,x=\"tip\", y=\"total_bill\",size=\"size\")\nfig.show()",
    "crumbs": [
      "Plot",
      "Plotly chart"
    ]
  },
  {
    "objectID": "plot/3 plotly.html#color-by-group-1",
    "href": "plot/3 plotly.html#color-by-group-1",
    "title": "Plotly chart",
    "section": "2.1 color by group",
    "text": "2.1 color by group\n\n\nCode\nimport random\nfrom siuba import _, mutate, filter, group_by, summarize,show_query\nfrom siuba import *\n\ndowjones2=dowjones&gt;&gt;mutate(type='old')\n\ndowjones3=dowjones&gt;&gt;mutate(Price=_.Price+random.random()*200,type='new')\n\ndowjones4=pd.concat([dowjones2, dowjones3], ignore_index = True)&gt;&gt; arrange(_.Date)\n\ndf = px.data.gapminder().query(\"continent=='Oceania'\")\n\n\n\n\nCode\ndowjones4.head()\n\n\n\n\n\n\n\n\n\n\nDate\nPrice\ntype\n\n\n\n\n0\n1914-12-01\n55.000000\nold\n\n\n649\n1914-12-01\n174.717257\nnew\n\n\n1\n1915-01-01\n56.550000\nold\n\n\n650\n1915-01-01\n176.267257\nnew\n\n\n2\n1915-02-01\n56.000000\nold\n\n\n\n\n\n\n\n\n\n\nCode\nimport plotly.express as px\n\nfig = px.line(dowjones4, x=\"Date\", y=\"Price\", color='type')\nfig.show()",
    "crumbs": [
      "Plot",
      "Plotly chart"
    ]
  },
  {
    "objectID": "plot/3 plotly.html#color-by-group-2",
    "href": "plot/3 plotly.html#color-by-group-2",
    "title": "Plotly chart",
    "section": "3.1 color by group",
    "text": "3.1 color by group\n\n\nCode\nfig = px.histogram(tips, x=\"total_bill\", color='sex', barmode='group')\nfig.show()",
    "crumbs": [
      "Plot",
      "Plotly chart"
    ]
  },
  {
    "objectID": "plot/3 plotly.html#color-by-group-3",
    "href": "plot/3 plotly.html#color-by-group-3",
    "title": "Plotly chart",
    "section": "5.1 color by group",
    "text": "5.1 color by group\n\n\nCode\nfig = px.box(tips, y=\"total_bill\",x='sex',color='sex')\nfig.show()",
    "crumbs": [
      "Plot",
      "Plotly chart"
    ]
  },
  {
    "objectID": "plot/3 plotly.html#color-by-group-4",
    "href": "plot/3 plotly.html#color-by-group-4",
    "title": "Plotly chart",
    "section": "6.1 color by group",
    "text": "6.1 color by group\n\n\nCode\nfig = px.strip(tips,x=\"day\", y=\"total_bill\",color='sex')\nfig.show()",
    "crumbs": [
      "Plot",
      "Plotly chart"
    ]
  },
  {
    "objectID": "data manipulation/0 data structure in Python .html#singular",
    "href": "data manipulation/0 data structure in Python .html#singular",
    "title": "Data structure in Python",
    "section": "1.1 singular",
    "text": "1.1 singular\n\n\nCode\na=1\ntype(a)\n\n\nint\n\n\n\n\nCode\na=1.3\ntype(a)\n\n\nfloat\n\n\n\n\nCode\na='hell'\ntype(a)\n\n\nstr",
    "crumbs": [
      "data manipulation",
      "Data structure in Python"
    ]
  },
  {
    "objectID": "data manipulation/0 data structure in Python .html#list",
    "href": "data manipulation/0 data structure in Python .html#list",
    "title": "Data structure in Python",
    "section": "1.2 list",
    "text": "1.2 list\n\n\nCode\na=[1,2,3]\n\na\n\n\n[1, 2, 3]\n\n\n\n\nCode\ntype(a) \n\n\nlist\n\n\n\n\nCode\nfruits = ['orange', 'apple', 'pear', 'banana', 'kiwi', 'apple', 'banana','apple']\n\n\n\n1.2.1 find length of the list with len()\n\n\nCode\nlen(fruits)\n\n\n8\n\n\n\n\n1.2.2 find how many time in the list with count()\n\n\nCode\nfruits.count('apple')\n\n\n3\n\n\n\n\n1.2.3 find locaiton of on the list with index()\nshow the first ‘apple’ index. python list start at 0\n\n\nCode\nfruits.index('apple')\n\n\n1\n\n\nall ‘apple’ in the list\n\n\nCode\n[index for index, value in enumerate(fruits) if value == 'apple']\n\n\n[1, 5, 7]\n\n\n\n\n1.2.4 reverse the list\n\n\nCode\nfruits.reverse()\nfruits\n\n\n['apple', 'banana', 'apple', 'kiwi', 'banana', 'pear', 'apple', 'orange']\n\n\n\n\n1.2.5 sort the list\n\n\nCode\nfruits.sort()\nfruits\n\n\n['apple', 'apple', 'apple', 'banana', 'banana', 'kiwi', 'orange', 'pear']\n\n\n\n\n1.2.6 add element on the list\n\n\nCode\nfruits.append('grape')\nfruits\n\n\n['apple',\n 'apple',\n 'apple',\n 'banana',\n 'banana',\n 'kiwi',\n 'orange',\n 'pear',\n 'grape']\n\n\n\n\n1.2.7 drop last element\n\n\nCode\nfruits.pop()\n\nfruits\n\n\n['apple', 'apple', 'apple', 'banana', 'banana', 'kiwi', 'orange', 'pear']\n\n\n\n\n1.2.8 List Comprehensions\nusing loop:\n\n\nCode\nsquares = []\nfor x in range(10):\n  squares.append(x**2)\n  \nsquares\n\n\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n\n\nusing List Comprehensions\n\n\nCode\nsquares = [x**2 for x in range(10)]\nsquares\n\n\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]",
    "crumbs": [
      "data manipulation",
      "Data structure in Python"
    ]
  },
  {
    "objectID": "data manipulation/0 data structure in Python .html#reverse-the-list",
    "href": "data manipulation/0 data structure in Python .html#reverse-the-list",
    "title": "Data structure in Python",
    "section": "1.3 reverse the list",
    "text": "1.3 reverse the list\n\n\nCode\nfruits.reverse()\nfruits\n\n\n['apple', 'banana', 'apple', 'kiwi', 'banana', 'pear', 'apple', 'orange']\n\n\n\n1.3.1 sort the list\n\n\nCode\nfruits.sort()\nfruits\n\n\n['apple', 'apple', 'apple', 'banana', 'banana', 'kiwi', 'orange', 'pear']\n\n\n\n\n1.3.2 add element on the list\n\n\nCode\nfruits.append('grape')\nfruits\n\n\n['apple',\n 'apple',\n 'apple',\n 'banana',\n 'banana',\n 'kiwi',\n 'orange',\n 'pear',\n 'grape']",
    "crumbs": [
      "data manipulation",
      "Data structure in Python"
    ]
  },
  {
    "objectID": "data manipulation/0 data structure in Python .html#drop-last-element",
    "href": "data manipulation/0 data structure in Python .html#drop-last-element",
    "title": "Data structure in Python",
    "section": "1.4 drop last element",
    "text": "1.4 drop last element\n\n\nCode\nfruits.pop()\n\nfruits\n\n\n['apple', 'apple', 'apple', 'banana', 'banana', 'kiwi', 'orange', 'pear']\n\n\n\n1.4.1 List Comprehensions\nusing loop:\n\n\nCode\nsquares = []\nfor x in range(10):\n  squares.append(x**2)\n  \nsquares\n\n\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n\n\nusing List Comprehensions\n\n\nCode\nsquares = [x**2 for x in range(10)]\nsquares\n\n\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]",
    "crumbs": [
      "data manipulation",
      "Data structure in Python"
    ]
  },
  {
    "objectID": "data manipulation/0 data structure in Python .html#tuples",
    "href": "data manipulation/0 data structure in Python .html#tuples",
    "title": "Data structure in Python",
    "section": "1.3 Tuples",
    "text": "1.3 Tuples\n\n\nCode\nfruits = ('orange', 'apple', 'pear', 'banana', 'kiwi', 'apple', 'banana','apple')\n\nfruits\n\n\n('orange', 'apple', 'pear', 'banana', 'kiwi', 'apple', 'banana', 'apple')\n\n\n\n\nCode\ntype(fruits)\n\n\ntuple\n\n\ntuple can not be modified.",
    "crumbs": [
      "data manipulation",
      "Data structure in Python"
    ]
  },
  {
    "objectID": "data manipulation/0 data structure in Python .html#sets",
    "href": "data manipulation/0 data structure in Python .html#sets",
    "title": "Data structure in Python",
    "section": "1.4 Sets",
    "text": "1.4 Sets\nA set is an unordered collection with no duplicate elements.\n\n\nCode\nbasket = {'apple', 'orange', 'apple', 'pear', 'orange', 'banana'}\n\nbasket\n\n\n{'apple', 'banana', 'orange', 'pear'}\n\n\n\n\nCode\ntype(basket)\n\n\nset",
    "crumbs": [
      "data manipulation",
      "Data structure in Python"
    ]
  },
  {
    "objectID": "data manipulation/0 data structure in Python .html#dictionaries",
    "href": "data manipulation/0 data structure in Python .html#dictionaries",
    "title": "Data structure in Python",
    "section": "1.5 Dictionaries",
    "text": "1.5 Dictionaries\n\n\nCode\ntel = {'jack': 4098, 'sape': 4139}\n\ntel\n\n\n{'jack': 4098, 'sape': 4139}\n\n\n\n\nCode\ntype(tel)\n\n\ndict\n\n\n\n\nCode\ntel['jack']\n\n\n4098",
    "crumbs": [
      "data manipulation",
      "Data structure in Python"
    ]
  },
  {
    "objectID": "classification/1 decision tree and hyperparameter tuning.html",
    "href": "classification/1 decision tree and hyperparameter tuning.html",
    "title": "Decision tree and hyperparameter tuning",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nimport time",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/1 decision tree and hyperparameter tuning.html#input-data",
    "href": "classification/1 decision tree and hyperparameter tuning.html#input-data",
    "title": "Decision tree and hyperparameter tuning",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/1 decision tree and hyperparameter tuning.html#data-eda",
    "href": "classification/1 decision tree and hyperparameter tuning.html#data-eda",
    "title": "Decision tree and hyperparameter tuning",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n\n\n0.7420382165605095\n0.18890814558058924",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/1 decision tree and hyperparameter tuning.html#data-wrangling",
    "href": "classification/1 decision tree and hyperparameter tuning.html#data-wrangling",
    "title": "Decision tree and hyperparameter tuning",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1046 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1308 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Dealing with missing numerical variables\ndata['Age'] = data.Age.fillna(data.Age.median())\ndata['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1309 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1309 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\ndata = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\nSex_male\n\n\n\n\n0\n1\n3\nBraund, Mr. Owen Harris\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\nTrue\n\n\n1\n2\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\nFalse\n\n\n2\n3\n3\nHeikkinen, Miss. Laina\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\nFalse\n\n\n3\n4\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\nFalse\n\n\n4\n5\n3\nAllen, Mr. William Henry\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\nTrue\n\n\n\n\n\n\n\n\n\n\nCode\n# Select features columns\ndata = data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()\n\n\n\n\n\n\n\n\n\n\nSex_male\nFare\nAge\nPclass\nSibSp\nrole\n\n\n\n\n0\nTrue\n7.2500\n22.0\n3\n1\ntrain\n\n\n1\nFalse\n71.2833\n38.0\n1\n1\ntrain\n\n\n2\nFalse\n7.9250\n26.0\n3\n0\ntrain\n\n\n3\nFalse\n53.1000\n35.0\n1\n1\ntrain\n\n\n4\nTrue\n8.0500\n35.0\n3\n0\ntrain",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/1 decision tree and hyperparameter tuning.html#split-data",
    "href": "classification/1 decision tree and hyperparameter tuning.html#split-data",
    "title": "Decision tree and hyperparameter tuning",
    "section": "2.4 split data",
    "text": "2.4 split data\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\nX_train = X_train.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 712 entries, 384 to 855\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Sex_male  712 non-null    bool   \n 1   Fare      712 non-null    float64\n 2   Age       712 non-null    float64\n 3   Pclass    712 non-null    int64  \n 4   SibSp     712 non-null    int64  \ndtypes: bool(1), float64(2), int64(2)\nmemory usage: 28.5 KB",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/1 decision tree and hyperparameter tuning.html#define-model",
    "href": "classification/1 decision tree and hyperparameter tuning.html#define-model",
    "title": "Decision tree and hyperparameter tuning",
    "section": "3.1 define model",
    "text": "3.1 define model\n\n\nCode\nmodel_dt = DecisionTreeClassifier()   #model with deph 3\nmodel_dt\n\n\nDecisionTreeClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriNot fittedDecisionTreeClassifier() \n\n\ndefalt hyper-parameters:\n\n\nCode\nmodel_dt.get_params()\n\n\n{'ccp_alpha': 0.0,\n 'class_weight': None,\n 'criterion': 'gini',\n 'max_depth': None,\n 'max_features': None,\n 'max_leaf_nodes': None,\n 'min_impurity_decrease': 0.0,\n 'min_samples_leaf': 1,\n 'min_samples_split': 2,\n 'min_weight_fraction_leaf': 0.0,\n 'monotonic_cst': None,\n 'random_state': None,\n 'splitter': 'best'}",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/1 decision tree and hyperparameter tuning.html#define-parameters",
    "href": "classification/1 decision tree and hyperparameter tuning.html#define-parameters",
    "title": "Decision tree and hyperparameter tuning",
    "section": "3.2 define parameters",
    "text": "3.2 define parameters\n\n\nCode\nparameters = {'criterion':['gini','entropy'],\n              'max_depth':np.arange(1,21).tolist()[0::2],\n              'min_samples_split':np.arange(2,11).tolist()[0::2],\n              'max_leaf_nodes':np.arange(3,26).tolist()[0::2]}",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/1 decision tree and hyperparameter tuning.html#define-grids",
    "href": "classification/1 decision tree and hyperparameter tuning.html#define-grids",
    "title": "Decision tree and hyperparameter tuning",
    "section": "3.3 define GridS",
    "text": "3.3 define GridS\ndefault cv is 5\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\nG1=GridSearchCV(DecisionTreeClassifier(), parameters, cv=10, n_jobs=-1)",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/1 decision tree and hyperparameter tuning.html#train-model",
    "href": "classification/1 decision tree and hyperparameter tuning.html#train-model",
    "title": "Decision tree and hyperparameter tuning",
    "section": "3.4 train model",
    "text": "3.4 train model\n\n\nCode\nstart_time = time.time()\n\nG1.fit(X_train,Y_train)\n\nend_time = time.time()\n\nduration = end_time - start_time\nduration\n\n\n3.4475159645080566\n\n\nbest parameters\n\n\nCode\nG1.best_params_\n\n\n{'criterion': 'gini',\n 'max_depth': 5,\n 'max_leaf_nodes': 7,\n 'min_samples_split': 2}\n\n\nbest model\n\n\nCode\nmodel_dt = G1.best_estimator_\n\n\nvariable importance\n\n\nCode\nimportances = model_dt.feature_importances_\nvi=pd.DataFrame({\"variable\":X_train.columns,\"importances\":importances})\nvi=vi.sort_values('importances',ascending=False)\nvi\n\n\n\n\n\n\n\n\n\n\nvariable\nimportances\n\n\n\n\n0\nSex_male\n0.633276\n\n\n3\nPclass\n0.224033\n\n\n2\nAge\n0.054571\n\n\n1\nFare\n0.046240\n\n\n4\nSibSp\n0.041881",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/1 decision tree and hyperparameter tuning.html#preformance",
    "href": "classification/1 decision tree and hyperparameter tuning.html#preformance",
    "title": "Decision tree and hyperparameter tuning",
    "section": "3.5 Preformance",
    "text": "3.5 Preformance\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = model_dt.predict(X_test) #always gets x and retuns y\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_test,Y_pred_dt)  \naccuracy\n\n\n0.7877094972067039\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_test,Y_pred_dt)  \nprecision_dt\n\n\n0.8\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_test,Y_pred_dt)  \nrecall_dt\n\n\n0.6486486486486487\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_test,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[93, 12],\n       [26, 48]])\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_test, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.7671814671814672",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/1 decision tree and hyperparameter tuning.html#k-fold-cross-validation",
    "href": "classification/1 decision tree and hyperparameter tuning.html#k-fold-cross-validation",
    "title": "Decision tree and hyperparameter tuning",
    "section": "3.6 k-Fold Cross-Validation",
    "text": "3.6 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(model_dt, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.818831872352999",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/2 Decision Tree on house price data.html",
    "href": "regression/2 Decision Tree on house price data.html",
    "title": "Decision tree",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split",
    "crumbs": [
      "Regression",
      "Decision tree"
    ]
  },
  {
    "objectID": "regression/2 Decision Tree on house price data.html#input-data",
    "href": "regression/2 Decision Tree on house price data.html#input-data",
    "title": "Decision tree",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nId = df_train['Id']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nId\nMSSubClass\nMSZoning\nLotFrontage\nLotArea\nStreet\nAlley\nLotShape\nLandContour\nUtilities\n...\nPoolArea\nPoolQC\nFence\nMiscFeature\nMiscVal\nMoSold\nYrSold\nSaleType\nSaleCondition\nSalePrice\n\n\n\n\n0\n1\n60\nRL\n65.0\n8450\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n2\n2008\nWD\nNormal\n208500\n\n\n1\n2\n20\nRL\n80.0\n9600\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2007\nWD\nNormal\n181500\n\n\n2\n3\n60\nRL\n68.0\n11250\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n9\n2008\nWD\nNormal\n223500\n\n\n3\n4\n70\nRL\n60.0\n9550\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n2\n2006\nWD\nAbnorml\n140000\n\n\n4\n5\n60\nRL\n84.0\n14260\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n12\n2008\nWD\nNormal\n250000\n\n\n\n\n5 rows × 81 columns\n\n\n\n\n\n\nCode\n#df_train.info()\n\n\n\n\nCode\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['SalePrice'], axis=1), df_test])",
    "crumbs": [
      "Regression",
      "Decision tree"
    ]
  },
  {
    "objectID": "regression/2 Decision Tree on house price data.html#data-eda",
    "href": "regression/2 Decision Tree on house price data.html#data-eda",
    "title": "Decision tree",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\nin step 1",
    "crumbs": [
      "Regression",
      "Decision tree"
    ]
  },
  {
    "objectID": "regression/2 Decision Tree on house price data.html#data-wrangling",
    "href": "regression/2 Decision Tree on house price data.html#data-wrangling",
    "title": "Decision tree",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling",
    "crumbs": [
      "Regression",
      "Decision tree"
    ]
  },
  {
    "objectID": "regression/2 Decision Tree on house price data.html#split-data",
    "href": "regression/2 Decision Tree on house price data.html#split-data",
    "title": "Decision tree",
    "section": "2.4 split data",
    "text": "2.4 split data\n\n\nCode\nY = df_train.SalePrice\nX = df_train.drop(['SalePrice'], axis=1)\n\n\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\nX_train = X_train.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nprint(X_train.shape)\nprint(X_test.shape)\n\n\n(1168, 80)\n(292, 80)\n\n\n\n\nCode\nprint(Y_train.shape)\nprint(Y_test.shape)\n\n\n(1168,)\n(292,)\n\n\n\n\nCode\ncategorical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].nunique() &lt; 10 and X_train[cname].dtype == \"object\"]\n                    \n                    \nnumerical_cols = numerical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].dtype in ['int64', 'float64']]\n\n\n\n\nCode\nprint(\"The total number of categorical columns:\", len(categorical_cols))\nprint(\"The total number of numerical columns:\", len(numerical_cols))\n\n\nThe total number of categorical columns: 40\nThe total number of numerical columns: 37\n\n\n\n\nCode\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train[my_cols].copy()\nX_test= X_test[my_cols].copy()\n\n\nX_final = df_test[my_cols].copy()",
    "crumbs": [
      "Regression",
      "Decision tree"
    ]
  },
  {
    "objectID": "regression/2 Decision Tree on house price data.html#pipelines-for-data-preprocessing",
    "href": "regression/2 Decision Tree on house price data.html#pipelines-for-data-preprocessing",
    "title": "Decision tree",
    "section": "2.5 Pipelines for Data Preprocessing",
    "text": "2.5 Pipelines for Data Preprocessing\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\nCode\nnumerical_transformer = Pipeline(steps=[\n    ('imputer_num', SimpleImputer(strategy='median')), \n    ('scaler', StandardScaler())\n])\n\n\n\n\nCode\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n\n\nCode\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)])",
    "crumbs": [
      "Regression",
      "Decision tree"
    ]
  },
  {
    "objectID": "regression/2 Decision Tree on house price data.html#define-model",
    "href": "regression/2 Decision Tree on house price data.html#define-model",
    "title": "Decision tree",
    "section": "3.1 define model",
    "text": "3.1 define model\n\n\nCode\nml_model = DecisionTreeRegressor(random_state=0)\nml_model\n\n\nDecisionTreeRegressor(random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeRegressor?Documentation for DecisionTreeRegressoriNot fittedDecisionTreeRegressor(random_state=0)",
    "crumbs": [
      "Regression",
      "Decision tree"
    ]
  },
  {
    "objectID": "regression/2 Decision Tree on house price data.html#define-pipline",
    "href": "regression/2 Decision Tree on house price data.html#define-pipline",
    "title": "Decision tree",
    "section": "3.2 define pipline",
    "text": "3.2 define pipline\n\n\nCode\npipeline = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model_dt', ml_model)\n         ]\n)",
    "crumbs": [
      "Regression",
      "Decision tree"
    ]
  },
  {
    "objectID": "regression/2 Decision Tree on house price data.html#train-model",
    "href": "regression/2 Decision Tree on house price data.html#train-model",
    "title": "Decision tree",
    "section": "3.3 train model",
    "text": "3.3 train model\n\n\nCode\npipeline.fit(X_train, Y_train)\n\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer_num',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['Id', 'MSSubClass',\n                                                   'LotFrontage', 'LotArea',\n                                                   'OverallQual', 'OverallCond',\n                                                   'YearBuilt', 'YearRemodAdd',\n                                                   'MasVnrArea', 'BsmtFinSF1',\n                                                   'BsmtFinSF2', 'BsmtUnfSF',\n                                                   'TotalBsmtSF', '1stFlrSF',\n                                                   '2ndFlrSF'...\n                                                   'LotConfig', 'LandSlope',\n                                                   'Condition1', 'Condition2',\n                                                   'BldgType', 'HouseStyle',\n                                                   'RoofStyle', 'RoofMatl',\n                                                   'MasVnrType', 'ExterQual',\n                                                   'ExterCond', 'Foundation',\n                                                   'BsmtQual', 'BsmtCond',\n                                                   'BsmtExposure',\n                                                   'BsmtFinType1',\n                                                   'BsmtFinType2', 'Heating',\n                                                   'HeatingQC', 'CentralAir',\n                                                   'Electrical', 'KitchenQual',\n                                                   'Functional', 'FireplaceQu', ...])])),\n                ('model_dt', DecisionTreeRegressor(random_state=0))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiFittedPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer_num',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['Id', 'MSSubClass',\n                                                   'LotFrontage', 'LotArea',\n                                                   'OverallQual', 'OverallCond',\n                                                   'YearBuilt', 'YearRemodAdd',\n                                                   'MasVnrArea', 'BsmtFinSF1',\n                                                   'BsmtFinSF2', 'BsmtUnfSF',\n                                                   'TotalBsmtSF', '1stFlrSF',\n                                                   '2ndFlrSF'...\n                                                   'LotConfig', 'LandSlope',\n                                                   'Condition1', 'Condition2',\n                                                   'BldgType', 'HouseStyle',\n                                                   'RoofStyle', 'RoofMatl',\n                                                   'MasVnrType', 'ExterQual',\n                                                   'ExterCond', 'Foundation',\n                                                   'BsmtQual', 'BsmtCond',\n                                                   'BsmtExposure',\n                                                   'BsmtFinType1',\n                                                   'BsmtFinType2', 'Heating',\n                                                   'HeatingQC', 'CentralAir',\n                                                   'Electrical', 'KitchenQual',\n                                                   'Functional', 'FireplaceQu', ...])])),\n                ('model_dt', DecisionTreeRegressor(random_state=0))])  preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('imputer_num',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('scaler', StandardScaler())]),\n                                 ['Id', 'MSSubClass', 'LotFrontage', 'LotArea',\n                                  'OverallQual', 'OverallCond', 'YearBuilt',\n                                  'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1',\n                                  'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF',\n                                  '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n                                  'GrLivArea', 'Bsm...\n                                 ['MSZoning', 'Street', 'Alley', 'LotShape',\n                                  'LandContour', 'Utilities', 'LotConfig',\n                                  'LandSlope', 'Condition1', 'Condition2',\n                                  'BldgType', 'HouseStyle', 'RoofStyle',\n                                  'RoofMatl', 'MasVnrType', 'ExterQual',\n                                  'ExterCond', 'Foundation', 'BsmtQual',\n                                  'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n                                  'BsmtFinType2', 'Heating', 'HeatingQC',\n                                  'CentralAir', 'Electrical', 'KitchenQual',\n                                  'Functional', 'FireplaceQu', ...])]) num['Id', 'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='median')  StandardScaler?Documentation for StandardScalerStandardScaler() cat['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'SaleType', 'SaleCondition']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='most_frequent')  OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore')  DecisionTreeRegressor?Documentation for DecisionTreeRegressorDecisionTreeRegressor(random_state=0) \n\n\n\n\nCode\nfitted_model=pipeline.steps[1][1]\n\n\n\n\nCode\nvar=pipeline[:-1].get_feature_names_out()\nvar\n\n\narray(['num__Id', 'num__MSSubClass', 'num__LotFrontage', 'num__LotArea',\n       'num__OverallQual', 'num__OverallCond', 'num__YearBuilt',\n       'num__YearRemodAdd', 'num__MasVnrArea', 'num__BsmtFinSF1',\n       'num__BsmtFinSF2', 'num__BsmtUnfSF', 'num__TotalBsmtSF',\n       'num__1stFlrSF', 'num__2ndFlrSF', 'num__LowQualFinSF',\n       'num__GrLivArea', 'num__BsmtFullBath', 'num__BsmtHalfBath',\n       'num__FullBath', 'num__HalfBath', 'num__BedroomAbvGr',\n       'num__KitchenAbvGr', 'num__TotRmsAbvGrd', 'num__Fireplaces',\n       'num__GarageYrBlt', 'num__GarageCars', 'num__GarageArea',\n       'num__WoodDeckSF', 'num__OpenPorchSF', 'num__EnclosedPorch',\n       'num__3SsnPorch', 'num__ScreenPorch', 'num__PoolArea',\n       'num__MiscVal', 'num__MoSold', 'num__YrSold',\n       'cat__MSZoning_C (all)', 'cat__MSZoning_FV', 'cat__MSZoning_RH',\n       'cat__MSZoning_RL', 'cat__MSZoning_RM', 'cat__Street_Grvl',\n       'cat__Street_Pave', 'cat__Alley_Grvl', 'cat__Alley_Pave',\n       'cat__LotShape_IR1', 'cat__LotShape_IR2', 'cat__LotShape_IR3',\n       'cat__LotShape_Reg', 'cat__LandContour_Bnk',\n       'cat__LandContour_HLS', 'cat__LandContour_Low',\n       'cat__LandContour_Lvl', 'cat__Utilities_AllPub',\n       'cat__Utilities_NoSeWa', 'cat__LotConfig_Corner',\n       'cat__LotConfig_CulDSac', 'cat__LotConfig_FR2',\n       'cat__LotConfig_FR3', 'cat__LotConfig_Inside',\n       'cat__LandSlope_Gtl', 'cat__LandSlope_Mod', 'cat__LandSlope_Sev',\n       'cat__Condition1_Artery', 'cat__Condition1_Feedr',\n       'cat__Condition1_Norm', 'cat__Condition1_PosA',\n       'cat__Condition1_PosN', 'cat__Condition1_RRAe',\n       'cat__Condition1_RRAn', 'cat__Condition1_RRNe',\n       'cat__Condition1_RRNn', 'cat__Condition2_Artery',\n       'cat__Condition2_Feedr', 'cat__Condition2_Norm',\n       'cat__Condition2_PosA', 'cat__Condition2_PosN',\n       'cat__Condition2_RRAe', 'cat__Condition2_RRAn',\n       'cat__Condition2_RRNn', 'cat__BldgType_1Fam',\n       'cat__BldgType_2fmCon', 'cat__BldgType_Duplex',\n       'cat__BldgType_Twnhs', 'cat__BldgType_TwnhsE',\n       'cat__HouseStyle_1.5Fin', 'cat__HouseStyle_1.5Unf',\n       'cat__HouseStyle_1Story', 'cat__HouseStyle_2.5Fin',\n       'cat__HouseStyle_2.5Unf', 'cat__HouseStyle_2Story',\n       'cat__HouseStyle_SFoyer', 'cat__HouseStyle_SLvl',\n       'cat__RoofStyle_Flat', 'cat__RoofStyle_Gable',\n       'cat__RoofStyle_Gambrel', 'cat__RoofStyle_Hip',\n       'cat__RoofStyle_Mansard', 'cat__RoofStyle_Shed',\n       'cat__RoofMatl_ClyTile', 'cat__RoofMatl_CompShg',\n       'cat__RoofMatl_Membran', 'cat__RoofMatl_Metal',\n       'cat__RoofMatl_Roll', 'cat__RoofMatl_Tar&Grv',\n       'cat__RoofMatl_WdShake', 'cat__RoofMatl_WdShngl',\n       'cat__MasVnrType_BrkCmn', 'cat__MasVnrType_BrkFace',\n       'cat__MasVnrType_Stone', 'cat__ExterQual_Ex', 'cat__ExterQual_Fa',\n       'cat__ExterQual_Gd', 'cat__ExterQual_TA', 'cat__ExterCond_Ex',\n       'cat__ExterCond_Fa', 'cat__ExterCond_Gd', 'cat__ExterCond_Po',\n       'cat__ExterCond_TA', 'cat__Foundation_BrkTil',\n       'cat__Foundation_CBlock', 'cat__Foundation_PConc',\n       'cat__Foundation_Slab', 'cat__Foundation_Stone',\n       'cat__Foundation_Wood', 'cat__BsmtQual_Ex', 'cat__BsmtQual_Fa',\n       'cat__BsmtQual_Gd', 'cat__BsmtQual_TA', 'cat__BsmtCond_Fa',\n       'cat__BsmtCond_Gd', 'cat__BsmtCond_Po', 'cat__BsmtCond_TA',\n       'cat__BsmtExposure_Av', 'cat__BsmtExposure_Gd',\n       'cat__BsmtExposure_Mn', 'cat__BsmtExposure_No',\n       'cat__BsmtFinType1_ALQ', 'cat__BsmtFinType1_BLQ',\n       'cat__BsmtFinType1_GLQ', 'cat__BsmtFinType1_LwQ',\n       'cat__BsmtFinType1_Rec', 'cat__BsmtFinType1_Unf',\n       'cat__BsmtFinType2_ALQ', 'cat__BsmtFinType2_BLQ',\n       'cat__BsmtFinType2_GLQ', 'cat__BsmtFinType2_LwQ',\n       'cat__BsmtFinType2_Rec', 'cat__BsmtFinType2_Unf',\n       'cat__Heating_Floor', 'cat__Heating_GasA', 'cat__Heating_GasW',\n       'cat__Heating_Grav', 'cat__Heating_OthW', 'cat__Heating_Wall',\n       'cat__HeatingQC_Ex', 'cat__HeatingQC_Fa', 'cat__HeatingQC_Gd',\n       'cat__HeatingQC_Po', 'cat__HeatingQC_TA', 'cat__CentralAir_N',\n       'cat__CentralAir_Y', 'cat__Electrical_FuseA',\n       'cat__Electrical_FuseF', 'cat__Electrical_FuseP',\n       'cat__Electrical_Mix', 'cat__Electrical_SBrkr',\n       'cat__KitchenQual_Ex', 'cat__KitchenQual_Fa',\n       'cat__KitchenQual_Gd', 'cat__KitchenQual_TA',\n       'cat__Functional_Maj1', 'cat__Functional_Maj2',\n       'cat__Functional_Min1', 'cat__Functional_Min2',\n       'cat__Functional_Mod', 'cat__Functional_Typ',\n       'cat__FireplaceQu_Ex', 'cat__FireplaceQu_Fa',\n       'cat__FireplaceQu_Gd', 'cat__FireplaceQu_Po',\n       'cat__FireplaceQu_TA', 'cat__GarageType_2Types',\n       'cat__GarageType_Attchd', 'cat__GarageType_Basment',\n       'cat__GarageType_BuiltIn', 'cat__GarageType_CarPort',\n       'cat__GarageType_Detchd', 'cat__GarageFinish_Fin',\n       'cat__GarageFinish_RFn', 'cat__GarageFinish_Unf',\n       'cat__GarageQual_Ex', 'cat__GarageQual_Fa', 'cat__GarageQual_Gd',\n       'cat__GarageQual_Po', 'cat__GarageQual_TA', 'cat__GarageCond_Ex',\n       'cat__GarageCond_Fa', 'cat__GarageCond_Gd', 'cat__GarageCond_Po',\n       'cat__GarageCond_TA', 'cat__PavedDrive_N', 'cat__PavedDrive_P',\n       'cat__PavedDrive_Y', 'cat__PoolQC_Ex', 'cat__PoolQC_Fa',\n       'cat__PoolQC_Gd', 'cat__Fence_GdPrv', 'cat__Fence_GdWo',\n       'cat__Fence_MnPrv', 'cat__Fence_MnWw', 'cat__MiscFeature_Gar2',\n       'cat__MiscFeature_Othr', 'cat__MiscFeature_Shed',\n       'cat__MiscFeature_TenC', 'cat__SaleType_COD', 'cat__SaleType_CWD',\n       'cat__SaleType_Con', 'cat__SaleType_ConLD', 'cat__SaleType_ConLI',\n       'cat__SaleType_ConLw', 'cat__SaleType_New', 'cat__SaleType_Oth',\n       'cat__SaleType_WD', 'cat__SaleCondition_Abnorml',\n       'cat__SaleCondition_AdjLand', 'cat__SaleCondition_Alloca',\n       'cat__SaleCondition_Family', 'cat__SaleCondition_Normal',\n       'cat__SaleCondition_Partial'], dtype=object)\n\n\nvariable importance\n\n\nCode\nimportances = fitted_model.feature_importances_\nvi=pd.DataFrame({\"variable\":var,\"importances\":importances})\nvi=vi.sort_values('importances',ascending=False)\nvi\n\n\n\n\n\n\n\n\n\n\nvariable\nimportances\n\n\n\n\n4\nnum__OverallQual\n0.629176\n\n\n16\nnum__GrLivArea\n0.095509\n\n\n9\nnum__BsmtFinSF1\n0.039396\n\n\n12\nnum__TotalBsmtSF\n0.035653\n\n\n7\nnum__YearRemodAdd\n0.027875\n\n\n...\n...\n...\n\n\n68\ncat__Condition1_PosN\n0.000000\n\n\n132\ncat__BsmtCond_Po\n0.000000\n\n\n130\ncat__BsmtCond_Fa\n0.000000\n\n\n69\ncat__Condition1_RRAe\n0.000000\n\n\n115\ncat__ExterCond_Ex\n0.000000\n\n\n\n\n231 rows × 2 columns",
    "crumbs": [
      "Regression",
      "Decision tree"
    ]
  },
  {
    "objectID": "regression/2 Decision Tree on house price data.html#preformance",
    "href": "regression/2 Decision Tree on house price data.html#preformance",
    "title": "Decision tree",
    "section": "3.4 Preformance",
    "text": "3.4 Preformance\n\n\nCode\nY_pred_dt =pipeline.predict(X_test) #always gets x and retuns y\n\n\nR 2\n\n\nCode\nfrom sklearn.metrics import r2_score\nr2_score(Y_test, Y_pred_dt)\n\n\n0.6555524431677492\n\n\nMAE\n\n\nCode\nfrom sklearn.metrics import mean_absolute_error\nmean_absolute_error(Y_test, Y_pred_dt)\n\n\n28831.33219178082\n\n\nRMSE\n\n\nCode\nfrom  math import sqrt\nfrom sklearn.metrics import mean_squared_error\nmse=mean_squared_error(Y_test, Y_pred_dt)\nrmse=sqrt(mse)\nrmse\n\n\n49771.861315729315",
    "crumbs": [
      "Regression",
      "Decision tree"
    ]
  },
  {
    "objectID": "regression/2 Decision Tree on house price data.html#k-fold-cross-validation",
    "href": "regression/2 Decision Tree on house price data.html#k-fold-cross-validation",
    "title": "Decision tree",
    "section": "3.5 k-Fold Cross-Validation",
    "text": "3.5 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \n\n\n\n\nCode\ncv_dt = cross_val_score(pipeline, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.6726918132634618\n\n\n\n\nCode\ncv_dt = cross_val_score(pipeline, X_train, Y_train, cv=kf_dt,scoring = 'neg_mean_squared_error')\nnp.mean(np.sqrt(np.abs(cv_dt)))\n\n\n42307.68442876806",
    "crumbs": [
      "Regression",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification/1.2 decision tree and hyperparameter tuning.html",
    "href": "classification/1.2 decision tree and hyperparameter tuning.html",
    "title": "Decision tree and hyperparameter tuning",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nimport time",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/1.2 decision tree and hyperparameter tuning.html#input-data",
    "href": "classification/1.2 decision tree and hyperparameter tuning.html#input-data",
    "title": "Decision tree and hyperparameter tuning",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/1.2 decision tree and hyperparameter tuning.html#data-eda",
    "href": "classification/1.2 decision tree and hyperparameter tuning.html#data-eda",
    "title": "Decision tree and hyperparameter tuning",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n\n\n0.7420382165605095\n0.18890814558058924",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/1.2 decision tree and hyperparameter tuning.html#data-wrangling",
    "href": "classification/1.2 decision tree and hyperparameter tuning.html#data-wrangling",
    "title": "Decision tree and hyperparameter tuning",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1046 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1308 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Dealing with missing numerical variables\ndata['Age'] = data.Age.fillna(data.Age.median())\ndata['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1309 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1309 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\ndata = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\nSex_male\n\n\n\n\n0\n1\n3\nBraund, Mr. Owen Harris\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\nTrue\n\n\n1\n2\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\nFalse\n\n\n2\n3\n3\nHeikkinen, Miss. Laina\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\nFalse\n\n\n3\n4\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\nFalse\n\n\n4\n5\n3\nAllen, Mr. William Henry\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\nTrue\n\n\n\n\n\n\n\n\n\n\nCode\n# Select features columns\ndata = data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()\n\n\n\n\n\n\n\n\n\n\nSex_male\nFare\nAge\nPclass\nSibSp\nrole\n\n\n\n\n0\nTrue\n7.2500\n22.0\n3\n1\ntrain\n\n\n1\nFalse\n71.2833\n38.0\n1\n1\ntrain\n\n\n2\nFalse\n7.9250\n26.0\n3\n0\ntrain\n\n\n3\nFalse\n53.1000\n35.0\n1\n1\ntrain\n\n\n4\nTrue\n8.0500\n35.0\n3\n0\ntrain",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/1.2 decision tree and hyperparameter tuning.html#split-data",
    "href": "classification/1.2 decision tree and hyperparameter tuning.html#split-data",
    "title": "Decision tree and hyperparameter tuning",
    "section": "2.4 split data",
    "text": "2.4 split data\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\nX_train = X_train.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 712 entries, 320 to 518\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Sex_male  712 non-null    bool   \n 1   Fare      712 non-null    float64\n 2   Age       712 non-null    float64\n 3   Pclass    712 non-null    int64  \n 4   SibSp     712 non-null    int64  \ndtypes: bool(1), float64(2), int64(2)\nmemory usage: 28.5 KB",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/1.2 decision tree and hyperparameter tuning.html#define-model",
    "href": "classification/1.2 decision tree and hyperparameter tuning.html#define-model",
    "title": "Decision tree and hyperparameter tuning",
    "section": "3.1 define model",
    "text": "3.1 define model\n\n\nCode\nmodel_dt = DecisionTreeClassifier()   #model with deph 3\nmodel_dt\n\n\nDecisionTreeClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriNot fittedDecisionTreeClassifier() \n\n\ndefalt hyper-parameters:\n\n\nCode\nmodel_dt.get_params()\n\n\n{'ccp_alpha': 0.0,\n 'class_weight': None,\n 'criterion': 'gini',\n 'max_depth': None,\n 'max_features': None,\n 'max_leaf_nodes': None,\n 'min_impurity_decrease': 0.0,\n 'min_samples_leaf': 1,\n 'min_samples_split': 2,\n 'min_weight_fraction_leaf': 0.0,\n 'monotonic_cst': None,\n 'random_state': None,\n 'splitter': 'best'}",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/1.2 decision tree and hyperparameter tuning.html#define-parameters",
    "href": "classification/1.2 decision tree and hyperparameter tuning.html#define-parameters",
    "title": "Decision tree and hyperparameter tuning",
    "section": "3.2 define parameters",
    "text": "3.2 define parameters\n\n\nCode\nparameters = {'criterion':['gini','entropy'],\n              'max_depth':np.arange(1,21).tolist()[0::2],\n              'min_samples_split':np.arange(2,11).tolist()[0::2],\n              'max_leaf_nodes':np.arange(3,26).tolist()[0::2]}\n\n\nall parameters combinations\n\n\nCode\nimport itertools\na = parameters.values()\ncombinations = list(itertools.product(*a))\n\n\n1200 combinations\n\n\nCode\nlen(combinations)\n\n\n1200\n\n\n\n\nCode\ncombinations[0:5]\n\n\n[('gini', 1, 2, 3),\n ('gini', 1, 2, 5),\n ('gini', 1, 2, 7),\n ('gini', 1, 2, 9),\n ('gini', 1, 2, 11)]",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/1.2 decision tree and hyperparameter tuning.html#define-grids",
    "href": "classification/1.2 decision tree and hyperparameter tuning.html#define-grids",
    "title": "Decision tree and hyperparameter tuning",
    "section": "3.3 define GridS",
    "text": "3.3 define GridS\n\n3.3.1 GridSearchCV\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\n\n# default cv is 5\nG1=GridSearchCV(DecisionTreeClassifier()\n                ,parameters\n                ,scoring='accuracy'\n                , cv=10, n_jobs=-1)\n\n\n\n\n3.3.2 RandomizedSearchCV\nmake a very small RandomizedSearch with 3 parameters combinations\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV\nG2=RandomizedSearchCV(DecisionTreeClassifier()\n                      ,parameters\n                    ,scoring='accuracy'\n                    ,n_iter = 3, cv=10, n_jobs=-1)",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/1.2 decision tree and hyperparameter tuning.html#train-model",
    "href": "classification/1.2 decision tree and hyperparameter tuning.html#train-model",
    "title": "Decision tree and hyperparameter tuning",
    "section": "3.4 train model",
    "text": "3.4 train model\n\n3.4.1 train GridSearchCV\n\n\nCode\nstart_time = time.time()\n\nG1.fit(X_train,Y_train)\n\nend_time = time.time()\nduration = end_time - start_time\nduration\n\n\n5.29243016242981\n\n\n\n\n3.4.2 train RandomizedSearchCV\n\n\nCode\nstart_time = time.time()\n\nG2.fit(X_train,Y_train)\n\nend_time = time.time()\nduration = end_time - start_time\nduration\n\n\n0.02891397476196289\n\n\n\n3.4.2.1 tunning result\n\n3.4.2.1.1 GridSearchCV\n\n\nCode\n# get the parameter names\ncolumn_results = [f\"param_{name}\" for name in parameters.keys()]\ncolumn_results += [\"mean_test_score\", \"std_test_score\", \"rank_test_score\"]\n\ncv_results = pd.DataFrame(G1.cv_results_)\ncv_results = cv_results[column_results].sort_values(\n    \"mean_test_score\", ascending=False\n)\n\n\ndef shorten_param(param_name):\n    if \"__\" in param_name:\n        return param_name.rsplit(\"__\", 1)[1]\n    return param_name\n\n\ncv_results = cv_results.rename(shorten_param, axis=1)\ncv_results.head()\n\n\n\n\n\n\n\n\n\n\nparam_criterion\nparam_max_depth\nparam_min_samples_split\nparam_max_leaf_nodes\nmean_test_score\nstd_test_score\nrank_test_score\n\n\n\n\n328\ngini\n11\n8\n13\n0.837089\n0.038231\n1\n\n\n506\ngini\n17\n4\n13\n0.837089\n0.038231\n1\n\n\n446\ngini\n15\n4\n13\n0.837089\n0.038231\n1\n\n\n387\ngini\n13\n6\n13\n0.837089\n0.038231\n1\n\n\n386\ngini\n13\n4\n13\n0.837089\n0.038231\n1\n\n\n\n\n\n\n\n\n\n\n3.4.2.1.2 RandomizedSearchCV\n\n\nCode\n# get the parameter names\ncolumn_results = [f\"param_{name}\" for name in parameters.keys()]\ncolumn_results += [\"mean_test_score\", \"std_test_score\", \"rank_test_score\"]\n\ncv_results = pd.DataFrame(G2.cv_results_)\ncv_results = cv_results[column_results].sort_values(\n    \"mean_test_score\", ascending=False\n)\n\n\ndef shorten_param(param_name):\n    if \"__\" in param_name:\n        return param_name.rsplit(\"__\", 1)[1]\n    return param_name\n\n\ncv_results = cv_results.rename(shorten_param, axis=1)\ncv_results.head()\n\n\n\n\n\n\n\n\n\n\nparam_criterion\nparam_max_depth\nparam_min_samples_split\nparam_max_leaf_nodes\nmean_test_score\nstd_test_score\nrank_test_score\n\n\n\n\n0\nentropy\n13\n8\n21\n0.830067\n0.037145\n1\n\n\n2\nentropy\n17\n8\n23\n0.827250\n0.038026\n2\n\n\n1\ngini\n1\n2\n13\n0.787989\n0.029869\n3\n\n\n\n\n\n\n\n\n\n\n\n3.4.2.2 tunning best parameters\n\n\nCode\nG1.best_params_\n\n\n{'criterion': 'gini',\n 'max_depth': 7,\n 'max_leaf_nodes': 13,\n 'min_samples_split': 2}\n\n\n\n\nCode\nG2.best_params_\n\n\n{'min_samples_split': 8,\n 'max_leaf_nodes': 21,\n 'max_depth': 13,\n 'criterion': 'entropy'}\n\n\n\n\n3.4.2.3 tunning best model\n\n\nCode\nmodel_dt = G1.best_estimator_\n\n\n\n\nCode\nmodel_dt2 = G2.best_estimator_\n\n\n\n\n3.4.2.4 variable importance\n\n\nCode\nimportances = model_dt.feature_importances_\nvi=pd.DataFrame({\"variable\":X_train.columns,\"importances\":importances})\nvi=vi.sort_values('importances',ascending=False)\nvi\n\n\n\n\n\n\n\n\n\n\nvariable\nimportances\n\n\n\n\n0\nSex_male\n0.525262\n\n\n3\nPclass\n0.141914\n\n\n2\nAge\n0.134637\n\n\n1\nFare\n0.121324\n\n\n4\nSibSp\n0.076863",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/1.2 decision tree and hyperparameter tuning.html#preformance",
    "href": "classification/1.2 decision tree and hyperparameter tuning.html#preformance",
    "title": "Decision tree and hyperparameter tuning",
    "section": "3.5 Preformance",
    "text": "3.5 Preformance\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = model_dt.predict(X_test) #always gets x and retuns y\nY_pred_dt2 = model_dt2.predict(X_test) #always gets x and retuns y\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_test,Y_pred_dt)  \naccuracy\n\n\n0.770949720670391\n\n\n\n\nCode\naccuracy2 = metrics.accuracy_score(Y_test,Y_pred_dt2)  \naccuracy2\n\n\n0.7541899441340782\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_test,Y_pred_dt)  \nprecision_dt\n\n\n0.8301886792452831\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_test,Y_pred_dt)  \nrecall_dt\n\n\n0.5789473684210527\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_test,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[94,  9],\n       [32, 44]])\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_test, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.7457843638221768",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/1.2 decision tree and hyperparameter tuning.html#k-fold-cross-validation",
    "href": "classification/1.2 decision tree and hyperparameter tuning.html#k-fold-cross-validation",
    "title": "Decision tree and hyperparameter tuning",
    "section": "3.6 k-Fold Cross-Validation",
    "text": "3.6 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(model_dt, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.8216487737614498",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/1.1 decision tree on titanic data.html",
    "href": "classification/1.1 decision tree on titanic data.html",
    "title": "Decision tree",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split",
    "crumbs": [
      "Classification",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification/1.1 decision tree on titanic data.html#input-data",
    "href": "classification/1.1 decision tree on titanic data.html#input-data",
    "title": "Decision tree",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification/1.1 decision tree on titanic data.html#data-eda",
    "href": "classification/1.1 decision tree on titanic data.html#data-eda",
    "title": "Decision tree",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n\n\n0.7420382165605095\n0.18890814558058924",
    "crumbs": [
      "Classification",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification/1.1 decision tree on titanic data.html#data-wrangling",
    "href": "classification/1.1 decision tree on titanic data.html#data-wrangling",
    "title": "Decision tree",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1046 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1308 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Dealing with missing numerical variables\ndata['Age'] = data.Age.fillna(data.Age.median())\ndata['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1309 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1309 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\ndata = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\nSex_male\n\n\n\n\n0\n1\n3\nBraund, Mr. Owen Harris\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\nTrue\n\n\n1\n2\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\nFalse\n\n\n2\n3\n3\nHeikkinen, Miss. Laina\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\nFalse\n\n\n3\n4\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\nFalse\n\n\n4\n5\n3\nAllen, Mr. William Henry\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\nTrue\n\n\n\n\n\n\n\n\n\n\nCode\n# Select features columns\ndata = data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()\n\n\n\n\n\n\n\n\n\n\nSex_male\nFare\nAge\nPclass\nSibSp\nrole\n\n\n\n\n0\nTrue\n7.2500\n22.0\n3\n1\ntrain\n\n\n1\nFalse\n71.2833\n38.0\n1\n1\ntrain\n\n\n2\nFalse\n7.9250\n26.0\n3\n0\ntrain\n\n\n3\nFalse\n53.1000\n35.0\n1\n1\ntrain\n\n\n4\nTrue\n8.0500\n35.0\n3\n0\ntrain",
    "crumbs": [
      "Classification",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification/1.1 decision tree on titanic data.html#split-data",
    "href": "classification/1.1 decision tree on titanic data.html#split-data",
    "title": "Decision tree",
    "section": "2.4 split data",
    "text": "2.4 split data\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\nX_train = X_train.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 712 entries, 330 to 466\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Sex_male  712 non-null    bool   \n 1   Fare      712 non-null    float64\n 2   Age       712 non-null    float64\n 3   Pclass    712 non-null    int64  \n 4   SibSp     712 non-null    int64  \ndtypes: bool(1), float64(2), int64(2)\nmemory usage: 28.5 KB",
    "crumbs": [
      "Classification",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification/1.1 decision tree on titanic data.html#define-model",
    "href": "classification/1.1 decision tree on titanic data.html#define-model",
    "title": "Decision tree",
    "section": "3.1 define model",
    "text": "3.1 define model\n\n\nCode\nmodel_dt = tree.DecisionTreeClassifier(max_depth=3)   #model with deph 3\nmodel_dt\n\n\nDecisionTreeClassifier(max_depth=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriNot fittedDecisionTreeClassifier(max_depth=3)",
    "crumbs": [
      "Classification",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification/1.1 decision tree on titanic data.html#train-model",
    "href": "classification/1.1 decision tree on titanic data.html#train-model",
    "title": "Decision tree",
    "section": "3.2 train model",
    "text": "3.2 train model\n\n\nCode\nmodel_dt.fit(X_train,Y_train)\n\n\nDecisionTreeClassifier(max_depth=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier(max_depth=3) \n\n\nvariable importance\n\n\nCode\nimportances = model_dt.feature_importances_\nvi=pd.DataFrame({\"variable\":X_train.columns,\"importances\":importances})\nvi=vi.sort_values('importances',ascending=False)\nvi\n\n\n\n\n\n\n\n\n\n\nvariable\nimportances\n\n\n\n\n0\nSex_male\n0.637694\n\n\n3\nPclass\n0.198801\n\n\n2\nAge\n0.071437\n\n\n1\nFare\n0.071284\n\n\n4\nSibSp\n0.020785",
    "crumbs": [
      "Classification",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification/1.1 decision tree on titanic data.html#preformance",
    "href": "classification/1.1 decision tree on titanic data.html#preformance",
    "title": "Decision tree",
    "section": "3.3 Preformance",
    "text": "3.3 Preformance\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = model_dt.predict(X_test) #always gets x and retuns y\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_test,Y_pred_dt)  \naccuracy\n\n\n0.8156424581005587\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_test,Y_pred_dt)  \nprecision_dt\n\n\n0.7887323943661971\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_test,Y_pred_dt)  \nrecall_dt\n\n\n0.7567567567567568\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_test,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[90, 15],\n       [18, 56]])\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_test, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.806949806949807",
    "crumbs": [
      "Classification",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification/1.1 decision tree on titanic data.html#k-fold-cross-validation",
    "href": "classification/1.1 decision tree on titanic data.html#k-fold-cross-validation",
    "title": "Decision tree",
    "section": "3.4 k-Fold Cross-Validation",
    "text": "3.4 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(model_dt, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.8160445188614203",
    "crumbs": [
      "Classification",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification/4 Support Vector Machines on titanic data.html",
    "href": "classification/4 Support Vector Machines on titanic data.html",
    "title": "Support Vector Machines",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split",
    "crumbs": [
      "Classification",
      "Support Vector Machines"
    ]
  },
  {
    "objectID": "classification/4 Support Vector Machines on titanic data.html#input-data",
    "href": "classification/4 Support Vector Machines on titanic data.html#input-data",
    "title": "Support Vector Machines",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "Support Vector Machines"
    ]
  },
  {
    "objectID": "classification/4 Support Vector Machines on titanic data.html#data-eda",
    "href": "classification/4 Support Vector Machines on titanic data.html#data-eda",
    "title": "Support Vector Machines",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n\n\n0.7420382165605095\n0.18890814558058924",
    "crumbs": [
      "Classification",
      "Support Vector Machines"
    ]
  },
  {
    "objectID": "classification/4 Support Vector Machines on titanic data.html#data-wrangling",
    "href": "classification/4 Support Vector Machines on titanic data.html#data-wrangling",
    "title": "Support Vector Machines",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1046 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1308 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Dealing with missing numerical variables\ndata['Age'] = data.Age.fillna(data.Age.median())\ndata['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1309 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1309 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\ndata = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\nSex_male\n\n\n\n\n0\n1\n3\nBraund, Mr. Owen Harris\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\nTrue\n\n\n1\n2\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\nFalse\n\n\n2\n3\n3\nHeikkinen, Miss. Laina\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\nFalse\n\n\n3\n4\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\nFalse\n\n\n4\n5\n3\nAllen, Mr. William Henry\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\nTrue\n\n\n\n\n\n\n\n\n\n\nCode\n# Select features columns\ndata = data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()\n\n\n\n\n\n\n\n\n\n\nSex_male\nFare\nAge\nPclass\nSibSp\nrole\n\n\n\n\n0\nTrue\n7.2500\n22.0\n3\n1\ntrain\n\n\n1\nFalse\n71.2833\n38.0\n1\n1\ntrain\n\n\n2\nFalse\n7.9250\n26.0\n3\n0\ntrain\n\n\n3\nFalse\n53.1000\n35.0\n1\n1\ntrain\n\n\n4\nTrue\n8.0500\n35.0\n3\n0\ntrain",
    "crumbs": [
      "Classification",
      "Support Vector Machines"
    ]
  },
  {
    "objectID": "classification/4 Support Vector Machines on titanic data.html#split-data",
    "href": "classification/4 Support Vector Machines on titanic data.html#split-data",
    "title": "Support Vector Machines",
    "section": "2.4 split data",
    "text": "2.4 split data\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\nX_train = X_train.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 712 entries, 65 to 380\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Sex_male  712 non-null    bool   \n 1   Fare      712 non-null    float64\n 2   Age       712 non-null    float64\n 3   Pclass    712 non-null    int64  \n 4   SibSp     712 non-null    int64  \ndtypes: bool(1), float64(2), int64(2)\nmemory usage: 28.5 KB",
    "crumbs": [
      "Classification",
      "Support Vector Machines"
    ]
  },
  {
    "objectID": "classification/4 Support Vector Machines on titanic data.html#define-model",
    "href": "classification/4 Support Vector Machines on titanic data.html#define-model",
    "title": "Support Vector Machines",
    "section": "3.1 define model",
    "text": "3.1 define model\nThe solvers implemented in the class Logistic Regression are “liblinear”, “newton-cg”, “lbfgs”, “sag” and “saga”. According to Scikit Documentation: The “liblinear” solver was the one used by default for historical reasons before version 0.22. Since then, default use is lbfgs Algorithm.\n\n\nCode\nfrom sklearn import svm\nml_model = svm.SVC(kernel=\"linear\")\nml_model\n\n\nSVC(kernel='linear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  SVC?Documentation for SVCiNot fittedSVC(kernel='linear')",
    "crumbs": [
      "Classification",
      "Support Vector Machines"
    ]
  },
  {
    "objectID": "classification/4 Support Vector Machines on titanic data.html#train-model",
    "href": "classification/4 Support Vector Machines on titanic data.html#train-model",
    "title": "Support Vector Machines",
    "section": "3.2 train model",
    "text": "3.2 train model\n\n\nCode\nml_model.fit(X_train,Y_train)\n\n\nSVC(kernel='linear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  SVC?Documentation for SVCiFittedSVC(kernel='linear')",
    "crumbs": [
      "Classification",
      "Support Vector Machines"
    ]
  },
  {
    "objectID": "classification/4 Support Vector Machines on titanic data.html#preformance",
    "href": "classification/4 Support Vector Machines on titanic data.html#preformance",
    "title": "Support Vector Machines",
    "section": "3.3 Preformance",
    "text": "3.3 Preformance\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = ml_model.predict(X_test) #always gets x and retuns y\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_test,Y_pred_dt)  \naccuracy\n\n\n0.7653631284916201\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_test,Y_pred_dt)  \nprecision_dt\n\n\n0.7\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_test,Y_pred_dt)  \nrecall_dt\n\n\n0.6363636363636364\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_test,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[95, 18],\n       [24, 42]])\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_test, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.7385358004827031",
    "crumbs": [
      "Classification",
      "Support Vector Machines"
    ]
  },
  {
    "objectID": "classification/4 Support Vector Machines on titanic data.html#k-fold-cross-validation",
    "href": "classification/4 Support Vector Machines on titanic data.html#k-fold-cross-validation",
    "title": "Support Vector Machines",
    "section": "3.4 k-Fold Cross-Validation",
    "text": "3.4 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(ml_model, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.7921107061952132",
    "crumbs": [
      "Classification",
      "Support Vector Machines"
    ]
  },
  {
    "objectID": "classification/5 KNN on titanic data.html",
    "href": "classification/5 KNN on titanic data.html",
    "title": "K-Nearest Neighbors",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split",
    "crumbs": [
      "Classification",
      "K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "classification/5 KNN on titanic data.html#input-data",
    "href": "classification/5 KNN on titanic data.html#input-data",
    "title": "K-Nearest Neighbors",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "classification/5 KNN on titanic data.html#data-eda",
    "href": "classification/5 KNN on titanic data.html#data-eda",
    "title": "K-Nearest Neighbors",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n\n\n0.7420382165605095\n0.18890814558058924",
    "crumbs": [
      "Classification",
      "K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "classification/5 KNN on titanic data.html#data-wrangling",
    "href": "classification/5 KNN on titanic data.html#data-wrangling",
    "title": "K-Nearest Neighbors",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1046 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1308 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Dealing with missing numerical variables\ndata['Age'] = data.Age.fillna(data.Age.median())\ndata['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1309 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1309 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\ndata = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\nSex_male\n\n\n\n\n0\n1\n3\nBraund, Mr. Owen Harris\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\nTrue\n\n\n1\n2\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\nFalse\n\n\n2\n3\n3\nHeikkinen, Miss. Laina\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\nFalse\n\n\n3\n4\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\nFalse\n\n\n4\n5\n3\nAllen, Mr. William Henry\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\nTrue\n\n\n\n\n\n\n\n\n\n\nCode\n# Select features columns\ndata = data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()\n\n\n\n\n\n\n\n\n\n\nSex_male\nFare\nAge\nPclass\nSibSp\nrole\n\n\n\n\n0\nTrue\n7.2500\n22.0\n3\n1\ntrain\n\n\n1\nFalse\n71.2833\n38.0\n1\n1\ntrain\n\n\n2\nFalse\n7.9250\n26.0\n3\n0\ntrain\n\n\n3\nFalse\n53.1000\n35.0\n1\n1\ntrain\n\n\n4\nTrue\n8.0500\n35.0\n3\n0\ntrain",
    "crumbs": [
      "Classification",
      "K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "classification/5 KNN on titanic data.html#split-data",
    "href": "classification/5 KNN on titanic data.html#split-data",
    "title": "K-Nearest Neighbors",
    "section": "2.4 split data",
    "text": "2.4 split data\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\nX_train = X_train.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 712 entries, 451 to 188\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Sex_male  712 non-null    bool   \n 1   Fare      712 non-null    float64\n 2   Age       712 non-null    float64\n 3   Pclass    712 non-null    int64  \n 4   SibSp     712 non-null    int64  \ndtypes: bool(1), float64(2), int64(2)\nmemory usage: 28.5 KB",
    "crumbs": [
      "Classification",
      "K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "classification/5 KNN on titanic data.html#define-model",
    "href": "classification/5 KNN on titanic data.html#define-model",
    "title": "K-Nearest Neighbors",
    "section": "3.1 define model",
    "text": "3.1 define model\nThe solvers implemented in the class Logistic Regression are “liblinear”, “newton-cg”, “lbfgs”, “sag” and “saga”. According to Scikit Documentation: The “liblinear” solver was the one used by default for historical reasons before version 0.22. Since then, default use is lbfgs Algorithm.\n\n\nCode\nfrom sklearn.neighbors import KNeighborsRegressor \nml_model = KNeighborsRegressor()\nml_model\n\n\nKNeighborsRegressor()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KNeighborsRegressor?Documentation for KNeighborsRegressoriNot fittedKNeighborsRegressor()",
    "crumbs": [
      "Classification",
      "K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "classification/5 KNN on titanic data.html#train-model",
    "href": "classification/5 KNN on titanic data.html#train-model",
    "title": "K-Nearest Neighbors",
    "section": "3.2 train model",
    "text": "3.2 train model\n\n\nCode\nml_model.fit(X_train,Y_train)\n\n\nKNeighborsRegressor()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KNeighborsRegressor?Documentation for KNeighborsRegressoriFittedKNeighborsRegressor()",
    "crumbs": [
      "Classification",
      "K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "classification/5 KNN on titanic data.html#preformance",
    "href": "classification/5 KNN on titanic data.html#preformance",
    "title": "K-Nearest Neighbors",
    "section": "3.3 Preformance",
    "text": "3.3 Preformance\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = ml_model.predict(X_test) #always gets x and retuns y\nY_pred_dt\n\n\narray([0.8, 0.4, 0.2, 0. , 0.2, 0. , 0. , 0.6, 0.2, 0. , 0.8, 0.2, 0.6,\n       0.6, 0.4, 1. , 0.8, 0. , 1. , 0.2, 0.2, 0.8, 0.4, 0. , 0.8, 0. ,\n       0.6, 0.8, 1. , 0.2, 0.2, 0.6, 0. , 0.2, 0.8, 1. , 0.2, 0.6, 0.6,\n       0.6, 0. , 0. , 0.2, 0.2, 0.8, 0.2, 0.4, 0.4, 0.2, 0.2, 0.4, 0.4,\n       0.4, 0.4, 1. , 1. , 0. , 0. , 0. , 1. , 0.4, 0.2, 0.8, 0.4, 0.2,\n       0.2, 0. , 0.6, 0.6, 0.2, 0.2, 0.2, 0.2, 0.8, 0.2, 0.2, 0.4, 0.8,\n       0.2, 0.2, 0.2, 1. , 0.2, 0.6, 0.6, 0.4, 0. , 0.4, 0.2, 0.4, 0.2,\n       0.4, 0. , 0.2, 0. , 0.6, 0.2, 0.2, 0.6, 0. , 0.6, 0.4, 0.4, 0.4,\n       0. , 1. , 0.2, 0.4, 0.2, 0. , 0. , 0.2, 0.2, 0. , 0.2, 0. , 0. ,\n       0. , 1. , 0.2, 0.2, 0.2, 0.8, 0.6, 0. , 0.2, 0.4, 0.6, 0.2, 0.8,\n       0. , 0.6, 0. , 0.6, 0. , 0.8, 0.2, 0.2, 0. , 0. , 0.6, 0.8, 0.6,\n       0.6, 0.6, 0.6, 0.4, 0.8, 0.6, 0.2, 0.8, 0.2, 0.4, 0. , 0.2, 0.8,\n       0.6, 0.6, 0.8, 0.2, 0.8, 0.2, 0.2, 0.8, 0. , 0.2, 0.6, 0. , 0.4,\n       0.2, 0.6, 0.8, 0.6, 0. , 0. , 0. , 0.2, 0. , 0.2])\n\n\n\n\nCode\n# its criteria is to round to 1 when higher than 0.5\nY_pred_dt = np.round(Y_pred_dt)  \n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_test,Y_pred_dt)  \naccuracy\n\n\n0.6983240223463687\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_test,Y_pred_dt)  \nprecision_dt\n\n\n0.6129032258064516\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_test,Y_pred_dt)  \nrecall_dt\n\n\n0.5588235294117647\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_test,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[87, 24],\n       [30, 38]])\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_test, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.6713036565977742",
    "crumbs": [
      "Classification",
      "K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "classification/5 KNN on titanic data.html#k-fold-cross-validation",
    "href": "classification/5 KNN on titanic data.html#k-fold-cross-validation",
    "title": "K-Nearest Neighbors",
    "section": "3.4 k-Fold Cross-Validation",
    "text": "3.4 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(ml_model, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.10856888813345673",
    "crumbs": [
      "Classification",
      "K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "classification/3 Logistic Regression on titanic data.html",
    "href": "classification/3 Logistic Regression on titanic data.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split",
    "crumbs": [
      "Classification",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "classification/3 Logistic Regression on titanic data.html#input-data",
    "href": "classification/3 Logistic Regression on titanic data.html#input-data",
    "title": "Logistic Regression",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "classification/3 Logistic Regression on titanic data.html#data-eda",
    "href": "classification/3 Logistic Regression on titanic data.html#data-eda",
    "title": "Logistic Regression",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n\n\n0.7420382165605095\n0.18890814558058924",
    "crumbs": [
      "Classification",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "classification/3 Logistic Regression on titanic data.html#data-wrangling",
    "href": "classification/3 Logistic Regression on titanic data.html#data-wrangling",
    "title": "Logistic Regression",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1046 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1308 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Dealing with missing numerical variables\ndata['Age'] = data.Age.fillna(data.Age.median())\ndata['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1309 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1309 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\ndata = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\nSex_male\n\n\n\n\n0\n1\n3\nBraund, Mr. Owen Harris\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\nTrue\n\n\n1\n2\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\nFalse\n\n\n2\n3\n3\nHeikkinen, Miss. Laina\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\nFalse\n\n\n3\n4\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\nFalse\n\n\n4\n5\n3\nAllen, Mr. William Henry\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\nTrue\n\n\n\n\n\n\n\n\n\n\nCode\n# Select features columns\ndata = data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()\n\n\n\n\n\n\n\n\n\n\nSex_male\nFare\nAge\nPclass\nSibSp\nrole\n\n\n\n\n0\nTrue\n7.2500\n22.0\n3\n1\ntrain\n\n\n1\nFalse\n71.2833\n38.0\n1\n1\ntrain\n\n\n2\nFalse\n7.9250\n26.0\n3\n0\ntrain\n\n\n3\nFalse\n53.1000\n35.0\n1\n1\ntrain\n\n\n4\nTrue\n8.0500\n35.0\n3\n0\ntrain",
    "crumbs": [
      "Classification",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "classification/3 Logistic Regression on titanic data.html#split-data",
    "href": "classification/3 Logistic Regression on titanic data.html#split-data",
    "title": "Logistic Regression",
    "section": "2.4 split data",
    "text": "2.4 split data\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\nX_train = X_train.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 712 entries, 791 to 454\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Sex_male  712 non-null    bool   \n 1   Fare      712 non-null    float64\n 2   Age       712 non-null    float64\n 3   Pclass    712 non-null    int64  \n 4   SibSp     712 non-null    int64  \ndtypes: bool(1), float64(2), int64(2)\nmemory usage: 28.5 KB",
    "crumbs": [
      "Classification",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "classification/3 Logistic Regression on titanic data.html#define-model",
    "href": "classification/3 Logistic Regression on titanic data.html#define-model",
    "title": "Logistic Regression",
    "section": "3.1 define model",
    "text": "3.1 define model\nThe solvers implemented in the class Logistic Regression are “liblinear”, “newton-cg”, “lbfgs”, “sag” and “saga”. According to Scikit Documentation: The “liblinear” solver was the one used by default for historical reasons before version 0.22. Since then, default use is lbfgs Algorithm.\n\n\nCode\nfrom sklearn.linear_model import LogisticRegression\nml_model = LogisticRegression(solver='liblinear')\nml_model\n\n\nLogisticRegression(solver='liblinear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniNot fittedLogisticRegression(solver='liblinear')",
    "crumbs": [
      "Classification",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "classification/3 Logistic Regression on titanic data.html#train-model",
    "href": "classification/3 Logistic Regression on titanic data.html#train-model",
    "title": "Logistic Regression",
    "section": "3.2 train model",
    "text": "3.2 train model\n\n\nCode\nml_model.fit(X_train,Y_train)\n\n\nLogisticRegression(solver='liblinear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression(solver='liblinear') \n\n\nvariable importance\n\n\nCode\ncoefficients = ml_model.coef_[0]\n\nfeature_importance = pd.DataFrame({'Feature': X_train.columns, 'Importance': np.abs(coefficients)})\nfeature_importance = feature_importance.sort_values('Importance', ascending=False)\nfeature_importance\n\n\n\n\n\n\n\n\n\n\nFeature\nImportance\n\n\n\n\n0\nSex_male\n2.343351\n\n\n3\nPclass\n0.793533\n\n\n4\nSibSp\n0.277801\n\n\n2\nAge\n0.027368\n\n\n1\nFare\n0.004206",
    "crumbs": [
      "Classification",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "classification/3 Logistic Regression on titanic data.html#preformance",
    "href": "classification/3 Logistic Regression on titanic data.html#preformance",
    "title": "Logistic Regression",
    "section": "3.3 Preformance",
    "text": "3.3 Preformance\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = ml_model.predict(X_test) #always gets x and retuns y\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_test,Y_pred_dt)  \naccuracy\n\n\n0.8491620111731844\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_test,Y_pred_dt)  \nprecision_dt\n\n\n0.84\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_test,Y_pred_dt)  \nrecall_dt\n\n\n0.6885245901639344\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_test,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[110,   8],\n       [ 19,  42]])\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_test, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.8103639899972214",
    "crumbs": [
      "Classification",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "classification/3 Logistic Regression on titanic data.html#k-fold-cross-validation",
    "href": "classification/3 Logistic Regression on titanic data.html#k-fold-cross-validation",
    "title": "Logistic Regression",
    "section": "3.4 k-Fold Cross-Validation",
    "text": "3.4 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(ml_model, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.7795035949965528",
    "crumbs": [
      "Classification",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "classification/6 Random Forest on titanic dat.html",
    "href": "classification/6 Random Forest on titanic dat.html",
    "title": "Classification with Random Forest",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split",
    "crumbs": [
      "Classification",
      "Classification with Random Forest"
    ]
  },
  {
    "objectID": "classification/6 Random Forest on titanic dat.html#input-data",
    "href": "classification/6 Random Forest on titanic dat.html#input-data",
    "title": "Classification with Random Forest",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "Classification with Random Forest"
    ]
  },
  {
    "objectID": "classification/6 Random Forest on titanic dat.html#data-eda",
    "href": "classification/6 Random Forest on titanic dat.html#data-eda",
    "title": "Classification with Random Forest",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n\n\n0.7420382165605095\n0.18890814558058924",
    "crumbs": [
      "Classification",
      "Classification with Random Forest"
    ]
  },
  {
    "objectID": "classification/6 Random Forest on titanic dat.html#data-wrangling",
    "href": "classification/6 Random Forest on titanic dat.html#data-wrangling",
    "title": "Classification with Random Forest",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1046 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1308 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Dealing with missing numerical variables\ndata['Age'] = data.Age.fillna(data.Age.median())\ndata['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1309 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1309 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\ndata = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\nSex_male\n\n\n\n\n0\n1\n3\nBraund, Mr. Owen Harris\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\nTrue\n\n\n1\n2\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\nFalse\n\n\n2\n3\n3\nHeikkinen, Miss. Laina\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\nFalse\n\n\n3\n4\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\nFalse\n\n\n4\n5\n3\nAllen, Mr. William Henry\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\nTrue\n\n\n\n\n\n\n\n\n\n\nCode\n# Select features columns\ndata = data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()\n\n\n\n\n\n\n\n\n\n\nSex_male\nFare\nAge\nPclass\nSibSp\nrole\n\n\n\n\n0\nTrue\n7.2500\n22.0\n3\n1\ntrain\n\n\n1\nFalse\n71.2833\n38.0\n1\n1\ntrain\n\n\n2\nFalse\n7.9250\n26.0\n3\n0\ntrain\n\n\n3\nFalse\n53.1000\n35.0\n1\n1\ntrain\n\n\n4\nTrue\n8.0500\n35.0\n3\n0\ntrain",
    "crumbs": [
      "Classification",
      "Classification with Random Forest"
    ]
  },
  {
    "objectID": "classification/6 Random Forest on titanic dat.html#split-data",
    "href": "classification/6 Random Forest on titanic dat.html#split-data",
    "title": "Classification with Random Forest",
    "section": "2.4 split data",
    "text": "2.4 split data\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\nX_train = X_train.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 712 entries, 710 to 744\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Sex_male  712 non-null    bool   \n 1   Fare      712 non-null    float64\n 2   Age       712 non-null    float64\n 3   Pclass    712 non-null    int64  \n 4   SibSp     712 non-null    int64  \ndtypes: bool(1), float64(2), int64(2)\nmemory usage: 28.5 KB",
    "crumbs": [
      "Classification",
      "Classification with Random Forest"
    ]
  },
  {
    "objectID": "classification/6 Random Forest on titanic dat.html#define-model",
    "href": "classification/6 Random Forest on titanic dat.html#define-model",
    "title": "Classification with Random Forest",
    "section": "3.1 define model",
    "text": "3.1 define model\nThe solvers implemented in the class Logistic Regression are “liblinear”, “newton-cg”, “lbfgs”, “sag” and “saga”. According to Scikit Documentation: The “liblinear” solver was the one used by default for historical reasons before version 0.22. Since then, default use is lbfgs Algorithm.\n\n\nCode\nfrom sklearn.ensemble import RandomForestClassifier\nml_model = RandomForestClassifier()\nml_model\n\n\nRandomForestClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriNot fittedRandomForestClassifier()",
    "crumbs": [
      "Classification",
      "Classification with Random Forest"
    ]
  },
  {
    "objectID": "classification/6 Random Forest on titanic dat.html#train-model",
    "href": "classification/6 Random Forest on titanic dat.html#train-model",
    "title": "Classification with Random Forest",
    "section": "3.2 train model",
    "text": "3.2 train model\n\n\nCode\nml_model.fit(X_train,Y_train)\n\n\nRandomForestClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriFittedRandomForestClassifier()",
    "crumbs": [
      "Classification",
      "Classification with Random Forest"
    ]
  },
  {
    "objectID": "classification/6 Random Forest on titanic dat.html#preformance",
    "href": "classification/6 Random Forest on titanic dat.html#preformance",
    "title": "Classification with Random Forest",
    "section": "3.3 Preformance",
    "text": "3.3 Preformance\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = ml_model.predict(X_test) #always gets x and retuns y\nY_pred_dt\n\n\narray([1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,\n       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n       0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1,\n       0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n       0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n       0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n       0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n       0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n       0, 0, 1])\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_test,Y_pred_dt)  \naccuracy\n\n\n0.8156424581005587\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_test,Y_pred_dt)  \nprecision_dt\n\n\n0.7761194029850746\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_test,Y_pred_dt)  \nrecall_dt\n\n\n0.7428571428571429\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_test,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[94, 15],\n       [18, 52]])\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_test, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.8026212319790301",
    "crumbs": [
      "Classification",
      "Classification with Random Forest"
    ]
  },
  {
    "objectID": "classification/6 Random Forest on titanic dat.html#k-fold-cross-validation",
    "href": "classification/6 Random Forest on titanic dat.html#k-fold-cross-validation",
    "title": "Classification with Random Forest",
    "section": "3.4 k-Fold Cross-Validation",
    "text": "3.4 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(ml_model, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.8160346695557962",
    "crumbs": [
      "Classification",
      "Classification with Random Forest"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#get-first-3-rows",
    "href": "data manipulation/1 Pandas.html#get-first-3-rows",
    "title": "Data manipulation with Pandas",
    "section": "0.15 get first 3 rows",
    "text": "0.15 get first 3 rows\n\n\nCode\ndf.head(3)\n\n\n\n\n\n\n\n\n\n\nFlightDate\nAirline\nOrigin\nDest\nCancelled\nDiverted\nCRSDepTime\nDepTime\nDepDelayMinutes\nDepDelay\n...\nWheelsOff\nWheelsOn\nTaxiIn\nCRSArrTime\nArrDelay\nArrDel15\nArrivalDelayGroups\nArrTimeBlk\nDistanceGroup\nDivAirportLandings\n\n\n\n\n0\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nGJT\nDEN\nFalse\nFalse\n1133\n1123.0\n0.0\n-10.0\n...\n1140.0\n1220.0\n8.0\n1245\n-17.0\n0.0\n-2.0\n1200-1259\n1\n0\n\n\n1\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nHRL\nIAH\nFalse\nFalse\n732\n728.0\n0.0\n-4.0\n...\n744.0\n839.0\n9.0\n849\n-1.0\n0.0\n-1.0\n0800-0859\n2\n0\n\n\n2\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nDRO\nDEN\nFalse\nFalse\n1529\n1514.0\n0.0\n-15.0\n...\n1535.0\n1622.0\n14.0\n1639\n-3.0\n0.0\n-1.0\n1600-1659\n2\n0\n\n\n\n\n3 rows × 61 columns",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#get-last-3-rows",
    "href": "data manipulation/1 Pandas.html#get-last-3-rows",
    "title": "Data manipulation with Pandas",
    "section": "0.16 get last 3 rows",
    "text": "0.16 get last 3 rows\n\n\nCode\ndf.tail(3)\n\n\n\n\n\n\n\n\n\n\nFlightDate\nAirline\nOrigin\nDest\nCancelled\nDiverted\nCRSDepTime\nDepTime\nDepDelayMinutes\nDepDelay\n...\nWheelsOff\nWheelsOn\nTaxiIn\nCRSArrTime\nArrDelay\nArrDel15\nArrivalDelayGroups\nArrTimeBlk\nDistanceGroup\nDivAirportLandings\n\n\n\n\n7\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nTYS\nIAH\nFalse\nFalse\n1129\n1117.0\n0.0\n-12.0\n...\n1139.0\n1255.0\n16.0\n1306\n5.0\n0.0\n0.0\n1300-1359\n4\n0\n\n\n8\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nIAH\nAEX\nFalse\nFalse\n1424\n1414.0\n0.0\n-10.0\n...\n1430.0\n1507.0\n6.0\n1524\n-11.0\n0.0\n-1.0\n1500-1559\n1\n0\n\n\n9\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nIAH\nMOB\nFalse\nFalse\n954\n947.0\n0.0\n-7.0\n...\n1004.0\n1104.0\n6.0\n1121\n-11.0\n0.0\n-1.0\n1100-1159\n2\n0\n\n\n\n\n3 rows × 61 columns",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#get-ramdon-5-rows",
    "href": "data manipulation/1 Pandas.html#get-ramdon-5-rows",
    "title": "Data manipulation with Pandas",
    "section": "0.17 get ramdon 5 rows",
    "text": "0.17 get ramdon 5 rows\n\n\nCode\ndf.sample(5, random_state=42)\n\n\n\n\n\n\n\n\n\n\nFlightDate\nAirline\nOrigin\nDest\nCancelled\nDiverted\nCRSDepTime\nDepTime\nDepDelayMinutes\nDepDelay\n...\nWheelsOff\nWheelsOn\nTaxiIn\nCRSArrTime\nArrDelay\nArrDel15\nArrivalDelayGroups\nArrTimeBlk\nDistanceGroup\nDivAirportLandings\n\n\n\n\n8\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nIAH\nAEX\nFalse\nFalse\n1424\n1414.0\n0.0\n-10.0\n...\n1430.0\n1507.0\n6.0\n1524\n-11.0\n0.0\n-1.0\n1500-1559\n1\n0\n\n\n1\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nHRL\nIAH\nFalse\nFalse\n732\n728.0\n0.0\n-4.0\n...\n744.0\n839.0\n9.0\n849\n-1.0\n0.0\n-1.0\n0800-0859\n2\n0\n\n\n5\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nDEN\nTUL\nFalse\nFalse\n955\n952.0\n0.0\n-3.0\n...\n1017.0\n1234.0\n4.0\n1240\n-2.0\n0.0\n-1.0\n1200-1259\n3\n0\n\n\n0\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nGJT\nDEN\nFalse\nFalse\n1133\n1123.0\n0.0\n-10.0\n...\n1140.0\n1220.0\n8.0\n1245\n-17.0\n0.0\n-2.0\n1200-1259\n1\n0\n\n\n7\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nTYS\nIAH\nFalse\nFalse\n1129\n1117.0\n0.0\n-12.0\n...\n1139.0\n1255.0\n16.0\n1306\n5.0\n0.0\n0.0\n1300-1359\n4\n0\n\n\n\n\n5 rows × 61 columns",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "classification/1.3 decision tree and fast tuning.html",
    "href": "classification/1.3 decision tree and fast tuning.html",
    "title": "Decision tree and fast tuning",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nimport time",
    "crumbs": [
      "Classification",
      "Decision tree and fast tuning"
    ]
  },
  {
    "objectID": "classification/1.3 decision tree and fast tuning.html#input-data",
    "href": "classification/1.3 decision tree and fast tuning.html#input-data",
    "title": "Decision tree and fast tuning",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "Decision tree and fast tuning"
    ]
  },
  {
    "objectID": "classification/1.3 decision tree and fast tuning.html#data-eda",
    "href": "classification/1.3 decision tree and fast tuning.html#data-eda",
    "title": "Decision tree and fast tuning",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n\n\n0.7420382165605095\n0.18890814558058924",
    "crumbs": [
      "Classification",
      "Decision tree and fast tuning"
    ]
  },
  {
    "objectID": "classification/1.3 decision tree and fast tuning.html#data-wrangling",
    "href": "classification/1.3 decision tree and fast tuning.html#data-wrangling",
    "title": "Decision tree and fast tuning",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1046 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1308 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Dealing with missing numerical variables\ndata['Age'] = data.Age.fillna(data.Age.median())\ndata['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1309 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1309 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\ndata = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\nSex_male\n\n\n\n\n0\n1\n3\nBraund, Mr. Owen Harris\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\nTrue\n\n\n1\n2\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\nFalse\n\n\n2\n3\n3\nHeikkinen, Miss. Laina\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\nFalse\n\n\n3\n4\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\nFalse\n\n\n4\n5\n3\nAllen, Mr. William Henry\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\nTrue\n\n\n\n\n\n\n\n\n\n\nCode\n# Select features columns\ndata = data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()\n\n\n\n\n\n\n\n\n\n\nSex_male\nFare\nAge\nPclass\nSibSp\nrole\n\n\n\n\n0\nTrue\n7.2500\n22.0\n3\n1\ntrain\n\n\n1\nFalse\n71.2833\n38.0\n1\n1\ntrain\n\n\n2\nFalse\n7.9250\n26.0\n3\n0\ntrain\n\n\n3\nFalse\n53.1000\n35.0\n1\n1\ntrain\n\n\n4\nTrue\n8.0500\n35.0\n3\n0\ntrain",
    "crumbs": [
      "Classification",
      "Decision tree and fast tuning"
    ]
  },
  {
    "objectID": "classification/1.3 decision tree and fast tuning.html#split-data",
    "href": "classification/1.3 decision tree and fast tuning.html#split-data",
    "title": "Decision tree and fast tuning",
    "section": "2.4 split data",
    "text": "2.4 split data\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\nX_train = X_train.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 712 entries, 701 to 753\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Sex_male  712 non-null    bool   \n 1   Fare      712 non-null    float64\n 2   Age       712 non-null    float64\n 3   Pclass    712 non-null    int64  \n 4   SibSp     712 non-null    int64  \ndtypes: bool(1), float64(2), int64(2)\nmemory usage: 28.5 KB",
    "crumbs": [
      "Classification",
      "Decision tree and fast tuning"
    ]
  },
  {
    "objectID": "classification/1.3 decision tree and fast tuning.html#define-model",
    "href": "classification/1.3 decision tree and fast tuning.html#define-model",
    "title": "Decision tree and fast tuning",
    "section": "3.1 define model",
    "text": "3.1 define model\n\n\nCode\nmodel_dt = DecisionTreeClassifier()   #model with deph 3\nmodel_dt\n\n\nDecisionTreeClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriNot fittedDecisionTreeClassifier() \n\n\ndefalt hyper-parameters:\n\n\nCode\nmodel_dt.get_params()\n\n\n{'ccp_alpha': 0.0,\n 'class_weight': None,\n 'criterion': 'gini',\n 'max_depth': None,\n 'max_features': None,\n 'max_leaf_nodes': None,\n 'min_impurity_decrease': 0.0,\n 'min_samples_leaf': 1,\n 'min_samples_split': 2,\n 'min_weight_fraction_leaf': 0.0,\n 'monotonic_cst': None,\n 'random_state': None,\n 'splitter': 'best'}",
    "crumbs": [
      "Classification",
      "Decision tree and fast tuning"
    ]
  },
  {
    "objectID": "classification/1.3 decision tree and fast tuning.html#define-parameters",
    "href": "classification/1.3 decision tree and fast tuning.html#define-parameters",
    "title": "Decision tree and fast tuning",
    "section": "3.2 define parameters",
    "text": "3.2 define parameters\n\n\nCode\nparameters = {'criterion':['gini','entropy'],\n              'max_depth':np.arange(1,21).tolist()[0::2],\n              'min_samples_split':np.arange(2,11).tolist()[0::2],\n              'max_leaf_nodes':np.arange(3,26).tolist()[0::2]}\n\n\nall parameters combinations\n\n\nCode\nimport itertools\na = parameters.values()\ncombinations = list(itertools.product(*a))\n\n\n1200 combinations\n\n\nCode\nlen(combinations)\n\n\n1200\n\n\n\n\nCode\ncombinations[0:5]\n\n\n[('gini', 1, 2, 3),\n ('gini', 1, 2, 5),\n ('gini', 1, 2, 7),\n ('gini', 1, 2, 9),\n ('gini', 1, 2, 11)]",
    "crumbs": [
      "Classification",
      "Decision tree and fast tuning"
    ]
  },
  {
    "objectID": "classification/1.3 decision tree and fast tuning.html#define-grids",
    "href": "classification/1.3 decision tree and fast tuning.html#define-grids",
    "title": "Decision tree and fast tuning",
    "section": "3.3 define GridS",
    "text": "3.3 define GridS\nusing Random Search limit to 100 combinations\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV\nG1=RandomizedSearchCV(DecisionTreeClassifier(), param_distributions=parameters,n_iter = 100, cv=10, n_jobs=-1)",
    "crumbs": [
      "Classification",
      "Decision tree and fast tuning"
    ]
  },
  {
    "objectID": "classification/1.3 decision tree and fast tuning.html#train-model",
    "href": "classification/1.3 decision tree and fast tuning.html#train-model",
    "title": "Decision tree and fast tuning",
    "section": "3.4 train model",
    "text": "3.4 train model\n\n\nCode\nstart_time = time.time()\n\nG1.fit(X_train,Y_train)\n\nend_time = time.time()\n\nduration = end_time - start_time\nduration\n\n\n2.8857500553131104\n\n\nbest parameters\n\n\nCode\nG1.best_params_\n\n\n{'min_samples_split': 2,\n 'max_leaf_nodes': 21,\n 'max_depth': 7,\n 'criterion': 'gini'}\n\n\nbest model\n\n\nCode\nmodel_dt = G1.best_estimator_\n\n\nvariable importance\n\n\nCode\nimportances = model_dt.feature_importances_\nvi=pd.DataFrame({\"variable\":X_train.columns,\"importances\":importances})\nvi=vi.sort_values('importances',ascending=False)\nvi\n\n\n\n\n\n\n\n\n\n\nvariable\nimportances\n\n\n\n\n0\nSex_male\n0.506731\n\n\n3\nPclass\n0.151602\n\n\n2\nAge\n0.144835\n\n\n1\nFare\n0.134373\n\n\n4\nSibSp\n0.062458",
    "crumbs": [
      "Classification",
      "Decision tree and fast tuning"
    ]
  },
  {
    "objectID": "classification/1.3 decision tree and fast tuning.html#preformance",
    "href": "classification/1.3 decision tree and fast tuning.html#preformance",
    "title": "Decision tree and fast tuning",
    "section": "3.5 Preformance",
    "text": "3.5 Preformance\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = model_dt.predict(X_test) #always gets x and retuns y\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_test,Y_pred_dt)  \naccuracy\n\n\n0.8100558659217877\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_test,Y_pred_dt)  \nprecision_dt\n\n\n0.75\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_test,Y_pred_dt)  \nrecall_dt\n\n\n0.75\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_test,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[94, 17],\n       [17, 51]])\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_test, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.7984234234234234",
    "crumbs": [
      "Classification",
      "Decision tree and fast tuning"
    ]
  },
  {
    "objectID": "classification/1.3 decision tree and fast tuning.html#k-fold-cross-validation",
    "href": "classification/1.3 decision tree and fast tuning.html#k-fold-cross-validation",
    "title": "Decision tree and fast tuning",
    "section": "3.6 k-Fold Cross-Validation",
    "text": "3.6 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(model_dt, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.8315177779966513",
    "crumbs": [
      "Classification",
      "Decision tree and fast tuning"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#select-column-by-column-names",
    "href": "data manipulation/1 Pandas.html#select-column-by-column-names",
    "title": "Data manipulation with Pandas",
    "section": "1.1 select column by column names",
    "text": "1.1 select column by column names\n\n\nCode\ndf[['FlightDate','Airline']]\n\n\n\n\n\n\n\n\n\n\nFlightDate\nAirline\n\n\n\n\n0\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\n\n\n1\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\n\n\n2\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\n\n\n3\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\n\n\n4\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\n\n\n5\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\n\n\n6\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\n\n\n7\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\n\n\n8\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\n\n\n9\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#first-3-columns",
    "href": "data manipulation/1 Pandas.html#first-3-columns",
    "title": "Data manipulation with Pandas",
    "section": "1.2 first 3 columns",
    "text": "1.2 first 3 columns\n\n\nCode\ndf[df.columns[:3]]\n\n\n\n\n\n\n\n\n\n\nFlightDate\nAirline\nOrigin\n\n\n\n\n0\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nGJT\n\n\n1\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nHRL\n\n\n2\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nDRO\n\n\n3\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nIAH\n\n\n4\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nDRO\n\n\n5\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nDEN\n\n\n6\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nIAH\n\n\n7\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nTYS\n\n\n8\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nIAH\n\n\n9\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nIAH",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#last-3-columns",
    "href": "data manipulation/1 Pandas.html#last-3-columns",
    "title": "Data manipulation with Pandas",
    "section": "1.3 last 3 columns",
    "text": "1.3 last 3 columns\n\n\nCode\ndf[df.columns[-3:]]\n\n\n\n\n\n\n\n\n\n\nArrTimeBlk\nDistanceGroup\nDivAirportLandings\n\n\n\n\n0\n1200-1259\n1\n0\n\n\n1\n0800-0859\n2\n0\n\n\n2\n1600-1659\n2\n0\n\n\n3\n1600-1659\n2\n0\n\n\n4\n1200-1259\n2\n0\n\n\n5\n1200-1259\n3\n0\n\n\n6\n2200-2259\n1\n0\n\n\n7\n1300-1359\n4\n0\n\n\n8\n1500-1559\n1\n0\n\n\n9\n1100-1159\n2\n0",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#select-3rd-columns",
    "href": "data manipulation/1 Pandas.html#select-3rd-columns",
    "title": "Data manipulation with Pandas",
    "section": "1.4 select 3rd columns",
    "text": "1.4 select 3rd columns\n\n\nCode\ndf.iloc[:,2]\n\n\n0    GJT\n1    HRL\n2    DRO\n3    IAH\n4    DRO\n5    DEN\n6    IAH\n7    TYS\n8    IAH\n9    IAH\nName: Origin, dtype: object",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#select-int-data-type-column",
    "href": "data manipulation/1 Pandas.html#select-int-data-type-column",
    "title": "Data manipulation with Pandas",
    "section": "1.5 select int data type column",
    "text": "1.5 select int data type column\n\n\nCode\ndf.select_dtypes('int')\n\n\n\n\n\n\n\n\n\n\nCRSDepTime\nYear\nQuarter\nMonth\nDayofMonth\nDayOfWeek\nDOT_ID_Marketing_Airline\nFlight_Number_Marketing_Airline\nDOT_ID_Operating_Airline\nFlight_Number_Operating_Airline\n...\nOriginStateFips\nOriginWac\nDestAirportID\nDestAirportSeqID\nDestCityMarketID\nDestStateFips\nDestWac\nCRSArrTime\nDistanceGroup\nDivAirportLandings\n\n\n\n\n0\n1133\n2022\n2\n4\n4\n1\n19977\n4301\n20445\n4301\n...\n8\n82\n11292\n1129202\n30325\n8\n82\n1245\n1\n0\n\n\n1\n732\n2022\n2\n4\n4\n1\n19977\n4299\n20445\n4299\n...\n48\n74\n12266\n1226603\n31453\n48\n74\n849\n2\n0\n\n\n2\n1529\n2022\n2\n4\n4\n1\n19977\n4298\n20445\n4298\n...\n8\n82\n11292\n1129202\n30325\n8\n82\n1639\n2\n0\n\n\n3\n1435\n2022\n2\n4\n4\n1\n19977\n4296\n20445\n4296\n...\n48\n74\n11973\n1197302\n31973\n28\n53\n1605\n2\n0\n\n\n4\n1135\n2022\n2\n4\n4\n1\n19977\n4295\n20445\n4295\n...\n8\n82\n11292\n1129202\n30325\n8\n82\n1245\n2\n0\n\n\n5\n955\n2022\n2\n4\n4\n1\n19977\n4294\n20445\n4294\n...\n8\n82\n15370\n1537002\n34653\n40\n73\n1240\n3\n0\n\n\n6\n2139\n2022\n2\n4\n4\n1\n19977\n4293\n20445\n4293\n...\n48\n74\n12915\n1291503\n31205\n22\n72\n2231\n1\n0\n\n\n7\n1129\n2022\n2\n4\n4\n1\n19977\n4292\n20445\n4292\n...\n47\n54\n12266\n1226603\n31453\n48\n74\n1306\n4\n0\n\n\n8\n1424\n2022\n2\n4\n4\n1\n19977\n4291\n20445\n4291\n...\n48\n74\n10185\n1018502\n30185\n22\n72\n1524\n1\n0\n\n\n9\n954\n2022\n2\n4\n4\n1\n19977\n4290\n20445\n4290\n...\n48\n74\n13422\n1342202\n30562\n1\n51\n1121\n2\n0\n\n\n\n\n10 rows × 23 columns",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#select-3-rows-and-4-column-cells",
    "href": "data manipulation/1 Pandas.html#select-3-rows-and-4-column-cells",
    "title": "Data manipulation with Pandas",
    "section": "2.1 select 3 rows and 4 column cells",
    "text": "2.1 select 3 rows and 4 column cells\n\n\nCode\ndf.iloc[2, 3]\n\n\n'DEN'",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#select-first-5-rows-and-first-5-column-cells",
    "href": "data manipulation/1 Pandas.html#select-first-5-rows-and-first-5-column-cells",
    "title": "Data manipulation with Pandas",
    "section": "2.2 select first 5 rows and first 5 column cells",
    "text": "2.2 select first 5 rows and first 5 column cells\n\n\nCode\ndf.iloc[:5, :5] \n\n\n\n\n\n\n\n\n\n\nFlightDate\nAirline\nOrigin\nDest\nCancelled\n\n\n\n\n0\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nGJT\nDEN\nFalse\n\n\n1\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nHRL\nIAH\nFalse\n\n\n2\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nDRO\nDEN\nFalse\n\n\n3\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nIAH\nGPT\nFalse\n\n\n4\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nDRO\nDEN\nFalse",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#select-3rd-rows",
    "href": "data manipulation/1 Pandas.html#select-3rd-rows",
    "title": "Data manipulation with Pandas",
    "section": "2.3 select 3rd rows",
    "text": "2.3 select 3rd rows\n\n\nCode\ndf.iloc[[2]]\n\n\n\n\n\n\n\n\n\n\nFlightDate\nAirline\nOrigin\nDest\nCancelled\nDiverted\nCRSDepTime\nDepTime\nDepDelayMinutes\nDepDelay\n...\nWheelsOff\nWheelsOn\nTaxiIn\nCRSArrTime\nArrDelay\nArrDel15\nArrivalDelayGroups\nArrTimeBlk\nDistanceGroup\nDivAirportLandings\n\n\n\n\n2\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nDRO\nDEN\nFalse\nFalse\n1529\n1514.0\n0.0\n-15.0\n...\n1535.0\n1622.0\n14.0\n1639\n-3.0\n0.0\n-1.0\n1600-1659\n2\n0\n\n\n\n\n1 rows × 61 columns",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "classification/6.1 Random Forest.html",
    "href": "classification/6.1 Random Forest.html",
    "title": "Random Forest",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split",
    "crumbs": [
      "Classification",
      "Random Forest"
    ]
  },
  {
    "objectID": "classification/6.1 Random Forest.html#input-data",
    "href": "classification/6.1 Random Forest.html#input-data",
    "title": "Random Forest",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "Random Forest"
    ]
  },
  {
    "objectID": "classification/6.1 Random Forest.html#data-eda",
    "href": "classification/6.1 Random Forest.html#data-eda",
    "title": "Random Forest",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n\n\n0.7420382165605095\n0.18890814558058924",
    "crumbs": [
      "Classification",
      "Random Forest"
    ]
  },
  {
    "objectID": "classification/6.1 Random Forest.html#data-wrangling",
    "href": "classification/6.1 Random Forest.html#data-wrangling",
    "title": "Random Forest",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1046 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1308 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Dealing with missing numerical variables\ndata['Age'] = data.Age.fillna(data.Age.median())\ndata['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1309 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1309 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\ndata = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\nSex_male\n\n\n\n\n0\n1\n3\nBraund, Mr. Owen Harris\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\nTrue\n\n\n1\n2\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\nFalse\n\n\n2\n3\n3\nHeikkinen, Miss. Laina\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\nFalse\n\n\n3\n4\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\nFalse\n\n\n4\n5\n3\nAllen, Mr. William Henry\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\nTrue\n\n\n\n\n\n\n\n\n\n\nCode\n# Select features columns\ndata = data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()\n\n\n\n\n\n\n\n\n\n\nSex_male\nFare\nAge\nPclass\nSibSp\nrole\n\n\n\n\n0\nTrue\n7.2500\n22.0\n3\n1\ntrain\n\n\n1\nFalse\n71.2833\n38.0\n1\n1\ntrain\n\n\n2\nFalse\n7.9250\n26.0\n3\n0\ntrain\n\n\n3\nFalse\n53.1000\n35.0\n1\n1\ntrain\n\n\n4\nTrue\n8.0500\n35.0\n3\n0\ntrain",
    "crumbs": [
      "Classification",
      "Random Forest"
    ]
  },
  {
    "objectID": "classification/6.1 Random Forest.html#split-data",
    "href": "classification/6.1 Random Forest.html#split-data",
    "title": "Random Forest",
    "section": "2.4 split data",
    "text": "2.4 split data\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\nX_train = X_train.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 712 entries, 206 to 819\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Sex_male  712 non-null    bool   \n 1   Fare      712 non-null    float64\n 2   Age       712 non-null    float64\n 3   Pclass    712 non-null    int64  \n 4   SibSp     712 non-null    int64  \ndtypes: bool(1), float64(2), int64(2)\nmemory usage: 28.5 KB",
    "crumbs": [
      "Classification",
      "Random Forest"
    ]
  },
  {
    "objectID": "classification/6.1 Random Forest.html#define-model",
    "href": "classification/6.1 Random Forest.html#define-model",
    "title": "Random Forest",
    "section": "3.1 define model",
    "text": "3.1 define model\nThe solvers implemented in the class Logistic Regression are “liblinear”, “newton-cg”, “lbfgs”, “sag” and “saga”. According to Scikit Documentation: The “liblinear” solver was the one used by default for historical reasons before version 0.22. Since then, default use is lbfgs Algorithm.\n\n\nCode\nfrom sklearn.ensemble import RandomForestClassifier\nml_model = RandomForestClassifier()\nml_model\n\n\nRandomForestClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriNot fittedRandomForestClassifier()",
    "crumbs": [
      "Classification",
      "Random Forest"
    ]
  },
  {
    "objectID": "classification/6.1 Random Forest.html#train-model",
    "href": "classification/6.1 Random Forest.html#train-model",
    "title": "Random Forest",
    "section": "3.2 train model",
    "text": "3.2 train model\n\n\nCode\nml_model.fit(X_train,Y_train)\n\n\nRandomForestClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriFittedRandomForestClassifier()",
    "crumbs": [
      "Classification",
      "Random Forest"
    ]
  },
  {
    "objectID": "classification/6.1 Random Forest.html#preformance",
    "href": "classification/6.1 Random Forest.html#preformance",
    "title": "Random Forest",
    "section": "3.3 Preformance",
    "text": "3.3 Preformance\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = ml_model.predict(X_test) #always gets x and retuns y\nY_pred_dt\n\n\narray([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n       0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,\n       0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,\n       0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,\n       1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n       0, 1, 0])\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_test,Y_pred_dt)  \naccuracy\n\n\n0.8044692737430168\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_test,Y_pred_dt)  \nprecision_dt\n\n\n0.7288135593220338\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_test,Y_pred_dt)  \nrecall_dt\n\n\n0.6935483870967742\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_test,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[101,  16],\n       [ 19,  43]])\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_test, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.7783981251723188",
    "crumbs": [
      "Classification",
      "Random Forest"
    ]
  },
  {
    "objectID": "classification/6.1 Random Forest.html#k-fold-cross-validation",
    "href": "classification/6.1 Random Forest.html#k-fold-cross-validation",
    "title": "Random Forest",
    "section": "3.4 k-Fold Cross-Validation",
    "text": "3.4 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(ml_model, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.8089825667290457",
    "crumbs": [
      "Classification",
      "Random Forest"
    ]
  },
  {
    "objectID": "classification/6.2 Random Forest.html",
    "href": "classification/6.2 Random Forest.html",
    "title": "Random Forest with pipeline and hyperparameter tuning",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nimport time",
    "crumbs": [
      "Classification",
      "Random Forest with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/6.2 Random Forest.html#input-data",
    "href": "classification/6.2 Random Forest.html#input-data",
    "title": "Random Forest with pipeline and hyperparameter tuning",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "Random Forest with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/6.2 Random Forest.html#data-eda",
    "href": "classification/6.2 Random Forest.html#data-eda",
    "title": "Random Forest with pipeline and hyperparameter tuning",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n\n\n0.7420382165605095\n0.18890814558058924",
    "crumbs": [
      "Classification",
      "Random Forest with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/6.2 Random Forest.html#data-wrangling",
    "href": "classification/6.2 Random Forest.html#data-wrangling",
    "title": "Random Forest with pipeline and hyperparameter tuning",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1046 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1308 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Dealing with missing numerical variables\ndata['Age'] = data.Age.fillna(data.Age.median())\ndata['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1309 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1309 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\ndata = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\nSex_male\n\n\n\n\n0\n1\n3\nBraund, Mr. Owen Harris\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\nTrue\n\n\n1\n2\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\nFalse\n\n\n2\n3\n3\nHeikkinen, Miss. Laina\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\nFalse\n\n\n3\n4\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\nFalse\n\n\n4\n5\n3\nAllen, Mr. William Henry\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\nTrue\n\n\n\n\n\n\n\n\n\n\nCode\n# Select features columns\ndata = data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()\n\n\n\n\n\n\n\n\n\n\nSex_male\nFare\nAge\nPclass\nSibSp\nrole\n\n\n\n\n0\nTrue\n7.2500\n22.0\n3\n1\ntrain\n\n\n1\nFalse\n71.2833\n38.0\n1\n1\ntrain\n\n\n2\nFalse\n7.9250\n26.0\n3\n0\ntrain\n\n\n3\nFalse\n53.1000\n35.0\n1\n1\ntrain\n\n\n4\nTrue\n8.0500\n35.0\n3\n0\ntrain",
    "crumbs": [
      "Classification",
      "Random Forest with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/6.2 Random Forest.html#split-data",
    "href": "classification/6.2 Random Forest.html#split-data",
    "title": "Random Forest with pipeline and hyperparameter tuning",
    "section": "2.4 split data",
    "text": "2.4 split data\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\nX_train = X_train.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 712 entries, 350 to 456\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Sex_male  712 non-null    bool   \n 1   Fare      712 non-null    float64\n 2   Age       712 non-null    float64\n 3   Pclass    712 non-null    int64  \n 4   SibSp     712 non-null    int64  \ndtypes: bool(1), float64(2), int64(2)\nmemory usage: 28.5 KB",
    "crumbs": [
      "Classification",
      "Random Forest with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/6.2 Random Forest.html#define-model",
    "href": "classification/6.2 Random Forest.html#define-model",
    "title": "Random Forest with pipeline and hyperparameter tuning",
    "section": "3.1 define model",
    "text": "3.1 define model\nThe solvers implemented in the class Logistic Regression are “liblinear”, “newton-cg”, “lbfgs”, “sag” and “saga”. According to Scikit Documentation: The “liblinear” solver was the one used by default for historical reasons before version 0.22. Since then, default use is lbfgs Algorithm.\n\n\nCode\nfrom sklearn.ensemble import RandomForestClassifier\nml_model = RandomForestClassifier()\nml_model\n\n\nRandomForestClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriNot fittedRandomForestClassifier()",
    "crumbs": [
      "Classification",
      "Random Forest with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/6.2 Random Forest.html#train-model",
    "href": "classification/6.2 Random Forest.html#train-model",
    "title": "Random Forest with pipeline and hyperparameter tuning",
    "section": "3.4 train model",
    "text": "3.4 train model\n\n\nCode\nstart_time = time.time()\n\nGridCV.fit(X_train, Y_train)\n\nend_time = time.time()\nduration = end_time - start_time\nduration\n\n\nFitting 10 folds for each of 27 candidates, totalling 270 fits\n\n\n8.21796202659607\n\n\n\n\nCode\n# get the parameter names\ncolumn_results = [f\"param_{name}\" for name in parameters.keys()]\ncolumn_results += [\"mean_test_score\", \"std_test_score\", \"rank_test_score\"]\n\ncv_results = pd.DataFrame(GridCV.cv_results_)\ncv_results = cv_results[column_results].sort_values(\n    \"mean_test_score\", ascending=False\n)\n\n\ndef shorten_param(param_name):\n    if \"__\" in param_name:\n        return param_name.rsplit(\"__\", 1)[1]\n    return param_name\n\n\ncv_results = cv_results.rename(shorten_param, axis=1)\ncv_results.head()\n\n\n\n\n\n\n\n\n\n\nmax_depth\nn_estimators\nmin_samples_leaf\nmean_test_score\nstd_test_score\nrank_test_score\n\n\n\n\n16\n30\n250\n3\n0.724570\n0.070025\n1\n\n\n17\n30\n300\n3\n0.723200\n0.073436\n2\n\n\n15\n30\n200\n3\n0.723181\n0.071370\n3\n\n\n22\n40\n250\n2\n0.723181\n0.077401\n4\n\n\n4\n20\n250\n2\n0.721811\n0.067437\n5\n\n\n\n\n\n\n\n\n\n3.4.0.1 tunning best parameters\n\n\nCode\nGridCV.best_params_\n\n\n{'model__max_depth': 30,\n 'model__min_samples_leaf': 3,\n 'model__n_estimators': 250}\n\n\n\n\n3.4.0.2 tunning best model\n\n\nCode\nml_model = GridCV.best_estimator_",
    "crumbs": [
      "Classification",
      "Random Forest with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/6.2 Random Forest.html#preformance",
    "href": "classification/6.2 Random Forest.html#preformance",
    "title": "Random Forest with pipeline and hyperparameter tuning",
    "section": "3.5 Preformance",
    "text": "3.5 Preformance\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = ml_model.predict(X_test) #always gets x and retuns y\nY_pred_dt\n\n\narray([0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n       0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n       0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n       1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,\n       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n       0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n       1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0])\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_test,Y_pred_dt)  \naccuracy\n\n\n0.7039106145251397\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_test,Y_pred_dt)  \nprecision_dt\n\n\n0.64\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_test,Y_pred_dt)  \nrecall_dt\n\n\n0.47761194029850745\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_test,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[94, 18],\n       [35, 32]])\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_test, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.6584488272921108",
    "crumbs": [
      "Classification",
      "Random Forest with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/6.2 Random Forest.html#k-fold-cross-validation",
    "href": "classification/6.2 Random Forest.html#k-fold-cross-validation",
    "title": "Random Forest with pipeline and hyperparameter tuning",
    "section": "3.6 k-Fold Cross-Validation",
    "text": "3.6 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(ml_model, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.7176696542893726",
    "crumbs": [
      "Classification",
      "Random Forest with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "data manipulation/2 siuba.html",
    "href": "data manipulation/2 siuba.html",
    "title": "Data manipulation with siuba",
    "section": "",
    "text": "siuba (小巴) is a port of dplyr and other R libraries with seamless support for pandas and SQL",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html#darkgrid",
    "href": "plot/1 seaborn.html#darkgrid",
    "title": "seaborn chart",
    "section": "10.1 darkgrid",
    "text": "10.1 darkgrid\n\n\nCode\nimport seaborn as sns\n\ndf = sns.load_dataset(\"tips\")\n\nsns.set_theme()\n# Equivalent to:\n# sns.set_style(\"darkgrid\")\n\nsns.boxplot(x = \"day\", y = \"total_bill\", data = df)",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html#darkgrid-1",
    "href": "plot/1 seaborn.html#darkgrid-1",
    "title": "seaborn chart",
    "section": "10.2 darkgrid",
    "text": "10.2 darkgrid\n\n\nCode\nimport seaborn as sns\n\ndf = sns.load_dataset(\"tips\")\nsns.set_style(\"whitegrid\")\n\nsns.boxplot(x = \"day\", y = \"total_bill\", data = df)",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html#darkgrid-themes",
    "href": "plot/1 seaborn.html#darkgrid-themes",
    "title": "seaborn chart",
    "section": "9.1 darkgrid themes",
    "text": "9.1 darkgrid themes\nIf you set the set_style function without any arguments the “darkgrid” theme will be used by default, which adds a gray background and white grid lines.\n\n\nCode\nimport seaborn as sns\n\ndf = sns.load_dataset(\"tips\")\n\nsns.set_theme()\n# Equivalent to:\n# sns.set_style(\"darkgrid\")\n\nsns.boxplot(x = \"day\", y = \"total_bill\", data = df)",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html#whitegrid-themes",
    "href": "plot/1 seaborn.html#whitegrid-themes",
    "title": "seaborn chart",
    "section": "9.2 whitegrid themes",
    "text": "9.2 whitegrid themes\nIf you want to add gray grid lines but with a white background set this theme.\n\n\nCode\nimport seaborn as sns\n\ndf = sns.load_dataset(\"tips\")\nsns.set_style(\"whitegrid\")\n\nsns.boxplot(x = \"day\", y = \"total_bill\", data = df)",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html#dark-themes",
    "href": "plot/1 seaborn.html#dark-themes",
    "title": "seaborn chart",
    "section": "9.3 dark themes",
    "text": "9.3 dark themes\nThe “dark” theme is the same as “darkgrid” but without the grid lines.\n\n\nCode\nimport seaborn as sns\n\ndf = sns.load_dataset(\"tips\")\n\nsns.set_style(\"dark\")\n\nsns.boxplot(x = \"day\", y = \"total_bill\", data = df)",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html#white-themes",
    "href": "plot/1 seaborn.html#white-themes",
    "title": "seaborn chart",
    "section": "9.4 white themes",
    "text": "9.4 white themes\nThe “white” theme is the same as “whitegrid” but without the gray grid lines.\n\n\nCode\nimport seaborn as sns\n\ndf = sns.load_dataset(\"tips\")\n\nsns.set_style(\"white\")\n\nsns.boxplot(x = \"day\", y = \"total_bill\", data = df)",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html#ticks-themes",
    "href": "plot/1 seaborn.html#ticks-themes",
    "title": "seaborn chart",
    "section": "9.5 ticks themes",
    "text": "9.5 ticks themes\nThe “ticks” theme is the same as the “white” theme but this theme adds ticks to the axes.\n\n\nCode\nimport seaborn as sns\n\ndf = sns.load_dataset(\"tips\")\n\nsns.set_style(\"ticks\")\n\nsns.boxplot(x = \"day\", y = \"total_bill\", data = df)",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html#add-title",
    "href": "plot/1 seaborn.html#add-title",
    "title": "seaborn chart",
    "section": "8.1 add title",
    "text": "8.1 add title\n\n\nCode\ndf = sns.load_dataset(\"tips\")\n\nax=sns.boxplot(x = \"day\", y = \"total_bill\", data = df)\n\nax.set_title(\"tips box plot \")\n\n\nText(0.5, 1.0, 'tips box plot ')",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html#adjust-size",
    "href": "plot/1 seaborn.html#adjust-size",
    "title": "seaborn chart",
    "section": "8.2 adjust size",
    "text": "8.2 adjust size\n\n\nCode\nplt.clf()\n\nplt.figure(figsize=(10, 6))\n\nax=sns.boxplot(x = \"day\", y = \"total_bill\", data = df)\nax.set_title(\"tips box plot \")\n\nplt.show()\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html#fivethirtyeight-themes",
    "href": "plot/1 seaborn.html#fivethirtyeight-themes",
    "title": "seaborn chart",
    "section": "9.6 fivethirtyeight themes",
    "text": "9.6 fivethirtyeight themes\n\n\nCode\nplt.clf()\n\nplt.style.use('fivethirtyeight')\n\nsns.boxplot(x = \"day\", y = \"total_bill\", data = df)\n\nplt.show()",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html#dark_background",
    "href": "plot/1 seaborn.html#dark_background",
    "title": "seaborn chart",
    "section": "9.9 dark_background",
    "text": "9.9 dark_background\n\n\nCode\nplt.clf()\nplt.style.use('dark_background')\n\nsns.boxplot(x = \"day\", y = \"total_bill\", data = df)\nfig.tight_layout()\nplt.show()",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html#tableau-colorblind1",
    "href": "plot/1 seaborn.html#tableau-colorblind1",
    "title": "seaborn chart",
    "section": "11.8 tableau-colorblind1",
    "text": "11.8 tableau-colorblind1\n\n\nCode\nmatplotlib.style.use('tableau-colorblind10')\n\nsns.boxplot(x = \"day\", y = \"total_bill\", data = df)\nfig.tight_layout()\nplt.show()",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html#tableau-colorblind10",
    "href": "plot/1 seaborn.html#tableau-colorblind10",
    "title": "seaborn chart",
    "section": "9.8 tableau-colorblind10",
    "text": "9.8 tableau-colorblind10\n\n\nCode\nplt.clf()\nplt.style.use('tableau-colorblind10')\n\nsns.boxplot(x = \"day\", y = \"total_bill\", data = df)\nfig.tight_layout()\nplt.show()",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html#ggplot",
    "href": "plot/1 seaborn.html#ggplot",
    "title": "seaborn chart",
    "section": "9.7 ggplot",
    "text": "9.7 ggplot\n\n\nCode\nplt.clf()\n\nplt.style.use('ggplot')\n\nsns.boxplot(x = \"day\", y = \"total_bill\", data = df)\nfig.tight_layout()\nplt.show()",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html#add-legend",
    "href": "plot/1 seaborn.html#add-legend",
    "title": "seaborn chart",
    "section": "10.3 add legend",
    "text": "10.3 add legend\n\n\nCode\ng = sns.FacetGrid(data=tips, col=\"day\",col_wrap=2, hue=\"sex\")\n\ng.map_dataframe(sns.scatterplot, x=\"total_bill\", y=\"tip\")\n\ng.add_legend()",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "plot/2 plotnine.html#xkcd",
    "href": "plot/2 plotnine.html#xkcd",
    "title": "plotnine chart",
    "section": "9.1 xkcd",
    "text": "9.1 xkcd\n\n\nCode\np=(\n    ggplot(data=tips)+aes(x=\"tip\",fill = 'sex')+ geom_histogram(position = 'dodge')+ theme_xkcd()\n)\n\np",
    "crumbs": [
      "Plot",
      "plotnine chart"
    ]
  },
  {
    "objectID": "plot/2 plotnine.html#theme_538",
    "href": "plot/2 plotnine.html#theme_538",
    "title": "plotnine chart",
    "section": "9.2 theme_538",
    "text": "9.2 theme_538\n\n\nCode\np=(\n    ggplot(data=tips)+aes(x=\"tip\",fill = 'sex')+ geom_histogram(position = 'dodge')+ theme_538()\n)\n\np",
    "crumbs": [
      "Plot",
      "plotnine chart"
    ]
  },
  {
    "objectID": "plot/2 plotnine.html#theme_dark",
    "href": "plot/2 plotnine.html#theme_dark",
    "title": "plotnine chart",
    "section": "9.3 theme_dark",
    "text": "9.3 theme_dark\n\n\nCode\np=(\n    ggplot(data=tips)+aes(x=\"tip\",fill = 'sex')+ geom_histogram(position = 'dodge')+ theme_dark()\n)\n\np",
    "crumbs": [
      "Plot",
      "plotnine chart"
    ]
  },
  {
    "objectID": "plot/2 plotnine.html#add-title",
    "href": "plot/2 plotnine.html#add-title",
    "title": "plotnine chart",
    "section": "8.1 add title",
    "text": "8.1 add title\n\n\nCode\np=(\n    ggplot(data=tips)+aes(x=\"tip\",fill = 'sex')+ geom_histogram(position = 'dodge')+ ggtitle(\"tip by sex\")\n)\n\np",
    "crumbs": [
      "Plot",
      "plotnine chart"
    ]
  },
  {
    "objectID": "plot/3 plotly.html#add-title",
    "href": "plot/3 plotly.html#add-title",
    "title": "Plotly chart",
    "section": "8.1 add title",
    "text": "8.1 add title\n\n\nCode\nfig = px.scatter(tips,x=\"tip\", y=\"total_bill\", title=\"total_bill title\").update_layout(title_x=0.5)\nfig.show()",
    "crumbs": [
      "Plot",
      "Plotly chart"
    ]
  },
  {
    "objectID": "plot/3 plotly.html#adjust-size",
    "href": "plot/3 plotly.html#adjust-size",
    "title": "Plotly chart",
    "section": "8.2 adjust size",
    "text": "8.2 adjust size\n\n\nCode\nfig = px.scatter(tips,x=\"tip\", y=\"total_bill\")\n\nfig.update_layout(\n    autosize=False\n    ,width=200\n    ,height=200\n    )\n    \nfig.show()",
    "crumbs": [
      "Plot",
      "Plotly chart"
    ]
  },
  {
    "objectID": "plot/3 plotly.html#ggplot2-themes",
    "href": "plot/3 plotly.html#ggplot2-themes",
    "title": "Plotly chart",
    "section": "9.1 ggplot2 themes",
    "text": "9.1 ggplot2 themes\n\n\nCode\nfig = px.scatter(tips,x=\"tip\", y=\"total_bill\",template=\"ggplot2\")\nfig.show()",
    "crumbs": [
      "Plot",
      "Plotly chart"
    ]
  },
  {
    "objectID": "plot/3 plotly.html#seaborn-themes",
    "href": "plot/3 plotly.html#seaborn-themes",
    "title": "Plotly chart",
    "section": "9.2 seaborn themes",
    "text": "9.2 seaborn themes\n\n\nCode\nfig = px.scatter(tips,x=\"tip\", y=\"total_bill\",template=\"seaborn\")\nfig.show()",
    "crumbs": [
      "Plot",
      "Plotly chart"
    ]
  },
  {
    "objectID": "plot/3 plotly.html#plotly_dark-themes",
    "href": "plot/3 plotly.html#plotly_dark-themes",
    "title": "Plotly chart",
    "section": "9.3 plotly_dark themes",
    "text": "9.3 plotly_dark themes\n\n\nCode\nfig = px.scatter(tips,x=\"tip\", y=\"total_bill\",template=\"plotly_dark\")\nfig.show()",
    "crumbs": [
      "Plot",
      "Plotly chart"
    ]
  },
  {
    "objectID": "plot/2 plotnine.html#adjust-size",
    "href": "plot/2 plotnine.html#adjust-size",
    "title": "plotnine chart",
    "section": "8.2 adjust size",
    "text": "8.2 adjust size\n\n\nCode\np=(\n    ggplot(data=tips)+aes(x=\"tip\",fill = 'sex')+ geom_histogram(position = 'dodge')+ ggtitle(\"tip by sex\")+ theme(figure_size=(4, 3)) \n)\n\np",
    "crumbs": [
      "Plot",
      "plotnine chart"
    ]
  },
  {
    "objectID": "classification/1.2 decision tree and hyperparameter tuning.html#save-model",
    "href": "classification/1.2 decision tree and hyperparameter tuning.html#save-model",
    "title": "Decision tree and hyperparameter tuning",
    "section": "3.7 save model",
    "text": "3.7 save model\n\n\nCode\nfrom joblib import dump, load\ndump(model_dt, 'trained_model.joblib') \n\n\n['trained_model.joblib']",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/1.2 decision tree and hyperparameter tuning.html#load-model",
    "href": "classification/1.2 decision tree and hyperparameter tuning.html#load-model",
    "title": "Decision tree and hyperparameter tuning",
    "section": "3.8 load model",
    "text": "3.8 load model\n\n\nCode\nmodel_dt_reload = load('trained_model.joblib') \n\n\n\n\nCode\nY_pred_dt = model_dt_reload.predict(X_test) #always gets x and retuns y\n\nY_pred_dt[0:5]\n\n\narray([0, 0, 0, 1, 0])",
    "crumbs": [
      "Classification",
      "Decision tree and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/1.1 decision tree on titanic data.html#save-model",
    "href": "classification/1.1 decision tree on titanic data.html#save-model",
    "title": "Decision tree",
    "section": "3.5 save model",
    "text": "3.5 save model\n\n\nCode\nfrom joblib import dump, load\ndump(model_dt, 'trained_model.joblib') \n\n\n['trained_model.joblib']",
    "crumbs": [
      "Classification",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification/1.1 decision tree on titanic data.html#load-model",
    "href": "classification/1.1 decision tree on titanic data.html#load-model",
    "title": "Decision tree",
    "section": "3.6 load model",
    "text": "3.6 load model\n\n\nCode\nmodel_dt_reload = load('trained_model.joblib') \n\n\n\n\nCode\nY_pred_dt = model_dt_reload.predict(X_test) #always gets x and retuns y\n\nY_pred_dt[0:5]\n\n\narray([1, 0, 0, 1, 1])",
    "crumbs": [
      "Classification",
      "Decision tree"
    ]
  },
  {
    "objectID": "data manipulation/2 siuba.html#drop-column",
    "href": "data manipulation/2 siuba.html#drop-column",
    "title": "Data manipulation with siuba",
    "section": "5 drop column",
    "text": "5 drop column\n\n\nCode\nsmall_mtcars &gt;&gt; select(~_.cyl)\n\n\n\n\n\n\n\n\n\n\nmpg\nhp\n\n\n\n\n0\n21.0\n110\n\n\n1\n21.0\n110\n\n\n2\n22.8\n93\n\n\n3\n21.4\n110\n\n\n4\n18.7\n175",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "classification/6.2 Random Forest.html#categorical_cols-and-numerical_cols",
    "href": "classification/6.2 Random Forest.html#categorical_cols-and-numerical_cols",
    "title": "Random Forest with pipeline and hyperparameter tuning",
    "section": "2.5 categorical_cols and numerical_cols",
    "text": "2.5 categorical_cols and numerical_cols\n\n\nCode\ncategorical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].nunique() &lt; 10 and X_train[cname].dtype == \"object\"]\n                    \n                    \nnumerical_cols = numerical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].dtype in ['int64', 'float64']]\n\n\n\n\nCode\nprint(\"The total number of categorical columns:\", len(categorical_cols))\nprint(\"The total number of numerical columns:\", len(numerical_cols))\n\n\nThe total number of categorical columns: 0\nThe total number of numerical columns: 4\n\n\n\n\nCode\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train[my_cols].copy()\n#X_val = X_val[my_cols].copy()\nX_test= X_test[my_cols].copy()\n\n\n#X_final = df_test[my_cols].copy()",
    "crumbs": [
      "Classification",
      "Random Forest with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/6.2 Random Forest.html#pipelines-for-data-preprocessing",
    "href": "classification/6.2 Random Forest.html#pipelines-for-data-preprocessing",
    "title": "Random Forest with pipeline and hyperparameter tuning",
    "section": "2.6 Pipelines for Data Preprocessing",
    "text": "2.6 Pipelines for Data Preprocessing\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\nCode\nnumerical_transformer = Pipeline(steps=[\n    ('imputer_num', SimpleImputer(strategy='median')), \n    ('scaler', StandardScaler())\n])\n\n\n\n\nCode\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n\n\nCode\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)])",
    "crumbs": [
      "Classification",
      "Random Forest with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/6.2 Random Forest.html#define-pipline",
    "href": "classification/6.2 Random Forest.html#define-pipline",
    "title": "Random Forest with pipeline and hyperparameter tuning",
    "section": "3.2 define pipline",
    "text": "3.2 define pipline\n\n\nCode\npipeline = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', ml_model)\n         ]\n)",
    "crumbs": [
      "Classification",
      "Random Forest with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/6.2 Random Forest.html#hyperparameter-tuning-set",
    "href": "classification/6.2 Random Forest.html#hyperparameter-tuning-set",
    "title": "Random Forest with pipeline and hyperparameter tuning",
    "section": "3.3 hyperparameter tuning set",
    "text": "3.3 hyperparameter tuning set\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\nparameters = {'model__max_depth':[20,30,40],\n                 'model__n_estimators':[200,250,300],\n                 'model__min_samples_leaf':[1,2,3]\n                \n                 }\n                 \n              \nGridCV = GridSearchCV(pipeline, parameters\n                    ,scoring='accuracy'\n                    ,cv=10\n                    ,n_jobs= -1, verbose=1)",
    "crumbs": [
      "Classification",
      "Random Forest with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/6.2 Random Forest.html#save-model",
    "href": "classification/6.2 Random Forest.html#save-model",
    "title": "Random Forest with pipeline and hyperparameter tuning",
    "section": "3.7 save model",
    "text": "3.7 save model\n\n\nCode\nfrom joblib import dump, load\ndump(ml_model, 'trained_model_6_2.joblib') \n\n\n['trained_model_6_2.joblib']\n\n\nsave trained pipeline\n\n\nCode\nfrom joblib import dump, load\ndump(GridCV, 'trained_pipeline_6_2.pkl', compress=True)  \n\n\n['trained_pipeline_6_2.pkl']",
    "crumbs": [
      "Classification",
      "Random Forest with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/6.2 Random Forest.html#load-model",
    "href": "classification/6.2 Random Forest.html#load-model",
    "title": "Random Forest with pipeline and hyperparameter tuning",
    "section": "3.8 load model",
    "text": "3.8 load model\n\n\nCode\nmodel_dt_reload = load('trained_model_6_2.joblib') \n\n\n\n\nCode\nY_pred_dt = model_dt_reload.predict(X_test) #always gets x and retuns y\n\nY_pred_dt[0:5]\n\n\narray([0, 0, 0, 1, 1])",
    "crumbs": [
      "Classification",
      "Random Forest with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/6.2 Random Forest.html#final-prediction",
    "href": "classification/6.2 Random Forest.html#final-prediction",
    "title": "Random Forest with pipeline and hyperparameter tuning",
    "section": "3.9 final prediction",
    "text": "3.9 final prediction",
    "crumbs": [
      "Classification",
      "Random Forest with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "regression/3 Random forest on house price data.html#categorical_cols-and-numerical_cols",
    "href": "regression/3 Random forest on house price data.html#categorical_cols-and-numerical_cols",
    "title": "Random forest and pipeline",
    "section": "2.5 categorical_cols and numerical_cols",
    "text": "2.5 categorical_cols and numerical_cols\n\n\nCode\ncategorical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].nunique() &lt; 10 and X_train[cname].dtype == \"object\"]\n                    \n                    \nnumerical_cols = numerical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].dtype in ['int64', 'float64']]\n\n\n\n\nCode\nprint(\"The total number of categorical columns:\", len(categorical_cols))\nprint(\"The total number of numerical columns:\", len(numerical_cols))\n\n\nThe total number of categorical columns: 40\nThe total number of numerical columns: 37\n\n\n\n\nCode\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train[my_cols].copy()\nX_test= X_test[my_cols].copy()\n\n\n#X_final = df_test[my_cols].copy()",
    "crumbs": [
      "Regression",
      "Random forest and pipeline"
    ]
  },
  {
    "objectID": "classification/7 XGboost.html",
    "href": "classification/7 XGboost.html",
    "title": "XGboost with pipeline",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nimport time",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7 XGboost.html#input-data",
    "href": "classification/7 XGboost.html#input-data",
    "title": "XGboost with pipeline",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7 XGboost.html#data-eda",
    "href": "classification/7 XGboost.html#data-eda",
    "title": "XGboost with pipeline",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n\n\n0.7420382165605095\n0.18890814558058924",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7 XGboost.html#data-wrangling",
    "href": "classification/7 XGboost.html#data-wrangling",
    "title": "XGboost with pipeline",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1046 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1308 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Dealing with missing numerical variables\ndata['Age'] = data.Age.fillna(data.Age.median())\ndata['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1309 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1309 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\ndata = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\nSex_male\n\n\n\n\n0\n1\n3\nBraund, Mr. Owen Harris\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\nTrue\n\n\n1\n2\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\nFalse\n\n\n2\n3\n3\nHeikkinen, Miss. Laina\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\nFalse\n\n\n3\n4\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\nFalse\n\n\n4\n5\n3\nAllen, Mr. William Henry\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\nTrue\n\n\n\n\n\n\n\n\n\n\nCode\n# Select features columns\ndata = data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()\n\n\n\n\n\n\n\n\n\n\nSex_male\nFare\nAge\nPclass\nSibSp\nrole\n\n\n\n\n0\nTrue\n7.2500\n22.0\n3\n1\ntrain\n\n\n1\nFalse\n71.2833\n38.0\n1\n1\ntrain\n\n\n2\nFalse\n7.9250\n26.0\n3\n0\ntrain\n\n\n3\nFalse\n53.1000\n35.0\n1\n1\ntrain\n\n\n4\nTrue\n8.0500\n35.0\n3\n0\ntrain",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7 XGboost.html#split-data",
    "href": "classification/7 XGboost.html#split-data",
    "title": "XGboost with pipeline",
    "section": "2.4 split data",
    "text": "2.4 split data\n60% training / 30% validation/ 10% testing\n\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\n#X_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\n\ntraining_size=0.6\nvalidation_size=0.3\ntesting_size=0.1\n\n\nX_train, X_val, Y_train, Y_val= train_test_split(X, Y, test_size=validation_size, random_state=1)\n\n\nX_train, X_test, Y_train, Y_test= train_test_split(X_train, Y_train, test_size=testing_size/training_size, random_state=1) \n    \n\nX_train = X_train.drop('role', axis=1)\nX_val = X_val.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 519 entries, 129 to 818\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Sex_male  519 non-null    bool   \n 1   Fare      519 non-null    float64\n 2   Age       519 non-null    float64\n 3   Pclass    519 non-null    int64  \n 4   SibSp     519 non-null    int64  \ndtypes: bool(1), float64(2), int64(2)\nmemory usage: 20.8 KB\n\n\n\n\nCode\nlen(X_train)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.5824915824915825\n\n\n\n\nCode\nlen(X_val)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.30078563411896747\n\n\n\n\nCode\nlen(X_test)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.11672278338945005",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7 XGboost.html#categorical_cols-and-numerical_cols",
    "href": "classification/7 XGboost.html#categorical_cols-and-numerical_cols",
    "title": "XGboost with pipeline",
    "section": "2.5 categorical_cols and numerical_cols",
    "text": "2.5 categorical_cols and numerical_cols\n\n\nCode\ncategorical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].nunique() &lt; 10 and X_train[cname].dtype == \"object\"]\n                    \n                    \nnumerical_cols = numerical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].dtype in ['int64', 'float64']]\n\n\n\n\nCode\nprint(\"The total number of categorical columns:\", len(categorical_cols))\nprint(\"The total number of numerical columns:\", len(numerical_cols))\n\n\nThe total number of categorical columns: 0\nThe total number of numerical columns: 4\n\n\n\n\nCode\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train[my_cols].copy()\nX_val = X_val[my_cols].copy()\nX_test= X_test[my_cols].copy()\n\n\n#X_final = df_test[my_cols].copy()",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7 XGboost.html#pipelines-for-data-preprocessing",
    "href": "classification/7 XGboost.html#pipelines-for-data-preprocessing",
    "title": "XGboost with pipeline",
    "section": "2.6 Pipelines for Data Preprocessing",
    "text": "2.6 Pipelines for Data Preprocessing\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\nCode\nnumerical_transformer = Pipeline(steps=[\n    ('imputer_num', SimpleImputer(strategy='median')), \n    ('scaler', StandardScaler())\n])\n\n\n\n\nCode\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n\n\nCode\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)])",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7 XGboost.html#define-model",
    "href": "classification/7 XGboost.html#define-model",
    "title": "XGboost with pipeline",
    "section": "3.1 define model",
    "text": "3.1 define model\n\n\nCode\nimport xgboost\nprint(xgboost.__version__)\n\n\n2.0.3\n\n\n\n\nCode\nfrom xgboost import XGBClassifier\nml_model = XGBClassifier()\nml_model\n\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. XGBClassifieriNot fittedXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...)",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7 XGboost.html#define-pipline",
    "href": "classification/7 XGboost.html#define-pipline",
    "title": "XGboost with pipeline",
    "section": "3.2 define pipline",
    "text": "3.2 define pipline\n\n\nCode\npipeline = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', ml_model)\n         ]\n)",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7 XGboost.html#train-model",
    "href": "classification/7 XGboost.html#train-model",
    "title": "XGboost with pipeline",
    "section": "3.3 train model",
    "text": "3.3 train model\n\n\nCode\nstart_time = time.time()\n\npipeline.fit(X_train, Y_train)\n\nend_time = time.time()\nduration = end_time - start_time\nduration\n\n\n0.04808187484741211",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7 XGboost.html#preformance",
    "href": "classification/7 XGboost.html#preformance",
    "title": "XGboost with pipeline",
    "section": "3.4 Preformance",
    "text": "3.4 Preformance\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = pipeline.predict(X_val) #always gets x and retuns y\nY_pred_dt\n\n\narray([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,\n       1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n       1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n       0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n       1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n       1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,\n       0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,\n       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n       1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n       0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n       1, 1, 1, 1])\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_val,Y_pred_dt)  \naccuracy\n\n\n0.6865671641791045\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_val,Y_pred_dt)  \nprecision_dt\n\n\n0.6631578947368421\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_val,Y_pred_dt)  \nrecall_dt\n\n\n0.5478260869565217\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_val,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[121,  32],\n       [ 52,  63]])\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_val, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.6693378800795681",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7 XGboost.html#k-fold-cross-validation",
    "href": "classification/7 XGboost.html#k-fold-cross-validation",
    "title": "XGboost with pipeline",
    "section": "3.5 k-Fold Cross-Validation",
    "text": "3.5 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(ml_model, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.6589245705750562",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7 XGboost.html#save-model",
    "href": "classification/7 XGboost.html#save-model",
    "title": "XGboost with pipeline",
    "section": "3.6 save model",
    "text": "3.6 save model\n\n\nCode\nfrom joblib import dump, load\ndump(ml_model, 'trained_model_7.joblib') \n\n\n['trained_model_7.joblib']\n\n\nsave trained pipeline\n\n\nCode\nfrom joblib import dump, load\ndump(pipeline, 'trained_pipeline_7.pkl', compress=True)  \n\n\n['trained_pipeline_7.pkl']",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7 XGboost.html#load-model",
    "href": "classification/7 XGboost.html#load-model",
    "title": "XGboost with pipeline",
    "section": "3.7 load model",
    "text": "3.7 load model\n\n\nCode\nmodel_dt_reload = load('trained_model_7.joblib')",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7 XGboost.html#final-prediction",
    "href": "classification/7 XGboost.html#final-prediction",
    "title": "XGboost with pipeline",
    "section": "3.8 final prediction",
    "text": "3.8 final prediction\n\n\nCode\nY_pred_dt_final = model_dt_reload.predict(X_test) #always gets x and retuns y\n\nY_pred_dt_final[0:5]\n\n\narray([0, 0, 0, 0, 0])",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost.html",
    "href": "classification/7.1 XGboost.html",
    "title": "XGboost with pipeline",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nimport time",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost.html#input-data",
    "href": "classification/7.1 XGboost.html#input-data",
    "title": "XGboost with pipeline",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost.html#data-eda",
    "href": "classification/7.1 XGboost.html#data-eda",
    "title": "XGboost with pipeline",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n\n\n0.7420382165605095\n0.18890814558058924",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost.html#data-wrangling",
    "href": "classification/7.1 XGboost.html#data-wrangling",
    "title": "XGboost with pipeline",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1046 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1308 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Dealing with missing numerical variables\ndata['Age'] = data.Age.fillna(data.Age.median())\ndata['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1309 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1309 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\ndata = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\nSex_male\n\n\n\n\n0\n1\n3\nBraund, Mr. Owen Harris\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\nTrue\n\n\n1\n2\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\nFalse\n\n\n2\n3\n3\nHeikkinen, Miss. Laina\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\nFalse\n\n\n3\n4\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\nFalse\n\n\n4\n5\n3\nAllen, Mr. William Henry\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\nTrue\n\n\n\n\n\n\n\n\n\n\nCode\n# Select features columns\ndata = data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()\n\n\n\n\n\n\n\n\n\n\nSex_male\nFare\nAge\nPclass\nSibSp\nrole\n\n\n\n\n0\nTrue\n7.2500\n22.0\n3\n1\ntrain\n\n\n1\nFalse\n71.2833\n38.0\n1\n1\ntrain\n\n\n2\nFalse\n7.9250\n26.0\n3\n0\ntrain\n\n\n3\nFalse\n53.1000\n35.0\n1\n1\ntrain\n\n\n4\nTrue\n8.0500\n35.0\n3\n0\ntrain",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost.html#split-data",
    "href": "classification/7.1 XGboost.html#split-data",
    "title": "XGboost with pipeline",
    "section": "2.4 split data",
    "text": "2.4 split data\n60% training / 30% validation/ 10% testing\n\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\n#X_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\n\ntraining_size=0.6\nvalidation_size=0.3\ntesting_size=0.1\n\n\nX_train, X_val, Y_train, Y_val= train_test_split(X, Y, test_size=validation_size, random_state=1)\n\n\nX_train, X_test, Y_train, Y_test= train_test_split(X_train, Y_train, test_size=testing_size/training_size, random_state=1) \n    \n\nX_train = X_train.drop('role', axis=1)\nX_val = X_val.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 519 entries, 129 to 818\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Sex_male  519 non-null    bool   \n 1   Fare      519 non-null    float64\n 2   Age       519 non-null    float64\n 3   Pclass    519 non-null    int64  \n 4   SibSp     519 non-null    int64  \ndtypes: bool(1), float64(2), int64(2)\nmemory usage: 20.8 KB\n\n\n\n\nCode\nlen(X_train)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.5824915824915825\n\n\n\n\nCode\nlen(X_val)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.30078563411896747\n\n\n\n\nCode\nlen(X_test)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.11672278338945005",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost.html#categorical_cols-and-numerical_cols",
    "href": "classification/7.1 XGboost.html#categorical_cols-and-numerical_cols",
    "title": "XGboost with pipeline",
    "section": "2.5 categorical_cols and numerical_cols",
    "text": "2.5 categorical_cols and numerical_cols\n\n\nCode\ncategorical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].nunique() &lt; 10 and X_train[cname].dtype == \"object\"]\n                    \n                    \nnumerical_cols = numerical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].dtype in ['int64', 'float64']]\n\n\n\n\nCode\nprint(\"The total number of categorical columns:\", len(categorical_cols))\nprint(\"The total number of numerical columns:\", len(numerical_cols))\n\n\nThe total number of categorical columns: 0\nThe total number of numerical columns: 4\n\n\n\n\nCode\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train[my_cols].copy()\nX_val = X_val[my_cols].copy()\nX_test= X_test[my_cols].copy()\n\n\n#X_final = df_test[my_cols].copy()",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost.html#pipelines-for-data-preprocessing",
    "href": "classification/7.1 XGboost.html#pipelines-for-data-preprocessing",
    "title": "XGboost with pipeline",
    "section": "2.6 Pipelines for Data Preprocessing",
    "text": "2.6 Pipelines for Data Preprocessing\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\nCode\nnumerical_transformer = Pipeline(steps=[\n    ('imputer_num', SimpleImputer(strategy='median')), \n    ('scaler', StandardScaler())\n])\n\n\n\n\nCode\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n\n\nCode\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)])",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost.html#define-model",
    "href": "classification/7.1 XGboost.html#define-model",
    "title": "XGboost with pipeline",
    "section": "3.1 define model",
    "text": "3.1 define model\n\n\nCode\nimport xgboost\nprint(xgboost.__version__)\n\n\n2.0.3\n\n\n\n\nCode\nfrom xgboost import XGBClassifier\nml_model = XGBClassifier()\nml_model\n\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. XGBClassifieriNot fittedXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...)",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost.html#define-pipline",
    "href": "classification/7.1 XGboost.html#define-pipline",
    "title": "XGboost with pipeline",
    "section": "3.2 define pipline",
    "text": "3.2 define pipline\n\n\nCode\npipeline = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', ml_model)\n         ]\n)",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost.html#train-model",
    "href": "classification/7.1 XGboost.html#train-model",
    "title": "XGboost with pipeline",
    "section": "3.3 train model",
    "text": "3.3 train model\n\n\nCode\nstart_time = time.time()\n\npipeline.fit(X_train, Y_train)\n\nend_time = time.time()\nduration = end_time - start_time\nduration\n\n\n0.06467294692993164",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost.html#preformance",
    "href": "classification/7.1 XGboost.html#preformance",
    "title": "XGboost with pipeline",
    "section": "3.4 Preformance",
    "text": "3.4 Preformance\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = pipeline.predict(X_val) #always gets x and retuns y\nY_pred_dt\n\n\narray([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,\n       1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n       1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n       0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n       1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n       1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,\n       0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,\n       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n       1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n       0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n       1, 1, 1, 1])\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_val,Y_pred_dt)  \naccuracy\n\n\n0.6865671641791045\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_val,Y_pred_dt)  \nprecision_dt\n\n\n0.6631578947368421\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_val,Y_pred_dt)  \nrecall_dt\n\n\n0.5478260869565217\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_val,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[121,  32],\n       [ 52,  63]])\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_val, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.6693378800795681",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost.html#k-fold-cross-validation",
    "href": "classification/7.1 XGboost.html#k-fold-cross-validation",
    "title": "XGboost with pipeline",
    "section": "3.5 k-Fold Cross-Validation",
    "text": "3.5 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(ml_model, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.6974794622852876",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost.html#save-model",
    "href": "classification/7.1 XGboost.html#save-model",
    "title": "XGboost with pipeline",
    "section": "3.6 save model",
    "text": "3.6 save model\n\n\nCode\nfrom joblib import dump, load\ndump(ml_model, 'trained_model_7.joblib') \n\n\n['trained_model_7.joblib']\n\n\nsave trained pipeline\n\n\nCode\nfrom joblib import dump, load\ndump(pipeline, 'trained_pipeline_7.pkl', compress=True)  \n\n\n['trained_pipeline_7.pkl']",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost.html#load-model",
    "href": "classification/7.1 XGboost.html#load-model",
    "title": "XGboost with pipeline",
    "section": "3.7 load model",
    "text": "3.7 load model\n\n\nCode\nmodel_dt_reload = load('trained_model_7.joblib')",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost.html#final-prediction",
    "href": "classification/7.1 XGboost.html#final-prediction",
    "title": "XGboost with pipeline",
    "section": "3.8 final prediction",
    "text": "3.8 final prediction\n\n\nCode\nY_pred_dt_final = model_dt_reload.predict(X_test) #always gets x and retuns y\n\nY_pred_dt_final[0:5]\n\n\narray([0, 0, 0, 0, 0])",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost.html",
    "href": "classification/7.2 XGboost.html",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nimport time",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost.html#input-data",
    "href": "classification/7.2 XGboost.html#input-data",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\n\n\n# Showing overview of the train dataset\ndf_train.head()",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost.html#data-eda",
    "href": "classification/7.2 XGboost.html#data-eda",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\nCode\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost.html#data-wrangling",
    "href": "classification/7.2 XGboost.html#data-wrangling",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n\n\nCode\n# Dealing with missing numerical variables\ndata['Age'] = data.Age.fillna(data.Age.median())\ndata['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\ndata = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\nCode\n# Select features columns\ndata = data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost.html#split-data",
    "href": "classification/7.2 XGboost.html#split-data",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "2.4 split data",
    "text": "2.4 split data\n60% training / 30% validation/ 10% testing\n\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\n#X_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\n\ntraining_size=0.6\nvalidation_size=0.3\ntesting_size=0.1\n\n\nX_train, X_val, Y_train, Y_val= train_test_split(X, Y, test_size=validation_size, random_state=1)\n\n\nX_train, X_test, Y_train, Y_test= train_test_split(X_train, Y_train, test_size=testing_size/training_size, random_state=1) \n    \n\nX_train = X_train.drop('role', axis=1)\nX_val = X_val.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n\n\nCode\nlen(X_train)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n\n\nCode\nlen(X_val)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n\n\nCode\nlen(X_test)/(len(X_train) +len(X_val) +len(X_test) )",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost.html#categorical_cols-and-numerical_cols",
    "href": "classification/7.2 XGboost.html#categorical_cols-and-numerical_cols",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "2.5 categorical_cols and numerical_cols",
    "text": "2.5 categorical_cols and numerical_cols\n\n\nCode\ncategorical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].nunique() &lt; 10 and X_train[cname].dtype == \"object\"]\n                    \n                    \nnumerical_cols = numerical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].dtype in ['int64', 'float64']]\n\n\n\n\nCode\nprint(\"The total number of categorical columns:\", len(categorical_cols))\nprint(\"The total number of numerical columns:\", len(numerical_cols))\n\n\n\n\nCode\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train[my_cols].copy()\nX_val = X_val[my_cols].copy()\nX_test= X_test[my_cols].copy()\n\n\n#X_final = df_test[my_cols].copy()",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost.html#pipelines-for-data-preprocessing",
    "href": "classification/7.2 XGboost.html#pipelines-for-data-preprocessing",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "2.6 Pipelines for Data Preprocessing",
    "text": "2.6 Pipelines for Data Preprocessing\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\nCode\nnumerical_transformer = Pipeline(steps=[\n    ('imputer_num', SimpleImputer(strategy='median')), \n    ('scaler', StandardScaler())\n])\n\n\n\n\nCode\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n\n\nCode\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)])",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost.html#define-model",
    "href": "classification/7.2 XGboost.html#define-model",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "3.1 define model",
    "text": "3.1 define model\n\n\nCode\nimport xgboost\nprint(xgboost.__version__)\n\n\n\n\nCode\nfrom xgboost import XGBClassifier\nml_model = XGBClassifier()\nml_model",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost.html#define-pipline",
    "href": "classification/7.2 XGboost.html#define-pipline",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "3.2 define pipline",
    "text": "3.2 define pipline\n\n\nCode\npipeline = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', ml_model)\n         ]\n)",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost.html#define-hyperparameter-tuning-set",
    "href": "classification/7.2 XGboost.html#define-hyperparameter-tuning-set",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "3.3 define hyperparameter tuning set",
    "text": "3.3 define hyperparameter tuning set\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\n\n\nparameters = {\n        'model__learning_rate': [0.01, 0.02,0.08,0.1],\n        'model__max_depth': [3, 5, 7,8,9,10,20],\n        'model__min_child_weight': [1, 3,5,8],\n        'model__subsample': [0.5, 0.7,0.9],\n        \n       # 'model__colsample__bytree': [0.5, 0.7],\n       \n        'model__n_estimators' : [100, 200, 500],\n        'model__objective': ['reg:squarederror']\n    }\n\n\nall parameters combinations\n\n\nCode\nimport itertools\na = parameters.values()\ncombinations = list(itertools.product(*a))\n\n\n1440 combinations\n\n\nCode\nlen(combinations)\n\n\n\n\nCode\ncombinations[0:5]\n\n\n\n\nCode\nGridCV = GridSearchCV(pipeline\n                ,parameters\n                ,scoring='accuracy'\n                , cv=10, n_jobs=-1)",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost.html#train-model",
    "href": "classification/7.2 XGboost.html#train-model",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "3.4 train model",
    "text": "3.4 train model\n\n\nCode\nstart_time = time.time()\n\nGridCV.fit(X_train, Y_train)\n\nend_time = time.time()\nduration = end_time - start_time\nduration\n\n\n\n3.4.0.1 tunning result\n\n3.4.0.1.1 GridSearchCV\n\n\nCode\n# get the parameter names\ncolumn_results = [f\"param_{name}\" for name in parameters.keys()]\ncolumn_results += [\"mean_test_score\", \"std_test_score\", \"rank_test_score\"]\n\ncv_results = pd.DataFrame(GridCV.cv_results_)\ncv_results = cv_results[column_results].sort_values(\n    \"mean_test_score\", ascending=False\n)\n\n\ndef shorten_param(param_name):\n    if \"__\" in param_name:\n        return param_name.rsplit(\"__\", 1)[1]\n    return param_name\n\n\ncv_results = cv_results.rename(shorten_param, axis=1)\ncv_results.head()\n\n\n\n\n\n3.4.0.2 tunning best parameters\n\n\nCode\nGridCV.best_params_\n\n\n\n\n3.4.0.3 tunning best model\n\n\nCode\nmodel_ml = GridCV.best_estimator_",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost.html#preformance",
    "href": "classification/7.2 XGboost.html#preformance",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "3.5 Preformance",
    "text": "3.5 Preformance\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = model_ml.predict(X_val) #always gets x and retuns y\nY_pred_dt\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_val,Y_pred_dt)  \naccuracy\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_val,Y_pred_dt)  \nprecision_dt\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_val,Y_pred_dt)  \nrecall_dt\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_val,Y_pred_dt)\nconfusion_matrix_dt\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_val, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost.html#k-fold-cross-validation",
    "href": "classification/7.2 XGboost.html#k-fold-cross-validation",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "3.6 k-Fold Cross-Validation",
    "text": "3.6 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(ml_model, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost.html#save-model",
    "href": "classification/7.2 XGboost.html#save-model",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "3.7 save model",
    "text": "3.7 save model\n\n\nCode\nfrom joblib import dump, load\ndump(model_ml, 'trained_model_7_2.joblib') \n\n\nsave trained pipeline\n\n\nCode\nfrom joblib import dump, load\ndump(GridCV, 'trained_GridCV_7_2.joblib', compress=True)",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost.html#load-model",
    "href": "classification/7.2 XGboost.html#load-model",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "3.8 load model",
    "text": "3.8 load model\n\n\nCode\nmodel_dt_reload = load('trained_model_7_2.joblib') \n\n\nload grid\n\n\nCode\nGrid_reload = load('trained_GridCV_7_2.joblib')",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost.html#final-prediction",
    "href": "classification/7.2 XGboost.html#final-prediction",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "3.9 final prediction",
    "text": "3.9 final prediction\n\n\nCode\nY_pred_dt_final = model_dt_reload.predict(X_test) #always gets x and retuns y\n\n#Y_pred_dt_final[0:5]",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost copy.html",
    "href": "classification/7.2 XGboost copy.html",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nimport time",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost copy.html#input-data",
    "href": "classification/7.2 XGboost copy.html#input-data",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\n\n\n# Showing overview of the train dataset\ndf_train.head()",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost copy.html#data-eda",
    "href": "classification/7.2 XGboost copy.html#data-eda",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\nCode\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost copy.html#data-wrangling",
    "href": "classification/7.2 XGboost copy.html#data-wrangling",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n\n\nCode\n# Dealing with missing numerical variables\ndata['Age'] = data.Age.fillna(data.Age.median())\ndata['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\ndata = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\nCode\n# Select features columns\ndata = data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost copy.html#split-data",
    "href": "classification/7.2 XGboost copy.html#split-data",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "2.4 split data",
    "text": "2.4 split data\n60% training / 30% validation/ 10% testing\n\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\n#X_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\n\ntraining_size=0.6\nvalidation_size=0.3\ntesting_size=0.1\n\n\nX_train, X_val, Y_train, Y_val= train_test_split(X, Y, test_size=validation_size, random_state=1)\n\n\nX_train, X_test, Y_train, Y_test= train_test_split(X_train, Y_train, test_size=testing_size/training_size, random_state=1) \n    \n\nX_train = X_train.drop('role', axis=1)\nX_val = X_val.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n\n\nCode\nlen(X_train)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n\n\nCode\nlen(X_val)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n\n\nCode\nlen(X_test)/(len(X_train) +len(X_val) +len(X_test) )",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost copy.html#categorical_cols-and-numerical_cols",
    "href": "classification/7.2 XGboost copy.html#categorical_cols-and-numerical_cols",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "2.5 categorical_cols and numerical_cols",
    "text": "2.5 categorical_cols and numerical_cols\n\n\nCode\ncategorical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].nunique() &lt; 10 and X_train[cname].dtype == \"object\"]\n                    \n                    \nnumerical_cols = numerical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].dtype in ['int64', 'float64']]\n\n\n\n\nCode\nprint(\"The total number of categorical columns:\", len(categorical_cols))\nprint(\"The total number of numerical columns:\", len(numerical_cols))\n\n\n\n\nCode\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train[my_cols].copy()\nX_val = X_val[my_cols].copy()\nX_test= X_test[my_cols].copy()\n\n\n#X_final = df_test[my_cols].copy()",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost copy.html#pipelines-for-data-preprocessing",
    "href": "classification/7.2 XGboost copy.html#pipelines-for-data-preprocessing",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "2.6 Pipelines for Data Preprocessing",
    "text": "2.6 Pipelines for Data Preprocessing\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\nCode\nnumerical_transformer = Pipeline(steps=[\n    ('imputer_num', SimpleImputer(strategy='median')), \n    ('scaler', StandardScaler())\n])\n\n\n\n\nCode\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n\n\nCode\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)])",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost copy.html#define-model",
    "href": "classification/7.2 XGboost copy.html#define-model",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "3.1 define model",
    "text": "3.1 define model\n\n\nCode\nimport xgboost\nprint(xgboost.__version__)\n\n\n\n\nCode\nfrom xgboost import XGBClassifier\nml_model = XGBClassifier()\nml_model",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost copy.html#define-pipline",
    "href": "classification/7.2 XGboost copy.html#define-pipline",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "3.2 define pipline",
    "text": "3.2 define pipline\n\n\nCode\npipeline = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', ml_model)\n         ]\n)",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost copy.html#define-hyperparameter-tuning-set",
    "href": "classification/7.2 XGboost copy.html#define-hyperparameter-tuning-set",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "3.3 define hyperparameter tuning set",
    "text": "3.3 define hyperparameter tuning set\n\n\nCode\nfrom sklearn.model_selection import GridSearchCV\n\n\nparameters = {\n        'model__learning_rate': [0.01, 0.02,0.08,0.1],\n        'model__max_depth': [3, 5, 7,8,9,10,15,20],\n        'model__min_child_weight': [1, 3,5,8,10],\n        'model__subsample': [0.5, 0.7,0.9],\n       # 'model__colsample__bytree': [0.5, 0.7],\n        'model__n_estimators' : [100, 200, 500],\n        'model__objective': ['reg:squarederror']\n    }\n\n\nall parameters combinations\n\n\nCode\nimport itertools\na = parameters.values()\ncombinations = list(itertools.product(*a))\n\n\n1440 combinations\n\n\nCode\nlen(combinations)\n\n\n\n\nCode\ncombinations[0:5]\n\n\n\n\nCode\nGridCV = GridSearchCV(pipeline\n                ,parameters\n                ,scoring='accuracy'\n                , cv=10, n_jobs=-1)",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost copy.html#train-model",
    "href": "classification/7.2 XGboost copy.html#train-model",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "3.4 train model",
    "text": "3.4 train model\n\n\nCode\nstart_time = time.time()\n\nGridCV.fit(X_train, Y_train)\n\nend_time = time.time()\nduration = end_time - start_time\nduration\n\n\n\n3.4.0.1 tunning result\n\n3.4.0.1.1 GridSearchCV\n\n\nCode\n# get the parameter names\ncolumn_results = [f\"param_{name}\" for name in parameters.keys()]\ncolumn_results += [\"mean_test_score\", \"std_test_score\", \"rank_test_score\"]\n\ncv_results = pd.DataFrame(GridCV.cv_results_)\ncv_results = cv_results[column_results].sort_values(\n    \"mean_test_score\", ascending=False\n)\n\n\ndef shorten_param(param_name):\n    if \"__\" in param_name:\n        return param_name.rsplit(\"__\", 1)[1]\n    return param_name\n\n\ncv_results = cv_results.rename(shorten_param, axis=1)\ncv_results.head()\n\n\n\n\n\n3.4.0.2 tunning best parameters\n\n\nCode\nGridCV.best_params_\n\n\n\n\n3.4.0.3 tunning best model\n\n\nCode\nmodel_ml = GridCV.best_estimator_",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost copy.html#preformance",
    "href": "classification/7.2 XGboost copy.html#preformance",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "3.5 Preformance",
    "text": "3.5 Preformance\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = model_ml.predict(X_val) #always gets x and retuns y\nY_pred_dt\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_val,Y_pred_dt)  \naccuracy\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_val,Y_pred_dt)  \nprecision_dt\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_val,Y_pred_dt)  \nrecall_dt\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_val,Y_pred_dt)\nconfusion_matrix_dt\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_val, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost copy.html#k-fold-cross-validation",
    "href": "classification/7.2 XGboost copy.html#k-fold-cross-validation",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "3.6 k-Fold Cross-Validation",
    "text": "3.6 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(ml_model, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost copy.html#save-model",
    "href": "classification/7.2 XGboost copy.html#save-model",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "3.7 save model",
    "text": "3.7 save model\n\n\nCode\nfrom joblib import dump, load\ndump(ml_model, 'trained_model_7_2.joblib') \n\n\nsave trained pipeline\n\n\nCode\nfrom joblib import dump, load\ndump(GridCV, 'trained_GridCV_7_2.pkl', compress=True)",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost copy.html#load-model",
    "href": "classification/7.2 XGboost copy.html#load-model",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "3.8 load model",
    "text": "3.8 load model\n\n\nCode\nmodel_dt_reload = load('trained_model_7_2.joblib')",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost copy.html#final-prediction",
    "href": "classification/7.2 XGboost copy.html#final-prediction",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "3.9 final prediction",
    "text": "3.9 final prediction\n\n\nCode\nY_pred_dt_final = model_dt_reload.predict(X_test) #always gets x and retuns y\n\nY_pred_dt_final[0:5]",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost.html",
    "href": "classification/7.3 XGboost.html",
    "title": "XGboost with pipeline and fast tuning",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nimport time",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost.html#input-data",
    "href": "classification/7.3 XGboost.html#input-data",
    "title": "XGboost with pipeline and fast tuning",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost.html#data-eda",
    "href": "classification/7.3 XGboost.html#data-eda",
    "title": "XGboost with pipeline and fast tuning",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n\n\n0.7420382165605095\n0.18890814558058924",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost.html#data-wrangling",
    "href": "classification/7.3 XGboost.html#data-wrangling",
    "title": "XGboost with pipeline and fast tuning",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1046 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1308 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Dealing with missing numerical variables\ndata['Age'] = data.Age.fillna(data.Age.median())\ndata['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Pclass       1309 non-null   int64  \n 2   Name         1309 non-null   object \n 3   Sex          1309 non-null   object \n 4   Age          1309 non-null   float64\n 5   SibSp        1309 non-null   int64  \n 6   Parch        1309 non-null   int64  \n 7   Ticket       1309 non-null   object \n 8   Fare         1309 non-null   float64\n 9   Cabin        295 non-null    object \n 10  Embarked     1307 non-null   object \n 11  role         1309 non-null   object \ndtypes: float64(2), int64(4), object(6)\nmemory usage: 132.9+ KB\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\ndata = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\nSex_male\n\n\n\n\n0\n1\n3\nBraund, Mr. Owen Harris\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\nTrue\n\n\n1\n2\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\nFalse\n\n\n2\n3\n3\nHeikkinen, Miss. Laina\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\nFalse\n\n\n3\n4\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\nFalse\n\n\n4\n5\n3\nAllen, Mr. William Henry\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\nTrue\n\n\n\n\n\n\n\n\n\n\nCode\n# Select features columns\ndata = data[['Sex_male', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()\n\n\n\n\n\n\n\n\n\n\nSex_male\nFare\nAge\nPclass\nSibSp\nrole\n\n\n\n\n0\nTrue\n7.2500\n22.0\n3\n1\ntrain\n\n\n1\nFalse\n71.2833\n38.0\n1\n1\ntrain\n\n\n2\nFalse\n7.9250\n26.0\n3\n0\ntrain\n\n\n3\nFalse\n53.1000\n35.0\n1\n1\ntrain\n\n\n4\nTrue\n8.0500\n35.0\n3\n0\ntrain",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost.html#split-data",
    "href": "classification/7.3 XGboost.html#split-data",
    "title": "XGboost with pipeline and fast tuning",
    "section": "2.4 split data",
    "text": "2.4 split data\n60% training / 30% validation/ 10% testing\n\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\n#X_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\n\ntraining_size=0.6\nvalidation_size=0.3\ntesting_size=0.1\n\n\nX_train, X_val, Y_train, Y_val= train_test_split(X, Y, test_size=validation_size, random_state=1)\n\n\nX_train, X_test, Y_train, Y_test= train_test_split(X_train, Y_train, test_size=testing_size/training_size, random_state=1) \n    \n\nX_train = X_train.drop('role', axis=1)\nX_val = X_val.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 519 entries, 129 to 818\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Sex_male  519 non-null    bool   \n 1   Fare      519 non-null    float64\n 2   Age       519 non-null    float64\n 3   Pclass    519 non-null    int64  \n 4   SibSp     519 non-null    int64  \ndtypes: bool(1), float64(2), int64(2)\nmemory usage: 20.8 KB\n\n\n\n\nCode\nlen(X_train)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.5824915824915825\n\n\n\n\nCode\nlen(X_val)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.30078563411896747\n\n\n\n\nCode\nlen(X_test)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.11672278338945005",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost.html#categorical_cols-and-numerical_cols",
    "href": "classification/7.3 XGboost.html#categorical_cols-and-numerical_cols",
    "title": "XGboost with pipeline and fast tuning",
    "section": "2.5 categorical_cols and numerical_cols",
    "text": "2.5 categorical_cols and numerical_cols\n\n\nCode\ncategorical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].nunique() &lt; 10 and X_train[cname].dtype == \"object\"]\n                    \n                    \nnumerical_cols = numerical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].dtype in ['int64', 'float64']]\n\n\n\n\nCode\nprint(\"The total number of categorical columns:\", len(categorical_cols))\nprint(\"The total number of numerical columns:\", len(numerical_cols))\n\n\nThe total number of categorical columns: 0\nThe total number of numerical columns: 4\n\n\n\n\nCode\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train[my_cols].copy()\nX_val = X_val[my_cols].copy()\nX_test= X_test[my_cols].copy()\n\n\n#X_final = df_test[my_cols].copy()",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost.html#pipelines-for-data-preprocessing",
    "href": "classification/7.3 XGboost.html#pipelines-for-data-preprocessing",
    "title": "XGboost with pipeline and fast tuning",
    "section": "2.6 Pipelines for Data Preprocessing",
    "text": "2.6 Pipelines for Data Preprocessing\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\nCode\nnumerical_transformer = Pipeline(steps=[\n    ('imputer_num', SimpleImputer(strategy='median')), \n    ('scaler', StandardScaler())\n])\n\n\n\n\nCode\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n\n\nCode\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)])",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost.html#define-model",
    "href": "classification/7.3 XGboost.html#define-model",
    "title": "XGboost with pipeline and fast tuning",
    "section": "3.1 define model",
    "text": "3.1 define model\n\n\nCode\nimport xgboost\nprint(xgboost.__version__)\n\n\n2.0.3\n\n\n\n\nCode\nfrom xgboost import XGBClassifier\nml_model = XGBClassifier()\nml_model\n\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. XGBClassifieriNot fittedXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...)",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost.html#define-pipline",
    "href": "classification/7.3 XGboost.html#define-pipline",
    "title": "XGboost with pipeline and fast tuning",
    "section": "3.2 define pipline",
    "text": "3.2 define pipline\n\n\nCode\npipeline = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', ml_model)\n         ]\n)",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost.html#define-hyperparameter-tuning-set",
    "href": "classification/7.3 XGboost.html#define-hyperparameter-tuning-set",
    "title": "XGboost with pipeline and fast tuning",
    "section": "3.3 define hyperparameter tuning set",
    "text": "3.3 define hyperparameter tuning set\n\n\nCode\n# explicitly require this experimental feature\nfrom sklearn.experimental import enable_halving_search_cv  # noqa\n\nfrom sklearn.model_selection import HalvingGridSearchCV\n\nparameters = {\n        'model__learning_rate': [0.01, 0.02,0.08,0.1],\n        'model__max_depth': [3, 5, 7,8,9,10,20],\n        'model__min_child_weight': [1, 3,5,8],\n        'model__subsample': [0.5, 0.7,0.9],\n        \n       # 'model__colsample__bytree': [0.5, 0.7],\n       \n        'model__n_estimators' : [100, 200, 500],\n        'model__objective': ['reg:squarederror']\n    }\n\n\nall parameters combinations\n\n\nCode\nfrom sklearn.model_selection import ParameterGrid\ncombinations=list(ParameterGrid(parameters))\n\n\n1440 combinations\n\n\nCode\nlen(combinations)\n\n\n1008\n\n\n\n\nCode\ncombinations[0:5]\n\n\n[{'model__learning_rate': 0.01,\n  'model__max_depth': 3,\n  'model__min_child_weight': 1,\n  'model__n_estimators': 100,\n  'model__objective': 'reg:squarederror',\n  'model__subsample': 0.5},\n {'model__learning_rate': 0.01,\n  'model__max_depth': 3,\n  'model__min_child_weight': 1,\n  'model__n_estimators': 100,\n  'model__objective': 'reg:squarederror',\n  'model__subsample': 0.7},\n {'model__learning_rate': 0.01,\n  'model__max_depth': 3,\n  'model__min_child_weight': 1,\n  'model__n_estimators': 100,\n  'model__objective': 'reg:squarederror',\n  'model__subsample': 0.9},\n {'model__learning_rate': 0.01,\n  'model__max_depth': 3,\n  'model__min_child_weight': 1,\n  'model__n_estimators': 200,\n  'model__objective': 'reg:squarederror',\n  'model__subsample': 0.5},\n {'model__learning_rate': 0.01,\n  'model__max_depth': 3,\n  'model__min_child_weight': 1,\n  'model__n_estimators': 200,\n  'model__objective': 'reg:squarederror',\n  'model__subsample': 0.7}]\n\n\nSearch over specified parameter values with successive halving.\nThe search strategy starts evaluating all the candidates with a small amount of resources and iteratively selects the best candidates, using more and more resources.\n\n\nCode\nGridCV = HalvingGridSearchCV(pipeline\n                ,parameters\n                ,scoring='accuracy'\n                ,max_resources=100\n                , cv=10, n_jobs=-1)",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost.html#train-model",
    "href": "classification/7.3 XGboost.html#train-model",
    "title": "XGboost with pipeline and fast tuning",
    "section": "3.4 train model",
    "text": "3.4 train model\n\n\nCode\nstart_time = time.time()\n\nRAY_AIR_NEW_OUTPUT=0\nGridCV.fit(X_train, Y_train)\n\nend_time = time.time()\nduration = end_time - start_time\nduration\n\n\n39.03060698509216\n\n\n\n3.4.0.1 tunning result\n\n3.4.0.1.1 GridSearchCV\n\n\nCode\n# get the parameter names\ncolumn_results = [f\"param_{name}\" for name in parameters.keys()]\ncolumn_results += [\"mean_test_score\", \"std_test_score\", \"rank_test_score\"]\n\ncv_results = pd.DataFrame(GridCV.cv_results_)\ncv_results = cv_results[column_results].sort_values(\n    \"mean_test_score\", ascending=False\n)\n\n\ndef shorten_param(param_name):\n    if \"__\" in param_name:\n        return param_name.rsplit(\"__\", 1)[1]\n    return param_name\n\n\ncv_results = cv_results.rename(shorten_param, axis=1)\ncv_results.head()\n\n\n\n\n\n\n\n\n\n\nlearning_rate\nmax_depth\nmin_child_weight\nsubsample\nn_estimators\nobjective\nmean_test_score\nstd_test_score\nrank_test_score\n\n\n\n\n927\n0.1\n9\n8\n0.5\n100\nreg:squarederror\n0.816667\n0.165831\n1\n\n\n783\n0.1\n3\n8\n0.5\n100\nreg:squarederror\n0.816667\n0.165831\n1\n\n\n963\n0.1\n10\n8\n0.5\n100\nreg:squarederror\n0.816667\n0.165831\n1\n\n\n999\n0.1\n20\n8\n0.5\n100\nreg:squarederror\n0.816667\n0.165831\n1\n\n\n855\n0.1\n7\n8\n0.5\n100\nreg:squarederror\n0.816667\n0.165831\n1\n\n\n\n\n\n\n\n\n\n\n\n3.4.0.2 tunning best parameters\n\n\nCode\nGridCV.best_params_\n\n\n{'model__learning_rate': 0.1,\n 'model__max_depth': 3,\n 'model__min_child_weight': 8,\n 'model__n_estimators': 100,\n 'model__objective': 'reg:squarederror',\n 'model__subsample': 0.5}\n\n\n\n\n3.4.0.3 tunning best model\n\n\nCode\nmodel_ml = GridCV.best_estimator_",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost.html#preformance",
    "href": "classification/7.3 XGboost.html#preformance",
    "title": "XGboost with pipeline and fast tuning",
    "section": "3.5 Preformance",
    "text": "3.5 Preformance\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = model_ml.predict(X_val) #always gets x and retuns y\nY_pred_dt\n\n\narray([1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n       1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n       1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n       0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n       1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n       1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,\n       0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,\n       0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n       1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n       0, 0, 1, 1])\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_val,Y_pred_dt)  \naccuracy\n\n\n0.6865671641791045\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_val,Y_pred_dt)  \nprecision_dt\n\n\n0.6867469879518072\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_val,Y_pred_dt)  \nrecall_dt\n\n\n0.4956521739130435\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_val,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[127,  26],\n       [ 58,  57]])\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_val, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.6628587666950838",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost.html#k-fold-cross-validation",
    "href": "classification/7.3 XGboost.html#k-fold-cross-validation",
    "title": "XGboost with pipeline and fast tuning",
    "section": "3.6 k-Fold Cross-Validation",
    "text": "3.6 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(ml_model, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.6897871545929799",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost.html#save-model",
    "href": "classification/7.3 XGboost.html#save-model",
    "title": "XGboost with pipeline and fast tuning",
    "section": "3.7 save model",
    "text": "3.7 save model\n\n\nCode\nfrom joblib import dump, load\ndump(model_ml, 'trained_model_7_2.joblib') \n\n\n['trained_model_7_2.joblib']\n\n\nsave trained pipeline\n\n\nCode\nfrom joblib import dump, load\ndump(GridCV, 'trained_GridCV_7_2.joblib', compress=True)  \n\n\n['trained_GridCV_7_2.joblib']",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost.html#load-model",
    "href": "classification/7.3 XGboost.html#load-model",
    "title": "XGboost with pipeline and fast tuning",
    "section": "3.8 load model",
    "text": "3.8 load model\n\n\nCode\nmodel_dt_reload = load('trained_model_7_2.joblib') \n\n\nload grid\n\n\nCode\nGrid_reload = load('trained_GridCV_7_2.joblib')",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost.html#final-prediction",
    "href": "classification/7.3 XGboost.html#final-prediction",
    "title": "XGboost with pipeline and fast tuning",
    "section": "3.9 final prediction",
    "text": "3.9 final prediction\n\n\nCode\nY_pred_dt_final = model_dt_reload.predict(X_test) #always gets x and retuns y\n\n#Y_pred_dt_final[0:5]",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html",
    "href": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "",
    "text": "import os\n#os.system('pip install xgboost')\n\n\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nimport time\nfrom siuba import *",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#input-data",
    "href": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#input-data",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "input data",
    "text": "input data\n\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\nPassengerId_test = df_test['PassengerId']\n\ndf_train = df_train &gt;&gt; select(~_.PassengerId)\ndf_test = df_test &gt;&gt; select(~_.PassengerId)\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#data-eda",
    "href": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#data-eda",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "data EDA",
    "text": "data EDA\n\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\n\n\n\n\n\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\n\n\n\n\n\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n\n0.7420382165605095\n0.18890814558058924",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#data-wrangling",
    "href": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#data-wrangling",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 11 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Pclass    1309 non-null   int64  \n 1   Name      1309 non-null   object \n 2   Sex       1309 non-null   object \n 3   Age       1046 non-null   float64\n 4   SibSp     1309 non-null   int64  \n 5   Parch     1309 non-null   int64  \n 6   Ticket    1309 non-null   object \n 7   Fare      1308 non-null   float64\n 8   Cabin     295 non-null    object \n 9   Embarked  1307 non-null   object \n 10  role      1309 non-null   object \ndtypes: float64(2), int64(3), object(6)\nmemory usage: 122.7+ KB\n\n\n\n# Dealing with missing numerical variables\n#data['Age'] = data.Age.fillna(data.Age.median())\n#data['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 11 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Pclass    1309 non-null   int64  \n 1   Name      1309 non-null   object \n 2   Sex       1309 non-null   object \n 3   Age       1046 non-null   float64\n 4   SibSp     1309 non-null   int64  \n 5   Parch     1309 non-null   int64  \n 6   Ticket    1309 non-null   object \n 7   Fare      1308 non-null   float64\n 8   Cabin     295 non-null    object \n 9   Embarked  1307 non-null   object \n 10  role      1309 non-null   object \ndtypes: float64(2), int64(3), object(6)\nmemory usage: 122.7+ KB\n\n\n\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\n#data = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\n\n\n\n\n\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\n\n\n\n\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\n\n\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\n\n\n2\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\n\n\n3\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\n\n\n4\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\n\n\n\n\n\n\n\n\n\n# Select features columns\n#data = data[['Sex', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()\n\n\n\n\n\n\n\n\n\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\n\n\n\n\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\n\n\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\n\n\n2\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\n\n\n3\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\n\n\n4\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#split-data",
    "href": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#split-data",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "split data",
    "text": "split data\n60% training / 30% validation/ 10% testing\n\n\nY=df_train['Survived']\nX=data[data.role =='train']\n\n#X_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\n\ntraining_size=0.8\nvalidation_size=0.1\ntesting_size=0.1\n\n\nX_train, X_val, Y_train, Y_val= train_test_split(X, Y, test_size=validation_size, random_state=1)\n\n\nX_train, X_test, Y_train, Y_test= train_test_split(X_train, Y_train, test_size=testing_size/training_size, random_state=1) \n    \n\nX_train = X_train.drop('role', axis=1)\nX_val = X_val.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\nX_train.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 700 entries, 658 to 554\nData columns (total 10 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Pclass    700 non-null    int64  \n 1   Name      700 non-null    object \n 2   Sex       700 non-null    object \n 3   Age       559 non-null    float64\n 4   SibSp     700 non-null    int64  \n 5   Parch     700 non-null    int64  \n 6   Ticket    700 non-null    object \n 7   Fare      700 non-null    float64\n 8   Cabin     160 non-null    object \n 9   Embarked  698 non-null    object \ndtypes: float64(2), int64(3), object(5)\nmemory usage: 60.2+ KB\n\n\n\nlen(X_train)/(len(X_train) +len(X_val) +len(X_test) )\n\n0.7856341189674523\n\n\n\nlen(X_val)/(len(X_train) +len(X_val) +len(X_test) )\n\n0.10101010101010101\n\n\n\nlen(X_test)/(len(X_train) +len(X_val) +len(X_test) )\n\n0.11335578002244669",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#categorical_cols-and-numerical_cols",
    "href": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#categorical_cols-and-numerical_cols",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "categorical_cols and numerical_cols",
    "text": "categorical_cols and numerical_cols\n\ncategorical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].nunique() &lt; 10 and X_train[cname].dtype == \"object\"]\n                    \n                    \nnumerical_cols = numerical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].dtype in ['int64', 'float64']]\n\n\nprint(\"The total number of categorical columns:\", len(categorical_cols))\nprint(\"The total number of numerical columns:\", len(numerical_cols))\n\nThe total number of categorical columns: 2\nThe total number of numerical columns: 5\n\n\n\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train[my_cols].copy()\nX_val = X_val[my_cols].copy()\nX_test= X_test[my_cols].copy()\n\nmy_cols\n\n#X_final = df_test[my_cols].copy()\n\n['Sex', 'Embarked', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#pipelines-for-data-preprocessing",
    "href": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#pipelines-for-data-preprocessing",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "Pipelines for Data Preprocessing",
    "text": "Pipelines for Data Preprocessing\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n\nnumerical_transformer = Pipeline(steps=[\n    ('imputer_num', SimpleImputer(strategy='median')), \n    ('scaler', StandardScaler())\n])\n\n\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)])",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#define-model",
    "href": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#define-model",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "define model",
    "text": "define model\n\nimport xgboost\nprint(xgboost.__version__)\n\n2.0.3\n\n\n\nfrom xgboost import XGBClassifier\nml_model = XGBClassifier()\nml_model\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. XGBClassifieriNot fittedXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...)",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#define-pipline",
    "href": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#define-pipline",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "define pipline",
    "text": "define pipline\n\npipeline = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', ml_model)\n         ]\n)\n\npipeline\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer_num',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['Pclass', 'Age', 'SibSp',\n                                                   'Parch', 'Fare']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('imputer_cat',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('onehot',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['Sex', 'Emba...\n                               feature_types=None, gamma=None, grow_policy=None,\n                               importance_type=None,\n                               interaction_constraints=None, learning_rate=None,\n                               max_bin=None, max_cat_threshold=None,\n                               max_cat_to_onehot=None, max_delta_step=None,\n                               max_depth=None, max_leaves=None,\n                               min_child_weight=None, missing=nan,\n                               monotone_constraints=None, multi_strategy=None,\n                               n_estimators=None, n_jobs=None,\n                               num_parallel_tree=None, random_state=None, ...))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiNot fittedPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer_num',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['Pclass', 'Age', 'SibSp',\n                                                   'Parch', 'Fare']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('imputer_cat',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('onehot',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['Sex', 'Emba...\n                               feature_types=None, gamma=None, grow_policy=None,\n                               importance_type=None,\n                               interaction_constraints=None, learning_rate=None,\n                               max_bin=None, max_cat_threshold=None,\n                               max_cat_to_onehot=None, max_delta_step=None,\n                               max_depth=None, max_leaves=None,\n                               min_child_weight=None, missing=nan,\n                               monotone_constraints=None, multi_strategy=None,\n                               n_estimators=None, n_jobs=None,\n                               num_parallel_tree=None, random_state=None, ...))])  preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('imputer_num',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('scaler', StandardScaler())]),\n                                 ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']),\n                                ('cat',\n                                 Pipeline(steps=[('imputer_cat',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('onehot',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 ['Sex', 'Embarked'])]) num['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='median')  StandardScaler?Documentation for StandardScalerStandardScaler() cat['Sex', 'Embarked']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='most_frequent')  OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore') XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...)",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#define-hyperparameter-tuning-set",
    "href": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#define-hyperparameter-tuning-set",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "define hyperparameter tuning set",
    "text": "define hyperparameter tuning set\n\nfrom sklearn.model_selection import GridSearchCV\n\n\nparameters = {\n        'model__learning_rate': [0.01, 0.02,0.08,0.1],\n        'model__max_depth': [3, 5, 7,8,9,10,20],\n        'model__min_child_weight': [1, 3,5,8],\n        'model__subsample': [0.5, 0.7,0.9],\n        \n       # 'model__colsample__bytree': [0.5, 0.7],\n       \n        'model__n_estimators' : [100, 200, 500],\n        'model__objective': ['reg:squarederror']\n    }\n\nall parameters combinations\n\nimport itertools\na = parameters.values()\ncombinations = list(itertools.product(*a))\n\n1440 combinations\n\nlen(combinations)\n\n1008\n\n\n\ncombinations[0:5]\n\n[(0.01, 3, 1, 0.5, 100, 'reg:squarederror'),\n (0.01, 3, 1, 0.5, 200, 'reg:squarederror'),\n (0.01, 3, 1, 0.5, 500, 'reg:squarederror'),\n (0.01, 3, 1, 0.7, 100, 'reg:squarederror'),\n (0.01, 3, 1, 0.7, 200, 'reg:squarederror')]\n\n\n\nGridCV = GridSearchCV(pipeline\n                ,parameters\n                ,scoring='accuracy'\n                , cv=10, n_jobs=-1)",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#train-model",
    "href": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#train-model",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "train model",
    "text": "train model\n\nstart_time = time.time()\n\nGridCV.fit(X_train, Y_train)\n\nend_time = time.time()\nduration = end_time - start_time\nduration\n\n156.45451498031616\n\n\n\ntunning result\n\nGridSearchCV\n\n# get the parameter names\ncolumn_results = [f\"param_{name}\" for name in parameters.keys()]\ncolumn_results += [\"mean_test_score\", \"std_test_score\", \"rank_test_score\"]\n\ncv_results = pd.DataFrame(GridCV.cv_results_)\ncv_results = cv_results[column_results].sort_values(\n    \"mean_test_score\", ascending=False\n)\n\n\ndef shorten_param(param_name):\n    if \"__\" in param_name:\n        return param_name.rsplit(\"__\", 1)[1]\n    return param_name\n\n\ncv_results = cv_results.rename(shorten_param, axis=1)\ncv_results.head()\n\n\n\n\n\n\n\n\n\nlearning_rate\nmax_depth\nmin_child_weight\nsubsample\nn_estimators\nobjective\nmean_test_score\nstd_test_score\nrank_test_score\n\n\n\n\n778\n0.1\n3\n5\n0.7\n200\nreg:squarederror\n0.842857\n0.053069\n1\n\n\n215\n0.01\n10\n8\n0.9\n500\nreg:squarederror\n0.838571\n0.034434\n2\n\n\n228\n0.01\n20\n3\n0.5\n200\nreg:squarederror\n0.838571\n0.046533\n2\n\n\n168\n0.01\n9\n5\n0.5\n500\nreg:squarederror\n0.838571\n0.046092\n4\n\n\n777\n0.1\n3\n5\n0.5\n200\nreg:squarederror\n0.838571\n0.046092\n4\n\n\n\n\n\n\n\n\n\n\n\ntunning best parameters\n\nGridCV.best_params_\n\n{'model__learning_rate': 0.1,\n 'model__max_depth': 3,\n 'model__min_child_weight': 5,\n 'model__n_estimators': 200,\n 'model__objective': 'reg:squarederror',\n 'model__subsample': 0.7}\n\n\n\n\ntunning best model\n\nmodel_ml = GridCV.best_estimator_",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#preformance",
    "href": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#preformance",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "Preformance",
    "text": "Preformance\n\n#Using predict method to test the model\nY_pred_dt = model_ml.predict(X_val) #always gets x and retuns y\nY_pred_dt\n\narray([1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n       1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n       1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n       0, 1])\n\n\n\nAccuracy\n\n\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_val,Y_pred_dt)  \naccuracy\n\n0.8\n\n\n\nPrecision\n\n\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_val,Y_pred_dt)  \nprecision_dt\n\n0.8275862068965517\n\n\n\nRecall\n\n\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_val,Y_pred_dt)  \nrecall_dt\n\n0.6486486486486487\n\n\n\nConfusion matrix\n\n\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_val,Y_pred_dt)\nconfusion_matrix_dt\n\narray([[48,  5],\n       [13, 24]])\n\n\n\nAUC - ROC Curve\n\n\nauc_dt = metrics.roc_auc_score(Y_val, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n0.7771545130035696",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#k-fold-cross-validation",
    "href": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#k-fold-cross-validation",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "k-Fold Cross-Validation",
    "text": "k-Fold Cross-Validation\n\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(pipeline, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n0.8114285714285714",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#save-model",
    "href": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#save-model",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "save model",
    "text": "save model\n\nfrom joblib import dump, load\ndump(model_ml, 'trained_model_7_2.joblib') \n\n['trained_model_7_2.joblib']\n\n\nsave trained pipeline\n\nfrom joblib import dump, load\ndump(GridCV, 'trained_GridCV_7_2.joblib', compress=True)  \n\n['trained_GridCV_7_2.joblib']",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#load-model",
    "href": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#load-model",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "load model",
    "text": "load model\n\nmodel_dt_reload = load('trained_model_7_2.joblib') \n\nload grid\n\nGrid_reload = load('trained_GridCV_7_2.joblib')",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#final-prediction",
    "href": "classification/7.2 XGboost with pipeline and hyperparameter tuning.html#final-prediction",
    "title": "XGboost with pipeline and hyperparameter tuning",
    "section": "final prediction",
    "text": "final prediction\n\nY_pred_dt_final = model_dt_reload.predict(X_test) #always gets x and retuns y\n\n#Y_pred_dt_final[0:5]",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and hyperparameter tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost with pipeline and fast tuning.html",
    "href": "classification/7.3 XGboost with pipeline and fast tuning.html",
    "title": "XGboost with pipeline and fast tuning",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nimport time\nfrom siuba import *",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost with pipeline and fast tuning.html#input-data",
    "href": "classification/7.3 XGboost with pipeline and fast tuning.html#input-data",
    "title": "XGboost with pipeline and fast tuning",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\nPassengerId_test = df_test['PassengerId']\n\ndf_train = df_train &gt;&gt; select(~_.PassengerId)\ndf_test = df_test &gt;&gt; select(~_.PassengerId)\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost with pipeline and fast tuning.html#data-eda",
    "href": "classification/7.3 XGboost with pipeline and fast tuning.html#data-eda",
    "title": "XGboost with pipeline and fast tuning",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n\n\n0.7420382165605095\n0.18890814558058924",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost with pipeline and fast tuning.html#data-wrangling",
    "href": "classification/7.3 XGboost with pipeline and fast tuning.html#data-wrangling",
    "title": "XGboost with pipeline and fast tuning",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 11 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Pclass    1309 non-null   int64  \n 1   Name      1309 non-null   object \n 2   Sex       1309 non-null   object \n 3   Age       1046 non-null   float64\n 4   SibSp     1309 non-null   int64  \n 5   Parch     1309 non-null   int64  \n 6   Ticket    1309 non-null   object \n 7   Fare      1308 non-null   float64\n 8   Cabin     295 non-null    object \n 9   Embarked  1307 non-null   object \n 10  role      1309 non-null   object \ndtypes: float64(2), int64(3), object(6)\nmemory usage: 122.7+ KB\n\n\n\n\nCode\n# Dealing with missing numerical variables\n#data['Age'] = data.Age.fillna(data.Age.median())\n#data['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 11 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Pclass    1309 non-null   int64  \n 1   Name      1309 non-null   object \n 2   Sex       1309 non-null   object \n 3   Age       1046 non-null   float64\n 4   SibSp     1309 non-null   int64  \n 5   Parch     1309 non-null   int64  \n 6   Ticket    1309 non-null   object \n 7   Fare      1308 non-null   float64\n 8   Cabin     295 non-null    object \n 9   Embarked  1307 non-null   object \n 10  role      1309 non-null   object \ndtypes: float64(2), int64(3), object(6)\nmemory usage: 122.7+ KB\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\n#data = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\n\n\n\n\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\n\n\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\n\n\n2\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\n\n\n3\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\n\n\n4\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\n\n\n\n\n\n\n\n\n\n\nCode\n# Select features columns\n#data = data[['Sex', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\n\n\n\n\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\n\n\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\n\n\n2\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\n\n\n3\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\n\n\n4\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost with pipeline and fast tuning.html#split-data",
    "href": "classification/7.3 XGboost with pipeline and fast tuning.html#split-data",
    "title": "XGboost with pipeline and fast tuning",
    "section": "2.4 split data",
    "text": "2.4 split data\n60% training / 30% validation/ 10% testing\n\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\n#X_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\n\ntraining_size=0.8\nvalidation_size=0.1\ntesting_size=0.1\n\n\nX_train, X_val, Y_train, Y_val= train_test_split(X, Y, test_size=validation_size, random_state=1)\n\n\nX_train, X_test, Y_train, Y_test= train_test_split(X_train, Y_train, test_size=testing_size/training_size, random_state=1) \n    \n\nX_train = X_train.drop('role', axis=1)\nX_val = X_val.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 700 entries, 658 to 554\nData columns (total 10 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Pclass    700 non-null    int64  \n 1   Name      700 non-null    object \n 2   Sex       700 non-null    object \n 3   Age       559 non-null    float64\n 4   SibSp     700 non-null    int64  \n 5   Parch     700 non-null    int64  \n 6   Ticket    700 non-null    object \n 7   Fare      700 non-null    float64\n 8   Cabin     160 non-null    object \n 9   Embarked  698 non-null    object \ndtypes: float64(2), int64(3), object(5)\nmemory usage: 60.2+ KB\n\n\n\n\nCode\nlen(X_train)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.7856341189674523\n\n\n\n\nCode\nlen(X_val)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.10101010101010101\n\n\n\n\nCode\nlen(X_test)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.11335578002244669",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost with pipeline and fast tuning.html#categorical_cols-and-numerical_cols",
    "href": "classification/7.3 XGboost with pipeline and fast tuning.html#categorical_cols-and-numerical_cols",
    "title": "XGboost with pipeline and fast tuning",
    "section": "2.5 categorical_cols and numerical_cols",
    "text": "2.5 categorical_cols and numerical_cols\n\n\nCode\ncategorical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].nunique() &lt; 10 and X_train[cname].dtype == \"object\"]\n                    \n                    \nnumerical_cols = numerical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].dtype in ['int64', 'float64']]\n\n\n\n\nCode\nprint(\"The total number of categorical columns:\", len(categorical_cols))\nprint(\"The total number of numerical columns:\", len(numerical_cols))\n\n\nThe total number of categorical columns: 2\nThe total number of numerical columns: 5\n\n\n\n\nCode\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train[my_cols].copy()\nX_val = X_val[my_cols].copy()\nX_test= X_test[my_cols].copy()\n\nmy_cols\n#X_final = df_test[my_cols].copy()\n\n\n['Sex', 'Embarked', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost with pipeline and fast tuning.html#pipelines-for-data-preprocessing",
    "href": "classification/7.3 XGboost with pipeline and fast tuning.html#pipelines-for-data-preprocessing",
    "title": "XGboost with pipeline and fast tuning",
    "section": "2.6 Pipelines for Data Preprocessing",
    "text": "2.6 Pipelines for Data Preprocessing\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\nCode\nnumerical_transformer = Pipeline(steps=[\n    ('imputer_num', SimpleImputer(strategy='median')), \n    ('scaler', StandardScaler())\n])\n\n\n\n\nCode\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n\n\nCode\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)])",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost with pipeline and fast tuning.html#define-model",
    "href": "classification/7.3 XGboost with pipeline and fast tuning.html#define-model",
    "title": "XGboost with pipeline and fast tuning",
    "section": "3.1 define model",
    "text": "3.1 define model\n\n\nCode\nimport xgboost\nprint(xgboost.__version__)\n\n\n2.0.3\n\n\n\n\nCode\nfrom xgboost import XGBClassifier\nml_model = XGBClassifier()\nml_model\n\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. XGBClassifieriNot fittedXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...)",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost with pipeline and fast tuning.html#define-pipline",
    "href": "classification/7.3 XGboost with pipeline and fast tuning.html#define-pipline",
    "title": "XGboost with pipeline and fast tuning",
    "section": "3.2 define pipline",
    "text": "3.2 define pipline\n\n\nCode\npipeline = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', ml_model)\n         ]\n)\n\npipeline\n\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer_num',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['Pclass', 'Age', 'SibSp',\n                                                   'Parch', 'Fare']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('imputer_cat',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('onehot',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['Sex', 'Emba...\n                               feature_types=None, gamma=None, grow_policy=None,\n                               importance_type=None,\n                               interaction_constraints=None, learning_rate=None,\n                               max_bin=None, max_cat_threshold=None,\n                               max_cat_to_onehot=None, max_delta_step=None,\n                               max_depth=None, max_leaves=None,\n                               min_child_weight=None, missing=nan,\n                               monotone_constraints=None, multi_strategy=None,\n                               n_estimators=None, n_jobs=None,\n                               num_parallel_tree=None, random_state=None, ...))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiNot fittedPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer_num',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['Pclass', 'Age', 'SibSp',\n                                                   'Parch', 'Fare']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('imputer_cat',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('onehot',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['Sex', 'Emba...\n                               feature_types=None, gamma=None, grow_policy=None,\n                               importance_type=None,\n                               interaction_constraints=None, learning_rate=None,\n                               max_bin=None, max_cat_threshold=None,\n                               max_cat_to_onehot=None, max_delta_step=None,\n                               max_depth=None, max_leaves=None,\n                               min_child_weight=None, missing=nan,\n                               monotone_constraints=None, multi_strategy=None,\n                               n_estimators=None, n_jobs=None,\n                               num_parallel_tree=None, random_state=None, ...))])  preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('imputer_num',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('scaler', StandardScaler())]),\n                                 ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']),\n                                ('cat',\n                                 Pipeline(steps=[('imputer_cat',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('onehot',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 ['Sex', 'Embarked'])]) num['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='median')  StandardScaler?Documentation for StandardScalerStandardScaler() cat['Sex', 'Embarked']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='most_frequent')  OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore') XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...)",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost with pipeline and fast tuning.html#define-hyperparameter-tuning-set",
    "href": "classification/7.3 XGboost with pipeline and fast tuning.html#define-hyperparameter-tuning-set",
    "title": "XGboost with pipeline and fast tuning",
    "section": "3.3 define hyperparameter tuning set",
    "text": "3.3 define hyperparameter tuning set\n\n\nCode\n# explicitly require this experimental feature\nfrom sklearn.experimental import enable_halving_search_cv  # noqa\n\nfrom sklearn.model_selection import HalvingGridSearchCV\n\nparameters = {\n        'model__learning_rate': [0.01, 0.02,0.08,0.1],\n        'model__max_depth': [3, 5, 7,8,9,10,20],\n        'model__min_child_weight': [1, 3,5,8],\n        'model__subsample': [0.5, 0.7,0.9],\n        \n       # 'model__colsample__bytree': [0.5, 0.7],\n       \n        'model__n_estimators' : [100, 200, 500],\n        'model__objective': ['reg:squarederror']\n    }\n\n\nall parameters combinations\n\n\nCode\nfrom sklearn.model_selection import ParameterGrid\ncombinations=list(ParameterGrid(parameters))\n\n\n1440 combinations\n\n\nCode\nlen(combinations)\n\n\n1008\n\n\n\n\nCode\ncombinations[0:5]\n\n\n[{'model__learning_rate': 0.01,\n  'model__max_depth': 3,\n  'model__min_child_weight': 1,\n  'model__n_estimators': 100,\n  'model__objective': 'reg:squarederror',\n  'model__subsample': 0.5},\n {'model__learning_rate': 0.01,\n  'model__max_depth': 3,\n  'model__min_child_weight': 1,\n  'model__n_estimators': 100,\n  'model__objective': 'reg:squarederror',\n  'model__subsample': 0.7},\n {'model__learning_rate': 0.01,\n  'model__max_depth': 3,\n  'model__min_child_weight': 1,\n  'model__n_estimators': 100,\n  'model__objective': 'reg:squarederror',\n  'model__subsample': 0.9},\n {'model__learning_rate': 0.01,\n  'model__max_depth': 3,\n  'model__min_child_weight': 1,\n  'model__n_estimators': 200,\n  'model__objective': 'reg:squarederror',\n  'model__subsample': 0.5},\n {'model__learning_rate': 0.01,\n  'model__max_depth': 3,\n  'model__min_child_weight': 1,\n  'model__n_estimators': 200,\n  'model__objective': 'reg:squarederror',\n  'model__subsample': 0.7}]\n\n\nSearch over specified parameter values with successive halving.\nThe search strategy starts evaluating all the candidates with a small amount of resources and iteratively selects the best candidates, using more and more resources.\n\n\nCode\nGridCV = HalvingGridSearchCV(pipeline\n                ,parameters\n                ,scoring='accuracy'\n                ,max_resources=100\n                , cv=10, n_jobs=-1)",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost with pipeline and fast tuning.html#train-model",
    "href": "classification/7.3 XGboost with pipeline and fast tuning.html#train-model",
    "title": "XGboost with pipeline and fast tuning",
    "section": "3.4 train model",
    "text": "3.4 train model\n\n\nCode\nstart_time = time.time()\n\nRAY_AIR_NEW_OUTPUT=0\nGridCV.fit(X_train, Y_train)\n\nend_time = time.time()\nduration = end_time - start_time\nduration\n\n\n46.2377769947052\n\n\n\n3.4.0.1 tunning result\n\n3.4.0.1.1 GridSearchCV\n\n\nCode\n# get the parameter names\ncolumn_results = [f\"param_{name}\" for name in parameters.keys()]\ncolumn_results += [\"mean_test_score\", \"std_test_score\", \"rank_test_score\"]\n\ncv_results = pd.DataFrame(GridCV.cv_results_)\ncv_results = cv_results[column_results].sort_values(\n    \"mean_test_score\", ascending=False\n)\n\n\ndef shorten_param(param_name):\n    if \"__\" in param_name:\n        return param_name.rsplit(\"__\", 1)[1]\n    return param_name\n\n\ncv_results = cv_results.rename(shorten_param, axis=1)\ncv_results.head()\n\n\n\n\n\n\n\n\n\n\nlearning_rate\nmax_depth\nmin_child_weight\nsubsample\nn_estimators\nobjective\nmean_test_score\nstd_test_score\nrank_test_score\n\n\n\n\n892\n0.1\n8\n8\n0.7\n100\nreg:squarederror\n0.8\n0.244949\n1\n\n\n199\n0.01\n10\n5\n0.7\n100\nreg:squarederror\n0.8\n0.244949\n1\n\n\n489\n0.02\n20\n5\n0.5\n200\nreg:squarederror\n0.8\n0.244949\n1\n\n\n356\n0.02\n7\n8\n0.9\n200\nreg:squarederror\n0.8\n0.244949\n1\n\n\n954\n0.1\n10\n5\n0.5\n100\nreg:squarederror\n0.8\n0.244949\n1\n\n\n\n\n\n\n\n\n\n\n\n3.4.0.2 tunning best parameters\n\n\nCode\nGridCV.best_params_\n\n\n{'model__learning_rate': 0.01,\n 'model__max_depth': 3,\n 'model__min_child_weight': 3,\n 'model__n_estimators': 100,\n 'model__objective': 'reg:squarederror',\n 'model__subsample': 0.7}\n\n\n\n\n3.4.0.3 tunning best model\n\n\nCode\nmodel_ml = GridCV.best_estimator_",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost with pipeline and fast tuning.html#preformance",
    "href": "classification/7.3 XGboost with pipeline and fast tuning.html#preformance",
    "title": "XGboost with pipeline and fast tuning",
    "section": "3.5 Preformance",
    "text": "3.5 Preformance\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = model_ml.predict(X_val) #always gets x and retuns y\nY_pred_dt\n\n\narray([1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n       1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n       1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n       0, 1])\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_val,Y_pred_dt)  \naccuracy\n\n\n0.7555555555555555\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_val,Y_pred_dt)  \nprecision_dt\n\n\n0.8260869565217391\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_val,Y_pred_dt)  \nrecall_dt\n\n\n0.5135135135135135\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_val,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[49,  4],\n       [18, 19]])\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_val, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.719020907700153",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost with pipeline and fast tuning.html#k-fold-cross-validation",
    "href": "classification/7.3 XGboost with pipeline and fast tuning.html#k-fold-cross-validation",
    "title": "XGboost with pipeline and fast tuning",
    "section": "3.6 k-Fold Cross-Validation",
    "text": "3.6 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(model_ml, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.8242857142857144",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost with pipeline and fast tuning.html#save-model",
    "href": "classification/7.3 XGboost with pipeline and fast tuning.html#save-model",
    "title": "XGboost with pipeline and fast tuning",
    "section": "3.7 save model",
    "text": "3.7 save model\n\n\nCode\nfrom joblib import dump, load\ndump(model_ml, 'trained_model_7_2.joblib') \n\n\n['trained_model_7_2.joblib']\n\n\nsave trained pipeline\n\n\nCode\nfrom joblib import dump, load\ndump(GridCV, 'trained_GridCV_7_2.joblib', compress=True)  \n\n\n['trained_GridCV_7_2.joblib']",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost with pipeline and fast tuning.html#load-model",
    "href": "classification/7.3 XGboost with pipeline and fast tuning.html#load-model",
    "title": "XGboost with pipeline and fast tuning",
    "section": "3.8 load model",
    "text": "3.8 load model\n\n\nCode\nmodel_dt_reload = load('trained_model_7_2.joblib') \n\n\nload grid\n\n\nCode\nGrid_reload = load('trained_GridCV_7_2.joblib')",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.3 XGboost with pipeline and fast tuning.html#final-prediction",
    "href": "classification/7.3 XGboost with pipeline and fast tuning.html#final-prediction",
    "title": "XGboost with pipeline and fast tuning",
    "section": "3.9 final prediction",
    "text": "3.9 final prediction\n\n\nCode\nY_pred_dt_final = model_dt_reload.predict(X_test) #always gets x and retuns y\n\n#Y_pred_dt_final[0:5]",
    "crumbs": [
      "Classification",
      "XGboost with pipeline and fast tuning"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost with pipeline.html",
    "href": "classification/7.1 XGboost with pipeline.html",
    "title": "XGboost with pipeline",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nimport time\nfrom siuba import *",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost with pipeline.html#input-data",
    "href": "classification/7.1 XGboost with pipeline.html#input-data",
    "title": "XGboost with pipeline",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\nPassengerId_test = df_test['PassengerId']\n\ndf_train = df_train &gt;&gt; select(~_.PassengerId)\ndf_test = df_test &gt;&gt; select(~_.PassengerId)\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost with pipeline.html#data-eda",
    "href": "classification/7.1 XGboost with pipeline.html#data-eda",
    "title": "XGboost with pipeline",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n\n\n0.7420382165605095\n0.18890814558058924",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost with pipeline.html#data-wrangling",
    "href": "classification/7.1 XGboost with pipeline.html#data-wrangling",
    "title": "XGboost with pipeline",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 11 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Pclass    1309 non-null   int64  \n 1   Name      1309 non-null   object \n 2   Sex       1309 non-null   object \n 3   Age       1046 non-null   float64\n 4   SibSp     1309 non-null   int64  \n 5   Parch     1309 non-null   int64  \n 6   Ticket    1309 non-null   object \n 7   Fare      1308 non-null   float64\n 8   Cabin     295 non-null    object \n 9   Embarked  1307 non-null   object \n 10  role      1309 non-null   object \ndtypes: float64(2), int64(3), object(6)\nmemory usage: 122.7+ KB\n\n\n\n\nCode\n# Dealing with missing numerical variables\n#data['Age'] = data.Age.fillna(data.Age.median())\n#data['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 11 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Pclass    1309 non-null   int64  \n 1   Name      1309 non-null   object \n 2   Sex       1309 non-null   object \n 3   Age       1046 non-null   float64\n 4   SibSp     1309 non-null   int64  \n 5   Parch     1309 non-null   int64  \n 6   Ticket    1309 non-null   object \n 7   Fare      1308 non-null   float64\n 8   Cabin     295 non-null    object \n 9   Embarked  1307 non-null   object \n 10  role      1309 non-null   object \ndtypes: float64(2), int64(3), object(6)\nmemory usage: 122.7+ KB\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\n#data = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\n\n\n\n\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\n\n\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\n\n\n2\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\n\n\n3\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\n\n\n4\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\n\n\n\n\n\n\n\n\n\n\nCode\n# Select features columns\n\n#data = data[['Sex', 'Fare', 'Age','Pclass', 'SibSp','role']]\n\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\n\n\n\n\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\n\n\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\n\n\n2\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\n\n\n3\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\n\n\n4\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\n\n\n\n\n\n\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 11 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Pclass    1309 non-null   int64  \n 1   Name      1309 non-null   object \n 2   Sex       1309 non-null   object \n 3   Age       1046 non-null   float64\n 4   SibSp     1309 non-null   int64  \n 5   Parch     1309 non-null   int64  \n 6   Ticket    1309 non-null   object \n 7   Fare      1308 non-null   float64\n 8   Cabin     295 non-null    object \n 9   Embarked  1307 non-null   object \n 10  role      1309 non-null   object \ndtypes: float64(2), int64(3), object(6)\nmemory usage: 122.7+ KB",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost with pipeline.html#split-data",
    "href": "classification/7.1 XGboost with pipeline.html#split-data",
    "title": "XGboost with pipeline",
    "section": "2.4 split data",
    "text": "2.4 split data\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\n#X_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\n\ntraining_size=0.8\nvalidation_size=0.1\ntesting_size=0.1\n\n\nX_train, X_val, Y_train, Y_val= train_test_split(X, Y, test_size=validation_size, random_state=1)\n\n\nX_train, X_test, Y_train, Y_test= train_test_split(X_train, Y_train, test_size=testing_size/training_size, random_state=1) \n    \n\nX_train = X_train.drop('role', axis=1)\nX_val = X_val.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 700 entries, 658 to 554\nData columns (total 10 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Pclass    700 non-null    int64  \n 1   Name      700 non-null    object \n 2   Sex       700 non-null    object \n 3   Age       559 non-null    float64\n 4   SibSp     700 non-null    int64  \n 5   Parch     700 non-null    int64  \n 6   Ticket    700 non-null    object \n 7   Fare      700 non-null    float64\n 8   Cabin     160 non-null    object \n 9   Embarked  698 non-null    object \ndtypes: float64(2), int64(3), object(5)\nmemory usage: 60.2+ KB\n\n\n\n\nCode\nlen(X_train)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.7856341189674523\n\n\n\n\nCode\nlen(X_val)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.10101010101010101\n\n\n\n\nCode\nlen(X_test)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.11335578002244669",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost with pipeline.html#categorical_cols-and-numerical_cols",
    "href": "classification/7.1 XGboost with pipeline.html#categorical_cols-and-numerical_cols",
    "title": "XGboost with pipeline",
    "section": "2.5 categorical_cols and numerical_cols",
    "text": "2.5 categorical_cols and numerical_cols\n\n\nCode\ncategorical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].nunique() &lt; 10 and X_train[cname].dtype == \"object\"]\n                    \n                    \nnumerical_cols = numerical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].dtype in ['int64', 'float64']]\n\n\n\n\nCode\nprint(\"The total number of categorical columns:\", len(categorical_cols))\nprint(\"The total number of numerical columns:\", len(numerical_cols))\n\n\nThe total number of categorical columns: 2\nThe total number of numerical columns: 5\n\n\n\n\nCode\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train[my_cols].copy()\nX_val = X_val[my_cols].copy()\nX_test= X_test[my_cols].copy()\n\n\n#X_final = df_test[my_cols].copy()\n\n\n\n\nCode\nmy_cols\n\n\n['Sex', 'Embarked', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost with pipeline.html#pipelines-for-data-preprocessing",
    "href": "classification/7.1 XGboost with pipeline.html#pipelines-for-data-preprocessing",
    "title": "XGboost with pipeline",
    "section": "2.6 Pipelines for Data Preprocessing",
    "text": "2.6 Pipelines for Data Preprocessing\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\nCode\nnumerical_transformer = Pipeline(steps=[\n    ('imputer_num', SimpleImputer(strategy='median')), \n    ('scaler', StandardScaler())\n])\n\n\n\n\nCode\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n\n\nCode\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)])",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost with pipeline.html#define-model",
    "href": "classification/7.1 XGboost with pipeline.html#define-model",
    "title": "XGboost with pipeline",
    "section": "3.1 define model",
    "text": "3.1 define model\n\n\nCode\nimport xgboost\nprint(xgboost.__version__)\n\n\n2.0.3\n\n\n\n\nCode\nfrom xgboost import XGBClassifier\nml_model = XGBClassifier()\nml_model\n\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. XGBClassifieriNot fittedXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...)",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost with pipeline.html#define-pipline",
    "href": "classification/7.1 XGboost with pipeline.html#define-pipline",
    "title": "XGboost with pipeline",
    "section": "3.2 define pipline",
    "text": "3.2 define pipline\n\n\nCode\npipeline = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', ml_model)\n         ]\n)\n\npipeline\n\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer_num',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['Pclass', 'Age', 'SibSp',\n                                                   'Parch', 'Fare']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('imputer_cat',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('onehot',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['Sex', 'Emba...\n                               feature_types=None, gamma=None, grow_policy=None,\n                               importance_type=None,\n                               interaction_constraints=None, learning_rate=None,\n                               max_bin=None, max_cat_threshold=None,\n                               max_cat_to_onehot=None, max_delta_step=None,\n                               max_depth=None, max_leaves=None,\n                               min_child_weight=None, missing=nan,\n                               monotone_constraints=None, multi_strategy=None,\n                               n_estimators=None, n_jobs=None,\n                               num_parallel_tree=None, random_state=None, ...))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiNot fittedPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer_num',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['Pclass', 'Age', 'SibSp',\n                                                   'Parch', 'Fare']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('imputer_cat',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('onehot',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['Sex', 'Emba...\n                               feature_types=None, gamma=None, grow_policy=None,\n                               importance_type=None,\n                               interaction_constraints=None, learning_rate=None,\n                               max_bin=None, max_cat_threshold=None,\n                               max_cat_to_onehot=None, max_delta_step=None,\n                               max_depth=None, max_leaves=None,\n                               min_child_weight=None, missing=nan,\n                               monotone_constraints=None, multi_strategy=None,\n                               n_estimators=None, n_jobs=None,\n                               num_parallel_tree=None, random_state=None, ...))])  preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('imputer_num',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('scaler', StandardScaler())]),\n                                 ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']),\n                                ('cat',\n                                 Pipeline(steps=[('imputer_cat',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('onehot',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 ['Sex', 'Embarked'])]) num['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='median')  StandardScaler?Documentation for StandardScalerStandardScaler() cat['Sex', 'Embarked']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='most_frequent')  OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore') XGBClassifierXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...)",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost with pipeline.html#train-model",
    "href": "classification/7.1 XGboost with pipeline.html#train-model",
    "title": "XGboost with pipeline",
    "section": "3.3 train model",
    "text": "3.3 train model\n\n\nCode\nstart_time = time.time()\n\npipeline.fit(X_train, Y_train)\n\nend_time = time.time()\nduration = end_time - start_time\nduration\n\n\n0.03803300857543945",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost with pipeline.html#preformance",
    "href": "classification/7.1 XGboost with pipeline.html#preformance",
    "title": "XGboost with pipeline",
    "section": "3.4 Preformance",
    "text": "3.4 Preformance\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = pipeline.predict(X_val) #always gets x and retuns y\nY_pred_dt\n\n\narray([1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n       1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,\n       1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n       0, 1])\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_val,Y_pred_dt)  \naccuracy\n\n\n0.7666666666666667\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_val,Y_pred_dt)  \nprecision_dt\n\n\n0.75\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_val,Y_pred_dt)  \nrecall_dt\n\n\n0.6486486486486487\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_val,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[45,  8],\n       [13, 24]])\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_val, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.7488526262111168",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost with pipeline.html#k-fold-cross-validation",
    "href": "classification/7.1 XGboost with pipeline.html#k-fold-cross-validation",
    "title": "XGboost with pipeline",
    "section": "3.5 k-Fold Cross-Validation",
    "text": "3.5 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(pipeline, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.8057142857142857",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost with pipeline.html#save-model",
    "href": "classification/7.1 XGboost with pipeline.html#save-model",
    "title": "XGboost with pipeline",
    "section": "3.6 save model",
    "text": "3.6 save model\n\n\nCode\nfrom joblib import dump, load\ndump(ml_model, 'trained_model_7.joblib') \n\n\n['trained_model_7.joblib']\n\n\nsave trained pipeline\n\n\nCode\nfrom joblib import dump, load\ndump(pipeline, 'trained_pipeline_7.joblib', compress=True)  \n\n\n['trained_pipeline_7.joblib']",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost with pipeline.html#load-model",
    "href": "classification/7.1 XGboost with pipeline.html#load-model",
    "title": "XGboost with pipeline",
    "section": "3.7 load model",
    "text": "3.7 load model\n\n\nCode\nmodel_dt_reload = load('trained_model_7.joblib') \n\n\nload pipeline\n\n\nCode\npipeline_reload = load('trained_pipeline_7.joblib')",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/7.1 XGboost with pipeline.html#final-prediction",
    "href": "classification/7.1 XGboost with pipeline.html#final-prediction",
    "title": "XGboost with pipeline",
    "section": "3.8 final prediction",
    "text": "3.8 final prediction\n\n\nCode\nY_pred_dt_final = pipeline_reload.predict(X_test) #always gets x and retuns y\n\nY_pred_dt_final[0:5]\n\n\narray([0, 0, 0, 1, 0])",
    "crumbs": [
      "Classification",
      "XGboost with pipeline"
    ]
  },
  {
    "objectID": "classification/8 Multiple Models using Pipeline.html",
    "href": "classification/8 Multiple Models using Pipeline.html",
    "title": "Multiple models using pipeline",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nimport time\nfrom siuba import *\n\nfrom sklearn.experimental import enable_halving_search_cv  # noqa\nfrom sklearn.model_selection import HalvingGridSearchCV",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline"
    ]
  },
  {
    "objectID": "classification/8 Multiple Models using Pipeline.html#input-data",
    "href": "classification/8 Multiple Models using Pipeline.html#input-data",
    "title": "Multiple models using pipeline",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\nPassengerId_test = df_test['PassengerId']\n\ndf_train = df_train &gt;&gt; select(~_.PassengerId)\ndf_test = df_test &gt;&gt; select(~_.PassengerId)\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline"
    ]
  },
  {
    "objectID": "classification/8 Multiple Models using Pipeline.html#data-eda",
    "href": "classification/8 Multiple Models using Pipeline.html#data-eda",
    "title": "Multiple models using pipeline",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n\n\n0.7420382165605095\n0.18890814558058924",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline"
    ]
  },
  {
    "objectID": "classification/8 Multiple Models using Pipeline.html#data-wrangling",
    "href": "classification/8 Multiple Models using Pipeline.html#data-wrangling",
    "title": "Multiple models using pipeline",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 11 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Pclass    1309 non-null   int64  \n 1   Name      1309 non-null   object \n 2   Sex       1309 non-null   object \n 3   Age       1046 non-null   float64\n 4   SibSp     1309 non-null   int64  \n 5   Parch     1309 non-null   int64  \n 6   Ticket    1309 non-null   object \n 7   Fare      1308 non-null   float64\n 8   Cabin     295 non-null    object \n 9   Embarked  1307 non-null   object \n 10  role      1309 non-null   object \ndtypes: float64(2), int64(3), object(6)\nmemory usage: 122.7+ KB\n\n\n\n\nCode\n# Dealing with missing numerical variables\n#data['Age'] = data.Age.fillna(data.Age.median())\n#data['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 11 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Pclass    1309 non-null   int64  \n 1   Name      1309 non-null   object \n 2   Sex       1309 non-null   object \n 3   Age       1046 non-null   float64\n 4   SibSp     1309 non-null   int64  \n 5   Parch     1309 non-null   int64  \n 6   Ticket    1309 non-null   object \n 7   Fare      1308 non-null   float64\n 8   Cabin     295 non-null    object \n 9   Embarked  1307 non-null   object \n 10  role      1309 non-null   object \ndtypes: float64(2), int64(3), object(6)\nmemory usage: 122.7+ KB\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\n#data = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\n\n\n\n\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\n\n\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\n\n\n2\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\n\n\n3\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\n\n\n4\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\n\n\n\n\n\n\n\n\n\n\nCode\n# Select features columns\n#data = data[['Sex', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\n\n\n\n\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\n\n\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\n\n\n2\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\n\n\n3\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\n\n\n4\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline"
    ]
  },
  {
    "objectID": "classification/8 Multiple Models using Pipeline.html#split-data",
    "href": "classification/8 Multiple Models using Pipeline.html#split-data",
    "title": "Multiple models using pipeline",
    "section": "2.4 split data",
    "text": "2.4 split data\n60% training / 30% validation/ 10% testing\n\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\n#X_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\n\ntraining_size=0.8\nvalidation_size=0.1\ntesting_size=0.1\n\n\nX_train, X_val, Y_train, Y_val= train_test_split(X, Y, test_size=validation_size, random_state=1)\n\n\nX_train, X_test, Y_train, Y_test= train_test_split(X_train, Y_train, test_size=testing_size/training_size, random_state=1) \n    \n\nX_train = X_train.drop('role', axis=1)\nX_val = X_val.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 700 entries, 658 to 554\nData columns (total 10 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Pclass    700 non-null    int64  \n 1   Name      700 non-null    object \n 2   Sex       700 non-null    object \n 3   Age       559 non-null    float64\n 4   SibSp     700 non-null    int64  \n 5   Parch     700 non-null    int64  \n 6   Ticket    700 non-null    object \n 7   Fare      700 non-null    float64\n 8   Cabin     160 non-null    object \n 9   Embarked  698 non-null    object \ndtypes: float64(2), int64(3), object(5)\nmemory usage: 60.2+ KB\n\n\n\n\nCode\nlen(X_train)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.7856341189674523\n\n\n\n\nCode\nlen(X_val)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.10101010101010101\n\n\n\n\nCode\nlen(X_test)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.11335578002244669",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline"
    ]
  },
  {
    "objectID": "classification/8 Multiple Models using Pipeline.html#categorical_cols-and-numerical_cols",
    "href": "classification/8 Multiple Models using Pipeline.html#categorical_cols-and-numerical_cols",
    "title": "Multiple models using pipeline",
    "section": "2.5 categorical_cols and numerical_cols",
    "text": "2.5 categorical_cols and numerical_cols\n\n\nCode\ncategorical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].nunique() &lt; 10 and X_train[cname].dtype == \"object\"]\n                    \n                    \nnumerical_cols = numerical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].dtype in ['int64', 'float64']]\n\n\n\n\nCode\nprint(\"The total number of categorical columns:\", len(categorical_cols))\nprint(\"The total number of numerical columns:\", len(numerical_cols))\n\n\nThe total number of categorical columns: 2\nThe total number of numerical columns: 5\n\n\n\n\nCode\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train[my_cols].copy()\nX_val = X_val[my_cols].copy()\nX_test= X_test[my_cols].copy()\n\nmy_cols\n#X_final = df_test[my_cols].copy()\n\n\n['Sex', 'Embarked', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline"
    ]
  },
  {
    "objectID": "classification/8 Multiple Models using Pipeline.html#pipelines-for-data-preprocessing",
    "href": "classification/8 Multiple Models using Pipeline.html#pipelines-for-data-preprocessing",
    "title": "Multiple models using pipeline",
    "section": "2.6 Pipelines for Data Preprocessing",
    "text": "2.6 Pipelines for Data Preprocessing\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\nCode\nnumerical_transformer = Pipeline(steps=[\n    ('imputer_num', SimpleImputer(strategy='median')), \n    ('scaler', StandardScaler())\n])\n\n\n\n\nCode\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n\n\nCode\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)])",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline"
    ]
  },
  {
    "objectID": "classification/8 Multiple Models using Pipeline.html#define-model",
    "href": "classification/8 Multiple Models using Pipeline.html#define-model",
    "title": "Multiple models using pipeline",
    "section": "3.1 define model",
    "text": "3.1 define model\n\n\nCode\nimport xgboost\nprint(xgboost.__version__)\n\n\n2.0.3\n\n\n\n3.1.1 XGB model\n\n\nCode\nfrom xgboost import XGBClassifier\nxgb_model = XGBClassifier()\nxgb_model\n\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. XGBClassifieriNot fittedXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...) \n\n\n\n\n3.1.2 Random Forest model\n\n\nCode\nfrom sklearn.ensemble import RandomForestClassifier\nrandom_forest_model = RandomForestClassifier()\nrandom_forest_model\n\n\nRandomForestClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriNot fittedRandomForestClassifier() \n\n\n\n\n3.1.3 Logistic Regression model\n\n\nCode\nfrom sklearn.linear_model import LogisticRegression\nLogisticRegression_model = LogisticRegression(solver='liblinear')\nLogisticRegression_model\n\n\nLogisticRegression(solver='liblinear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniNot fittedLogisticRegression(solver='liblinear')",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline"
    ]
  },
  {
    "objectID": "classification/8 Multiple Models using Pipeline.html#define-pipline",
    "href": "classification/8 Multiple Models using Pipeline.html#define-pipline",
    "title": "Multiple models using pipeline",
    "section": "3.2 define pipline",
    "text": "3.2 define pipline\n\n\nCode\npipeline_xgb = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', xgb_model)\n         ]\n)\n\npipeline_rf = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', random_forest_model)\n         ]\n)\n\npipeline_lr = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', LogisticRegression_model)\n         ]\n)",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline"
    ]
  },
  {
    "objectID": "classification/8 Multiple Models using Pipeline.html#define-gridsearch",
    "href": "classification/8 Multiple Models using Pipeline.html#define-gridsearch",
    "title": "Multiple models using pipeline",
    "section": "3.3 define GridSearch",
    "text": "3.3 define GridSearch\n\n\nCode\n# Grid_xgb = HalvingGridSearchCV(pipeline_xgb\n#                 ,parameters\n#                 ,scoring='accuracy'\n#                 ,max_resources=100\n#                 , cv=10, n_jobs=-1)\n#                 \n# Grid_rf = HalvingGridSearchCV(pipeline_rf\n#                 ,parameters\n#                 ,scoring='accuracy'\n#                 ,max_resources=100\n#                 , cv=10, n_jobs=-1)\n#                 \n# Grid_lr = HalvingGridSearchCV(pipeline_lr\n#                 ,parameters\n#                 ,scoring='accuracy'\n#                 ,max_resources=100\n#                 , cv=10, n_jobs=-1)",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline"
    ]
  },
  {
    "objectID": "classification/8 Multiple Models using Pipeline.html#train-model",
    "href": "classification/8 Multiple Models using Pipeline.html#train-model",
    "title": "Multiple models using pipeline",
    "section": "3.4 train model",
    "text": "3.4 train model\n\n\nCode\nstart_time = time.time()\n\n\npipelines = [pipeline_xgb, pipeline_rf, pipeline_lr, ]\nfor pipe in pipelines:\n    pipe.fit(X_train,Y_train)\n\n\nend_time = time.time()\nduration = end_time - start_time\nduration\n\n\n0.10587692260742188",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline"
    ]
  },
  {
    "objectID": "classification/8 Multiple Models using Pipeline.html#preformance",
    "href": "classification/8 Multiple Models using Pipeline.html#preformance",
    "title": "Multiple models using pipeline",
    "section": "3.5 Preformance",
    "text": "3.5 Preformance\n\n\nCode\ngrid_dict = {0: 'XGB', 1: 'random forest', 2: 'linear regression'}\n\nfor i, model in enumerate(pipelines):\n    print('{} Test Accuracy: {}'.format(grid_dict[i],\n    model.score(X_test,Y_test)))\n    #print('{} Best Params: {}'.format(grid_dict[i],          model.best_params_))\n\n\nXGB Test Accuracy: 0.8514851485148515\nrandom forest Test Accuracy: 0.801980198019802\nlinear regression Test Accuracy: 0.7821782178217822\n\n\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = pipeline_xgb.predict(X_test) #always gets x and retuns y\nY_pred_dt\n\n\narray([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n       1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,\n       0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,\n       1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0])\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_test,Y_pred_dt)  \naccuracy\n\n\n0.8514851485148515\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_test,Y_pred_dt)  \nprecision_dt\n\n\n0.8064516129032258\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_test,Y_pred_dt)  \nrecall_dt\n\n\n0.7352941176470589\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_test,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[61,  6],\n       [ 9, 25]])\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_test, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.8228709394205443",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline"
    ]
  },
  {
    "objectID": "classification/8 Multiple Models using Pipeline.html#k-fold-cross-validation",
    "href": "classification/8 Multiple Models using Pipeline.html#k-fold-cross-validation",
    "title": "Multiple models using pipeline",
    "section": "3.6 k-Fold Cross-Validation",
    "text": "3.6 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(pipeline_xgb, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.8042857142857143",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline"
    ]
  },
  {
    "objectID": "classification/8 Multiple Models using Pipeline.html#save-model",
    "href": "classification/8 Multiple Models using Pipeline.html#save-model",
    "title": "Multiple models using pipeline",
    "section": "3.7 save model",
    "text": "3.7 save model\n\n\nCode\nfrom joblib import dump, load\ndump(pipeline_xgb, 'trained_pipeline_8.joblib', compress=True)  \n\n\n['trained_pipeline_8.joblib']",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline"
    ]
  },
  {
    "objectID": "classification/8 Multiple Models using Pipeline.html#load-model",
    "href": "classification/8 Multiple Models using Pipeline.html#load-model",
    "title": "Multiple models using pipeline",
    "section": "3.8 load model",
    "text": "3.8 load model\n\n\nCode\nmodel_reload = load('trained_pipeline_8.joblib')",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline"
    ]
  },
  {
    "objectID": "classification/8 Multiple Models using Pipeline.html#final-prediction",
    "href": "classification/8 Multiple Models using Pipeline.html#final-prediction",
    "title": "Multiple models using pipeline",
    "section": "3.9 final prediction",
    "text": "3.9 final prediction\n\n\nCode\nY_pred_dt_final = model_reload.predict(X_val) #always gets x and retuns y\n\nY_pred_dt_final[0:5]\n\n\narray([1, 0, 1, 1, 1])",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline"
    ]
  },
  {
    "objectID": "classification/8.2 Multiple Models using Pipeline and tuning.html",
    "href": "classification/8.2 Multiple Models using Pipeline and tuning.html",
    "title": "Multiple models using pipeline and tuning",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nimport time\nfrom siuba import *\n\nfrom sklearn.experimental import enable_halving_search_cv  # noqa\nfrom sklearn.model_selection import HalvingGridSearchCV",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline and tuning"
    ]
  },
  {
    "objectID": "classification/8.2 Multiple Models using Pipeline and tuning.html#input-data",
    "href": "classification/8.2 Multiple Models using Pipeline and tuning.html#input-data",
    "title": "Multiple models using pipeline and tuning",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = df_train['PassengerId']\nPassengerId_test = df_test['PassengerId']\n\ndf_train = df_train &gt;&gt; select(~_.PassengerId)\ndf_test = df_test &gt;&gt; select(~_.PassengerId)\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline and tuning"
    ]
  },
  {
    "objectID": "classification/8.2 Multiple Models using Pipeline and tuning.html#data-eda",
    "href": "classification/8.2 Multiple Models using Pipeline and tuning.html#data-eda",
    "title": "Multiple models using pipeline and tuning",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nsns.countplot(x='Survived', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.catplot(x='Survived', col='Sex', kind='count', data=df_train);\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint(df_train[df_train.Sex == 'female'].Survived.sum()/df_train[df_train.Sex == 'female'].Survived.count())\nprint(df_train[df_train.Sex == 'male'].Survived.sum()/df_train[df_train.Sex == 'male'].Survived.count())\n\n\n0.7420382165605095\n0.18890814558058924",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline and tuning"
    ]
  },
  {
    "objectID": "classification/8.2 Multiple Models using Pipeline and tuning.html#data-wrangling",
    "href": "classification/8.2 Multiple Models using Pipeline and tuning.html#data-wrangling",
    "title": "Multiple models using pipeline and tuning",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nsurvived_train = df_train.Survived\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['Survived'], axis=1), df_test])\n\n\n\n\nCode\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 11 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Pclass    1309 non-null   int64  \n 1   Name      1309 non-null   object \n 2   Sex       1309 non-null   object \n 3   Age       1046 non-null   float64\n 4   SibSp     1309 non-null   int64  \n 5   Parch     1309 non-null   int64  \n 6   Ticket    1309 non-null   object \n 7   Fare      1308 non-null   float64\n 8   Cabin     295 non-null    object \n 9   Embarked  1307 non-null   object \n 10  role      1309 non-null   object \ndtypes: float64(2), int64(3), object(6)\nmemory usage: 122.7+ KB\n\n\n\n\nCode\n# Dealing with missing numerical variables\n#data['Age'] = data.Age.fillna(data.Age.median())\n#data['Fare'] = data.Fare.fillna(data.Fare.median())\n\n# Check out info of data\ndata.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1309 entries, 0 to 417\nData columns (total 11 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Pclass    1309 non-null   int64  \n 1   Name      1309 non-null   object \n 2   Sex       1309 non-null   object \n 3   Age       1046 non-null   float64\n 4   SibSp     1309 non-null   int64  \n 5   Parch     1309 non-null   int64  \n 6   Ticket    1309 non-null   object \n 7   Fare      1308 non-null   float64\n 8   Cabin     295 non-null    object \n 9   Embarked  1307 non-null   object \n 10  role      1309 non-null   object \ndtypes: float64(2), int64(3), object(6)\nmemory usage: 122.7+ KB\n\n\n\n\nCode\n# Tranform Sex feature to numeric value\n# create a new column for each of the options in 'Sex'\n# creates a new column for female, called 'Sex_female', \n# creates a new column for 'Sex_male'\n# more then two categorical values it is better to use one-hot-encode\n#data = pd.get_dummies(data, columns=['Sex'], drop_first=True)\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\n\n\n\n\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\n\n\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\n\n\n2\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\n\n\n3\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\n\n\n4\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain\n\n\n\n\n\n\n\n\n\n\nCode\n# Select features columns\n#data = data[['Sex', 'Fare', 'Age','Pclass', 'SibSp','role']]\ndata.head()\n\n\n\n\n\n\n\n\n\n\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nrole\n\n\n\n\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\ntrain\n\n\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\ntrain\n\n\n2\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\ntrain\n\n\n3\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\ntrain\n\n\n4\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\ntrain",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline and tuning"
    ]
  },
  {
    "objectID": "classification/8.2 Multiple Models using Pipeline and tuning.html#split-data",
    "href": "classification/8.2 Multiple Models using Pipeline and tuning.html#split-data",
    "title": "Multiple models using pipeline and tuning",
    "section": "2.4 split data",
    "text": "2.4 split data\n60% training / 30% validation/ 10% testing\n\n\n\nCode\nY=df_train['Survived']\nX=data[data.role =='train']\n\n#X_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\n\ntraining_size=0.8\nvalidation_size=0.1\ntesting_size=0.1\n\n\nX_train, X_val, Y_train, Y_val= train_test_split(X, Y, test_size=validation_size, random_state=1)\n\n\nX_train, X_test, Y_train, Y_test= train_test_split(X_train, Y_train, test_size=testing_size/training_size, random_state=1) \n    \n\nX_train = X_train.drop('role', axis=1)\nX_val = X_val.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nX_train.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 700 entries, 658 to 554\nData columns (total 10 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Pclass    700 non-null    int64  \n 1   Name      700 non-null    object \n 2   Sex       700 non-null    object \n 3   Age       559 non-null    float64\n 4   SibSp     700 non-null    int64  \n 5   Parch     700 non-null    int64  \n 6   Ticket    700 non-null    object \n 7   Fare      700 non-null    float64\n 8   Cabin     160 non-null    object \n 9   Embarked  698 non-null    object \ndtypes: float64(2), int64(3), object(5)\nmemory usage: 60.2+ KB\n\n\n\n\nCode\nlen(X_train)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.7856341189674523\n\n\n\n\nCode\nlen(X_val)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.10101010101010101\n\n\n\n\nCode\nlen(X_test)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.11335578002244669",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline and tuning"
    ]
  },
  {
    "objectID": "classification/8.2 Multiple Models using Pipeline and tuning.html#categorical_cols-and-numerical_cols",
    "href": "classification/8.2 Multiple Models using Pipeline and tuning.html#categorical_cols-and-numerical_cols",
    "title": "Multiple models using pipeline and tuning",
    "section": "2.5 categorical_cols and numerical_cols",
    "text": "2.5 categorical_cols and numerical_cols\n\n\nCode\ncategorical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].nunique() &lt; 10 and X_train[cname].dtype == \"object\"]\n                    \n                    \nnumerical_cols = numerical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].dtype in ['int64', 'float64']]\n\n\n\n\nCode\nprint(\"The total number of categorical columns:\", len(categorical_cols))\nprint(\"The total number of numerical columns:\", len(numerical_cols))\n\n\nThe total number of categorical columns: 2\nThe total number of numerical columns: 5\n\n\n\n\nCode\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train[my_cols].copy()\nX_val = X_val[my_cols].copy()\nX_test= X_test[my_cols].copy()\n\nmy_cols\n#X_final = df_test[my_cols].copy()\n\n\n['Sex', 'Embarked', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline and tuning"
    ]
  },
  {
    "objectID": "classification/8.2 Multiple Models using Pipeline and tuning.html#pipelines-for-data-preprocessing",
    "href": "classification/8.2 Multiple Models using Pipeline and tuning.html#pipelines-for-data-preprocessing",
    "title": "Multiple models using pipeline and tuning",
    "section": "2.6 Pipelines for Data Preprocessing",
    "text": "2.6 Pipelines for Data Preprocessing\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\nCode\nnumerical_transformer = Pipeline(steps=[\n    ('imputer_num', SimpleImputer(strategy='median')), \n    ('scaler', StandardScaler())\n])\n\n\n\n\nCode\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n\n\nCode\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)])",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline and tuning"
    ]
  },
  {
    "objectID": "classification/8.2 Multiple Models using Pipeline and tuning.html#define-model",
    "href": "classification/8.2 Multiple Models using Pipeline and tuning.html#define-model",
    "title": "Multiple models using pipeline and tuning",
    "section": "3.1 define model",
    "text": "3.1 define model\n\n\nCode\nimport xgboost\nprint(xgboost.__version__)\n\n\n2.0.3\n\n\n\n3.1.1 XGB model\n\n\nCode\nfrom xgboost import XGBClassifier\nxgb_model = XGBClassifier()\nxgb_model\n\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. XGBClassifieriNot fittedXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...) \n\n\n\n\n3.1.2 Random Forest model\n\n\nCode\nfrom sklearn.ensemble import RandomForestClassifier\nrandom_forest_model = RandomForestClassifier()\nrandom_forest_model\n\n\nRandomForestClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriNot fittedRandomForestClassifier() \n\n\n\n\n3.1.3 Logistic Regression model\n\n\nCode\nfrom sklearn.linear_model import LogisticRegression\nLogisticRegression_model = LogisticRegression(solver='liblinear')\nLogisticRegression_model\n\n\nLogisticRegression(solver='liblinear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniNot fittedLogisticRegression(solver='liblinear')",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline and tuning"
    ]
  },
  {
    "objectID": "classification/8.2 Multiple Models using Pipeline and tuning.html#define-pipline",
    "href": "classification/8.2 Multiple Models using Pipeline and tuning.html#define-pipline",
    "title": "Multiple models using pipeline and tuning",
    "section": "3.2 define pipline",
    "text": "3.2 define pipline\n\n\nCode\npipeline_xgb = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', xgb_model)\n         ]\n)\n\npipeline_rf = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', random_forest_model)\n         ]\n)\n\npipeline_lr = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', LogisticRegression_model)\n         ]\n)",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline and tuning"
    ]
  },
  {
    "objectID": "classification/8.2 Multiple Models using Pipeline and tuning.html#define-gridsearch",
    "href": "classification/8.2 Multiple Models using Pipeline and tuning.html#define-gridsearch",
    "title": "Multiple models using pipeline and tuning",
    "section": "3.3 define GridSearch",
    "text": "3.3 define GridSearch\n\n\nCode\nparameters_xgb= {\n        'model__learning_rate': [0.01, 0.02,0.08,0.1],\n        'model__max_depth': [3, 5, 7,8,9,10,20],\n        'model__min_child_weight': [1, 3,5,8],\n        'model__subsample': [0.5, 0.7,0.9],\n        \n       # 'model__colsample__bytree': [0.5, 0.7],\n       \n        'model__n_estimators' : [100, 200, 500],\n        'model__objective': ['reg:squarederror']\n    }\n\n\nGrid_xgb = HalvingGridSearchCV(pipeline_xgb\n                ,parameters_xgb \n                ,scoring='accuracy'\n                ,max_resources=100\n                , cv=10, n_jobs=-1)\n                \n                \nparameters_rf = {'model__max_depth':[20,30,40],\n                 'model__n_estimators':[200,250,300],\n                 'model__min_samples_leaf':[1,2,3]\n                 }                \n                \n\nGrid_rf = HalvingGridSearchCV(pipeline_rf\n                ,parameters_rf\n                ,scoring='accuracy'\n                ,max_resources=100\n                , cv=10, n_jobs=-1)",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline and tuning"
    ]
  },
  {
    "objectID": "classification/8.2 Multiple Models using Pipeline and tuning.html#train-model",
    "href": "classification/8.2 Multiple Models using Pipeline and tuning.html#train-model",
    "title": "Multiple models using pipeline and tuning",
    "section": "3.4 train model",
    "text": "3.4 train model\n\n\nCode\nstart_time = time.time()\n\n\nGrids = [Grid_xgb, Grid_rf,pipeline_xgb,pipeline_rf,pipeline_lr]\nfor Grid in Grids:\n    Grid.fit(X_train,Y_train)\n\n\nend_time = time.time()\nduration = end_time - start_time\nduration\n\n\n45.8913140296936",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline and tuning"
    ]
  },
  {
    "objectID": "classification/8.2 Multiple Models using Pipeline and tuning.html#preformance",
    "href": "classification/8.2 Multiple Models using Pipeline and tuning.html#preformance",
    "title": "Multiple models using pipeline and tuning",
    "section": "3.5 Preformance",
    "text": "3.5 Preformance\n\n\nCode\ngrid_dict = {0: 'XGB', 1: 'random forest', 2: 'XGB non tune',3: 'ramdon forest non tune',4:'linear regression non tune' }\n\nfor i, model in enumerate(Grids):\n    print('{} Test Accuracy: {}'.format(grid_dict[i],\n    model.score(X_test,Y_test)))\n    #print('{} Best Params: {}'.format(grid_dict[i], model.best_params_))\n\n\nXGB Test Accuracy: 0.8712871287128713\nrandom forest Test Accuracy: 0.8217821782178217\nXGB non tune Test Accuracy: 0.8514851485148515\nramdon forest non tune Test Accuracy: 0.8217821782178217\nlinear regression non tune Test Accuracy: 0.7821782178217822\n\n\n\n\nCode\nbest_ml=Grid_xgb.best_estimator_\n\n\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = best_ml.predict(X_test) #always gets x and retuns y\nY_pred_dt\n\n\narray([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n       0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n       0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_test,Y_pred_dt)  \naccuracy\n\n\n0.8712871287128713\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_test,Y_pred_dt)  \nprecision_dt\n\n\n0.9565217391304348\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_test,Y_pred_dt)  \nrecall_dt\n\n\n0.6470588235294118\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_test,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[66,  1],\n       [12, 22]])\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_test, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.8160667251975416",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline and tuning"
    ]
  },
  {
    "objectID": "classification/8.2 Multiple Models using Pipeline and tuning.html#k-fold-cross-validation",
    "href": "classification/8.2 Multiple Models using Pipeline and tuning.html#k-fold-cross-validation",
    "title": "Multiple models using pipeline and tuning",
    "section": "3.6 k-Fold Cross-Validation",
    "text": "3.6 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(pipeline_xgb, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.7985714285714286",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline and tuning"
    ]
  },
  {
    "objectID": "classification/8.2 Multiple Models using Pipeline and tuning.html#save-model",
    "href": "classification/8.2 Multiple Models using Pipeline and tuning.html#save-model",
    "title": "Multiple models using pipeline and tuning",
    "section": "3.7 save model",
    "text": "3.7 save model\n\n\nCode\nfrom joblib import dump, load\ndump(Grid_xgb, 'trained_grid_8_2.joblib', compress=True)  \n\n\n['trained_grid_8_2.joblib']",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline and tuning"
    ]
  },
  {
    "objectID": "classification/8.2 Multiple Models using Pipeline and tuning.html#load-model",
    "href": "classification/8.2 Multiple Models using Pipeline and tuning.html#load-model",
    "title": "Multiple models using pipeline and tuning",
    "section": "3.8 load model",
    "text": "3.8 load model\n\n\nCode\nmodel_reload = load('trained_grid_8_2.joblib') \n\n\n\n\nCode\nbest_ml=model_reload.best_estimator_",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline and tuning"
    ]
  },
  {
    "objectID": "classification/8.2 Multiple Models using Pipeline and tuning.html#final-prediction",
    "href": "classification/8.2 Multiple Models using Pipeline and tuning.html#final-prediction",
    "title": "Multiple models using pipeline and tuning",
    "section": "3.9 final prediction",
    "text": "3.9 final prediction\n\n\nCode\nY_pred_dt_final =best_ml.predict(X_val) #always gets x and retuns y\n\nY_pred_dt_final[0:5]\n\n\narray([1, 0, 1, 1, 1])",
    "crumbs": [
      "Classification",
      "Multiple models using pipeline and tuning"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html#change-name",
    "href": "plot/1 seaborn.html#change-name",
    "title": "seaborn chart",
    "section": "8.3 change name",
    "text": "8.3 change name\n\n\nCode\nax=sns.boxplot(x = \"day\", y = \"total_bill\", data = df)\nax.set_title(\"tips box plot \")\nax.set(xlabel='x-axis label', ylabel='y-axis label')",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "plot/3 plotly.html#change-x-y-name",
    "href": "plot/3 plotly.html#change-x-y-name",
    "title": "Plotly chart",
    "section": "8.3 change x y name",
    "text": "8.3 change x y name\n\n\nCode\nfig = px.scatter(tips,x=\"tip\", y=\"total_bill\"\n                ,labels={\n                     \"tip\": \"new x label)\",\n                     \"total_bill\": \"new y label\"\n                 }\n\n)\n\n    \nfig.show()",
    "crumbs": [
      "Plot",
      "Plotly chart"
    ]
  },
  {
    "objectID": "plot/1 seaborn.html#change-x-y-name",
    "href": "plot/1 seaborn.html#change-x-y-name",
    "title": "seaborn chart",
    "section": "8.3 change x y name",
    "text": "8.3 change x y name\n\n\nCode\nax=sns.boxplot(x = \"day\", y = \"total_bill\", data = df)\nax.set_title(\"tips box plot \")\nax.set(xlabel='x-axis label', ylabel='y-axis label')",
    "crumbs": [
      "Plot",
      "seaborn chart"
    ]
  },
  {
    "objectID": "plot/2 plotnine.html#change-x-y-name",
    "href": "plot/2 plotnine.html#change-x-y-name",
    "title": "plotnine chart",
    "section": "8.3 change x y name",
    "text": "8.3 change x y name\n\n\nCode\np=(\n    ggplot(data=tips)+aes(x=\"tip\",y=\"total_bill\")+ geom_point()+ scale_x_continuous(name=\"new x name\")+ scale_y_continuous(name=\"new y name\")\n)\n\np",
    "crumbs": [
      "Plot",
      "plotnine chart"
    ]
  },
  {
    "objectID": "data manipulation/2 siuba.html#append",
    "href": "data manipulation/2 siuba.html#append",
    "title": "Data manipulation with siuba",
    "section": "9 Append",
    "text": "9 Append\n\n9.1 append by row\n\n\nCode\n# not available in siuba yet\n#from siuba import bind_rows\n\n\n\n\nCode\n# using pandas\n\n# get 1 to 4 rows\ndata1=mtcars.iloc[0:4]\n\n# get 9 rows\ndata2=mtcars.iloc[10:11]\n\ndata3=pd.concat([data1, data2], ignore_index = True,axis=0)\n\ndata3\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n3\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n4\n17.8\n6\n167.6\n123\n3.92\n3.440\n18.90\n1\n0\n4\n4\n\n\n\n\n\n\n\n\n\n\n9.2 append by column\n\n\nCode\n# not available in siuba yet\n#from siuba import bind_columns\n\n\n\n\nCode\n# using pandas\ndata1=small_mtcars&gt;&gt;select(_.mpg)\n\ndata2=small_mtcars&gt;&gt;select(_.cyl)\n\ndata3=pd.concat([data1, data2],axis=1)\n\ndata3\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\n\n\n\n\n0\n21.0\n6\n\n\n1\n21.0\n6\n\n\n2\n22.8\n4\n\n\n3\n21.4\n6\n\n\n4\n18.7\n8\n\n\n\n\n\n\n\n\n\n\n9.3 Dropping NA values\n\n\n9.4 keep NA values",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#load-package",
    "href": "data manipulation/1 Pandas.html#load-package",
    "title": "Data manipulation with Pandas",
    "section": "0.1 load package",
    "text": "0.1 load package\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nimport seaborn as sns\n\nfrom siuba.data import mtcars,penguins\n\nfrom siuba.siu import call\nfrom siuba import _, mutate, filter, group_by, summarize,show_query\nfrom siuba import *\n\nfrom siuba.data import mtcars,penguins\n\n\n\n\nCode\nsmall_mtcars = mtcars[[\"cyl\", \"mpg\",'hp']]\nsmall_mtcars=small_mtcars.head(5)",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#select-column",
    "href": "data manipulation/1 Pandas.html#select-column",
    "title": "Data manipulation with Pandas",
    "section": "0.2 select column",
    "text": "0.2 select column\n\n0.2.1 select by name\n\n\nCode\nsmall_mtcars [[\"cyl\", \"mpg\",'hp']]\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\nhp\n\n\n\n\n0\n6\n21.0\n110\n\n\n1\n6\n21.0\n110\n\n\n2\n4\n22.8\n93\n\n\n3\n6\n21.4\n110\n\n\n4\n8\n18.7\n175\n\n\n\n\n\n\n\n\nother way:\n\n\nCode\nsmall_mtcars.filter(items=['cyl', 'mpg','hp'])\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\nhp\n\n\n\n\n0\n6\n21.0\n110\n\n\n1\n6\n21.0\n110\n\n\n2\n4\n22.8\n93\n\n\n3\n6\n21.4\n110\n\n\n4\n8\n18.7\n175\n\n\n\n\n\n\n\n\n\n\n0.2.2 select columns by name match with ‘p’\n\n\nCode\nsmall_mtcars.loc[:,small_mtcars.columns.str.contains(\"p\")]\n\n\n\n\n\n\n\n\n\n\nmpg\nhp\n\n\n\n\n0\n21.0\n110\n\n\n1\n21.0\n110\n\n\n2\n22.8\n93\n\n\n3\n21.4\n110\n\n\n4\n18.7\n175\n\n\n\n\n\n\n\n\nother way:\n\n\nCode\nsmall_mtcars.filter(regex='p.*', axis=1)\n\n\n\n\n\n\n\n\n\n\nmpg\nhp\n\n\n\n\n0\n21.0\n110\n\n\n1\n21.0\n110\n\n\n2\n22.8\n93\n\n\n3\n21.4\n110\n\n\n4\n18.7\n175\n\n\n\n\n\n\n\n\n\n\n0.2.3 select columns by index\n\n0.2.3.1 select first and 3rd columns\n\n\nCode\nsmall_mtcars.iloc[[0,2]]\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\nhp\n\n\n\n\n0\n6\n21.0\n110\n\n\n2\n4\n22.8\n93\n\n\n\n\n\n\n\n\n\n\n0.2.3.2 select first to 3rd columns\n\n\nCode\nsmall_mtcars[0:3]\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\nhp\n\n\n\n\n0\n6\n21.0\n110\n\n\n1\n6\n21.0\n110\n\n\n2\n4\n22.8\n93",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#drop-column",
    "href": "data manipulation/1 Pandas.html#drop-column",
    "title": "Data manipulation with Pandas",
    "section": "0.3 drop column",
    "text": "0.3 drop column\n\n\nCode\nsmall_mtcars.drop('cyl', axis=1)\n\n\n\n\n\n\n\n\n\n\nmpg\nhp\n\n\n\n\n0\n21.0\n110\n\n\n1\n21.0\n110\n\n\n2\n22.8\n93\n\n\n3\n21.4\n110\n\n\n4\n18.7\n175",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#renaming-column",
    "href": "data manipulation/1 Pandas.html#renaming-column",
    "title": "Data manipulation with Pandas",
    "section": "0.4 Renaming column",
    "text": "0.4 Renaming column\n\n\nCode\nsmall_mtcars.rename(columns={'mpg':\"new_name_mpg\", 'cyl':'new_name_cyl'})\n\n\n\n\n\n\n\n\n\n\nnew_name_cyl\nnew_name_mpg\nhp\n\n\n\n\n0\n6\n21.0\n110\n\n\n1\n6\n21.0\n110\n\n\n2\n4\n22.8\n93\n\n\n3\n6\n21.4\n110\n\n\n4\n8\n18.7\n175",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#create-column",
    "href": "data manipulation/1 Pandas.html#create-column",
    "title": "Data manipulation with Pandas",
    "section": "0.5 Create column",
    "text": "0.5 Create column\n\n0.5.1 Mutate\n\n\nCode\nsmall_mtcars['mpg2'] = small_mtcars['mpg']+1\n\nsmall_mtcars['mpg3'] = if_else(small_mtcars['mpg']&gt; 20, \"long\", \"short\")\n\nsmall_mtcars['mpg4'] = small_mtcars[\"mpg\"].case_when([\n    (small_mtcars.eval(\"mpg &lt; 19\"), \"short\"), \n    (small_mtcars.eval(\"mpg &lt;= 22\"), \"Medium\"), \n    (small_mtcars.eval(\"mpg &gt; 22\"), \"long\")\n])\n\n\nsmall_mtcars\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\nhp\nmpg2\nmpg3\nmpg4\n\n\n\n\n0\n6\n21.0\n110\n22.0\nlong\nMedium\n\n\n1\n6\n21.0\n110\n22.0\nlong\nMedium\n\n\n2\n4\n22.8\n93\n23.8\nlong\nlong\n\n\n3\n6\n21.4\n110\n22.4\nlong\nMedium\n\n\n4\n8\n18.7\n175\n19.7\nshort\nshort\n\n\n\n\n\n\n\n\n\n\n0.5.2 Transmute,create column and only keep this column\n\n\nCode\nsmall_mtcars['mpg2'] = small_mtcars['mpg']+1\n\nnew_data=small_mtcars[['mpg2']]\n\nnew_data\n\n\n\n\n\n\n\n\n\n\nmpg2\n\n\n\n\n0\n22.0\n\n\n1\n22.0\n\n\n2\n23.8\n\n\n3\n22.4\n\n\n4\n19.7",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#filter-rows",
    "href": "data manipulation/1 Pandas.html#filter-rows",
    "title": "Data manipulation with Pandas",
    "section": "0.6 Filter rows",
    "text": "0.6 Filter rows\n\n\nCode\nmtcars[(mtcars['gear'] ==4)]\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n7\n24.4\n4\n146.7\n62\n3.69\n3.190\n20.00\n1\n0\n4\n2\n\n\n8\n22.8\n4\n140.8\n95\n3.92\n3.150\n22.90\n1\n0\n4\n2\n\n\n9\n19.2\n6\n167.6\n123\n3.92\n3.440\n18.30\n1\n0\n4\n4\n\n\n10\n17.8\n6\n167.6\n123\n3.92\n3.440\n18.90\n1\n0\n4\n4\n\n\n17\n32.4\n4\n78.7\n66\n4.08\n2.200\n19.47\n1\n1\n4\n1\n\n\n18\n30.4\n4\n75.7\n52\n4.93\n1.615\n18.52\n1\n1\n4\n2\n\n\n19\n33.9\n4\n71.1\n65\n4.22\n1.835\n19.90\n1\n1\n4\n1\n\n\n25\n27.3\n4\n79.0\n66\n4.08\n1.935\n18.90\n1\n1\n4\n1\n\n\n31\n21.4\n4\n121.0\n109\n4.11\n2.780\n18.60\n1\n1\n4\n2\n\n\n\n\n\n\n\n\nother way:\n\n\nCode\nmtcars.query('gear==4')\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n7\n24.4\n4\n146.7\n62\n3.69\n3.190\n20.00\n1\n0\n4\n2\n\n\n8\n22.8\n4\n140.8\n95\n3.92\n3.150\n22.90\n1\n0\n4\n2\n\n\n9\n19.2\n6\n167.6\n123\n3.92\n3.440\n18.30\n1\n0\n4\n4\n\n\n10\n17.8\n6\n167.6\n123\n3.92\n3.440\n18.90\n1\n0\n4\n4\n\n\n17\n32.4\n4\n78.7\n66\n4.08\n2.200\n19.47\n1\n1\n4\n1\n\n\n18\n30.4\n4\n75.7\n52\n4.93\n1.615\n18.52\n1\n1\n4\n2\n\n\n19\n33.9\n4\n71.1\n65\n4.22\n1.835\n19.90\n1\n1\n4\n1\n\n\n25\n27.3\n4\n79.0\n66\n4.08\n1.935\n18.90\n1\n1\n4\n1\n\n\n31\n21.4\n4\n121.0\n109\n4.11\n2.780\n18.60\n1\n1\n4\n2\n\n\n\n\n\n\n\n\n\n0.6.1 Filters with AND conditions\n\n\nCode\nmtcars[(mtcars['cyl'] &gt;4)&(mtcars['gear'] ==5) ]\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n28\n15.8\n8\n351.0\n264\n4.22\n3.17\n14.5\n0\n1\n5\n4\n\n\n29\n19.7\n6\n145.0\n175\n3.62\n2.77\n15.5\n0\n1\n5\n6\n\n\n30\n15.0\n8\n301.0\n335\n3.54\n3.57\n14.6\n0\n1\n5\n8\n\n\n\n\n\n\n\n\nother way:\n\n\nCode\nmtcars.query('cyl&gt;4 and gear==5')\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n28\n15.8\n8\n351.0\n264\n4.22\n3.17\n14.5\n0\n1\n5\n4\n\n\n29\n19.7\n6\n145.0\n175\n3.62\n2.77\n15.5\n0\n1\n5\n6\n\n\n30\n15.0\n8\n301.0\n335\n3.54\n3.57\n14.6\n0\n1\n5\n8\n\n\n\n\n\n\n\n\n\n\n0.6.2 Filters with OR conditions\n\n\nCode\nmtcars[(mtcars['cyl'] ==6) |(mtcars['gear'] ==5) ]\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n3\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n5\n18.1\n6\n225.0\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n\n\n9\n19.2\n6\n167.6\n123\n3.92\n3.440\n18.30\n1\n0\n4\n4\n\n\n10\n17.8\n6\n167.6\n123\n3.92\n3.440\n18.90\n1\n0\n4\n4\n\n\n26\n26.0\n4\n120.3\n91\n4.43\n2.140\n16.70\n0\n1\n5\n2\n\n\n27\n30.4\n4\n95.1\n113\n3.77\n1.513\n16.90\n1\n1\n5\n2\n\n\n28\n15.8\n8\n351.0\n264\n4.22\n3.170\n14.50\n0\n1\n5\n4\n\n\n29\n19.7\n6\n145.0\n175\n3.62\n2.770\n15.50\n0\n1\n5\n6\n\n\n30\n15.0\n8\n301.0\n335\n3.54\n3.570\n14.60\n0\n1\n5\n8\n\n\n\n\n\n\n\n\nother way:\n\n\nCode\nmtcars.query('cyl==6 or gear==5')\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n3\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n5\n18.1\n6\n225.0\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n\n\n9\n19.2\n6\n167.6\n123\n3.92\n3.440\n18.30\n1\n0\n4\n4\n\n\n10\n17.8\n6\n167.6\n123\n3.92\n3.440\n18.90\n1\n0\n4\n4\n\n\n26\n26.0\n4\n120.3\n91\n4.43\n2.140\n16.70\n0\n1\n5\n2\n\n\n27\n30.4\n4\n95.1\n113\n3.77\n1.513\n16.90\n1\n1\n5\n2\n\n\n28\n15.8\n8\n351.0\n264\n4.22\n3.170\n14.50\n0\n1\n5\n4\n\n\n29\n19.7\n6\n145.0\n175\n3.62\n2.770\n15.50\n0\n1\n5\n6\n\n\n30\n15.0\n8\n301.0\n335\n3.54\n3.570\n14.60\n0\n1\n5\n8\n\n\n\n\n\n\n\n\n\n\n0.6.3 filter row with index\n\n0.6.3.1 5th rows\n\n\nCode\n# not in siuba, in pandas\nmtcars.iloc[[4]]\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n4\n18.7\n8\n360.0\n175\n3.15\n3.44\n17.02\n0\n0\n3\n2\n\n\n\n\n\n\n\n\n\n\n0.6.3.2 1 and 5tj rows\n\n\nCode\n# not in siuba, in pandas\nmtcars.iloc[[0,4]]\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\n21.0\n6\n160.0\n110\n3.90\n2.62\n16.46\n0\n1\n4\n4\n\n\n4\n18.7\n8\n360.0\n175\n3.15\n3.44\n17.02\n0\n0\n3\n2\n\n\n\n\n\n\n\n\n\n\n0.6.3.3 1 to 5th rows\n\n\nCode\n# not in siuba, in pandas\nmtcars.iloc[0:4]\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n3\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n\n\n\n\n\n\n\n\n0.6.3.4 get ramdon 5 rows\n\n\nCode\nmtcars.sample(5, random_state=42)\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n29\n19.7\n6\n145.0\n175\n3.62\n2.770\n15.50\n0\n1\n5\n6\n\n\n15\n10.4\n8\n460.0\n215\n3.00\n5.424\n17.82\n0\n0\n3\n4\n\n\n24\n19.2\n8\n400.0\n175\n3.08\n3.845\n17.05\n0\n0\n3\n2\n\n\n17\n32.4\n4\n78.7\n66\n4.08\n2.200\n19.47\n1\n1\n4\n1\n\n\n8\n22.8\n4\n140.8\n95\n3.92\n3.150\n22.90\n1\n0\n4\n2",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#append",
    "href": "data manipulation/1 Pandas.html#append",
    "title": "Data manipulation with Pandas",
    "section": "0.7 Append",
    "text": "0.7 Append\n\n0.7.1 append by row\n\n\nCode\n# not available in siuba yet\n#from siuba import bind_rows\n\n\n\n\nCode\n# using pandas\n\n# get 1 to 4 rows\ndata1=mtcars.iloc[0:4]\n\n# get 9 rows\ndata2=mtcars.iloc[10:11]\n\ndata3=pd.concat([data1, data2], ignore_index = True,axis=0)\n\ndata3\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n3\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n4\n17.8\n6\n167.6\n123\n3.92\n3.440\n18.90\n1\n0\n4\n4\n\n\n\n\n\n\n\n\n\n\n0.7.2 append by column\n\n\nCode\n# not available in siuba yet\n#from siuba import bind_columns\n\n\n\n\nCode\n# using pandas\ndata1=small_mtcars&gt;&gt;select(_.mpg)\n\ndata2=small_mtcars&gt;&gt;select(_.cyl)\n\ndata3=pd.concat([data1, data2],axis=1)\n\ndata3\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\n\n\n\n\n0\n21.0\n6\n\n\n1\n21.0\n6\n\n\n2\n22.8\n4\n\n\n3\n21.4\n6\n\n\n4\n18.7\n8\n\n\n\n\n\n\n\n\n\n\n0.7.3 Dropping NA values\n\n\n0.7.4 keep NA values",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#group-by",
    "href": "data manipulation/1 Pandas.html#group-by",
    "title": "Data manipulation with Pandas",
    "section": "0.8 group by",
    "text": "0.8 group by\n\n0.8.1 average,min,max,sum\n\n\nCode\nmtcars.groupby(\"cyl\")[\"hp\"].mean()\n\n\ncyl\n4     82.636364\n6    122.285714\n8    209.214286\nName: hp, dtype: float64\n\n\n\n\nCode\nmtcars.groupby(\"cyl\")[\"hp\"].min()\n\n\ncyl\n4     52\n6    105\n8    150\nName: hp, dtype: int64\n\n\n\n\nCode\nmtcars.groupby(\"cyl\")[\"hp\"].max()\n\n\ncyl\n4    113\n6    175\n8    335\nName: hp, dtype: int64\n\n\n\n\nCode\nmtcars.groupby(\"cyl\")[\"hp\"].sum()\n\n\ncyl\n4     909\n6     856\n8    2929\nName: hp, dtype: int64\n\n\n\n\n0.8.2 count\n\n\nCode\nmtcars.groupby(\"cyl\")[\"cyl\"].count()\n\n\ncyl\n4    11\n6     7\n8    14\nName: cyl, dtype: int64",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#order-rows",
    "href": "data manipulation/1 Pandas.html#order-rows",
    "title": "Data manipulation with Pandas",
    "section": "0.9 order rows",
    "text": "0.9 order rows\n\n\nCode\nsmall_mtcars.sort_values('hp')\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\nhp\nmpg2\nmpg3\nmpg4\n\n\n\n\n2\n4\n22.8\n93\n23.8\nlong\nlong\n\n\n0\n6\n21.0\n110\n22.0\nlong\nMedium\n\n\n1\n6\n21.0\n110\n22.0\nlong\nMedium\n\n\n3\n6\n21.4\n110\n22.4\nlong\nMedium\n\n\n4\n8\n18.7\n175\n19.7\nshort\nshort\n\n\n\n\n\n\n\n\n\n0.9.1 Sort in descending order\n\n\nCode\nsmall_mtcars.sort_values('hp',ascending=False)\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\nhp\nmpg2\nmpg3\nmpg4\n\n\n\n\n4\n8\n18.7\n175\n19.7\nshort\nshort\n\n\n0\n6\n21.0\n110\n22.0\nlong\nMedium\n\n\n1\n6\n21.0\n110\n22.0\nlong\nMedium\n\n\n3\n6\n21.4\n110\n22.4\nlong\nMedium\n\n\n2\n4\n22.8\n93\n23.8\nlong\nlong\n\n\n\n\n\n\n\n\n\n\n0.9.2 Arrange by multiple variables\n\n\nCode\nsmall_mtcars.sort_values(by=['cyl','mpg'])\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\nhp\nmpg2\nmpg3\nmpg4\n\n\n\n\n2\n4\n22.8\n93\n23.8\nlong\nlong\n\n\n0\n6\n21.0\n110\n22.0\nlong\nMedium\n\n\n1\n6\n21.0\n110\n22.0\nlong\nMedium\n\n\n3\n6\n21.4\n110\n22.4\nlong\nMedium\n\n\n4\n8\n18.7\n175\n19.7\nshort\nshort",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#join",
    "href": "data manipulation/1 Pandas.html#join",
    "title": "Data manipulation with Pandas",
    "section": "0.10 join",
    "text": "0.10 join\n\n\nCode\nlhs = pd.DataFrame({'id': [1,2,3], 'val': ['lhs.1', 'lhs.2', 'lhs.3']})\nrhs = pd.DataFrame({'id': [1,2,4], 'val': ['rhs.1', 'rhs.2', 'rhs.3']})\n\n\n\n\nCode\nlhs\n\n\n\n\n\n\n\n\n\n\nid\nval\n\n\n\n\n0\n1\nlhs.1\n\n\n1\n2\nlhs.2\n\n\n2\n3\nlhs.3\n\n\n\n\n\n\n\n\n\n\nCode\nrhs\n\n\n\n\n\n\n\n\n\n\nid\nval\n\n\n\n\n0\n1\nrhs.1\n\n\n1\n2\nrhs.2\n\n\n2\n4\nrhs.3\n\n\n\n\n\n\n\n\n\n0.10.1 inner_join\n\n\nCode\nresult=pd.merge(lhs, rhs, on='id', how='inner')\nresult\n\n\n\n\n\n\n\n\n\n\nid\nval_x\nval_y\n\n\n\n\n0\n1\nlhs.1\nrhs.1\n\n\n1\n2\nlhs.2\nrhs.2\n\n\n\n\n\n\n\n\n\n\n0.10.2 full join\n\n\nCode\nresult=pd.merge(lhs, rhs, on='id', how='outer')\nresult\n\n\n\n\n\n\n\n\n\n\nid\nval_x\nval_y\n\n\n\n\n0\n1\nlhs.1\nrhs.1\n\n\n1\n2\nlhs.2\nrhs.2\n\n\n2\n3\nlhs.3\nNaN\n\n\n3\n4\nNaN\nrhs.3\n\n\n\n\n\n\n\n\n\n\n0.10.3 left join\n\n\nCode\nresult=pd.merge(lhs, rhs, on='id', how='left')\nresult\n\n\n\n\n\n\n\n\n\n\nid\nval_x\nval_y\n\n\n\n\n0\n1\nlhs.1\nrhs.1\n\n\n1\n2\nlhs.2\nrhs.2\n\n\n2\n3\nlhs.3\nNaN\n\n\n\n\n\n\n\n\n\n\n0.10.4 anti join\nkeep data in left which not in right\n\n\nCode\n#in siuba\nresult=lhs &gt;&gt; anti_join(_, rhs, on=\"id\")\nresult\n\n\n\n\n\n\n\n\n\n\nid\nval\n\n\n\n\n2\n3\nlhs.3\n\n\n\n\n\n\n\n\nkeep data in right which not in left\n\n\nCode\n#in siuba\nresult=rhs &gt;&gt; anti_join(_, lhs, on=\"id\")\nresult\n\n\n\n\n\n\n\n\n\n\nid\nval\n\n\n\n\n2\n4\nrhs.3",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#reshape-tables",
    "href": "data manipulation/1 Pandas.html#reshape-tables",
    "title": "Data manipulation with Pandas",
    "section": "0.11 Reshape tables",
    "text": "0.11 Reshape tables\n\n\nCode\ncosts = pd.DataFrame({\n    'id': [1,2],\n    'price_x': [.1, .2],\n    'price_y': [.4, .5],\n    'price_z': [.7, .8]\n})\n\ncosts\n\n\n\n\n\n\n\n\n\n\nid\nprice_x\nprice_y\nprice_z\n\n\n\n\n0\n1\n0.1\n0.4\n0.7\n\n\n1\n2\n0.2\n0.5\n0.8\n\n\n\n\n\n\n\n\n\n0.11.1 Gather data long(wide to long)\n\n\nCode\n# selecting each variable manually\nlong_date=pd.melt(costs,id_vars=['id'], value_vars=['price_x', 'price_y','price_z'])\n\nlong_date\n#costs &gt;&gt; gather('measure', 'value', _.price_x, _.price_y, _.price_z)\n\n\n\n\n\n\n\n\n\n\nid\nvariable\nvalue\n\n\n\n\n0\n1\nprice_x\n0.1\n\n\n1\n2\nprice_x\n0.2\n\n\n2\n1\nprice_y\n0.4\n\n\n3\n2\nprice_y\n0.5\n\n\n4\n1\nprice_z\n0.7\n\n\n5\n2\nprice_z\n0.8\n\n\n\n\n\n\n\n\n\n\n0.11.2 Spread data wide (long to wide)\n\n\nCode\nlong_date.pivot(index=\"id\", columns=\"variable\", values=\"value\")\n\n\n\n\n\n\n\n\n\nvariable\nprice_x\nprice_y\nprice_z\n\n\nid\n\n\n\n\n\n\n\n1\n0.1\n0.4\n0.7\n\n\n2\n0.2\n0.5\n0.8",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#string",
    "href": "data manipulation/1 Pandas.html#string",
    "title": "Data manipulation with Pandas",
    "section": "0.12 string",
    "text": "0.12 string\n\n\nCode\ndf = pd.DataFrame({'text': ['abc', 'DDD','1243c','aeEe'], 'num': [3, 4,7,8]})\n\ndf\n\n\n\n\n\n\n\n\n\n\ntext\nnum\n\n\n\n\n0\nabc\n3\n\n\n1\nDDD\n4\n\n\n2\n1243c\n7\n\n\n3\naeEe\n8\n\n\n\n\n\n\n\n\n\n0.12.1 upper case\n\n\nCode\ndf&gt;&gt; mutate(text_new=_.text.str.upper())\n\n\n\n\n\n\n\n\n\n\ntext\nnum\ntext_new\n\n\n\n\n0\nabc\n3\nABC\n\n\n1\nDDD\n4\nDDD\n\n\n2\n1243c\n7\n1243C\n\n\n3\naeEe\n8\nAEEE\n\n\n\n\n\n\n\n\n\n\n0.12.2 lower case\n\n\nCode\ndf&gt;&gt; mutate(text_new=_.text.str.lower())\n\n\n\n\n\n\n\n\n\n\ntext\nnum\ntext_new\n\n\n\n\n0\nabc\n3\nabc\n\n\n1\nDDD\n4\nddd\n\n\n2\n1243c\n7\n1243c\n\n\n3\naeEe\n8\naeee\n\n\n\n\n\n\n\n\n\n\n0.12.3 match\n\n\nCode\ndf&gt;&gt; mutate(text_new1=if_else(_.text== \"abc\",'T','F')\n            ,text_new2=if_else(_.text.str.startswith(\"a\"),'T','F')\n            ,text_new3=if_else(_.text.str.endswith(\"c\"),'T','F')\n            ,text_new4=if_else(_.text.str.contains(\"4\"),'T','F')\n\n)\n\n\n\n\n\n\n\n\n\n\ntext\nnum\ntext_new1\ntext_new2\ntext_new3\ntext_new4\n\n\n\n\n0\nabc\n3\nT\nT\nT\nF\n\n\n1\nDDD\n4\nF\nF\nF\nF\n\n\n2\n1243c\n7\nF\nF\nT\nT\n\n\n3\naeEe\n8\nF\nT\nF\nF\n\n\n\n\n\n\n\n\n\n\n0.12.4 concatenation\n\n\nCode\ndf&gt;&gt; mutate(text_new1=_.text+' is '+_.text\n)\n\n\n\n\n\n\n\n\n\n\ntext\nnum\ntext_new1\n\n\n\n\n0\nabc\n3\nabc is abc\n\n\n1\nDDD\n4\nDDD is DDD\n\n\n2\n1243c\n7\n1243c is 1243c\n\n\n3\naeEe\n8\naeEe is aeEe\n\n\n\n\n\n\n\n\n\n\n0.12.5 replace\nUse .str.replace(…, regex=True) with regular expressions to replace patterns in strings.\nFor example, the code below uses “p.”, where . is called a wildcard–which matches any character.\n\n\nCode\ndf&gt;&gt; mutate(text_new1=_.text.str.replace(\"a.\", \"XX\", regex=True)\n)\n\n\n\n\n\n\n\n\n\n\ntext\nnum\ntext_new1\n\n\n\n\n0\nabc\n3\nXXc\n\n\n1\nDDD\n4\nDDD\n\n\n2\n1243c\n7\n1243c\n\n\n3\naeEe\n8\nXXEe\n\n\n\n\n\n\n\n\n\n\n0.12.6 extract\nUse str.extract() with a regular expression to pull out a matching piece of text.\nFor example the regular expression “^(.*) ” contains the following pieces:\n\na matches the literal letter “a”\n.* has a . which matches anything, and * which modifies it to apply 0 or more times.\n\n\n\nCode\ndf&gt;&gt; mutate(text_new1=_.text.str.extract(\"a(.*)\")\n            ,text_new2=_.text.str.extract(\"(.*)c\")\n)\n\n\n\n\n\n\n\n\n\n\ntext\nnum\ntext_new1\ntext_new2\n\n\n\n\n0\nabc\n3\nbc\nab\n\n\n1\nDDD\n4\nNaN\nNaN\n\n\n2\n1243c\n7\nNaN\n1243\n\n\n3\naeEe\n8\neEe\nNaN",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/1 Pandas.html#date",
    "href": "data manipulation/1 Pandas.html#date",
    "title": "Data manipulation with Pandas",
    "section": "0.13 date",
    "text": "0.13 date\n\n\nCode\ndf_dates = pd.DataFrame({\n    \"dates\": pd.to_datetime([\"2021-01-02\", \"2021-02-03\"]),\n    \"raw\": [\"2023-04-05 06:07:08\", \"2024-05-06 07:08:09\"],\n})\ndf_dates\n\n\n\n\n\n\n\n\n\n\ndates\nraw\n\n\n\n\n0\n2021-01-02\n2023-04-05 06:07:08\n\n\n1\n2021-02-03\n2024-05-06 07:08:09\n\n\n\n\n\n\n\n\n\n\nCode\nfrom datetime import datetime\n\ndf_date=df_dates&gt;&gt;mutate(month=_.dates.dt.month_name()\n                  ,date_format_raw = call(pd.to_datetime, _.raw)\n                  ,date_format_raw_year=_.date_format_raw.dt.year\n\n)\n\ndf_date\n\n\n\n\n\n\n\n\n\n\ndates\nraw\nmonth\ndate_format_raw\ndate_format_raw_year\n\n\n\n\n0\n2021-01-02\n2023-04-05 06:07:08\nJanuary\n2023-04-05 06:07:08\n2023\n\n\n1\n2021-02-03\n2024-05-06 07:08:09\nFebruary\n2024-05-06 07:08:09\n2024\n\n\n\n\n\n\n\n\n\n\nCode\ndf_date.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2 entries, 0 to 1\nData columns (total 5 columns):\n #   Column                Non-Null Count  Dtype         \n---  ------                --------------  -----         \n 0   dates                 2 non-null      datetime64[ns]\n 1   raw                   2 non-null      object        \n 2   month                 2 non-null      object        \n 3   date_format_raw       2 non-null      datetime64[ns]\n 4   date_format_raw_year  2 non-null      int32         \ndtypes: datetime64[ns](2), int32(1), object(2)\nmemory usage: 204.0+ bytes",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "regression/1 Linear Regression.html",
    "href": "regression/1 Linear Regression.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Code\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)",
    "crumbs": [
      "Regression",
      "Linear Regression"
    ]
  },
  {
    "objectID": "regression/1 Linear Regression.html#load-package",
    "href": "regression/1 Linear Regression.html#load-package",
    "title": "Linear Regression",
    "section": "",
    "text": "Code\nfrom siuba.data import mtcars,penguins\n\nfrom siuba.siu import call\nfrom siuba import _, mutate, filter, group_by, summarize,show_query\nfrom siuba import *\n\nfrom siuba.data import mtcars,penguins\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split",
    "crumbs": [
      "Regression",
      "Linear Regression"
    ]
  },
  {
    "objectID": "regression/1 Linear Regression.html#input-data",
    "href": "regression/1 Linear Regression.html#input-data",
    "title": "Linear Regression",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nId = df_train['Id']\n\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nId\nMSSubClass\nMSZoning\nLotFrontage\nLotArea\nStreet\nAlley\nLotShape\nLandContour\nUtilities\n...\nPoolArea\nPoolQC\nFence\nMiscFeature\nMiscVal\nMoSold\nYrSold\nSaleType\nSaleCondition\nSalePrice\n\n\n\n\n0\n1\n60\nRL\n65.0\n8450\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n2\n2008\nWD\nNormal\n208500\n\n\n1\n2\n20\nRL\n80.0\n9600\nPave\nNaN\nReg\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n5\n2007\nWD\nNormal\n181500\n\n\n2\n3\n60\nRL\n68.0\n11250\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n9\n2008\nWD\nNormal\n223500\n\n\n3\n4\n70\nRL\n60.0\n9550\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n2\n2006\nWD\nAbnorml\n140000\n\n\n4\n5\n60\nRL\n84.0\n14260\nPave\nNaN\nIR1\nLvl\nAllPub\n...\n0\nNaN\nNaN\nNaN\n0\n12\n2008\nWD\nNormal\n250000\n\n\n\n\n5 rows × 81 columns\n\n\n\n\n\n\nCode\n#df_train.info()\n\n\n\n\nCode\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['SalePrice'], axis=1), df_test])",
    "crumbs": [
      "Regression",
      "Linear Regression"
    ]
  },
  {
    "objectID": "regression/1 Linear Regression.html#data-eda",
    "href": "regression/1 Linear Regression.html#data-eda",
    "title": "Linear Regression",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\nin step 1",
    "crumbs": [
      "Regression",
      "Linear Regression"
    ]
  },
  {
    "objectID": "regression/1 Linear Regression.html#data-wrangling",
    "href": "regression/1 Linear Regression.html#data-wrangling",
    "title": "Linear Regression",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling",
    "crumbs": [
      "Regression",
      "Linear Regression"
    ]
  },
  {
    "objectID": "regression/1 Linear Regression.html#split-data",
    "href": "regression/1 Linear Regression.html#split-data",
    "title": "Linear Regression",
    "section": "2.4 split data",
    "text": "2.4 split data\n\n\nCode\nY = df_train.SalePrice\nX = df_train.drop(['SalePrice'], axis=1)\n\n\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\nX_train = X_train.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nprint(X_train.shape)\nprint(X_test.shape)\n\n\n(1168, 80)\n(292, 80)\n\n\n\n\nCode\nprint(Y_train.shape)\nprint(Y_test.shape)\n\n\n(1168,)\n(292,)\n\n\n\n\nCode\ncategorical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].nunique() &lt; 10 and X_train[cname].dtype == \"object\"]\n                    \n                    \nnumerical_cols = numerical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].dtype in ['int64', 'float64']]\n\n\n\n\nCode\nprint(\"The total number of categorical columns:\", len(categorical_cols))\nprint(\"The total number of numerical columns:\", len(numerical_cols))\n\n\nThe total number of categorical columns: 40\nThe total number of numerical columns: 37\n\n\n\n\nCode\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train[my_cols].copy()\nX_test= X_test[my_cols].copy()\n\n\nX_final = df_test[my_cols].copy()",
    "crumbs": [
      "Regression",
      "Linear Regression"
    ]
  },
  {
    "objectID": "regression/1 Linear Regression.html#pipelines-for-data-preprocessing",
    "href": "regression/1 Linear Regression.html#pipelines-for-data-preprocessing",
    "title": "Linear Regression",
    "section": "2.5 Pipelines for Data Preprocessing",
    "text": "2.5 Pipelines for Data Preprocessing\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\nCode\nnumerical_transformer = Pipeline(steps=[\n    ('imputer_num', SimpleImputer(strategy='median'))\n   # ,('scaler', StandardScaler())\n])\n\n\n\n\nCode\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n\n\nCode\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)])",
    "crumbs": [
      "Regression",
      "Linear Regression"
    ]
  },
  {
    "objectID": "regression/1 Linear Regression.html#define-model",
    "href": "regression/1 Linear Regression.html#define-model",
    "title": "Linear Regression",
    "section": "3.1 define model",
    "text": "3.1 define model\n\n\nCode\nfrom sklearn.linear_model import LinearRegression\nml_model = LinearRegression()\nml_model\n\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniNot fittedLinearRegression()",
    "crumbs": [
      "Regression",
      "Linear Regression"
    ]
  },
  {
    "objectID": "regression/1 Linear Regression.html#define-pipline",
    "href": "regression/1 Linear Regression.html#define-pipline",
    "title": "Linear Regression",
    "section": "3.2 define pipline",
    "text": "3.2 define pipline\n\n\nCode\npipeline = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model_dt', ml_model)\n         ]\n)",
    "crumbs": [
      "Regression",
      "Linear Regression"
    ]
  },
  {
    "objectID": "regression/1 Linear Regression.html#train-model",
    "href": "regression/1 Linear Regression.html#train-model",
    "title": "Linear Regression",
    "section": "3.3 train model",
    "text": "3.3 train model\n\n\nCode\npipeline.fit(X_train, Y_train)\n\n\nPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer_num',\n                                                                   SimpleImputer(strategy='median'))]),\n                                                  ['Id', 'MSSubClass',\n                                                   'LotFrontage', 'LotArea',\n                                                   'OverallQual', 'OverallCond',\n                                                   'YearBuilt', 'YearRemodAdd',\n                                                   'MasVnrArea', 'BsmtFinSF1',\n                                                   'BsmtFinSF2', 'BsmtUnfSF',\n                                                   'TotalBsmtSF', '1stFlrSF',\n                                                   '2ndFlrSF', 'LowQualFinSF',\n                                                   'GrLivArea',...\n                                                   'LandContour', 'Utilities',\n                                                   'LotConfig', 'LandSlope',\n                                                   'Condition1', 'Condition2',\n                                                   'BldgType', 'HouseStyle',\n                                                   'RoofStyle', 'RoofMatl',\n                                                   'MasVnrType', 'ExterQual',\n                                                   'ExterCond', 'Foundation',\n                                                   'BsmtQual', 'BsmtCond',\n                                                   'BsmtExposure',\n                                                   'BsmtFinType1',\n                                                   'BsmtFinType2', 'Heating',\n                                                   'HeatingQC', 'CentralAir',\n                                                   'Electrical', 'KitchenQual',\n                                                   'Functional', 'FireplaceQu', ...])])),\n                ('model_dt', LinearRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiFittedPipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer_num',\n                                                                   SimpleImputer(strategy='median'))]),\n                                                  ['Id', 'MSSubClass',\n                                                   'LotFrontage', 'LotArea',\n                                                   'OverallQual', 'OverallCond',\n                                                   'YearBuilt', 'YearRemodAdd',\n                                                   'MasVnrArea', 'BsmtFinSF1',\n                                                   'BsmtFinSF2', 'BsmtUnfSF',\n                                                   'TotalBsmtSF', '1stFlrSF',\n                                                   '2ndFlrSF', 'LowQualFinSF',\n                                                   'GrLivArea',...\n                                                   'LandContour', 'Utilities',\n                                                   'LotConfig', 'LandSlope',\n                                                   'Condition1', 'Condition2',\n                                                   'BldgType', 'HouseStyle',\n                                                   'RoofStyle', 'RoofMatl',\n                                                   'MasVnrType', 'ExterQual',\n                                                   'ExterCond', 'Foundation',\n                                                   'BsmtQual', 'BsmtCond',\n                                                   'BsmtExposure',\n                                                   'BsmtFinType1',\n                                                   'BsmtFinType2', 'Heating',\n                                                   'HeatingQC', 'CentralAir',\n                                                   'Electrical', 'KitchenQual',\n                                                   'Functional', 'FireplaceQu', ...])])),\n                ('model_dt', LinearRegression())])  preprocessor: ColumnTransformer?Documentation for preprocessor: ColumnTransformerColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('imputer_num',\n                                                  SimpleImputer(strategy='median'))]),\n                                 ['Id', 'MSSubClass', 'LotFrontage', 'LotArea',\n                                  'OverallQual', 'OverallCond', 'YearBuilt',\n                                  'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1',\n                                  'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF',\n                                  '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n                                  'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath',\n                                  'F...\n                                 ['MSZoning', 'Street', 'Alley', 'LotShape',\n                                  'LandContour', 'Utilities', 'LotConfig',\n                                  'LandSlope', 'Condition1', 'Condition2',\n                                  'BldgType', 'HouseStyle', 'RoofStyle',\n                                  'RoofMatl', 'MasVnrType', 'ExterQual',\n                                  'ExterCond', 'Foundation', 'BsmtQual',\n                                  'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n                                  'BsmtFinType2', 'Heating', 'HeatingQC',\n                                  'CentralAir', 'Electrical', 'KitchenQual',\n                                  'Functional', 'FireplaceQu', ...])]) num['Id', 'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='median') cat['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'SaleType', 'SaleCondition']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='most_frequent')  OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(handle_unknown='ignore')  LinearRegression?Documentation for LinearRegressionLinearRegression() \n\n\n\n\nCode\nfitted_model=pipeline.steps[1][1]\n\n\n\n\nCode\nvar=pipeline[:-1].get_feature_names_out()\nvar\n\n\narray(['num__Id', 'num__MSSubClass', 'num__LotFrontage', 'num__LotArea',\n       'num__OverallQual', 'num__OverallCond', 'num__YearBuilt',\n       'num__YearRemodAdd', 'num__MasVnrArea', 'num__BsmtFinSF1',\n       'num__BsmtFinSF2', 'num__BsmtUnfSF', 'num__TotalBsmtSF',\n       'num__1stFlrSF', 'num__2ndFlrSF', 'num__LowQualFinSF',\n       'num__GrLivArea', 'num__BsmtFullBath', 'num__BsmtHalfBath',\n       'num__FullBath', 'num__HalfBath', 'num__BedroomAbvGr',\n       'num__KitchenAbvGr', 'num__TotRmsAbvGrd', 'num__Fireplaces',\n       'num__GarageYrBlt', 'num__GarageCars', 'num__GarageArea',\n       'num__WoodDeckSF', 'num__OpenPorchSF', 'num__EnclosedPorch',\n       'num__3SsnPorch', 'num__ScreenPorch', 'num__PoolArea',\n       'num__MiscVal', 'num__MoSold', 'num__YrSold',\n       'cat__MSZoning_C (all)', 'cat__MSZoning_FV', 'cat__MSZoning_RH',\n       'cat__MSZoning_RL', 'cat__MSZoning_RM', 'cat__Street_Grvl',\n       'cat__Street_Pave', 'cat__Alley_Grvl', 'cat__Alley_Pave',\n       'cat__LotShape_IR1', 'cat__LotShape_IR2', 'cat__LotShape_IR3',\n       'cat__LotShape_Reg', 'cat__LandContour_Bnk',\n       'cat__LandContour_HLS', 'cat__LandContour_Low',\n       'cat__LandContour_Lvl', 'cat__Utilities_AllPub',\n       'cat__Utilities_NoSeWa', 'cat__LotConfig_Corner',\n       'cat__LotConfig_CulDSac', 'cat__LotConfig_FR2',\n       'cat__LotConfig_FR3', 'cat__LotConfig_Inside',\n       'cat__LandSlope_Gtl', 'cat__LandSlope_Mod', 'cat__LandSlope_Sev',\n       'cat__Condition1_Artery', 'cat__Condition1_Feedr',\n       'cat__Condition1_Norm', 'cat__Condition1_PosA',\n       'cat__Condition1_PosN', 'cat__Condition1_RRAe',\n       'cat__Condition1_RRAn', 'cat__Condition1_RRNe',\n       'cat__Condition1_RRNn', 'cat__Condition2_Artery',\n       'cat__Condition2_Feedr', 'cat__Condition2_Norm',\n       'cat__Condition2_PosN', 'cat__Condition2_RRAn',\n       'cat__Condition2_RRNn', 'cat__BldgType_1Fam',\n       'cat__BldgType_2fmCon', 'cat__BldgType_Duplex',\n       'cat__BldgType_Twnhs', 'cat__BldgType_TwnhsE',\n       'cat__HouseStyle_1.5Fin', 'cat__HouseStyle_1.5Unf',\n       'cat__HouseStyle_1Story', 'cat__HouseStyle_2.5Fin',\n       'cat__HouseStyle_2.5Unf', 'cat__HouseStyle_2Story',\n       'cat__HouseStyle_SFoyer', 'cat__HouseStyle_SLvl',\n       'cat__RoofStyle_Flat', 'cat__RoofStyle_Gable',\n       'cat__RoofStyle_Gambrel', 'cat__RoofStyle_Hip',\n       'cat__RoofStyle_Mansard', 'cat__RoofStyle_Shed',\n       'cat__RoofMatl_ClyTile', 'cat__RoofMatl_CompShg',\n       'cat__RoofMatl_Membran', 'cat__RoofMatl_Roll',\n       'cat__RoofMatl_Tar&Grv', 'cat__RoofMatl_WdShake',\n       'cat__RoofMatl_WdShngl', 'cat__MasVnrType_BrkCmn',\n       'cat__MasVnrType_BrkFace', 'cat__MasVnrType_Stone',\n       'cat__ExterQual_Ex', 'cat__ExterQual_Fa', 'cat__ExterQual_Gd',\n       'cat__ExterQual_TA', 'cat__ExterCond_Ex', 'cat__ExterCond_Fa',\n       'cat__ExterCond_Gd', 'cat__ExterCond_Po', 'cat__ExterCond_TA',\n       'cat__Foundation_BrkTil', 'cat__Foundation_CBlock',\n       'cat__Foundation_PConc', 'cat__Foundation_Slab',\n       'cat__Foundation_Stone', 'cat__Foundation_Wood',\n       'cat__BsmtQual_Ex', 'cat__BsmtQual_Fa', 'cat__BsmtQual_Gd',\n       'cat__BsmtQual_TA', 'cat__BsmtCond_Fa', 'cat__BsmtCond_Gd',\n       'cat__BsmtCond_Po', 'cat__BsmtCond_TA', 'cat__BsmtExposure_Av',\n       'cat__BsmtExposure_Gd', 'cat__BsmtExposure_Mn',\n       'cat__BsmtExposure_No', 'cat__BsmtFinType1_ALQ',\n       'cat__BsmtFinType1_BLQ', 'cat__BsmtFinType1_GLQ',\n       'cat__BsmtFinType1_LwQ', 'cat__BsmtFinType1_Rec',\n       'cat__BsmtFinType1_Unf', 'cat__BsmtFinType2_ALQ',\n       'cat__BsmtFinType2_BLQ', 'cat__BsmtFinType2_GLQ',\n       'cat__BsmtFinType2_LwQ', 'cat__BsmtFinType2_Rec',\n       'cat__BsmtFinType2_Unf', 'cat__Heating_Floor', 'cat__Heating_GasA',\n       'cat__Heating_GasW', 'cat__Heating_Grav', 'cat__Heating_OthW',\n       'cat__Heating_Wall', 'cat__HeatingQC_Ex', 'cat__HeatingQC_Fa',\n       'cat__HeatingQC_Gd', 'cat__HeatingQC_Po', 'cat__HeatingQC_TA',\n       'cat__CentralAir_N', 'cat__CentralAir_Y', 'cat__Electrical_FuseA',\n       'cat__Electrical_FuseF', 'cat__Electrical_FuseP',\n       'cat__Electrical_Mix', 'cat__Electrical_SBrkr',\n       'cat__KitchenQual_Ex', 'cat__KitchenQual_Fa',\n       'cat__KitchenQual_Gd', 'cat__KitchenQual_TA',\n       'cat__Functional_Maj1', 'cat__Functional_Maj2',\n       'cat__Functional_Min1', 'cat__Functional_Min2',\n       'cat__Functional_Mod', 'cat__Functional_Sev',\n       'cat__Functional_Typ', 'cat__FireplaceQu_Ex',\n       'cat__FireplaceQu_Fa', 'cat__FireplaceQu_Gd',\n       'cat__FireplaceQu_Po', 'cat__FireplaceQu_TA',\n       'cat__GarageType_2Types', 'cat__GarageType_Attchd',\n       'cat__GarageType_Basment', 'cat__GarageType_BuiltIn',\n       'cat__GarageType_CarPort', 'cat__GarageType_Detchd',\n       'cat__GarageFinish_Fin', 'cat__GarageFinish_RFn',\n       'cat__GarageFinish_Unf', 'cat__GarageQual_Ex',\n       'cat__GarageQual_Fa', 'cat__GarageQual_Gd', 'cat__GarageQual_Po',\n       'cat__GarageQual_TA', 'cat__GarageCond_Ex', 'cat__GarageCond_Fa',\n       'cat__GarageCond_Gd', 'cat__GarageCond_Po', 'cat__GarageCond_TA',\n       'cat__PavedDrive_N', 'cat__PavedDrive_P', 'cat__PavedDrive_Y',\n       'cat__PoolQC_Ex', 'cat__PoolQC_Fa', 'cat__PoolQC_Gd',\n       'cat__Fence_GdPrv', 'cat__Fence_GdWo', 'cat__Fence_MnPrv',\n       'cat__Fence_MnWw', 'cat__MiscFeature_Gar2',\n       'cat__MiscFeature_Othr', 'cat__MiscFeature_Shed',\n       'cat__SaleType_COD', 'cat__SaleType_CWD', 'cat__SaleType_Con',\n       'cat__SaleType_ConLD', 'cat__SaleType_ConLI',\n       'cat__SaleType_ConLw', 'cat__SaleType_New', 'cat__SaleType_Oth',\n       'cat__SaleType_WD', 'cat__SaleCondition_Abnorml',\n       'cat__SaleCondition_AdjLand', 'cat__SaleCondition_Alloca',\n       'cat__SaleCondition_Family', 'cat__SaleCondition_Normal',\n       'cat__SaleCondition_Partial'], dtype=object)\n\n\nvariable importance\n\n\nCode\n#importances = fitted_model.feature_importances_\n#vi=pd.DataFrame({\"variable\":var,\"importances\":importances})\n#vi=vi.sort_values('importances',ascending=False)\n#vi\n\n\n\n\nCode\nfitted_model.intercept_\n\n\n-736277.43942112\n\n\n\n\nCode\nprint('Coefficents:',fitted_model.coef_)\n\n\nCoefficents: [ 2.00063833e-01 -7.23577849e+00  2.78942213e+01  7.62922811e-01\n  7.73108358e+03  7.08904721e+03  2.32096735e+02  1.82015831e+01\n  2.13711287e+01  1.52992003e+01  1.19423510e+01 -5.42644954e+00\n  2.18150716e+01  2.46509721e+01  3.39695950e+01 -2.67932400e+01\n  3.18273193e+01 -3.30611916e+02 -1.41075969e+03  4.58790988e+03\n  3.99064038e+03 -4.54446484e+03 -2.05591680e+04  1.18559755e+03\n  4.76107645e+03 -4.35490448e+00  1.33785889e+03  2.64936968e+01\n  1.63174964e+01 -1.57242146e+01  5.34422455e+00  2.42185724e+01\n  2.54088448e+00  6.35602232e+01  7.28673174e+00 -2.61641310e+02\n  1.16619852e+01 -1.82773070e+04  9.88672633e+03  2.97671622e+03\n  6.25094702e+03 -8.37082549e+02 -1.21549942e+04  1.21549942e+04\n -4.19769031e+02  4.19769031e+02 -2.97551099e+03  2.41536678e+03\n  3.86930311e+03 -3.30915890e+03 -3.11492819e+03  9.43308478e+03\n -1.10236180e+04  4.70546138e+03  1.45260677e+04 -1.45260677e+04\n  1.76895490e+03  9.84753268e+03 -4.18241113e+03 -7.67263370e+03\n  2.38557252e+02  2.14337079e+04  3.07752331e+04 -5.22089410e+04\n -1.04998666e+04 -5.38665092e+02  1.10030995e+04 -1.34713151e+04\n  9.62425349e+03 -9.80245692e+03  7.26632178e+03  2.82412111e+02\n  6.13621688e+03  7.44844310e+04  4.29829007e+04  3.49559739e+04\n -2.17777780e+05  1.70674682e+04  4.82870063e+04  6.42407321e+03\n -2.75529210e+03  1.61650573e+03 -3.60930723e+03 -1.67597962e+03\n  5.29472609e+03  1.90413526e+04  8.21246143e+03 -2.06996712e+04\n -9.00107234e+03 -3.29304492e+03  1.83071395e+03 -1.38546560e+03\n -1.45984806e+04 -2.39986446e+04 -1.56166566e+04 -2.31556338e+04\n -1.10752377e+04  8.84446534e+04 -6.34745363e+05  8.44014123e+04\n  1.78865228e+05  7.72604841e+04  7.75706906e+04  6.50890108e+04\n  1.51558537e+05 -6.51945484e+03  1.86681887e+03  4.65263596e+03\n  1.24920942e+04  5.53850021e+03 -6.58276317e+03 -1.14478313e+04\n  9.92273047e+03 -4.23986829e+03 -1.21560544e+04  1.38408295e+04\n -7.36763734e+03  4.37755040e+03  5.58346788e+03  1.04024388e+04\n  5.82797615e+03  5.87566325e+02 -2.67789995e+04  1.36710744e+04\n -6.82992826e+03 -4.12293538e+03 -2.71821080e+03 -2.60681394e+04\n -2.45557233e+04  7.30616001e+04 -2.24377374e+04  2.39362001e+02\n  8.10618577e+03 -3.02942365e+03 -5.31612412e+03 -3.16148062e+03\n -5.25466978e+02  6.01342827e+03 -3.31668535e+03 -6.31933992e+02\n  1.62213866e+03  2.46124554e+03 -6.21073346e+03  1.04124882e+04\n -9.93867637e+03  5.70903066e+02  2.70477302e+03  1.87753917e+04\n  5.10628497e+03  1.49465271e+03 -9.26817148e+03 -2.56065099e+04\n  9.49835206e+03  1.33366177e+03  6.20885061e+03 -1.67706667e+03\n -5.01142388e+03 -8.54021826e+02  2.29285922e+03 -2.29285922e+03\n  1.97802903e+04  1.63042906e+04  1.51897937e+04 -6.62442287e+04\n  1.49698541e+04  2.11350223e+04 -3.76795596e+03 -8.70879170e+03\n -8.65827468e+03  7.28472784e+03  3.75360098e+03  1.23940410e+04\n  1.27155700e+04 -1.90830289e+02 -5.97987769e+04  2.38416673e+04\n -2.19880944e+03 -5.78920172e+03  1.75024074e+03  7.17614312e+03\n -9.38372700e+02 -1.38995931e+04  3.07927614e+03  4.56436858e+03\n  2.12519805e+03 -1.05002899e+03  5.18077927e+03 -3.74778963e+02\n -1.54119146e+03  1.91597042e+03  8.55709547e+04 -2.58108838e+04\n -1.54539236e+04 -2.60423194e+04 -1.82638280e+04 -8.11984287e+04\n  1.47285038e+04  2.20453463e+04  2.43388452e+04  2.00857334e+04\n  5.62099005e+03 -6.22493261e+03  6.03942563e+02  5.12180070e+04\n -3.13691905e+04 -1.98488164e+04 -8.34920336e+03  4.38465977e+03\n  4.53200819e+03 -5.67464598e+02 -7.16259879e+04  3.01214765e+04\n  4.15045114e+04 -1.53340189e+04  5.18328948e+03  3.47199275e+04\n -2.47403521e+03  1.50058902e+03 -8.26867742e+03  4.87628541e+03\n -1.05332495e+04 -9.67011039e+03 -6.23150656e+03  1.24672861e+03\n  1.10042266e+04 -9.05410933e+03 -1.68623818e+03  4.72089889e+03]",
    "crumbs": [
      "Regression",
      "Linear Regression"
    ]
  },
  {
    "objectID": "regression/1 Linear Regression.html#preformance",
    "href": "regression/1 Linear Regression.html#preformance",
    "title": "Linear Regression",
    "section": "3.4 Preformance",
    "text": "3.4 Preformance\n\n\nCode\nY_pred_dt =pipeline.predict(X_test) #always gets x and retuns y\n\n\nR 2\n\n\nCode\nfrom sklearn.metrics import r2_score\nr2_score(Y_test, Y_pred_dt)\n\n\n0.8813245079668873\n\n\nMAE\n\n\nCode\nfrom sklearn.metrics import mean_absolute_error\nmean_absolute_error(Y_test, Y_pred_dt)\n\n\n18325.533081970603\n\n\nRMSE\n\n\nCode\nfrom  math import sqrt\nfrom sklearn.metrics import mean_squared_error\nmse=mean_squared_error(Y_test, Y_pred_dt)\nrmse=sqrt(mse)\nrmse\n\n\n28222.81096381496",
    "crumbs": [
      "Regression",
      "Linear Regression"
    ]
  },
  {
    "objectID": "regression/1 Linear Regression.html#k-fold-cross-validation",
    "href": "regression/1 Linear Regression.html#k-fold-cross-validation",
    "title": "Linear Regression",
    "section": "3.5 k-Fold Cross-Validation",
    "text": "3.5 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \n\n\n\n\nCode\ncv_dt = cross_val_score(pipeline, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.734162022812417\n\n\n\n\nCode\ncv_dt = cross_val_score(pipeline, X_train, Y_train, cv=kf_dt,scoring = 'neg_mean_squared_error')\nnp.mean(np.sqrt(np.abs(cv_dt)))\n\n\n42971.22356218268",
    "crumbs": [
      "Regression",
      "Linear Regression"
    ]
  },
  {
    "objectID": "regression/8 Multiple Models using Pipeline and fast tuning.html",
    "href": "regression/8 Multiple Models using Pipeline and fast tuning.html",
    "title": "Multiple models using pipeline and fast tuning",
    "section": "",
    "text": "with pipeline and tunning",
    "crumbs": [
      "Regression",
      "Multiple models using pipeline and fast tuning"
    ]
  },
  {
    "objectID": "regression/8 Multiple Models using Pipeline and fast tuning.html#input-data",
    "href": "regression/8 Multiple Models using Pipeline and fast tuning.html#input-data",
    "title": "Multiple models using pipeline and fast tuning",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/train.csv')\ndf_test = pd.read_csv('./data/test.csv')\n\n# Store our test passenger IDs for easy access\nId = df_train['Id']\nId_test = df_test['Id']\n\ndf_train = df_train &gt;&gt; select(~_.Id,~_.PoolQC,~_.Fence,~_.MiscFeature,~_.Alley)\ndf_test = df_test &gt;&gt; select(~_.Id,~_.PoolQC,~_.Fence,~_.MiscFeature,~_.Alley)\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nMSSubClass\nMSZoning\nLotFrontage\nLotArea\nStreet\nLotShape\nLandContour\nUtilities\nLotConfig\nLandSlope\n...\nEnclosedPorch\n3SsnPorch\nScreenPorch\nPoolArea\nMiscVal\nMoSold\nYrSold\nSaleType\nSaleCondition\nSalePrice\n\n\n\n\n0\n60\nRL\n65.0\n8450\nPave\nReg\nLvl\nAllPub\nInside\nGtl\n...\n0\n0\n0\n0\n0\n2\n2008\nWD\nNormal\n208500\n\n\n1\n20\nRL\n80.0\n9600\nPave\nReg\nLvl\nAllPub\nFR2\nGtl\n...\n0\n0\n0\n0\n0\n5\n2007\nWD\nNormal\n181500\n\n\n2\n60\nRL\n68.0\n11250\nPave\nIR1\nLvl\nAllPub\nInside\nGtl\n...\n0\n0\n0\n0\n0\n9\n2008\nWD\nNormal\n223500\n\n\n3\n70\nRL\n60.0\n9550\nPave\nIR1\nLvl\nAllPub\nCorner\nGtl\n...\n272\n0\n0\n0\n0\n2\n2006\nWD\nAbnorml\n140000\n\n\n4\n60\nRL\n84.0\n14260\nPave\nIR1\nLvl\nAllPub\nFR2\nGtl\n...\n0\n0\n0\n0\n0\n12\n2008\nWD\nNormal\n250000\n\n\n\n\n5 rows × 76 columns",
    "crumbs": [
      "Regression",
      "Multiple models using pipeline and fast tuning"
    ]
  },
  {
    "objectID": "regression/8 Multiple Models using Pipeline and fast tuning.html#data-eda",
    "href": "regression/8 Multiple Models using Pipeline and fast tuning.html#data-eda",
    "title": "Multiple models using pipeline and fast tuning",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\nin step 1",
    "crumbs": [
      "Regression",
      "Multiple models using pipeline and fast tuning"
    ]
  },
  {
    "objectID": "regression/8 Multiple Models using Pipeline and fast tuning.html#data-wrangling",
    "href": "regression/8 Multiple Models using Pipeline and fast tuning.html#data-wrangling",
    "title": "Multiple models using pipeline and fast tuning",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling",
    "crumbs": [
      "Regression",
      "Multiple models using pipeline and fast tuning"
    ]
  },
  {
    "objectID": "regression/8 Multiple Models using Pipeline and fast tuning.html#data-wrangling-1",
    "href": "regression/8 Multiple Models using Pipeline and fast tuning.html#data-wrangling-1",
    "title": "Multiple models using pipeline and fast tuning",
    "section": "2.4 Data Wrangling",
    "text": "2.4 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\n#SalePrice_train = df_train.SalePrice\n\n\n\ndf_train['role'] = 'train'\ndf_test['role'] = 'test'\n\n# Concatenate training and test sets\ndata = pd.concat([df_train.drop(['SalePrice'], axis=1), df_test])\n\n\n\n\nCode\ndata.shape\n\n\n(2919, 76)",
    "crumbs": [
      "Regression",
      "Multiple models using pipeline and fast tuning"
    ]
  },
  {
    "objectID": "regression/8 Multiple Models using Pipeline and fast tuning.html#split-data",
    "href": "regression/8 Multiple Models using Pipeline and fast tuning.html#split-data",
    "title": "Multiple models using pipeline and fast tuning",
    "section": "2.5 split data",
    "text": "2.5 split data\n\n\nCode\nY = df_train.SalePrice\nX = df_train.drop(['SalePrice'], axis=1)\n\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\nX_train = X_train.drop('role', axis=1)\nX_test = X_test.drop('role', axis=1)\n\n\n\n\nCode\nprint(X_train.shape)\nprint(X_test.shape)\n\n\n(1168, 75)\n(292, 75)\n\n\n\n\nCode\nprint(Y_train.shape)\nprint(Y_test.shape)\n\n\n(1168,)\n(292,)\n\n\n\n\nCode\ncategorical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].nunique() &lt; 10 and X_train[cname].dtype == \"object\"]\n                    \n                    \nnumerical_cols = numerical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].dtype in ['int64', 'float64']]\n\n\n\n\nCode\nprint(\"The total number of categorical columns:\", len(categorical_cols))\nprint(\"The total number of numerical columns:\", len(numerical_cols))\n\n\nThe total number of categorical columns: 36\nThe total number of numerical columns: 36\n\n\n\n\nCode\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train[my_cols].copy()\nX_test= X_test[my_cols].copy()\n\n\n#X_final = df_test[my_cols].copy()",
    "crumbs": [
      "Regression",
      "Multiple models using pipeline and fast tuning"
    ]
  },
  {
    "objectID": "regression/8 Multiple Models using Pipeline and fast tuning.html#pipelines-for-data-preprocessing",
    "href": "regression/8 Multiple Models using Pipeline and fast tuning.html#pipelines-for-data-preprocessing",
    "title": "Multiple models using pipeline and fast tuning",
    "section": "2.6 Pipelines for Data Preprocessing",
    "text": "2.6 Pipelines for Data Preprocessing\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\nCode\nnumerical_transformer = Pipeline(steps=[\n    ('imputer_num', SimpleImputer(strategy='median')), \n    ('scaler', StandardScaler())\n])\n\nnumerical_transformer_non_scaler = Pipeline(steps=[\n    ('imputer_num', SimpleImputer(strategy='median'))\n   #, ('scaler', StandardScaler())\n])\n\n\n\n\nCode\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n\n\nCode\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)])\n    \n    \npreprocessor_non_scaler = ColumnTransformer(transformers=[\n    ('num', numerical_transformer_non_scaler, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)])",
    "crumbs": [
      "Regression",
      "Multiple models using pipeline and fast tuning"
    ]
  },
  {
    "objectID": "regression/8 Multiple Models using Pipeline and fast tuning.html#define-model",
    "href": "regression/8 Multiple Models using Pipeline and fast tuning.html#define-model",
    "title": "Multiple models using pipeline and fast tuning",
    "section": "3.1 define model",
    "text": "3.1 define model\n\n3.1.1 XGB model\n\n\nCode\nfrom xgboost import XGBRegressor\nxgb_model = XGBRegressor()\nxgb_model\n\n\nXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=None, n_jobs=None,\n             num_parallel_tree=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. XGBRegressoriNot fittedXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=None, n_jobs=None,\n             num_parallel_tree=None, random_state=None, ...) \n\n\n\n\n3.1.2 Random Forest model\n\n\nCode\nfrom sklearn.ensemble import RandomForestRegressor\nrandom_forest_model = RandomForestRegressor()\nrandom_forest_model\n\n\nRandomForestRegressor()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestRegressor?Documentation for RandomForestRegressoriNot fittedRandomForestRegressor() \n\n\n\n\n3.1.3 Linear Regression model\n\n\nCode\nfrom sklearn.linear_model import LinearRegression\nLinearRegression_model = LinearRegression()\nLinearRegression_model\n\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniNot fittedLinearRegression()",
    "crumbs": [
      "Regression",
      "Multiple models using pipeline and fast tuning"
    ]
  },
  {
    "objectID": "regression/8 Multiple Models using Pipeline and fast tuning.html#define-pipline",
    "href": "regression/8 Multiple Models using Pipeline and fast tuning.html#define-pipline",
    "title": "Multiple models using pipeline and fast tuning",
    "section": "3.2 define pipline",
    "text": "3.2 define pipline\n\n\nCode\npipeline_xgb = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', xgb_model)\n         ]\n)\n\npipeline_rf = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', random_forest_model)\n         ]\n)\n\npipeline_lr = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor_non_scaler), \n         ('model', LinearRegression_model)\n         ]\n)",
    "crumbs": [
      "Regression",
      "Multiple models using pipeline and fast tuning"
    ]
  },
  {
    "objectID": "regression/8 Multiple Models using Pipeline and fast tuning.html#define-gridsearch",
    "href": "regression/8 Multiple Models using Pipeline and fast tuning.html#define-gridsearch",
    "title": "Multiple models using pipeline and fast tuning",
    "section": "3.3 define GridSearch",
    "text": "3.3 define GridSearch\n\n\nCode\nparameters_xgb= {\n        'model__learning_rate': [0.01, 0.02,0.08,0.1],\n        'model__max_depth': [3, 5, 7,8,9,10,20],\n        'model__min_child_weight': [1, 3,5,8],\n        'model__subsample': [0.5, 0.7,0.9],\n        \n       # 'model__colsample__bytree': [0.5, 0.7],\n       \n        'model__n_estimators' : [100, 200, 500],\n        'model__objective': ['reg:squarederror']\n    }\n\n\nGrid_xgb = HalvingGridSearchCV(pipeline_xgb\n                ,parameters_xgb \n                #,scoring='neg_root_mean_squared_error'\n                ,max_resources=100\n                , cv=10, n_jobs=-1)\n                \n                \nparameters_rf = {'model__max_depth':[20,30,40],\n                 'model__n_estimators':[200,250,300],\n                 'model__min_samples_leaf':[1,2,3]\n                 }                \n                \n\nGrid_rf = HalvingGridSearchCV(pipeline_rf\n                ,parameters_rf\n                #,scoring='neg_root_mean_squared_error'\n                ,max_resources=100\n                , cv=10, n_jobs=-1)",
    "crumbs": [
      "Regression",
      "Multiple models using pipeline and fast tuning"
    ]
  },
  {
    "objectID": "regression/8 Multiple Models using Pipeline and fast tuning.html#train-model",
    "href": "regression/8 Multiple Models using Pipeline and fast tuning.html#train-model",
    "title": "Multiple models using pipeline and fast tuning",
    "section": "3.4 train model",
    "text": "3.4 train model\n\n\nCode\nstart_time = time.time()\n\n\n#Grids = [Grid_xgb,Grid_rf,pipeline_xgb,pipeline_rf,pipeline_lr]\n\n\nGrids = [pipeline_xgb,pipeline_rf,pipeline_lr\n        ,Grid_xgb,Grid_rf]\n\n\nfor Grid in Grids:\n    Grid.fit(X_train,Y_train)\n\n\nend_time = time.time()\nduration = end_time - start_time\nprint(\"traning time: \",duration)\n\n\ntraning time:  109.65193772315979",
    "crumbs": [
      "Regression",
      "Multiple models using pipeline and fast tuning"
    ]
  },
  {
    "objectID": "regression/8 Multiple Models using Pipeline and fast tuning.html#preformance",
    "href": "regression/8 Multiple Models using Pipeline and fast tuning.html#preformance",
    "title": "Multiple models using pipeline and fast tuning",
    "section": "3.5 Preformance",
    "text": "3.5 Preformance\n\n\nCode\n#grid_dict = {0: 'XGB', 1: 'random forest', 2: 'XGB non tune',3: 'ramdon forest non tune',4:'linear regression non tune' }\n\ngrid_dict = {0: 'XGB', 1: 'random forest',2: 'linear regression'\n            ,3: 'XGB tuning'\n            ,4: 'random forest tuning'\n            }\n\n\nfor i, model in enumerate(Grids):\n    print('{} Test Accuracy: {}'.format(grid_dict[i],\n    model.score(X_test,Y_test)))\n    #print('{} Best Params: {}'.format(grid_dict[i], model.best_params_))\n\n\nXGB Test Accuracy: 0.9057078022089059\nrandom forest Test Accuracy: 0.9031434181419291\nlinear regression Test Accuracy: 0.9098597408133744\nXGB tuning Test Accuracy: 0.9095699901822996\nrandom forest tuning Test Accuracy: 0.90321867930246\n\n\n\n\nCode\nbest_ml=pipeline_xgb\n\n\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = best_ml.predict(X_test) #always gets x and retuns y\n\n\nR 2\n\n\nCode\nfrom sklearn.metrics import r2_score\nr2_score(Y_test, Y_pred_dt)\n\n\n0.9057078022089059\n\n\nMAE\n\n\nCode\nfrom sklearn.metrics import mean_absolute_error\nmean_absolute_error(Y_test, Y_pred_dt)\n\n\n17311.796540560787\n\n\nRMSE\n\n\nCode\nfrom  math import sqrt\nfrom sklearn.metrics import mean_squared_error\nmse=mean_squared_error(Y_test, Y_pred_dt)\nrmse=sqrt(mse)\nrmse\n\n\n27225.550798118966",
    "crumbs": [
      "Regression",
      "Multiple models using pipeline and fast tuning"
    ]
  },
  {
    "objectID": "regression/8 Multiple Models using Pipeline and fast tuning.html#k-fold-cross-validation",
    "href": "regression/8 Multiple Models using Pipeline and fast tuning.html#k-fold-cross-validation",
    "title": "Multiple models using pipeline and fast tuning",
    "section": "3.6 k-Fold Cross-Validation",
    "text": "3.6 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \n\n\n\n\nCode\n#cv_dt = cross_val_score(optimised_model_pipeline, X_train, Y_train, cv=kf_dt,scoring = 'neg_mean_squared_error')\n#np.mean(np.sqrt(np.abs(cv_dt)))",
    "crumbs": [
      "Regression",
      "Multiple models using pipeline and fast tuning"
    ]
  },
  {
    "objectID": "clustering/0 Mall Customers.html",
    "href": "clustering/0 Mall Customers.html",
    "title": "Mall Customers Dataset",
    "section": "",
    "text": "Code\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns",
    "crumbs": [
      "Clustering",
      "Mall Customers Dataset"
    ]
  },
  {
    "objectID": "clustering/0 Mall Customers.html#download-data",
    "href": "clustering/0 Mall Customers.html#download-data",
    "title": "Mall Customers Dataset",
    "section": "2.1 download data",
    "text": "2.1 download data\nhttps://www.kaggle.com/datasets/shwetabh123/mall-customer",
    "crumbs": [
      "Clustering",
      "Mall Customers Dataset"
    ]
  },
  {
    "objectID": "clustering/0 Mall Customers.html#input-data",
    "href": "clustering/0 Mall Customers.html#input-data",
    "title": "Mall Customers Dataset",
    "section": "2.2 input data",
    "text": "2.2 input data\n\n\nCode\n# Loading the data\ndf_train = pd.read_csv('./data/Mall_Customers.csv')\n\n# Showing overview of the train dataset\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nCustomerID\nGenre\nAge\nAnnual Income (k$)\nSpending Score (1-100)\n\n\n\n\n0\n1\nMale\n19\n15\n39\n\n\n1\n2\nMale\n21\n15\n81\n\n\n2\n3\nFemale\n20\n16\n6\n\n\n3\n4\nFemale\n23\n16\n77\n\n\n4\n5\nFemale\n31\n17\n40",
    "crumbs": [
      "Clustering",
      "Mall Customers Dataset"
    ]
  },
  {
    "objectID": "clustering/0 Mall Customers.html#data-eda",
    "href": "clustering/0 Mall Customers.html#data-eda",
    "title": "Mall Customers Dataset",
    "section": "2.3 data EDA",
    "text": "2.3 data EDA\n\n\nCode\ndf_train.describe()\n\n\n\n\n\n\n\n\n\n\nCustomerID\nAge\nAnnual Income (k$)\nSpending Score (1-100)\n\n\n\n\ncount\n200.000000\n200.000000\n200.000000\n200.000000\n\n\nmean\n100.500000\n38.850000\n60.560000\n50.200000\n\n\nstd\n57.879185\n13.969007\n26.264721\n25.823522\n\n\nmin\n1.000000\n18.000000\n15.000000\n1.000000\n\n\n25%\n50.750000\n28.750000\n41.500000\n34.750000\n\n\n50%\n100.500000\n36.000000\n61.500000\n50.000000\n\n\n75%\n150.250000\n49.000000\n78.000000\n73.000000\n\n\nmax\n200.000000\n70.000000\n137.000000\n99.000000\n\n\n\n\n\n\n\n\n\n\nCode\ndf_train.describe(include=[object])\n\n\n\n\n\n\n\n\n\n\nGenre\n\n\n\n\ncount\n200\n\n\nunique\n2\n\n\ntop\nFemale\n\n\nfreq\n112\n\n\n\n\n\n\n\n\nMissing Data\n\n\nCode\ndf_train.isnull().sum()\n\n\nCustomerID                0\nGenre                     0\nAge                       0\nAnnual Income (k$)        0\nSpending Score (1-100)    0\ndtype: int64",
    "crumbs": [
      "Clustering",
      "Mall Customers Dataset"
    ]
  },
  {
    "objectID": "clustering/0 Mall Customers.html#feature",
    "href": "clustering/0 Mall Customers.html#feature",
    "title": "Mall Customers Dataset",
    "section": "2.4 feature",
    "text": "2.4 feature\n\n\nCode\nimport sweetviz as sv\nmy_report = sv.analyze(df_train)\n\n\n\n\n\n\n\nCode\nmy_report.show_notebook()\n\n\n \n\n\nhttps://www.kaggle.com/code/sangwookchn/clustering-techniques-using-scikit-learn",
    "crumbs": [
      "Clustering",
      "Mall Customers Dataset"
    ]
  },
  {
    "objectID": "regression/0 house price data.html#feature",
    "href": "regression/0 house price data.html#feature",
    "title": "Housing Prices Dataset",
    "section": "2.4 feature",
    "text": "2.4 feature\n\n\nCode\nimport sweetviz as sv\nmy_report = sv.analyze(df_train)\n\n\n\n\n\n\n\nCode\nmy_report.show_notebook()",
    "crumbs": [
      "Regression",
      "Housing Prices Dataset"
    ]
  },
  {
    "objectID": "classification 2/0 hotel booking data.html",
    "href": "classification 2/0 hotel booking data.html",
    "title": "Hotel booking Dataset",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split"
  },
  {
    "objectID": "classification 2/0 hotel booking data.html#download-data",
    "href": "classification 2/0 hotel booking data.html#download-data",
    "title": "Hotel booking Dataset",
    "section": "2.1 download data",
    "text": "2.1 download data\nhttps://www.kaggle.com/c/titanic/data\n\n\nCode\nimport pandas as pd\nurl='https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv'\nhotels=pd.read_csv(url)\n\n\n\n\nCode\nhotels.head()\n\n\n\n\n\n\n\n\n\n\nhotel\nis_canceled\nlead_time\narrival_date_year\narrival_date_month\narrival_date_week_number\narrival_date_day_of_month\nstays_in_weekend_nights\nstays_in_week_nights\nadults\n...\ndeposit_type\nagent\ncompany\ndays_in_waiting_list\ncustomer_type\nadr\nrequired_car_parking_spaces\ntotal_of_special_requests\nreservation_status\nreservation_status_date\n\n\n\n\n0\nResort Hotel\n0\n342\n2015\nJuly\n27\n1\n0\n0\n2\n...\nNo Deposit\nNaN\nNaN\n0\nTransient\n0.0\n0\n0\nCheck-Out\n2015-07-01\n\n\n1\nResort Hotel\n0\n737\n2015\nJuly\n27\n1\n0\n0\n2\n...\nNo Deposit\nNaN\nNaN\n0\nTransient\n0.0\n0\n0\nCheck-Out\n2015-07-01\n\n\n2\nResort Hotel\n0\n7\n2015\nJuly\n27\n1\n0\n1\n1\n...\nNo Deposit\nNaN\nNaN\n0\nTransient\n75.0\n0\n0\nCheck-Out\n2015-07-02\n\n\n3\nResort Hotel\n0\n13\n2015\nJuly\n27\n1\n0\n1\n1\n...\nNo Deposit\n304.0\nNaN\n0\nTransient\n75.0\n0\n0\nCheck-Out\n2015-07-02\n\n\n4\nResort Hotel\n0\n14\n2015\nJuly\n27\n1\n0\n2\n2\n...\nNo Deposit\n240.0\nNaN\n0\nTransient\n98.0\n0\n1\nCheck-Out\n2015-07-03\n\n\n\n\n5 rows × 32 columns"
  },
  {
    "objectID": "classification 2/0 hotel booking data.html#data-eda",
    "href": "classification 2/0 hotel booking data.html#data-eda",
    "title": "Hotel booking Dataset",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nhotels.describe()\n\n\n\n\n\n\n\n\n\n\nis_canceled\nlead_time\narrival_date_year\narrival_date_week_number\narrival_date_day_of_month\nstays_in_weekend_nights\nstays_in_week_nights\nadults\nchildren\nbabies\nis_repeated_guest\nprevious_cancellations\nprevious_bookings_not_canceled\nbooking_changes\nagent\ncompany\ndays_in_waiting_list\nadr\nrequired_car_parking_spaces\ntotal_of_special_requests\n\n\n\n\ncount\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n119386.000000\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n103050.000000\n6797.000000\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n\n\nmean\n0.370416\n104.011416\n2016.156554\n27.165173\n15.798241\n0.927599\n2.500302\n1.856403\n0.103890\n0.007949\n0.031912\n0.087118\n0.137097\n0.221124\n86.693382\n189.266735\n2.321149\n101.831122\n0.062518\n0.571363\n\n\nstd\n0.482918\n106.863097\n0.707476\n13.605138\n8.780829\n0.998613\n1.908286\n0.579261\n0.398561\n0.097436\n0.175767\n0.844336\n1.497437\n0.652306\n110.774548\n131.655015\n17.594721\n50.535790\n0.245291\n0.792798\n\n\nmin\n0.000000\n0.000000\n2015.000000\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n6.000000\n0.000000\n-6.380000\n0.000000\n0.000000\n\n\n25%\n0.000000\n18.000000\n2016.000000\n16.000000\n8.000000\n0.000000\n1.000000\n2.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n9.000000\n62.000000\n0.000000\n69.290000\n0.000000\n0.000000\n\n\n50%\n0.000000\n69.000000\n2016.000000\n28.000000\n16.000000\n1.000000\n2.000000\n2.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n14.000000\n179.000000\n0.000000\n94.575000\n0.000000\n0.000000\n\n\n75%\n1.000000\n160.000000\n2017.000000\n38.000000\n23.000000\n2.000000\n3.000000\n2.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n229.000000\n270.000000\n0.000000\n126.000000\n0.000000\n1.000000\n\n\nmax\n1.000000\n737.000000\n2017.000000\n53.000000\n31.000000\n19.000000\n50.000000\n55.000000\n10.000000\n10.000000\n1.000000\n26.000000\n72.000000\n21.000000\n535.000000\n543.000000\n391.000000\n5400.000000\n8.000000\n5.000000\n\n\n\n\n\n\n\n\n\n\nCode\nhotels.describe(include=[object])\n\n\n\n\n\n\n\n\n\n\nhotel\narrival_date_month\nmeal\ncountry\nmarket_segment\ndistribution_channel\nreserved_room_type\nassigned_room_type\ndeposit_type\ncustomer_type\nreservation_status\nreservation_status_date\n\n\n\n\ncount\n119390\n119390\n119390\n118902\n119390\n119390\n119390\n119390\n119390\n119390\n119390\n119390\n\n\nunique\n2\n12\n5\n177\n8\n5\n10\n12\n3\n4\n3\n926\n\n\ntop\nCity Hotel\nAugust\nBB\nPRT\nOnline TA\nTA/TO\nA\nA\nNo Deposit\nTransient\nCheck-Out\n2015-10-21\n\n\nfreq\n79330\n13877\n92310\n48590\n56477\n97870\n85994\n74053\n104641\n89613\n75166\n1461\n\n\n\n\n\n\n\n\nMissing Data\n\n\nCode\nhotels.isnull().sum()\n\n\nhotel                                  0\nis_canceled                            0\nlead_time                              0\narrival_date_year                      0\narrival_date_month                     0\narrival_date_week_number               0\narrival_date_day_of_month              0\nstays_in_weekend_nights                0\nstays_in_week_nights                   0\nadults                                 0\nchildren                               4\nbabies                                 0\nmeal                                   0\ncountry                              488\nmarket_segment                         0\ndistribution_channel                   0\nis_repeated_guest                      0\nprevious_cancellations                 0\nprevious_bookings_not_canceled         0\nreserved_room_type                     0\nassigned_room_type                     0\nbooking_changes                        0\ndeposit_type                           0\nagent                              16340\ncompany                           112593\ndays_in_waiting_list                   0\ncustomer_type                          0\nadr                                    0\nrequired_car_parking_spaces            0\ntotal_of_special_requests              0\nreservation_status                     0\nreservation_status_date                0\ndtype: int64\n\n\n\n\nCode\n#import math\n#hotels=hotels&gt;&gt; filter(math.isnan(_.children)==False)\nfrom siuba.siu import call\nfrom siuba import _, mutate, filter, group_by, summarize,show_query\nfrom siuba import *\n\nhotels &gt;&gt; group_by(_.children)  &gt;&gt; summarize(n = _.shape[0])\n\n\n\n\n\n\n\n\n\n\nchildren\nn\n\n\n\n\n0\n0.0\n110796\n\n\n1\n1.0\n4861\n\n\n2\n2.0\n3652\n\n\n3\n3.0\n76\n\n\n4\n10.0\n1\n\n\n5\nNaN\n4\n\n\n\n\n\n\n\n\n\n\nCode\n#import math\nhotels=hotels&gt;&gt;mutate(children=if_else(_.children &gt; 0, True, False))\n\n# Create a boolean mask and apply it\nmask = pd.notna(hotels['children'])\nhotels = hotels[mask]\n\n\n\n\nCode\nimport sweetviz as sv\nmy_report = sv.analyze(hotels)\n\n\n\n\n\n\n\nCode\nmy_report.show_notebook()"
  },
  {
    "objectID": "classification 2/0 hotel booking data.html#feature-vs-target",
    "href": "classification 2/0 hotel booking data.html#feature-vs-target",
    "title": "Hotel booking Dataset",
    "section": "2.3 feature vs target",
    "text": "2.3 feature vs target\n\n\nCode\nmy_report2 = sv.analyze(hotels,target_feat='children')\n\n\n\n\n\n\n\nCode\nmy_report2.show_notebook()"
  },
  {
    "objectID": "classification 2/0 hotel booking data.html#compare-train-data-and-test-data",
    "href": "classification 2/0 hotel booking data.html#compare-train-data-and-test-data",
    "title": "Hotel booking Dataset",
    "section": "2.4 compare train data and test data",
    "text": "2.4 compare train data and test data\n\n\nCode\n#compare = sv.compare(source=df_train, compare=df_test)\n\n\n\n\nCode\n#compare.show_notebook()"
  },
  {
    "objectID": "classification 2/0 hotel booking data.html#data-dictionary",
    "href": "classification 2/0 hotel booking data.html#data-dictionary",
    "title": "Hotel booking Dataset",
    "section": "2.5 data dictionary",
    "text": "2.5 data dictionary"
  },
  {
    "objectID": "intro/1 input output.html#csv-1",
    "href": "intro/1 input output.html#csv-1",
    "title": "input output",
    "section": "2.1 CSV",
    "text": "2.1 CSV",
    "crumbs": [
      "Intro",
      "input output"
    ]
  },
  {
    "objectID": "intro/1 input output.html#excel-1",
    "href": "intro/1 input output.html#excel-1",
    "title": "input output",
    "section": "2.2 excel",
    "text": "2.2 excel",
    "crumbs": [
      "Intro",
      "input output"
    ]
  },
  {
    "objectID": "classification2/0 hotel booking data.html",
    "href": "classification2/0 hotel booking data.html",
    "title": "Hotel booking Dataset",
    "section": "",
    "text": "Code\nimport os\n#os.system('pip install xgboost')\n\n\n\n\nCode\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split",
    "crumbs": [
      "Classification2",
      "Hotel booking Dataset"
    ]
  },
  {
    "objectID": "classification2/0 hotel booking data.html#download-data",
    "href": "classification2/0 hotel booking data.html#download-data",
    "title": "Hotel booking Dataset",
    "section": "2.1 download data",
    "text": "2.1 download data\nhttps://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-02-11/readme.md\n\n\nCode\nimport pandas as pd\nurl='https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv'\nhotels=pd.read_csv(url)\n\nhotels.to_csv('data/hotels.csv', index=False)  \n\n\n\n\nCode\nhotels.head()\n\n\n\n\n\n\n\n\n\n\nhotel\nis_canceled\nlead_time\narrival_date_year\narrival_date_month\narrival_date_week_number\narrival_date_day_of_month\nstays_in_weekend_nights\nstays_in_week_nights\nadults\n...\ndeposit_type\nagent\ncompany\ndays_in_waiting_list\ncustomer_type\nadr\nrequired_car_parking_spaces\ntotal_of_special_requests\nreservation_status\nreservation_status_date\n\n\n\n\n0\nResort Hotel\n0\n342\n2015\nJuly\n27\n1\n0\n0\n2\n...\nNo Deposit\nNaN\nNaN\n0\nTransient\n0.0\n0\n0\nCheck-Out\n2015-07-01\n\n\n1\nResort Hotel\n0\n737\n2015\nJuly\n27\n1\n0\n0\n2\n...\nNo Deposit\nNaN\nNaN\n0\nTransient\n0.0\n0\n0\nCheck-Out\n2015-07-01\n\n\n2\nResort Hotel\n0\n7\n2015\nJuly\n27\n1\n0\n1\n1\n...\nNo Deposit\nNaN\nNaN\n0\nTransient\n75.0\n0\n0\nCheck-Out\n2015-07-02\n\n\n3\nResort Hotel\n0\n13\n2015\nJuly\n27\n1\n0\n1\n1\n...\nNo Deposit\n304.0\nNaN\n0\nTransient\n75.0\n0\n0\nCheck-Out\n2015-07-02\n\n\n4\nResort Hotel\n0\n14\n2015\nJuly\n27\n1\n0\n2\n2\n...\nNo Deposit\n240.0\nNaN\n0\nTransient\n98.0\n0\n1\nCheck-Out\n2015-07-03\n\n\n\n\n5 rows × 32 columns",
    "crumbs": [
      "Classification2",
      "Hotel booking Dataset"
    ]
  },
  {
    "objectID": "classification2/0 hotel booking data.html#data-eda",
    "href": "classification2/0 hotel booking data.html#data-eda",
    "title": "Hotel booking Dataset",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\nhotels.describe()\n\n\n\n\n\n\n\n\n\n\nis_canceled\nlead_time\narrival_date_year\narrival_date_week_number\narrival_date_day_of_month\nstays_in_weekend_nights\nstays_in_week_nights\nadults\nchildren\nbabies\nis_repeated_guest\nprevious_cancellations\nprevious_bookings_not_canceled\nbooking_changes\nagent\ncompany\ndays_in_waiting_list\nadr\nrequired_car_parking_spaces\ntotal_of_special_requests\n\n\n\n\ncount\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n119386.000000\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n103050.000000\n6797.000000\n119390.000000\n119390.000000\n119390.000000\n119390.000000\n\n\nmean\n0.370416\n104.011416\n2016.156554\n27.165173\n15.798241\n0.927599\n2.500302\n1.856403\n0.103890\n0.007949\n0.031912\n0.087118\n0.137097\n0.221124\n86.693382\n189.266735\n2.321149\n101.831122\n0.062518\n0.571363\n\n\nstd\n0.482918\n106.863097\n0.707476\n13.605138\n8.780829\n0.998613\n1.908286\n0.579261\n0.398561\n0.097436\n0.175767\n0.844336\n1.497437\n0.652306\n110.774548\n131.655015\n17.594721\n50.535790\n0.245291\n0.792798\n\n\nmin\n0.000000\n0.000000\n2015.000000\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n6.000000\n0.000000\n-6.380000\n0.000000\n0.000000\n\n\n25%\n0.000000\n18.000000\n2016.000000\n16.000000\n8.000000\n0.000000\n1.000000\n2.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n9.000000\n62.000000\n0.000000\n69.290000\n0.000000\n0.000000\n\n\n50%\n0.000000\n69.000000\n2016.000000\n28.000000\n16.000000\n1.000000\n2.000000\n2.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n14.000000\n179.000000\n0.000000\n94.575000\n0.000000\n0.000000\n\n\n75%\n1.000000\n160.000000\n2017.000000\n38.000000\n23.000000\n2.000000\n3.000000\n2.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n229.000000\n270.000000\n0.000000\n126.000000\n0.000000\n1.000000\n\n\nmax\n1.000000\n737.000000\n2017.000000\n53.000000\n31.000000\n19.000000\n50.000000\n55.000000\n10.000000\n10.000000\n1.000000\n26.000000\n72.000000\n21.000000\n535.000000\n543.000000\n391.000000\n5400.000000\n8.000000\n5.000000\n\n\n\n\n\n\n\n\n\n\nCode\nhotels.describe(include=[object])\n\n\n\n\n\n\n\n\n\n\nhotel\narrival_date_month\nmeal\ncountry\nmarket_segment\ndistribution_channel\nreserved_room_type\nassigned_room_type\ndeposit_type\ncustomer_type\nreservation_status\nreservation_status_date\n\n\n\n\ncount\n119390\n119390\n119390\n118902\n119390\n119390\n119390\n119390\n119390\n119390\n119390\n119390\n\n\nunique\n2\n12\n5\n177\n8\n5\n10\n12\n3\n4\n3\n926\n\n\ntop\nCity Hotel\nAugust\nBB\nPRT\nOnline TA\nTA/TO\nA\nA\nNo Deposit\nTransient\nCheck-Out\n2015-10-21\n\n\nfreq\n79330\n13877\n92310\n48590\n56477\n97870\n85994\n74053\n104641\n89613\n75166\n1461\n\n\n\n\n\n\n\n\nMissing Data\n\n\nCode\nhotels.isnull().sum()\n\n\nhotel                                  0\nis_canceled                            0\nlead_time                              0\narrival_date_year                      0\narrival_date_month                     0\narrival_date_week_number               0\narrival_date_day_of_month              0\nstays_in_weekend_nights                0\nstays_in_week_nights                   0\nadults                                 0\nchildren                               4\nbabies                                 0\nmeal                                   0\ncountry                              488\nmarket_segment                         0\ndistribution_channel                   0\nis_repeated_guest                      0\nprevious_cancellations                 0\nprevious_bookings_not_canceled         0\nreserved_room_type                     0\nassigned_room_type                     0\nbooking_changes                        0\ndeposit_type                           0\nagent                              16340\ncompany                           112593\ndays_in_waiting_list                   0\ncustomer_type                          0\nadr                                    0\nrequired_car_parking_spaces            0\ntotal_of_special_requests              0\nreservation_status                     0\nreservation_status_date                0\ndtype: int64\n\n\n\n\nCode\n#import math\n#hotels=hotels&gt;&gt; filter(math.isnan(_.children)==False)\nfrom siuba.siu import call\nfrom siuba import _, mutate, filter, group_by, summarize,show_query\nfrom siuba import *\n\nhotels &gt;&gt; group_by(_.children)  &gt;&gt; summarize(n = _.shape[0])\n\n\n\n\n\n\n\n\n\n\nchildren\nn\n\n\n\n\n0\n0.0\n110796\n\n\n1\n1.0\n4861\n\n\n2\n2.0\n3652\n\n\n3\n3.0\n76\n\n\n4\n10.0\n1\n\n\n5\nNaN\n4\n\n\n\n\n\n\n\n\n\n\nCode\n#import math\nhotels=hotels&gt;&gt;mutate(children=if_else(_.children &gt; 0, True, False))\n\n# Create a boolean mask and apply it\nmask = pd.notna(hotels['children'])\nhotels = hotels[mask]\n\n\n\n\nCode\nimport sweetviz as sv\nmy_report = sv.analyze(hotels)\n\n\n\n\n\n\n\nCode\nmy_report.show_notebook()",
    "crumbs": [
      "Classification2",
      "Hotel booking Dataset"
    ]
  },
  {
    "objectID": "classification2/0 hotel booking data.html#feature-vs-target",
    "href": "classification2/0 hotel booking data.html#feature-vs-target",
    "title": "Hotel booking Dataset",
    "section": "2.3 feature vs target",
    "text": "2.3 feature vs target\n\n\nCode\nmy_report2 = sv.analyze(hotels,target_feat='children')\n\n\n\n\n\n\n\nCode\nmy_report2.show_notebook()",
    "crumbs": [
      "Classification2",
      "Hotel booking Dataset"
    ]
  },
  {
    "objectID": "classification2/0 hotel booking data.html#compare-train-data-and-test-data",
    "href": "classification2/0 hotel booking data.html#compare-train-data-and-test-data",
    "title": "Hotel booking Dataset",
    "section": "2.4 compare train data and test data",
    "text": "2.4 compare train data and test data\n\n\nCode\n#compare = sv.compare(source=df_train, compare=df_test)\n\n\n\n\nCode\n#compare.show_notebook()",
    "crumbs": [
      "Classification2",
      "Hotel booking Dataset"
    ]
  },
  {
    "objectID": "classification2/0 hotel booking data.html#data-dictionary",
    "href": "classification2/0 hotel booking data.html#data-dictionary",
    "title": "Hotel booking Dataset",
    "section": "2.5 data dictionary",
    "text": "2.5 data dictionary\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nhotel\ncharacter\nHotel (H1 = Resort Hotel or H2 = City Hotel)\n\n\nis_canceled\ndouble\nValue indicating if the booking was canceled (1) or not (0)\n\n\nlead_time\ndouble\nNumber of days that elapsed between the entering date of the booking into the PMS and the arrival date\n\n\narrival_date_year\ndouble\nYear of arrival date\n\n\narrival_date_month\ncharacter\nMonth of arrival date\n\n\narrival_date_week_number\ndouble\nWeek number of year for arrival date\n\n\narrival_date_day_of_month\ndouble\nDay of arrival date\n\n\nstays_in_weekend_nights\ndouble\nNumber of weekend nights (Saturday or Sunday) the guest stayed or booked to stay at the hotel\n\n\nstays_in_week_nights\ndouble\nNumber of week nights (Monday to Friday) the guest stayed or booked to stay at the hotel\n\n\nadults\ndouble\nNumber of adults\n\n\nchildren\ndouble\nNumber of children\n\n\nbabies\ndouble\nNumber of babies\n\n\nmeal\ncharacter\nType of meal booked. Categories are presented in standard hospitality meal packages:\nUndefined/SC – no meal package;\nBB – Bed & Breakfast;\nHB – Half board (breakfast and one other meal – usually dinner);\nFB – Full board (breakfast, lunch and dinner)\n\n\ncountry\ncharacter\nCountry of origin. Categories are represented in the ISO 3155–3:2013 format\n\n\nmarket_segment\ncharacter\nMarket segment designation. In categories, the term “TA” means “Travel Agents” and “TO” means “Tour Operators”\n\n\ndistribution_channel\ncharacter\nBooking distribution channel. The term “TA” means “Travel Agents” and “TO” means “Tour Operators”\n\n\nis_repeated_guest\ndouble\nValue indicating if the booking name was from a repeated guest (1) or not (0)\n\n\nprevious_cancellations\ndouble\nNumber of previous bookings that were cancelled by the customer prior to the current booking\n\n\nprevious_bookings_not_canceled\ndouble\nNumber of previous bookings not cancelled by the customer prior to the current booking\n\n\nreserved_room_type\ncharacter\nCode of room type reserved. Code is presented instead of designation for anonymity reasons\n\n\nassigned_room_type\ncharacter\nCode for the type of room assigned to the booking. Sometimes the assigned room type differs from the reserved room type due to hotel operation reasons (e.g. overbooking) or by customer request. Code is presented instead of designation for anonymity reasons\n\n\nbooking_changes\ndouble\nNumber of changes/amendments made to the booking from the moment the booking was entered on the PMS until the moment of check-in or cancellation\n\n\ndeposit_type\ncharacter\nIndication on if the customer made a deposit to guarantee the booking. This variable can assume three categories:\nNo Deposit – no deposit was made;\nNon Refund – a deposit was made in the value of the total stay cost;\nRefundable – a deposit was made with a value under the total cost of stay.\n\n\nagent\ncharacter\nID of the travel agency that made the booking\n\n\ncompany\ncharacter\nID of the company/entity that made the booking or responsible for paying the booking. ID is presented instead of designation for anonymity reasons\n\n\ndays_in_waiting_list\ndouble\nNumber of days the booking was in the waiting list before it was confirmed to the customer\n\n\ncustomer_type\ncharacter\nType of booking, assuming one of four categories:\nContract - when the booking has an allotment or other type of contract associated to it;\nGroup – when the booking is associated to a group;\nTransient – when the booking is not part of a group or contract, and is not associated to other transient booking;\nTransient-party – when the booking is transient, but is associated to at least other transient booking\n\n\nadr\ndouble\nAverage Daily Rate as defined by dividing the sum of all lodging transactions by the total number of staying nights\n\n\nrequired_car_parking_spaces\ndouble\nNumber of car parking spaces required by the customer\n\n\ntotal_of_special_requests\ndouble\nNumber of special requests made by the customer (e.g. twin bed or high floor)\n\n\nreservation_status\ncharacter\nReservation last status, assuming one of three categories:\nCanceled – booking was canceled by the customer;\nCheck-Out – customer has checked in but already departed;\nNo-Show – customer did not check-in and did inform the hotel of the reason why\n\n\nreservation_status_date\ndouble\nDate at which the last status was set. This variable can be used in conjunction with the ReservationStatus to understand when was the booking canceled or when did the customer checked-out of the hotel",
    "crumbs": [
      "Classification2",
      "Hotel booking Dataset"
    ]
  },
  {
    "objectID": "classification2/1 decision tree on hotel booking data.html",
    "href": "classification2/1 decision tree on hotel booking data.html",
    "title": "Decision tree",
    "section": "",
    "text": "8590 booking with children.110800 bookings with no children.\nOnly 7% booking with children.Its imbalanced data.Why it causes problems?\nThe model cannot learn to predict the minority class well because of class imbalance.\nModel is only able to learn a simple heuristic (e.g. always predict the dominate class) and it gets stuck in a sub optimal solution.\nAn accuracy of over 90% can be misleading because the model may not have predictive power on the rare class.\nSince we have enough rare class data(at least 1K).Let handle imbalanced data with down sample before training.\nIf there is no enough rare class data, we will do over smaple method.",
    "crumbs": [
      "Classification2",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification2/1 decision tree on hotel booking data.html#download-data",
    "href": "classification2/1 decision tree on hotel booking data.html#download-data",
    "title": "Decision tree",
    "section": "2.1 download data",
    "text": "2.1 download data\n\n\nCode\nimport pandas as pd\n#url='https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv'\nhotels=pd.read_csv('data/hotels.csv')\n\n\n\n\nCode\nhotels.head()\n\n\n\n\n\n\n\n\n\n\nhotel\nis_canceled\nlead_time\narrival_date_year\narrival_date_month\narrival_date_week_number\narrival_date_day_of_month\nstays_in_weekend_nights\nstays_in_week_nights\nadults\n...\ndeposit_type\nagent\ncompany\ndays_in_waiting_list\ncustomer_type\nadr\nrequired_car_parking_spaces\ntotal_of_special_requests\nreservation_status\nreservation_status_date\n\n\n\n\n0\nResort Hotel\n0\n342\n2015\nJuly\n27\n1\n0\n0\n2\n...\nNo Deposit\nNaN\nNaN\n0\nTransient\n0.0\n0\n0\nCheck-Out\n2015-07-01\n\n\n1\nResort Hotel\n0\n737\n2015\nJuly\n27\n1\n0\n0\n2\n...\nNo Deposit\nNaN\nNaN\n0\nTransient\n0.0\n0\n0\nCheck-Out\n2015-07-01\n\n\n2\nResort Hotel\n0\n7\n2015\nJuly\n27\n1\n0\n1\n1\n...\nNo Deposit\nNaN\nNaN\n0\nTransient\n75.0\n0\n0\nCheck-Out\n2015-07-02\n\n\n3\nResort Hotel\n0\n13\n2015\nJuly\n27\n1\n0\n1\n1\n...\nNo Deposit\n304.0\nNaN\n0\nTransient\n75.0\n0\n0\nCheck-Out\n2015-07-02\n\n\n4\nResort Hotel\n0\n14\n2015\nJuly\n27\n1\n0\n2\n2\n...\nNo Deposit\n240.0\nNaN\n0\nTransient\n98.0\n0\n1\nCheck-Out\n2015-07-03\n\n\n\n\n5 rows × 32 columns",
    "crumbs": [
      "Classification2",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification2/1 decision tree on hotel booking data.html#data-eda",
    "href": "classification2/1 decision tree on hotel booking data.html#data-eda",
    "title": "Decision tree",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\nMissing Data\n\n\nCode\nhotels.isnull().sum()\n\n\nhotel                                  0\nis_canceled                            0\nlead_time                              0\narrival_date_year                      0\narrival_date_month                     0\narrival_date_week_number               0\narrival_date_day_of_month              0\nstays_in_weekend_nights                0\nstays_in_week_nights                   0\nadults                                 0\nchildren                               4\nbabies                                 0\nmeal                                   0\ncountry                              488\nmarket_segment                         0\ndistribution_channel                   0\nis_repeated_guest                      0\nprevious_cancellations                 0\nprevious_bookings_not_canceled         0\nreserved_room_type                     0\nassigned_room_type                     0\nbooking_changes                        0\ndeposit_type                           0\nagent                              16340\ncompany                           112593\ndays_in_waiting_list                   0\ncustomer_type                          0\nadr                                    0\nrequired_car_parking_spaces            0\ntotal_of_special_requests              0\nreservation_status                     0\nreservation_status_date                0\ndtype: int64\n\n\n\n\nCode\n#import math\n#hotels=hotels&gt;&gt; filter(math.isnan(_.children)==False)\nfrom siuba.siu import call\nfrom siuba import _, mutate, filter, group_by, summarize,show_query\nfrom siuba import *\n\nhotels &gt;&gt; group_by(_.children)  &gt;&gt; summarize(n = _.shape[0])\n\n\n\n\n\n\n\n\n\n\nchildren\nn\n\n\n\n\n0\n0.0\n110796\n\n\n1\n1.0\n4861\n\n\n2\n2.0\n3652\n\n\n3\n3.0\n76\n\n\n4\n10.0\n1\n\n\n5\nNaN\n4\n\n\n\n\n\n\n\n\n\n\nCode\n#import math\nhotels=hotels&gt;&gt;mutate(children=if_else(_.children &gt; 0, True, False))\n\n# Create a boolean mask and apply it\nmask = pd.notna(hotels['children'])\nhotels = hotels[mask]",
    "crumbs": [
      "Classification2",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification2/1 decision tree on hotel booking data.html#data-wrangling",
    "href": "classification2/1 decision tree on hotel booking data.html#data-wrangling",
    "title": "Decision tree",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nchildren_train = hotels .children\n\n# Concatenate training and test sets\ndata  = hotels",
    "crumbs": [
      "Classification2",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification2/1 decision tree on hotel booking data.html#categorical_cols-and-numerical_cols",
    "href": "classification2/1 decision tree on hotel booking data.html#categorical_cols-and-numerical_cols",
    "title": "Decision tree",
    "section": "2.4 categorical_cols and numerical_cols",
    "text": "2.4 categorical_cols and numerical_cols\n\n\nCode\ncategorical_cols = [cname for cname in data \n                    if data[cname].nunique() &lt; 10 and data[cname].dtype == \"object\"]\n                    \n                    \nnumerical_cols = numerical_cols = [cname for cname in data.columns \n                    if data[cname].dtype in ['int64', 'float64']]\n\n\n\n\nCode\nprint(\"The total number of categorical columns:\", len(categorical_cols))\nprint(\"The total number of numerical columns:\", len(numerical_cols))\n\n\nThe total number of categorical columns: 7\nThe total number of numerical columns: 19\n\n\n\n\nCode\ndata &gt;&gt; group_by(_.children)  &gt;&gt; summarize(n = _.shape[0])\n\n\n\n\n\n\n\n\n\n\nchildren\nn\n\n\n\n\n0\nFalse\n110800\n\n\n1\nTrue\n8590",
    "crumbs": [
      "Classification2",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification2/1 decision tree on hotel booking data.html#split-data",
    "href": "classification2/1 decision tree on hotel booking data.html#split-data",
    "title": "Decision tree",
    "section": "2.5 split data",
    "text": "2.5 split data\n\n\nCode\nY=data['children']\nX=data.drop('children', axis=1)\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.8)\n\n\n\n\nCode\nfrom imblearn.under_sampling import RandomUnderSampler\n\nros=RandomUnderSampler(random_state=0)\n\nX_train_resample,Y_train_resample=ros.fit_resample(X_train,Y_train)\n\n\nbefore split\n\n\nCode\nY.value_counts()\n\n\nchildren\nFalse    110800\nTrue       8590\nName: count, dtype: int64\n\n\nafter split before downsample:\n\n\nCode\nY_train.value_counts()\n\n\nchildren\nFalse    88646\nTrue      6866\nName: count, dtype: int64\n\n\nafter split after downsample:\n\n\nCode\nY_train_resample.value_counts()\n\n\nchildren\nFalse    6866\nTrue     6866\nName: count, dtype: int64\n\n\n\n\nCode\nX_train_resample.shape\n\n\n(13732, 31)",
    "crumbs": [
      "Classification2",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification2/1 decision tree on hotel booking data.html#define-model",
    "href": "classification2/1 decision tree on hotel booking data.html#define-model",
    "title": "Decision tree",
    "section": "3.1 define model",
    "text": "3.1 define model\n\n\nCode\nml_model = tree.DecisionTreeClassifier()  \nml_model\n\n\nDecisionTreeClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriNot fittedDecisionTreeClassifier()",
    "crumbs": [
      "Classification2",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification2/1 decision tree on hotel booking data.html#train-model",
    "href": "classification2/1 decision tree on hotel booking data.html#train-model",
    "title": "Decision tree",
    "section": "3.2 train model",
    "text": "3.2 train model\ntrain on resample data\n\n\nCode\nml_model.fit(X_train_resample[numerical_cols],Y_train_resample)\n\n\nDecisionTreeClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier() \n\n\nvariable importance\n\n\nCode\nimportances = ml_model.feature_importances_\nvi=pd.DataFrame({\"variable\":X_train_resample[numerical_cols].columns,\"importances\":importances})\n#vi=vi.sort_values('importances',ascending=False)\n#vi",
    "crumbs": [
      "Classification2",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification2/1 decision tree on hotel booking data.html#preformance",
    "href": "classification2/1 decision tree on hotel booking data.html#preformance",
    "title": "Decision tree",
    "section": "3.3 Preformance",
    "text": "3.3 Preformance\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = ml_model.predict(X_test[numerical_cols]) #always gets x and retuns y\n\n\n\n\nCode\nimport collections, numpy\ncollections.Counter(Y_pred_dt)\n\n\nCounter({False: 17588, True: 6290})\n\n\npredict 25% have children,the truth is only 7% have children,because we have make downsample before training.\n\n\nCode\n5977/(5977+17901)\n\n\n0.2503140966580116\n\n\n\n\nCode\ncollections.Counter(Y_test)\n\n1762/(22116+1762)\n\n\n0.07379177485551554\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_test,Y_pred_dt)  \naccuracy\n\n\n0.7809699304799397\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_test,Y_pred_dt)  \nprecision_dt\n\n\n0.22130365659777423\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_test,Y_pred_dt)  \nrecall_dt\n\n\n0.8074245939675174\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_test,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[17256,  4898],\n       [  332,  1392]])\n\n\n\n\nCode\nfrom sklearn.metrics import ConfusionMatrixDisplay\nConfusionMatrixDisplay(confusion_matrix_dt).plot()\n\n\n\n\n\n\n\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_test, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.7931679257641144\n\n\n\n\nCode\nfpr, tpr, thresholds = metrics.roc_curve(Y_test, Y_pred_dt)\nroc_auc = metrics.auc(fpr, tpr)\n\ndisplay = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,\n                                 estimator_name='example estimator')\n\ndisplay.plot()\n\nplt.show()",
    "crumbs": [
      "Classification2",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification2/1 decision tree on hotel booking data.html#k-fold-cross-validation",
    "href": "classification2/1 decision tree on hotel booking data.html#k-fold-cross-validation",
    "title": "Decision tree",
    "section": "3.4 k-Fold Cross-Validation",
    "text": "3.4 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(ml_model, X_train[numerical_cols], Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.9292445080288813",
    "crumbs": [
      "Classification2",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification2/1 decision tree on hotel booking data.html#save-model",
    "href": "classification2/1 decision tree on hotel booking data.html#save-model",
    "title": "Decision tree",
    "section": "3.5 save model",
    "text": "3.5 save model\n\n\nCode\nfrom joblib import dump, load\ndump(ml_model, 'trained_model_1.joblib') \n\n\n['trained_model_1.joblib']",
    "crumbs": [
      "Classification2",
      "Decision tree"
    ]
  },
  {
    "objectID": "classification2/1 decision tree on hotel booking data.html#load-model",
    "href": "classification2/1 decision tree on hotel booking data.html#load-model",
    "title": "Decision tree",
    "section": "3.6 load model",
    "text": "3.6 load model\n\n\nCode\nml_model_reload = load('trained_model_1.joblib') \n\n\n\n\nCode\nY_pred_dt = ml_model_reload.predict(X_test[numerical_cols]) #always gets x and retuns y\n\nY_pred_dt[0:5]\n\n\narray([False, False, False, False, False])",
    "crumbs": [
      "Classification2",
      "Decision tree"
    ]
  },
  {
    "objectID": "intro/1 input output.html",
    "href": "intro/1 input output.html",
    "title": "input output",
    "section": "",
    "text": "Code\nimport pandas as pd\ndata=pd.read_csv('data/Book3.csv')\ndata\n\n\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1241\nrhth\n\n\n1\n35235\nrjyyj\n\n\n\n\n\n\n\n\nCSV online\n\n\nCode\nimport pandas as pd\nurl='https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv'\nhotels=pd.read_csv(url)\n\n\n\n\n\nsheet_name=0 read first sheet.\nsheet_name=1 read second sheet.\n.sheet_name=‘Sheet1’ read ‘Sheet1’ sheet.\n\n\nCode\nimport pandas as pd\ndata=pd.read_excel('data/Book1.xlsx',sheet_name=0)\ndata\n\n\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1241\nrhth\n\n\n1\n35235\nrjyyj\n\n\n\n\n\n\n\n\n\n\n\nparquet format is one of the best for data analytic\n\n\nCode\ndata= pd.read_parquet(\"data/df.parquet\")\ndata.shape\n\n\n(100, 61)\n\n\n\n\nCode\ndata.head()\n\n\n\n\n\n\n\n\n\n\nFlightDate\nAirline\nOrigin\nDest\nCancelled\nDiverted\nCRSDepTime\nDepTime\nDepDelayMinutes\nDepDelay\n...\nWheelsOff\nWheelsOn\nTaxiIn\nCRSArrTime\nArrDelay\nArrDel15\nArrivalDelayGroups\nArrTimeBlk\nDistanceGroup\nDivAirportLandings\n\n\n\n\n0\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nGJT\nDEN\nFalse\nFalse\n1133\n1123.0\n0.0\n-10.0\n...\n1140.0\n1220.0\n8.0\n1245\n-17.0\n0.0\n-2.0\n1200-1259\n1\n0\n\n\n1\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nHRL\nIAH\nFalse\nFalse\n732\n728.0\n0.0\n-4.0\n...\n744.0\n839.0\n9.0\n849\n-1.0\n0.0\n-1.0\n0800-0859\n2\n0\n\n\n2\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nDRO\nDEN\nFalse\nFalse\n1529\n1514.0\n0.0\n-15.0\n...\n1535.0\n1622.0\n14.0\n1639\n-3.0\n0.0\n-1.0\n1600-1659\n2\n0\n\n\n3\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nIAH\nGPT\nFalse\nFalse\n1435\n1430.0\n0.0\n-5.0\n...\n1446.0\n1543.0\n4.0\n1605\n-18.0\n0.0\n-2.0\n1600-1659\n2\n0\n\n\n4\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nDRO\nDEN\nFalse\nFalse\n1135\n1135.0\n0.0\n0.0\n...\n1154.0\n1243.0\n8.0\n1245\n6.0\n0.0\n0.0\n1200-1259\n2\n0\n\n\n\n\n5 rows × 61 columns\n\n\n\n\nread parquet zip\n\n\nCode\ndata= pd.read_parquet(\"data/df.parquet.gzip\")\ndata.shape\n\n\n(100, 61)\n\n\n\n\n\n\n\nCode\ndata=pd.read_feather(\"data/file.feather\")",
    "crumbs": [
      "Intro",
      "input output"
    ]
  },
  {
    "objectID": "intro/1 input output.html#read-csv",
    "href": "intro/1 input output.html#read-csv",
    "title": "input output",
    "section": "",
    "text": "Code\nimport pandas as pd\ndata=pd.read_csv('data/Book3.csv')\ndata\n\n\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1241\nrhth\n\n\n1\n35235\nrjyyj\n\n\n\n\n\n\n\n\nCSV online\n\n\nCode\nimport pandas as pd\nurl='https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv'\nhotels=pd.read_csv(url)",
    "crumbs": [
      "Intro",
      "input output"
    ]
  },
  {
    "objectID": "intro/1 input output.html#read-excel",
    "href": "intro/1 input output.html#read-excel",
    "title": "input output",
    "section": "",
    "text": "sheet_name=0 read first sheet.\nsheet_name=1 read second sheet.\n.sheet_name=‘Sheet1’ read ‘Sheet1’ sheet.\n\n\nCode\nimport pandas as pd\ndata=pd.read_excel('data/Book1.xlsx',sheet_name=0)\ndata\n\n\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1241\nrhth\n\n\n1\n35235\nrjyyj",
    "crumbs": [
      "Intro",
      "input output"
    ]
  },
  {
    "objectID": "intro/1 input output.html#read-parquet",
    "href": "intro/1 input output.html#read-parquet",
    "title": "input output",
    "section": "",
    "text": "parquet format is one of the best for data analytic\n\n\nCode\ndata= pd.read_parquet(\"data/df.parquet\")\ndata.shape\n\n\n(100, 61)\n\n\n\n\nCode\ndata.head()\n\n\n\n\n\n\n\n\n\n\nFlightDate\nAirline\nOrigin\nDest\nCancelled\nDiverted\nCRSDepTime\nDepTime\nDepDelayMinutes\nDepDelay\n...\nWheelsOff\nWheelsOn\nTaxiIn\nCRSArrTime\nArrDelay\nArrDel15\nArrivalDelayGroups\nArrTimeBlk\nDistanceGroup\nDivAirportLandings\n\n\n\n\n0\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nGJT\nDEN\nFalse\nFalse\n1133\n1123.0\n0.0\n-10.0\n...\n1140.0\n1220.0\n8.0\n1245\n-17.0\n0.0\n-2.0\n1200-1259\n1\n0\n\n\n1\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nHRL\nIAH\nFalse\nFalse\n732\n728.0\n0.0\n-4.0\n...\n744.0\n839.0\n9.0\n849\n-1.0\n0.0\n-1.0\n0800-0859\n2\n0\n\n\n2\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nDRO\nDEN\nFalse\nFalse\n1529\n1514.0\n0.0\n-15.0\n...\n1535.0\n1622.0\n14.0\n1639\n-3.0\n0.0\n-1.0\n1600-1659\n2\n0\n\n\n3\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nIAH\nGPT\nFalse\nFalse\n1435\n1430.0\n0.0\n-5.0\n...\n1446.0\n1543.0\n4.0\n1605\n-18.0\n0.0\n-2.0\n1600-1659\n2\n0\n\n\n4\n2022-04-04\nCommutair Aka Champlain Enterprises, Inc.\nDRO\nDEN\nFalse\nFalse\n1135\n1135.0\n0.0\n0.0\n...\n1154.0\n1243.0\n8.0\n1245\n6.0\n0.0\n0.0\n1200-1259\n2\n0\n\n\n\n\n5 rows × 61 columns\n\n\n\n\nread parquet zip\n\n\nCode\ndata= pd.read_parquet(\"data/df.parquet.gzip\")\ndata.shape\n\n\n(100, 61)",
    "crumbs": [
      "Intro",
      "input output"
    ]
  },
  {
    "objectID": "intro/1 input output.html#read-feather",
    "href": "intro/1 input output.html#read-feather",
    "title": "input output",
    "section": "",
    "text": "Code\ndata=pd.read_feather(\"data/file.feather\")",
    "crumbs": [
      "Intro",
      "input output"
    ]
  },
  {
    "objectID": "intro/1 input output.html#write-csv",
    "href": "intro/1 input output.html#write-csv",
    "title": "input output",
    "section": "2.1 write CSV",
    "text": "2.1 write CSV\n\n\nCode\ndata.head().to_csv('data/out.csv', index=False)",
    "crumbs": [
      "Intro",
      "input output"
    ]
  },
  {
    "objectID": "intro/1 input output.html#write-excel",
    "href": "intro/1 input output.html#write-excel",
    "title": "input output",
    "section": "2.2 write excel",
    "text": "2.2 write excel\n\n\nCode\ndata.head().to_excel('data/out.xlsx')",
    "crumbs": [
      "Intro",
      "input output"
    ]
  },
  {
    "objectID": "intro/1 input output.html#write-parquet",
    "href": "intro/1 input output.html#write-parquet",
    "title": "input output",
    "section": "2.3 write parquet",
    "text": "2.3 write parquet\n\n\nCode\ndata.head(100).to_parquet('data/df.parquet') \n\n\noutput to zip format\n\n\nCode\ndata.head(100).to_parquet('data/df.parquet.gzip',\n              compression='gzip')",
    "crumbs": [
      "Intro",
      "input output"
    ]
  },
  {
    "objectID": "intro/1 input output.html#write-feather",
    "href": "intro/1 input output.html#write-feather",
    "title": "input output",
    "section": "2.4 write feather",
    "text": "2.4 write feather\n\n\nCode\ndata.head(100).to_feather(\"data/file.feather\")",
    "crumbs": [
      "Intro",
      "input output"
    ]
  },
  {
    "objectID": "data manipulation/1 input output.html",
    "href": "data manipulation/1 input output.html",
    "title": "input & output in Python",
    "section": "",
    "text": "Data input and ouput in python",
    "crumbs": [
      "data manipulation",
      "input & output in Python"
    ]
  },
  {
    "objectID": "data manipulation/1 input output.html#read-csv",
    "href": "data manipulation/1 input output.html#read-csv",
    "title": "input & output in Python",
    "section": "1.1 read CSV",
    "text": "1.1 read CSV\n\n\nCode\nimport pandas as pd\ndata=pd.read_csv('data/Book3.csv')\ndata\n\n\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1241\nrhth\n\n\n1\n35235\nrjyyj\n\n\n\n\n\n\n\n\nread CSV online\n\n\nCode\nimport pandas as pd\nurl='https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv'\nhotels=pd.read_csv(url)",
    "crumbs": [
      "data manipulation",
      "input & output in Python"
    ]
  },
  {
    "objectID": "data manipulation/1 input output.html#read-excel",
    "href": "data manipulation/1 input output.html#read-excel",
    "title": "input & output in Python",
    "section": "1.2 read excel",
    "text": "1.2 read excel\nsheet_name=0 read first sheet.\nsheet_name=1 read second sheet.\n.sheet_name=‘Sheet1’ read ‘Sheet1’ sheet.\n\n\nCode\nimport pandas as pd\ndata_excel=pd.read_excel('data/Book1.xlsx',sheet_name=0)\ndata_excel\n\n\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1241\nrhth\n\n\n1\n35235\nrjyyj",
    "crumbs": [
      "data manipulation",
      "input & output in Python"
    ]
  },
  {
    "objectID": "data manipulation/1 input output.html#read-parquet",
    "href": "data manipulation/1 input output.html#read-parquet",
    "title": "input & output in Python",
    "section": "1.3 read parquet",
    "text": "1.3 read parquet\nparquet format is one of the best for data analytic\n\n\nCode\ndata= pd.read_parquet(\"data/df.parquet\")\ndata.shape\n\n\n(100, 62)\n\n\n\n\nCode\ndata.head()\n\n\n\n\n\n\n\n\n\n\nFlightDate\nAirline\nOrigin\nDest\nCancelled\nDiverted\nCRSDepTime\nDepTime\nDepDelayMinutes\nDepDelay\n...\nWheelsOn\nTaxiIn\nCRSArrTime\nArrDelay\nArrDel15\nArrivalDelayGroups\nArrTimeBlk\nDistanceGroup\nDivAirportLandings\n__index_level_0__\n\n\n\n\n0\n2022-04-04 00:00:00+00:00\nCommutair Aka Champlain Enterprises, Inc.\nGJT\nDEN\nFalse\nFalse\n1133\n1123.0\n0.0\n-10.0\n...\n1220.0\n8.0\n1245\n-17.0\n0.0\n-2.0\n1200-1259\n1\n0\n0\n\n\n1\n2022-04-04 00:00:00+00:00\nCommutair Aka Champlain Enterprises, Inc.\nHRL\nIAH\nFalse\nFalse\n732\n728.0\n0.0\n-4.0\n...\n839.0\n9.0\n849\n-1.0\n0.0\n-1.0\n0800-0859\n2\n0\n1\n\n\n2\n2022-04-04 00:00:00+00:00\nCommutair Aka Champlain Enterprises, Inc.\nDRO\nDEN\nFalse\nFalse\n1529\n1514.0\n0.0\n-15.0\n...\n1622.0\n14.0\n1639\n-3.0\n0.0\n-1.0\n1600-1659\n2\n0\n2\n\n\n3\n2022-04-04 00:00:00+00:00\nCommutair Aka Champlain Enterprises, Inc.\nIAH\nGPT\nFalse\nFalse\n1435\n1430.0\n0.0\n-5.0\n...\n1543.0\n4.0\n1605\n-18.0\n0.0\n-2.0\n1600-1659\n2\n0\n3\n\n\n4\n2022-04-04 00:00:00+00:00\nCommutair Aka Champlain Enterprises, Inc.\nDRO\nDEN\nFalse\nFalse\n1135\n1135.0\n0.0\n0.0\n...\n1243.0\n8.0\n1245\n6.0\n0.0\n0.0\n1200-1259\n2\n0\n4\n\n\n\n\n5 rows × 62 columns\n\n\n\n\nread parquet zip\n\n\nCode\ndata= pd.read_parquet(\"data/df.parquet.gzip\")\ndata.shape\n\n\n(100, 62)",
    "crumbs": [
      "data manipulation",
      "input & output in Python"
    ]
  },
  {
    "objectID": "data manipulation/1 input output.html#read-feather",
    "href": "data manipulation/1 input output.html#read-feather",
    "title": "input & output in Python",
    "section": "1.4 read feather",
    "text": "1.4 read feather\n\n\nCode\ndata=pd.read_feather(\"data/feather_file.feather\")\ndata.head()\n\n\n\n\n\n\n\n\n\n\nFlightDate\nAirline\nOrigin\nDest\nCancelled\nDiverted\nCRSDepTime\nDepTime\nDepDelayMinutes\nDepDelay\n...\nWheelsOn\nTaxiIn\nCRSArrTime\nArrDelay\nArrDel15\nArrivalDelayGroups\nArrTimeBlk\nDistanceGroup\nDivAirportLandings\n__index_level_0__\n\n\n\n\n0\n2022-04-04 00:00:00+00:00\nCommutair Aka Champlain Enterprises, Inc.\nGJT\nDEN\nFalse\nFalse\n1133\n1123.0\n0.0\n-10.0\n...\n1220.0\n8.0\n1245\n-17.0\n0.0\n-2.0\n1200-1259\n1\n0\n0\n\n\n1\n2022-04-04 00:00:00+00:00\nCommutair Aka Champlain Enterprises, Inc.\nHRL\nIAH\nFalse\nFalse\n732\n728.0\n0.0\n-4.0\n...\n839.0\n9.0\n849\n-1.0\n0.0\n-1.0\n0800-0859\n2\n0\n1\n\n\n2\n2022-04-04 00:00:00+00:00\nCommutair Aka Champlain Enterprises, Inc.\nDRO\nDEN\nFalse\nFalse\n1529\n1514.0\n0.0\n-15.0\n...\n1622.0\n14.0\n1639\n-3.0\n0.0\n-1.0\n1600-1659\n2\n0\n2\n\n\n3\n2022-04-04 00:00:00+00:00\nCommutair Aka Champlain Enterprises, Inc.\nIAH\nGPT\nFalse\nFalse\n1435\n1430.0\n0.0\n-5.0\n...\n1543.0\n4.0\n1605\n-18.0\n0.0\n-2.0\n1600-1659\n2\n0\n3\n\n\n4\n2022-04-04 00:00:00+00:00\nCommutair Aka Champlain Enterprises, Inc.\nDRO\nDEN\nFalse\nFalse\n1135\n1135.0\n0.0\n0.0\n...\n1243.0\n8.0\n1245\n6.0\n0.0\n0.0\n1200-1259\n2\n0\n4\n\n\n\n\n5 rows × 62 columns",
    "crumbs": [
      "data manipulation",
      "input & output in Python"
    ]
  },
  {
    "objectID": "data manipulation/1 input output.html#write-csv",
    "href": "data manipulation/1 input output.html#write-csv",
    "title": "input & output in Python",
    "section": "2.1 write CSV",
    "text": "2.1 write CSV\n\n\nCode\ndata.head().to_csv('data/out.csv', index=False)",
    "crumbs": [
      "data manipulation",
      "input & output in Python"
    ]
  },
  {
    "objectID": "data manipulation/1 input output.html#write-excel",
    "href": "data manipulation/1 input output.html#write-excel",
    "title": "input & output in Python",
    "section": "2.2 write excel",
    "text": "2.2 write excel\n\n\nCode\ndata_excel.to_excel('data/out.xlsx')",
    "crumbs": [
      "data manipulation",
      "input & output in Python"
    ]
  },
  {
    "objectID": "data manipulation/1 input output.html#write-parquet",
    "href": "data manipulation/1 input output.html#write-parquet",
    "title": "input & output in Python",
    "section": "2.3 write parquet",
    "text": "2.3 write parquet\n\n\nCode\ndata.head(100).to_parquet('data/df.parquet') \n\n\noutput to zip format\n\n\nCode\ndata.head(100).to_parquet('data/df.parquet.gzip',\n              compression='gzip')",
    "crumbs": [
      "data manipulation",
      "input & output in Python"
    ]
  },
  {
    "objectID": "data manipulation/1 input output.html#write-feather",
    "href": "data manipulation/1 input output.html#write-feather",
    "title": "input & output in Python",
    "section": "2.4 write feather",
    "text": "2.4 write feather\n\n\nCode\ndata.head(100).to_feather(\"data/feather_file.feather\")",
    "crumbs": [
      "data manipulation",
      "input & output in Python"
    ]
  },
  {
    "objectID": "data manipulation/2 data structure in Python .html#singular",
    "href": "data manipulation/2 data structure in Python .html#singular",
    "title": "Data structure in Python",
    "section": "1.1 singular",
    "text": "1.1 singular\n\n\nCode\na=1\ntype(a)\n\n\nint\n\n\n\n\nCode\na=1.3\ntype(a)\n\n\nfloat\n\n\n\n\nCode\na='hell'\ntype(a)\n\n\nstr\n\n\n\n\nCode\na= True\ntype(a)\n\n\nbool",
    "crumbs": [
      "data manipulation",
      "Data structure in Python"
    ]
  },
  {
    "objectID": "data manipulation/2 data structure in Python .html#list",
    "href": "data manipulation/2 data structure in Python .html#list",
    "title": "Data structure in Python",
    "section": "1.2 list",
    "text": "1.2 list\n\n\nCode\na=[1,2,3]\n\na\n\n\n[1, 2, 3]\n\n\n\n\nCode\ntype(a) \n\n\nlist\n\n\n\n\nCode\nfruits = ['orange', 'apple', 'pear', 'banana', 'kiwi', 'apple', 'banana','apple']\n\n\n\n1.2.1 find length of the list with len()\n\n\nCode\nlen(fruits)\n\n\n8\n\n\n\n\n1.2.2 find how many time in the list with count()\n\n\nCode\nfruits.count('apple')\n\n\n3\n\n\n\n\n1.2.3 find locaiton of on the list with index()\nshow the first ‘apple’ index. python list start at 0\n\n\nCode\nfruits.index('apple')\n\n\n1\n\n\nall ‘apple’ in the list\n\n\nCode\n[index for index, value in enumerate(fruits) if value == 'apple']\n\n\n[1, 5, 7]\n\n\n\n\n1.2.4 reverse the list\n\n\nCode\nfruits.reverse()\nfruits\n\n\n['apple', 'banana', 'apple', 'kiwi', 'banana', 'pear', 'apple', 'orange']\n\n\n\n\n1.2.5 sort the list\n\n\nCode\nfruits.sort()\nfruits\n\n\n['apple', 'apple', 'apple', 'banana', 'banana', 'kiwi', 'orange', 'pear']\n\n\n\n\n1.2.6 add element on the list\n\n\nCode\nfruits.append('grape')\nfruits\n\n\n['apple',\n 'apple',\n 'apple',\n 'banana',\n 'banana',\n 'kiwi',\n 'orange',\n 'pear',\n 'grape']\n\n\n\n\n1.2.7 drop last element\n\n\nCode\nfruits.pop()\n\nfruits\n\n\n['apple', 'apple', 'apple', 'banana', 'banana', 'kiwi', 'orange', 'pear']\n\n\n\n\n1.2.8 List Comprehensions\nusing loop:\n\n\nCode\nsquares = []\nfor x in range(10):\n  squares.append(x**2)\n  \nsquares\n\n\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n\n\nusing List Comprehensions\n\n\nCode\nsquares = [x**2 for x in range(10)]\nsquares\n\n\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n\n\n\n\n1.2.9 list to Tuples\n\n\nCode\ntuple(squares)\n\n\n(0, 1, 4, 9, 16, 25, 36, 49, 64, 81)\n\n\n\n\n1.2.10 list to set\n\n\nCode\nset(squares)\n\n\n{0, 1, 4, 9, 16, 25, 36, 49, 64, 81}\n\n\n\n\n1.2.11 list to dictionary\n\n1.2.11.1 one list to dictionary\n\n\nCode\nlist=['a', 1, 'b', 2, 'c', 3]\n\ndef convert(lst):\n   res_dict = {}\n   for i in range(0, len(lst), 2):\n       res_dict[lst[i]] = lst[i + 1]\n   return res_dict\n \nconvert(list)\n\n\n{'a': 1, 'b': 2, 'c': 3}\n\n\n\n\n1.2.11.2 two list to dictionary\n\n\nCode\nimport itertools\n\nkeys = ('name', 'age', 'food')\n\nvalues = ('Monty', 42, 'spam')\n\ndict(zip(keys, values))\n\n\n{'name': 'Monty', 'age': 42, 'food': 'spam'}",
    "crumbs": [
      "data manipulation",
      "Data structure in Python"
    ]
  },
  {
    "objectID": "data manipulation/2 data structure in Python .html#tuples",
    "href": "data manipulation/2 data structure in Python .html#tuples",
    "title": "Data structure in Python",
    "section": "1.3 Tuples",
    "text": "1.3 Tuples\n\n\nCode\nfruits = ('orange', 'apple', 'pear', 'banana', 'kiwi', 'apple', 'banana','apple')\n\nfruits\n\n\n('orange', 'apple', 'pear', 'banana', 'kiwi', 'apple', 'banana', 'apple')\n\n\n\n\nCode\ntype(fruits)\n\n\ntuple\n\n\ntuple can not be modified.",
    "crumbs": [
      "data manipulation",
      "Data structure in Python"
    ]
  },
  {
    "objectID": "data manipulation/2 data structure in Python .html#sets",
    "href": "data manipulation/2 data structure in Python .html#sets",
    "title": "Data structure in Python",
    "section": "1.4 Sets",
    "text": "1.4 Sets\nA set is an unordered collection with no duplicate elements.\n\n\nCode\nbasket = {'apple', 'orange', 'apple', 'pear', 'orange', 'banana'}\n\nbasket\n\n\n{'apple', 'banana', 'orange', 'pear'}\n\n\n\n\nCode\ntype(basket)\n\n\nset",
    "crumbs": [
      "data manipulation",
      "Data structure in Python"
    ]
  },
  {
    "objectID": "data manipulation/2 data structure in Python .html#dictionaries",
    "href": "data manipulation/2 data structure in Python .html#dictionaries",
    "title": "Data structure in Python",
    "section": "1.5 Dictionaries",
    "text": "1.5 Dictionaries\n\n\nCode\ntel = {'jack': 4098, 'sape': 4139}\n\ntel\n\n\n{'jack': 4098, 'sape': 4139}\n\n\n\n\nCode\ntype(tel)\n\n\ndict\n\n\n\n\nCode\ntel['jack']\n\n\n4098",
    "crumbs": [
      "data manipulation",
      "Data structure in Python"
    ]
  },
  {
    "objectID": "data manipulation/3 Pandas.html",
    "href": "data manipulation/3 Pandas.html",
    "title": "Data manipulation with Pandas",
    "section": "",
    "text": "pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.\nCode\nimport pandas as pd\nprint('pandas version', pd.__version__)\n\n\npandas version 2.2.1\nCode\nimport os\n#os.system('pip show pandas')",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/3 Pandas.html#load-package",
    "href": "data manipulation/3 Pandas.html#load-package",
    "title": "Data manipulation with Pandas",
    "section": "0.1 load package",
    "text": "0.1 load package\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nimport seaborn as sns\n\nfrom siuba.data import mtcars,penguins\n\nfrom siuba.siu import call\nfrom siuba import _, mutate, filter, group_by, summarize,show_query\nfrom siuba import *\n\nfrom siuba.data import mtcars,penguins\n\n\n\n\nCode\nsmall_mtcars = mtcars[[\"cyl\", \"mpg\",'hp']]\nsmall_mtcars=small_mtcars.head(5)",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/3 Pandas.html#select-column",
    "href": "data manipulation/3 Pandas.html#select-column",
    "title": "Data manipulation with Pandas",
    "section": "0.2 select column",
    "text": "0.2 select column\n\n0.2.1 get column names\n\n\nCode\nlist(small_mtcars)\n\n\n['cyl', 'mpg', 'hp']\n\n\n\n\n0.2.2 select by name\n\n\nCode\nsmall_mtcars [[\"cyl\", \"mpg\",'hp']]\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\nhp\n\n\n\n\n0\n6\n21.0\n110\n\n\n1\n6\n21.0\n110\n\n\n2\n4\n22.8\n93\n\n\n3\n6\n21.4\n110\n\n\n4\n8\n18.7\n175\n\n\n\n\n\n\n\n\nother way:\n\n\nCode\nsmall_mtcars.filter(items=['cyl', 'mpg','hp'])\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\nhp\n\n\n\n\n0\n6\n21.0\n110\n\n\n1\n6\n21.0\n110\n\n\n2\n4\n22.8\n93\n\n\n3\n6\n21.4\n110\n\n\n4\n8\n18.7\n175\n\n\n\n\n\n\n\n\n\n\n0.2.3 select columns by name match with ‘p’\n\n\nCode\nsmall_mtcars.loc[:,small_mtcars.columns.str.contains(\"p\")]\n\n\n\n\n\n\n\n\n\n\nmpg\nhp\n\n\n\n\n0\n21.0\n110\n\n\n1\n21.0\n110\n\n\n2\n22.8\n93\n\n\n3\n21.4\n110\n\n\n4\n18.7\n175\n\n\n\n\n\n\n\n\nother way:\n\n\nCode\nsmall_mtcars.filter(regex='p.*', axis=1)\n\n\n\n\n\n\n\n\n\n\nmpg\nhp\n\n\n\n\n0\n21.0\n110\n\n\n1\n21.0\n110\n\n\n2\n22.8\n93\n\n\n3\n21.4\n110\n\n\n4\n18.7\n175\n\n\n\n\n\n\n\n\n\n\n0.2.4 select columns by index\n\n0.2.4.1 select first and 3rd columns\n\n\nCode\nsmall_mtcars.iloc[[0,2]]\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\nhp\n\n\n\n\n0\n6\n21.0\n110\n\n\n2\n4\n22.8\n93\n\n\n\n\n\n\n\n\n\n\n0.2.4.2 select first to 3rd columns\n\n\nCode\nsmall_mtcars[0:3]\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\nhp\n\n\n\n\n0\n6\n21.0\n110\n\n\n1\n6\n21.0\n110\n\n\n2\n4\n22.8\n93",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/3 Pandas.html#drop-column",
    "href": "data manipulation/3 Pandas.html#drop-column",
    "title": "Data manipulation with Pandas",
    "section": "0.3 drop column",
    "text": "0.3 drop column\n\n\nCode\nsmall_mtcars.drop('cyl', axis=1)\n\n\n\n\n\n\n\n\n\n\nmpg\nhp\n\n\n\n\n0\n21.0\n110\n\n\n1\n21.0\n110\n\n\n2\n22.8\n93\n\n\n3\n21.4\n110\n\n\n4\n18.7\n175",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/3 Pandas.html#renaming-column",
    "href": "data manipulation/3 Pandas.html#renaming-column",
    "title": "Data manipulation with Pandas",
    "section": "0.4 Renaming column",
    "text": "0.4 Renaming column\n\n\nCode\nsmall_mtcars.rename(columns={'mpg':\"new_name_mpg\", 'cyl':'new_name_cyl'})\n\n\n\n\n\n\n\n\n\n\nnew_name_cyl\nnew_name_mpg\nhp\n\n\n\n\n0\n6\n21.0\n110\n\n\n1\n6\n21.0\n110\n\n\n2\n4\n22.8\n93\n\n\n3\n6\n21.4\n110\n\n\n4\n8\n18.7\n175",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/3 Pandas.html#create-column",
    "href": "data manipulation/3 Pandas.html#create-column",
    "title": "Data manipulation with Pandas",
    "section": "0.5 Create column",
    "text": "0.5 Create column\n\n0.5.1 Mutate\n\n\nCode\nsmall_mtcars['mpg2'] = small_mtcars['mpg']+1\n\n\nsmall_mtcars['mpg3']  = np.where(small_mtcars['mpg']&gt; 20, \"long\", \"short\")\n\n\nsmall_mtcars['mpg4'] =np.where(small_mtcars[\"mpg\"]&lt;19, \"short\",\n                   np.where(small_mtcars[\"mpg\"]&lt;=22, \"Medium\",\n                   np.where(small_mtcars[\"mpg\"]&gt;22, \"long\",\"else\")))\n\n\nsmall_mtcars\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\nhp\nmpg2\nmpg3\nmpg4\n\n\n\n\n0\n6\n21.0\n110\n22.0\nlong\nMedium\n\n\n1\n6\n21.0\n110\n22.0\nlong\nMedium\n\n\n2\n4\n22.8\n93\n23.8\nlong\nlong\n\n\n3\n6\n21.4\n110\n22.4\nlong\nMedium\n\n\n4\n8\n18.7\n175\n19.7\nshort\nshort\n\n\n\n\n\n\n\n\n\n\n0.5.2 Transmute,create column and only keep this column\n\n\nCode\nsmall_mtcars['mpg2'] = small_mtcars['mpg']+1\n\nnew_data=small_mtcars[['mpg2']]\n\nnew_data\n\n\n\n\n\n\n\n\n\n\nmpg2\n\n\n\n\n0\n22.0\n\n\n1\n22.0\n\n\n2\n23.8\n\n\n3\n22.4\n\n\n4\n19.7",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/3 Pandas.html#filter-rows",
    "href": "data manipulation/3 Pandas.html#filter-rows",
    "title": "Data manipulation with Pandas",
    "section": "0.6 Filter rows",
    "text": "0.6 Filter rows\n\n\nCode\nmtcars[(mtcars['gear'] ==4)]\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n7\n24.4\n4\n146.7\n62\n3.69\n3.190\n20.00\n1\n0\n4\n2\n\n\n8\n22.8\n4\n140.8\n95\n3.92\n3.150\n22.90\n1\n0\n4\n2\n\n\n9\n19.2\n6\n167.6\n123\n3.92\n3.440\n18.30\n1\n0\n4\n4\n\n\n10\n17.8\n6\n167.6\n123\n3.92\n3.440\n18.90\n1\n0\n4\n4\n\n\n17\n32.4\n4\n78.7\n66\n4.08\n2.200\n19.47\n1\n1\n4\n1\n\n\n18\n30.4\n4\n75.7\n52\n4.93\n1.615\n18.52\n1\n1\n4\n2\n\n\n19\n33.9\n4\n71.1\n65\n4.22\n1.835\n19.90\n1\n1\n4\n1\n\n\n25\n27.3\n4\n79.0\n66\n4.08\n1.935\n18.90\n1\n1\n4\n1\n\n\n31\n21.4\n4\n121.0\n109\n4.11\n2.780\n18.60\n1\n1\n4\n2\n\n\n\n\n\n\n\n\nother way:\n\n\nCode\nmtcars.query('gear==4')\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n7\n24.4\n4\n146.7\n62\n3.69\n3.190\n20.00\n1\n0\n4\n2\n\n\n8\n22.8\n4\n140.8\n95\n3.92\n3.150\n22.90\n1\n0\n4\n2\n\n\n9\n19.2\n6\n167.6\n123\n3.92\n3.440\n18.30\n1\n0\n4\n4\n\n\n10\n17.8\n6\n167.6\n123\n3.92\n3.440\n18.90\n1\n0\n4\n4\n\n\n17\n32.4\n4\n78.7\n66\n4.08\n2.200\n19.47\n1\n1\n4\n1\n\n\n18\n30.4\n4\n75.7\n52\n4.93\n1.615\n18.52\n1\n1\n4\n2\n\n\n19\n33.9\n4\n71.1\n65\n4.22\n1.835\n19.90\n1\n1\n4\n1\n\n\n25\n27.3\n4\n79.0\n66\n4.08\n1.935\n18.90\n1\n1\n4\n1\n\n\n31\n21.4\n4\n121.0\n109\n4.11\n2.780\n18.60\n1\n1\n4\n2\n\n\n\n\n\n\n\n\n\n0.6.1 Filters with AND conditions\n\n\nCode\nmtcars[(mtcars['cyl'] &gt;4)&(mtcars['gear'] ==5) ]\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n28\n15.8\n8\n351.0\n264\n4.22\n3.17\n14.5\n0\n1\n5\n4\n\n\n29\n19.7\n6\n145.0\n175\n3.62\n2.77\n15.5\n0\n1\n5\n6\n\n\n30\n15.0\n8\n301.0\n335\n3.54\n3.57\n14.6\n0\n1\n5\n8\n\n\n\n\n\n\n\n\nother way:\n\n\nCode\nmtcars.query('cyl&gt;4 and gear==5')\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n28\n15.8\n8\n351.0\n264\n4.22\n3.17\n14.5\n0\n1\n5\n4\n\n\n29\n19.7\n6\n145.0\n175\n3.62\n2.77\n15.5\n0\n1\n5\n6\n\n\n30\n15.0\n8\n301.0\n335\n3.54\n3.57\n14.6\n0\n1\n5\n8\n\n\n\n\n\n\n\n\n\n\n0.6.2 Filters with OR conditions\n\n\nCode\nmtcars[(mtcars['cyl'] ==6) |(mtcars['gear'] ==5) ]\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n3\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n5\n18.1\n6\n225.0\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n\n\n9\n19.2\n6\n167.6\n123\n3.92\n3.440\n18.30\n1\n0\n4\n4\n\n\n10\n17.8\n6\n167.6\n123\n3.92\n3.440\n18.90\n1\n0\n4\n4\n\n\n26\n26.0\n4\n120.3\n91\n4.43\n2.140\n16.70\n0\n1\n5\n2\n\n\n27\n30.4\n4\n95.1\n113\n3.77\n1.513\n16.90\n1\n1\n5\n2\n\n\n28\n15.8\n8\n351.0\n264\n4.22\n3.170\n14.50\n0\n1\n5\n4\n\n\n29\n19.7\n6\n145.0\n175\n3.62\n2.770\n15.50\n0\n1\n5\n6\n\n\n30\n15.0\n8\n301.0\n335\n3.54\n3.570\n14.60\n0\n1\n5\n8\n\n\n\n\n\n\n\n\nother way:\n\n\nCode\nmtcars.query('cyl==6 or gear==5')\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n3\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n5\n18.1\n6\n225.0\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n\n\n9\n19.2\n6\n167.6\n123\n3.92\n3.440\n18.30\n1\n0\n4\n4\n\n\n10\n17.8\n6\n167.6\n123\n3.92\n3.440\n18.90\n1\n0\n4\n4\n\n\n26\n26.0\n4\n120.3\n91\n4.43\n2.140\n16.70\n0\n1\n5\n2\n\n\n27\n30.4\n4\n95.1\n113\n3.77\n1.513\n16.90\n1\n1\n5\n2\n\n\n28\n15.8\n8\n351.0\n264\n4.22\n3.170\n14.50\n0\n1\n5\n4\n\n\n29\n19.7\n6\n145.0\n175\n3.62\n2.770\n15.50\n0\n1\n5\n6\n\n\n30\n15.0\n8\n301.0\n335\n3.54\n3.570\n14.60\n0\n1\n5\n8\n\n\n\n\n\n\n\n\n\n\n0.6.3 filter row with index\n\n0.6.3.1 5th rows\n\n\nCode\n# not in siuba, in pandas\nmtcars.iloc[[4]]\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n4\n18.7\n8\n360.0\n175\n3.15\n3.44\n17.02\n0\n0\n3\n2\n\n\n\n\n\n\n\n\n\n\n0.6.3.2 1 and 5tj rows\n\n\nCode\n# not in siuba, in pandas\nmtcars.iloc[[0,4]]\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\n21.0\n6\n160.0\n110\n3.90\n2.62\n16.46\n0\n1\n4\n4\n\n\n4\n18.7\n8\n360.0\n175\n3.15\n3.44\n17.02\n0\n0\n3\n2\n\n\n\n\n\n\n\n\n\n\n0.6.3.3 1 to 5th rows\n\n\nCode\n# not in siuba, in pandas\nmtcars.iloc[0:4]\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n3\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n\n\n\n\n\n\n\n\n0.6.3.4 get ramdon 5 rows\n\n\nCode\nmtcars.sample(5, random_state=42)\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n29\n19.7\n6\n145.0\n175\n3.62\n2.770\n15.50\n0\n1\n5\n6\n\n\n15\n10.4\n8\n460.0\n215\n3.00\n5.424\n17.82\n0\n0\n3\n4\n\n\n24\n19.2\n8\n400.0\n175\n3.08\n3.845\n17.05\n0\n0\n3\n2\n\n\n17\n32.4\n4\n78.7\n66\n4.08\n2.200\n19.47\n1\n1\n4\n1\n\n\n8\n22.8\n4\n140.8\n95\n3.92\n3.150\n22.90\n1\n0\n4\n2",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/3 Pandas.html#append",
    "href": "data manipulation/3 Pandas.html#append",
    "title": "Data manipulation with Pandas",
    "section": "0.7 Append",
    "text": "0.7 Append\n\n0.7.1 append by row\n\n\nCode\n# not available in siuba yet\n#from siuba import bind_rows\n\n\n\n\nCode\n# using pandas\n\n# get 1 to 4 rows\ndata1=mtcars.iloc[0:4]\n\n# get 9 rows\ndata2=mtcars.iloc[10:11]\n\ndata3=pd.concat([data1, data2], ignore_index = True,axis=0)\n\ndata3\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n3\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n4\n17.8\n6\n167.6\n123\n3.92\n3.440\n18.90\n1\n0\n4\n4\n\n\n\n\n\n\n\n\n\n\n0.7.2 append by column\n\n\nCode\n# not available in siuba yet\n#from siuba import bind_columns\n\n\n\n\nCode\n# using pandas\ndata1=small_mtcars&gt;&gt;select(_.mpg)\n\ndata2=small_mtcars&gt;&gt;select(_.cyl)\n\ndata3=pd.concat([data1, data2],axis=1)\n\ndata3\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\n\n\n\n\n0\n21.0\n6\n\n\n1\n21.0\n6\n\n\n2\n22.8\n4\n\n\n3\n21.4\n6\n\n\n4\n18.7\n8\n\n\n\n\n\n\n\n\n\n\n0.7.3 Dropping NA values\n\n\n0.7.4 keep NA values",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/3 Pandas.html#group-by",
    "href": "data manipulation/3 Pandas.html#group-by",
    "title": "Data manipulation with Pandas",
    "section": "0.8 group by",
    "text": "0.8 group by\n\n0.8.1 average,min,max,sum\n\n\nCode\nmtcars.groupby(\"cyl\")[\"hp\"].mean()\n\n\ncyl\n4     82.636364\n6    122.285714\n8    209.214286\nName: hp, dtype: float64\n\n\n\n\nCode\nmtcars.groupby(\"cyl\")[\"hp\"].min()\n\n\ncyl\n4     52\n6    105\n8    150\nName: hp, dtype: int64\n\n\n\n\nCode\nmtcars.groupby(\"cyl\")[\"hp\"].max()\n\n\ncyl\n4    113\n6    175\n8    335\nName: hp, dtype: int64\n\n\n\n\nCode\nmtcars.groupby(\"cyl\")[\"hp\"].sum()\n\n\ncyl\n4     909\n6     856\n8    2929\nName: hp, dtype: int64\n\n\n\n\n0.8.2 count record and count distinct record\n\n\nCode\nmtcars.groupby(\"cyl\")[\"hp\"].count()\n\n\ncyl\n4    11\n6     7\n8    14\nName: hp, dtype: int64\n\n\n\n\nCode\nmtcars.groupby(\"cyl\")[\"hp\"].nunique()\n\n\ncyl\n4    10\n6     4\n8     9\nName: hp, dtype: int64",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/3 Pandas.html#order-rows",
    "href": "data manipulation/3 Pandas.html#order-rows",
    "title": "Data manipulation with Pandas",
    "section": "0.9 order rows",
    "text": "0.9 order rows\n\n\nCode\nsmall_mtcars.sort_values('hp')\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\nhp\nmpg2\nmpg3\nmpg4\n\n\n\n\n2\n4\n22.8\n93\n23.8\nlong\nlong\n\n\n0\n6\n21.0\n110\n22.0\nlong\nMedium\n\n\n1\n6\n21.0\n110\n22.0\nlong\nMedium\n\n\n3\n6\n21.4\n110\n22.4\nlong\nMedium\n\n\n4\n8\n18.7\n175\n19.7\nshort\nshort\n\n\n\n\n\n\n\n\n\n0.9.1 Sort in descending order\n\n\nCode\nsmall_mtcars.sort_values('hp',ascending=False)\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\nhp\nmpg2\nmpg3\nmpg4\n\n\n\n\n4\n8\n18.7\n175\n19.7\nshort\nshort\n\n\n0\n6\n21.0\n110\n22.0\nlong\nMedium\n\n\n1\n6\n21.0\n110\n22.0\nlong\nMedium\n\n\n3\n6\n21.4\n110\n22.4\nlong\nMedium\n\n\n2\n4\n22.8\n93\n23.8\nlong\nlong\n\n\n\n\n\n\n\n\n\n\n0.9.2 Arrange by multiple variables\n\n\nCode\nsmall_mtcars.sort_values(by=['cyl','mpg'])\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\nhp\nmpg2\nmpg3\nmpg4\n\n\n\n\n2\n4\n22.8\n93\n23.8\nlong\nlong\n\n\n0\n6\n21.0\n110\n22.0\nlong\nMedium\n\n\n1\n6\n21.0\n110\n22.0\nlong\nMedium\n\n\n3\n6\n21.4\n110\n22.4\nlong\nMedium\n\n\n4\n8\n18.7\n175\n19.7\nshort\nshort",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/3 Pandas.html#join",
    "href": "data manipulation/3 Pandas.html#join",
    "title": "Data manipulation with Pandas",
    "section": "0.10 join",
    "text": "0.10 join\n\n\nCode\nlhs = pd.DataFrame({'id': [1,2,3], 'val': ['lhs.1', 'lhs.2', 'lhs.3']})\nrhs = pd.DataFrame({'id': [1,2,4], 'val': ['rhs.1', 'rhs.2', 'rhs.3']})\n\n\n\n\nCode\nlhs\n\n\n\n\n\n\n\n\n\n\nid\nval\n\n\n\n\n0\n1\nlhs.1\n\n\n1\n2\nlhs.2\n\n\n2\n3\nlhs.3\n\n\n\n\n\n\n\n\n\n\nCode\nrhs\n\n\n\n\n\n\n\n\n\n\nid\nval\n\n\n\n\n0\n1\nrhs.1\n\n\n1\n2\nrhs.2\n\n\n2\n4\nrhs.3\n\n\n\n\n\n\n\n\n\n0.10.1 inner_join\n\n\nCode\nresult=pd.merge(lhs, rhs, on='id', how='inner')\nresult\n\n\n\n\n\n\n\n\n\n\nid\nval_x\nval_y\n\n\n\n\n0\n1\nlhs.1\nrhs.1\n\n\n1\n2\nlhs.2\nrhs.2\n\n\n\n\n\n\n\n\n\n\n0.10.2 full join\n\n\nCode\nresult=pd.merge(lhs, rhs, on='id', how='outer')\nresult\n\n\n\n\n\n\n\n\n\n\nid\nval_x\nval_y\n\n\n\n\n0\n1\nlhs.1\nrhs.1\n\n\n1\n2\nlhs.2\nrhs.2\n\n\n2\n3\nlhs.3\nNaN\n\n\n3\n4\nNaN\nrhs.3\n\n\n\n\n\n\n\n\n\n\n0.10.3 left join\n\n\nCode\nresult=pd.merge(lhs, rhs, on='id', how='left')\nresult\n\n\n\n\n\n\n\n\n\n\nid\nval_x\nval_y\n\n\n\n\n0\n1\nlhs.1\nrhs.1\n\n\n1\n2\nlhs.2\nrhs.2\n\n\n2\n3\nlhs.3\nNaN\n\n\n\n\n\n\n\n\n\n\n0.10.4 anti join\nkeep data in left which not in right\n\n\nCode\n#in siuba\nresult=lhs &gt;&gt; anti_join(_, rhs, on=\"id\")\nresult\n\n\n\n\n\n\n\n\n\n\nid\nval\n\n\n\n\n2\n3\nlhs.3\n\n\n\n\n\n\n\n\nkeep data in right which not in left\n\n\nCode\n#in siuba\nresult=rhs &gt;&gt; anti_join(_, lhs, on=\"id\")\nresult\n\n\n\n\n\n\n\n\n\n\nid\nval\n\n\n\n\n2\n4\nrhs.3",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/3 Pandas.html#reshape-tables",
    "href": "data manipulation/3 Pandas.html#reshape-tables",
    "title": "Data manipulation with Pandas",
    "section": "0.11 Reshape tables",
    "text": "0.11 Reshape tables\n\n\nCode\ncosts = pd.DataFrame({\n    'id': [1,2],\n    'price_x': [.1, .2],\n    'price_y': [.4, .5],\n    'price_z': [.7, .8]\n})\n\ncosts\n\n\n\n\n\n\n\n\n\n\nid\nprice_x\nprice_y\nprice_z\n\n\n\n\n0\n1\n0.1\n0.4\n0.7\n\n\n1\n2\n0.2\n0.5\n0.8\n\n\n\n\n\n\n\n\n\n0.11.1 Gather data long(wide to long)\n\n\nCode\n# selecting each variable manually\nlong_date=pd.melt(costs,id_vars=['id'], value_vars=['price_x', 'price_y','price_z'])\n\nlong_date\n#costs &gt;&gt; gather('measure', 'value', _.price_x, _.price_y, _.price_z)\n\n\n\n\n\n\n\n\n\n\nid\nvariable\nvalue\n\n\n\n\n0\n1\nprice_x\n0.1\n\n\n1\n2\nprice_x\n0.2\n\n\n2\n1\nprice_y\n0.4\n\n\n3\n2\nprice_y\n0.5\n\n\n4\n1\nprice_z\n0.7\n\n\n5\n2\nprice_z\n0.8\n\n\n\n\n\n\n\n\n\n\n0.11.2 Spread data wide (long to wide)\n\n\nCode\nlong_date.pivot(index=\"id\", columns=\"variable\", values=\"value\")\n\n\n\n\n\n\n\n\n\nvariable\nprice_x\nprice_y\nprice_z\n\n\nid\n\n\n\n\n\n\n\n1\n0.1\n0.4\n0.7\n\n\n2\n0.2\n0.5\n0.8",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/3 Pandas.html#string",
    "href": "data manipulation/3 Pandas.html#string",
    "title": "Data manipulation with Pandas",
    "section": "0.12 string",
    "text": "0.12 string\n\n\nCode\ndf = pd.DataFrame({'text': ['abc', 'DDD','1243c','aeEe'], 'num': [3, 4,7,8]})\n\ndf\n\n\n\n\n\n\n\n\n\n\ntext\nnum\n\n\n\n\n0\nabc\n3\n\n\n1\nDDD\n4\n\n\n2\n1243c\n7\n\n\n3\naeEe\n8\n\n\n\n\n\n\n\n\n\n0.12.1 upper case\n\n\nCode\ndf&gt;&gt; mutate(text_new=_.text.str.upper())\n\n\n\n\n\n\n\n\n\n\ntext\nnum\ntext_new\n\n\n\n\n0\nabc\n3\nABC\n\n\n1\nDDD\n4\nDDD\n\n\n2\n1243c\n7\n1243C\n\n\n3\naeEe\n8\nAEEE\n\n\n\n\n\n\n\n\n\n\n0.12.2 lower case\n\n\nCode\ndf&gt;&gt; mutate(text_new=_.text.str.lower())\n\n\n\n\n\n\n\n\n\n\ntext\nnum\ntext_new\n\n\n\n\n0\nabc\n3\nabc\n\n\n1\nDDD\n4\nddd\n\n\n2\n1243c\n7\n1243c\n\n\n3\naeEe\n8\naeee\n\n\n\n\n\n\n\n\n\n\n0.12.3 match\n\n\nCode\ndf&gt;&gt; mutate(text_new1=if_else(_.text== \"abc\",'T','F')\n            ,text_new2=if_else(_.text.str.startswith(\"a\"),'T','F')\n            ,text_new3=if_else(_.text.str.endswith(\"c\"),'T','F')\n            ,text_new4=if_else(_.text.str.contains(\"4\"),'T','F')\n\n)\n\n\n\n\n\n\n\n\n\n\ntext\nnum\ntext_new1\ntext_new2\ntext_new3\ntext_new4\n\n\n\n\n0\nabc\n3\nT\nT\nT\nF\n\n\n1\nDDD\n4\nF\nF\nF\nF\n\n\n2\n1243c\n7\nF\nF\nT\nT\n\n\n3\naeEe\n8\nF\nT\nF\nF\n\n\n\n\n\n\n\n\n\n\n0.12.4 concatenation\n\n\nCode\ndf&gt;&gt; mutate(text_new1=_.text+' is '+_.text\n)\n\n\n\n\n\n\n\n\n\n\ntext\nnum\ntext_new1\n\n\n\n\n0\nabc\n3\nabc is abc\n\n\n1\nDDD\n4\nDDD is DDD\n\n\n2\n1243c\n7\n1243c is 1243c\n\n\n3\naeEe\n8\naeEe is aeEe\n\n\n\n\n\n\n\n\n\n\n0.12.5 replace\nUse .str.replace(…, regex=True) with regular expressions to replace patterns in strings.\nFor example, the code below uses “p.”, where . is called a wildcard–which matches any character.\n\n\nCode\ndf&gt;&gt; mutate(text_new1=_.text.str.replace(\"a.\", \"XX\", regex=True)\n)\n\n\n\n\n\n\n\n\n\n\ntext\nnum\ntext_new1\n\n\n\n\n0\nabc\n3\nXXc\n\n\n1\nDDD\n4\nDDD\n\n\n2\n1243c\n7\n1243c\n\n\n3\naeEe\n8\nXXEe\n\n\n\n\n\n\n\n\n\n\n0.12.6 extract\nUse str.extract() with a regular expression to pull out a matching piece of text.\nFor example the regular expression “^(.*) ” contains the following pieces:\n\na matches the literal letter “a”\n.* has a . which matches anything, and * which modifies it to apply 0 or more times.\n\n\n\nCode\ndf&gt;&gt; mutate(text_new1=_.text.str.extract(\"a(.*)\")\n            ,text_new2=_.text.str.extract(\"(.*)c\")\n)\n\n\n\n\n\n\n\n\n\n\ntext\nnum\ntext_new1\ntext_new2\n\n\n\n\n0\nabc\n3\nbc\nab\n\n\n1\nDDD\n4\nNaN\nNaN\n\n\n2\n1243c\n7\nNaN\n1243\n\n\n3\naeEe\n8\neEe\nNaN",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/3 Pandas.html#date",
    "href": "data manipulation/3 Pandas.html#date",
    "title": "Data manipulation with Pandas",
    "section": "0.13 date",
    "text": "0.13 date\n\n\nCode\ndf_dates = pd.DataFrame({\n    \"dates\": pd.to_datetime([\"2021-01-02\", \"2021-02-03\"]),\n    \"raw\": [\"2023-04-05 06:07:08\", \"2024-05-06 07:08:09\"],\n})\ndf_dates\n\n\n\n\n\n\n\n\n\n\ndates\nraw\n\n\n\n\n0\n2021-01-02\n2023-04-05 06:07:08\n\n\n1\n2021-02-03\n2024-05-06 07:08:09\n\n\n\n\n\n\n\n\n\n\nCode\nfrom datetime import datetime\n\ndf_date=df_dates&gt;&gt;mutate(month=_.dates.dt.month_name()\n                  ,date_format_raw = call(pd.to_datetime, _.raw)\n                  ,date_format_raw_year=_.date_format_raw.dt.year\n\n)\n\ndf_date\n\n\n\n\n\n\n\n\n\n\ndates\nraw\nmonth\ndate_format_raw\ndate_format_raw_year\n\n\n\n\n0\n2021-01-02\n2023-04-05 06:07:08\nJanuary\n2023-04-05 06:07:08\n2023\n\n\n1\n2021-02-03\n2024-05-06 07:08:09\nFebruary\n2024-05-06 07:08:09\n2024\n\n\n\n\n\n\n\n\n\n\nCode\ndf_date.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2 entries, 0 to 1\nData columns (total 5 columns):\n #   Column                Non-Null Count  Dtype         \n---  ------                --------------  -----         \n 0   dates                 2 non-null      datetime64[ns]\n 1   raw                   2 non-null      object        \n 2   month                 2 non-null      object        \n 3   date_format_raw       2 non-null      datetime64[ns]\n 4   date_format_raw_year  2 non-null      int32         \ndtypes: datetime64[ns](2), int32(1), object(2)\nmemory usage: 204.0+ bytes",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "data manipulation/4 siuba.html",
    "href": "data manipulation/4 siuba.html",
    "title": "Data manipulation with siuba",
    "section": "",
    "text": "siuba (小巴) is a port of dplyr and other R libraries with seamless support for pandas and SQL",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/4 siuba.html#comparison-with-different-python-dataframe-package",
    "href": "data manipulation/4 siuba.html#comparison-with-different-python-dataframe-package",
    "title": "Data manipulation with siuba",
    "section": "1 Comparison with different python dataframe package",
    "text": "1 Comparison with different python dataframe package",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/4 siuba.html#package-download",
    "href": "data manipulation/4 siuba.html#package-download",
    "title": "Data manipulation with siuba",
    "section": "2 Package download",
    "text": "2 Package download\n\n\nCode\nimport os\nos.system('pip install siuba')",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/4 siuba.html#load-package",
    "href": "data manipulation/4 siuba.html#load-package",
    "title": "Data manipulation with siuba",
    "section": "3 load package",
    "text": "3 load package\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nimport seaborn as sns\n\n\n\nfrom siuba.siu import call\nfrom siuba import _, mutate, filter, group_by, summarize,show_query\nfrom siuba import *\n\nfrom siuba.data import mtcars,penguins\n\n\n\n\nCode\nimport os\nos.system('pip show siuba')\n\n\nName: siuba\nVersion: 0.4.4\nSummary: A package for quick, scrappy analyses with pandas and SQL\nHome-page: https://github.com/machow/siuba\nAuthor: Michael Chow\nAuthor-email: mc_al_gh_siuba@fastmail.com\nLicense: MIT\nLocation: /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages\nRequires: numpy, pandas, PyYAML, SQLAlchemy\nRequired-by: \n\n\n0\n\n\n\n\nCode\nsmall_mtcars = mtcars &gt;&gt; select(_.cyl, _.mpg, _.hp)&gt;&gt; head(5)",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/4 siuba.html#select-column",
    "href": "data manipulation/4 siuba.html#select-column",
    "title": "Data manipulation with siuba",
    "section": "4 select column",
    "text": "4 select column\n\n4.1 get column names\n\n\nCode\nlist(small_mtcars)\n\n\n['cyl', 'mpg', 'hp']\n\n\n\n\n4.2 select columns by name\n\n\nCode\nsmall_mtcars &gt;&gt; select(_.cyl, _.mpg)\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\n\n\n\n\n0\n6\n21.0\n\n\n1\n6\n21.0\n\n\n2\n4\n22.8\n\n\n3\n6\n21.4\n\n\n4\n8\n18.7\n\n\n\n\n\n\n\n\n\n\n4.3 select columns by name match with ‘p’\n\n\nCode\nsmall_mtcars &gt;&gt; select(_.contains(\"p\"))\n\n\n\n\n\n\n\n\n\n\nmpg\nhp\n\n\n\n\n0\n21.0\n110\n\n\n1\n21.0\n110\n\n\n2\n22.8\n93\n\n\n3\n21.4\n110\n\n\n4\n18.7\n175\n\n\n\n\n\n\n\n\n\n\n4.4 select columns by index\n\n4.4.1 select first and 3rd columns\n\n\nCode\nsmall_mtcars &gt;&gt; select(0,2)\n\n\n\n\n\n\n\n\n\n\ncyl\nhp\n\n\n\n\n0\n6\n110\n\n\n1\n6\n110\n\n\n2\n4\n93\n\n\n3\n6\n110\n\n\n4\n8\n175\n\n\n\n\n\n\n\n\n\n\n4.4.2 select first to 3rd columns\n\n\nCode\nsmall_mtcars &gt;&gt; select(_[0:3])\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\nhp\n\n\n\n\n0\n6\n21.0\n110\n\n\n1\n6\n21.0\n110\n\n\n2\n4\n22.8\n93\n\n\n3\n6\n21.4\n110\n\n\n4\n8\n18.7\n175",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/4 siuba.html#drop-column",
    "href": "data manipulation/4 siuba.html#drop-column",
    "title": "Data manipulation with siuba",
    "section": "5 drop column",
    "text": "5 drop column\n\n\nCode\nsmall_mtcars &gt;&gt; select(~_.cyl)\n\n\n\n\n\n\n\n\n\n\nmpg\nhp\n\n\n\n\n0\n21.0\n110\n\n\n1\n21.0\n110\n\n\n2\n22.8\n93\n\n\n3\n21.4\n110\n\n\n4\n18.7\n175",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/4 siuba.html#renaming-column",
    "href": "data manipulation/4 siuba.html#renaming-column",
    "title": "Data manipulation with siuba",
    "section": "6 Renaming column",
    "text": "6 Renaming column\n\n\nCode\nsmall_mtcars &gt;&gt; rename(new_name_mpg = _.mpg)\n\n\n\n\n\n\n\n\n\n\ncyl\nnew_name_mpg\nhp\n\n\n\n\n0\n6\n21.0\n110\n\n\n1\n6\n21.0\n110\n\n\n2\n4\n22.8\n93\n\n\n3\n6\n21.4\n110\n\n\n4\n8\n18.7\n175",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/4 siuba.html#create-column",
    "href": "data manipulation/4 siuba.html#create-column",
    "title": "Data manipulation with siuba",
    "section": "7 Create column",
    "text": "7 Create column\n\n7.1 Mutate\n\n\nCode\nmtcars.head()&gt;&gt; mutate(gear2 = _.gear+1\n                      ,gear3=if_else(_.gear &gt; 3, \"long\", \"short\")\n                       ,qsec2=case_when({\n                                          _.qsec &lt;= 17: \"short\",\n                                          _.qsec &lt;= 18: \"Medium\",\n                                          True: \"long\"\n                                                     })\n                       )\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\ngear2\ngear3\nqsec2\n\n\n\n\n0\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n5\nlong\nshort\n\n\n1\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n5\nlong\nMedium\n\n\n2\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n5\nlong\nlong\n\n\n3\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n4\nshort\nlong\n\n\n4\n18.7\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n4\nshort\nMedium\n\n\n\n\n\n\n\n\n\n\n7.2 Transmute,create column and only keep this column\n\n\nCode\nmtcars.head()&gt;&gt; transmute(gear2 = _.gear+1)\n\n\n\n\n\n\n\n\n\n\ngear2\n\n\n\n\n0\n5\n\n\n1\n5\n\n\n2\n5\n\n\n3\n4\n\n\n4\n4",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/4 siuba.html#filter-rows",
    "href": "data manipulation/4 siuba.html#filter-rows",
    "title": "Data manipulation with siuba",
    "section": "8 Filter rows",
    "text": "8 Filter rows\n\n\nCode\nmtcars&gt;&gt; filter(_.gear ==4)\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n7\n24.4\n4\n146.7\n62\n3.69\n3.190\n20.00\n1\n0\n4\n2\n\n\n8\n22.8\n4\n140.8\n95\n3.92\n3.150\n22.90\n1\n0\n4\n2\n\n\n9\n19.2\n6\n167.6\n123\n3.92\n3.440\n18.30\n1\n0\n4\n4\n\n\n10\n17.8\n6\n167.6\n123\n3.92\n3.440\n18.90\n1\n0\n4\n4\n\n\n17\n32.4\n4\n78.7\n66\n4.08\n2.200\n19.47\n1\n1\n4\n1\n\n\n18\n30.4\n4\n75.7\n52\n4.93\n1.615\n18.52\n1\n1\n4\n2\n\n\n19\n33.9\n4\n71.1\n65\n4.22\n1.835\n19.90\n1\n1\n4\n1\n\n\n25\n27.3\n4\n79.0\n66\n4.08\n1.935\n18.90\n1\n1\n4\n1\n\n\n31\n21.4\n4\n121.0\n109\n4.11\n2.780\n18.60\n1\n1\n4\n2\n\n\n\n\n\n\n\n\n\n8.1 Filters with AND conditions\n\n\nCode\nmtcars &gt;&gt; filter((_.cyl &gt;4) & (_.gear == 5))\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n28\n15.8\n8\n351.0\n264\n4.22\n3.17\n14.5\n0\n1\n5\n4\n\n\n29\n19.7\n6\n145.0\n175\n3.62\n2.77\n15.5\n0\n1\n5\n6\n\n\n30\n15.0\n8\n301.0\n335\n3.54\n3.57\n14.6\n0\n1\n5\n8\n\n\n\n\n\n\n\n\n\n\n8.2 Filters with OR conditions\n\n\nCode\nmtcars &gt;&gt; filter((_.cyl == 6) | (_.gear == 5))\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n3\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n5\n18.1\n6\n225.0\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n\n\n9\n19.2\n6\n167.6\n123\n3.92\n3.440\n18.30\n1\n0\n4\n4\n\n\n10\n17.8\n6\n167.6\n123\n3.92\n3.440\n18.90\n1\n0\n4\n4\n\n\n26\n26.0\n4\n120.3\n91\n4.43\n2.140\n16.70\n0\n1\n5\n2\n\n\n27\n30.4\n4\n95.1\n113\n3.77\n1.513\n16.90\n1\n1\n5\n2\n\n\n28\n15.8\n8\n351.0\n264\n4.22\n3.170\n14.50\n0\n1\n5\n4\n\n\n29\n19.7\n6\n145.0\n175\n3.62\n2.770\n15.50\n0\n1\n5\n6\n\n\n30\n15.0\n8\n301.0\n335\n3.54\n3.570\n14.60\n0\n1\n5\n8\n\n\n\n\n\n\n\n\n\n\n8.3 filter row with index\n\n8.3.1 first 3\n\n\nCode\nsmall_mtcars&gt;&gt;head(3)\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\nhp\n\n\n\n\n0\n6\n21.0\n110\n\n\n1\n6\n21.0\n110\n\n\n2\n4\n22.8\n93\n\n\n\n\n\n\n\n\n\n\n8.3.2 last 3\n\n\nCode\n# not in siuba, in pandas\nsmall_mtcars.tail(3)\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\nhp\n\n\n\n\n2\n4\n22.8\n93\n\n\n3\n6\n21.4\n110\n\n\n4\n8\n18.7\n175\n\n\n\n\n\n\n\n\n\n\n8.3.3 5th rows\n\n\nCode\n# not in siuba, in pandas\nmtcars.iloc[[4]]\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n4\n18.7\n8\n360.0\n175\n3.15\n3.44\n17.02\n0\n0\n3\n2\n\n\n\n\n\n\n\n\n\n\n8.3.4 1 and 5th rows\n\n\nCode\n# not in siuba, in pandas\nmtcars.iloc[[0,4]]\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\n21.0\n6\n160.0\n110\n3.90\n2.62\n16.46\n0\n1\n4\n4\n\n\n4\n18.7\n8\n360.0\n175\n3.15\n3.44\n17.02\n0\n0\n3\n2\n\n\n\n\n\n\n\n\n\n\n8.3.5 1 to 5th rows\n\n\nCode\n# not in siuba, in pandas\nmtcars.iloc[0:4]\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n3\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n\n\n\n\n\n\n\n\n8.3.6 get ramdon 5 rows\n\n\nCode\nmtcars.sample(5, random_state=42)\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n29\n19.7\n6\n145.0\n175\n3.62\n2.770\n15.50\n0\n1\n5\n6\n\n\n15\n10.4\n8\n460.0\n215\n3.00\n5.424\n17.82\n0\n0\n3\n4\n\n\n24\n19.2\n8\n400.0\n175\n3.08\n3.845\n17.05\n0\n0\n3\n2\n\n\n17\n32.4\n4\n78.7\n66\n4.08\n2.200\n19.47\n1\n1\n4\n1\n\n\n8\n22.8\n4\n140.8\n95\n3.92\n3.150\n22.90\n1\n0\n4\n2",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/4 siuba.html#append",
    "href": "data manipulation/4 siuba.html#append",
    "title": "Data manipulation with siuba",
    "section": "9 Append",
    "text": "9 Append\n\n9.1 append by row\n\n\nCode\n# not available in siuba yet\n#from siuba import bind_rows\n\n\n\n\nCode\n# using pandas\n\n# get 1 to 4 rows\ndata1=mtcars.iloc[0:4]\n\n# get 9 rows\ndata2=mtcars.iloc[10:11]\n\ndata3=pd.concat([data1, data2], ignore_index = True,axis=0)\n\ndata3\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n3\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n4\n17.8\n6\n167.6\n123\n3.92\n3.440\n18.90\n1\n0\n4\n4\n\n\n\n\n\n\n\n\n\n\n9.2 append by column\n\n\nCode\n# not available in siuba yet\n#from siuba import bind_columns\n\n\n\n\nCode\n# using pandas\ndata1=small_mtcars&gt;&gt;select(_.mpg)\n\ndata2=small_mtcars&gt;&gt;select(_.cyl)\n\ndata3=pd.concat([data1, data2],axis=1)\n\ndata3\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\n\n\n\n\n0\n21.0\n6\n\n\n1\n21.0\n6\n\n\n2\n22.8\n4\n\n\n3\n21.4\n6\n\n\n4\n18.7\n8\n\n\n\n\n\n\n\n\n\n\n9.3 Dropping NA values\n\n\n9.4 keep NA values",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/4 siuba.html#group-by",
    "href": "data manipulation/4 siuba.html#group-by",
    "title": "Data manipulation with siuba",
    "section": "10 group by",
    "text": "10 group by\n\n10.1 average,min,max,sum\n\n\nCode\ntbl_query = (mtcars\n  &gt;&gt; group_by(_.cyl)\n  &gt;&gt; summarize(avg_hp = _.hp.mean()\n              ,min_hp=_.hp.min()\n              ,max_hp=_.hp.max()\n              ,totol_disp=_.disp.sum()\n  )\n  )\n\ntbl_query\n\n\n\n\n\n\n\n\n\n\ncyl\navg_hp\nmin_hp\nmax_hp\ntotol_disp\n\n\n\n\n0\n4\n82.636364\n52\n113\n1156.5\n\n\n1\n6\n122.285714\n105\n175\n1283.2\n\n\n2\n8\n209.214286\n150\n335\n4943.4\n\n\n\n\n\n\n\n\n\n\n10.2 count record and count distinct record\n\n\nCode\nmtcars &gt;&gt; group_by(_.cyl)  &gt;&gt; summarize(n = _.shape[0])\n\n\n\n\n\n\n\n\n\n\ncyl\nn\n\n\n\n\n0\n4\n11\n\n\n1\n6\n7\n\n\n2\n8\n14\n\n\n\n\n\n\n\n\n\n\nCode\nmtcars &gt;&gt; group_by(_.cyl)  &gt;&gt; summarize(n = _.hp.nunique())\n\n\n\n\n\n\n\n\n\n\ncyl\nn\n\n\n\n\n0\n4\n10\n\n\n1\n6\n4\n\n\n2\n8\n9",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/4 siuba.html#order-rows",
    "href": "data manipulation/4 siuba.html#order-rows",
    "title": "Data manipulation with siuba",
    "section": "11 order rows",
    "text": "11 order rows\n\n\nCode\nsmall_mtcars &gt;&gt; arrange(_.hp)\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\nhp\n\n\n\n\n2\n4\n22.8\n93\n\n\n0\n6\n21.0\n110\n\n\n1\n6\n21.0\n110\n\n\n3\n6\n21.4\n110\n\n\n4\n8\n18.7\n175\n\n\n\n\n\n\n\n\n\n11.1 Sort in descending order\n\n\nCode\nsmall_mtcars &gt;&gt; arrange(-_.hp)\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\nhp\n\n\n\n\n4\n8\n18.7\n175\n\n\n0\n6\n21.0\n110\n\n\n1\n6\n21.0\n110\n\n\n3\n6\n21.4\n110\n\n\n2\n4\n22.8\n93\n\n\n\n\n\n\n\n\n\n\n11.2 Arrange by multiple variables\n\n\nCode\nsmall_mtcars &gt;&gt; arrange(_.cyl, -_.mpg)\n\n\n\n\n\n\n\n\n\n\ncyl\nmpg\nhp\n\n\n\n\n2\n4\n22.8\n93\n\n\n3\n6\n21.4\n110\n\n\n0\n6\n21.0\n110\n\n\n1\n6\n21.0\n110\n\n\n4\n8\n18.7\n175",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/4 siuba.html#join",
    "href": "data manipulation/4 siuba.html#join",
    "title": "Data manipulation with siuba",
    "section": "12 join",
    "text": "12 join\n\n\nCode\nlhs = pd.DataFrame({'id': [1,2,3], 'val': ['lhs.1', 'lhs.2', 'lhs.3']})\nrhs = pd.DataFrame({'id': [1,2,4], 'val': ['rhs.1', 'rhs.2', 'rhs.3']})\n\n\n\n\nCode\nlhs\n\n\n\n\n\n\n\n\n\n\nid\nval\n\n\n\n\n0\n1\nlhs.1\n\n\n1\n2\nlhs.2\n\n\n2\n3\nlhs.3\n\n\n\n\n\n\n\n\n\n\nCode\nrhs\n\n\n\n\n\n\n\n\n\n\nid\nval\n\n\n\n\n0\n1\nrhs.1\n\n\n1\n2\nrhs.2\n\n\n2\n4\nrhs.3\n\n\n\n\n\n\n\n\n\n12.1 inner_join\n\n\nCode\nresult=lhs &gt;&gt; inner_join(_, rhs, on=\"id\")\nresult\n\n\n\n\n\n\n\n\n\n\nid\nval_x\nval_y\n\n\n\n\n0\n1\nlhs.1\nrhs.1\n\n\n1\n2\nlhs.2\nrhs.2\n\n\n\n\n\n\n\n\n\n\n12.2 full join\n\n\nCode\nresult=rhs &gt;&gt; full_join(_, lhs, on=\"id\")\nresult\n\n\n\n\n\n\n\n\n\n\nid\nval_x\nval_y\n\n\n\n\n0\n1\nrhs.1\nlhs.1\n\n\n1\n2\nrhs.2\nlhs.2\n\n\n2\n3\nNaN\nlhs.3\n\n\n3\n4\nrhs.3\nNaN\n\n\n\n\n\n\n\n\n\n\n12.3 left join\n\n\nCode\nresult=lhs &gt;&gt; left_join(_, rhs, on=\"id\")\nresult\n\n\n\n\n\n\n\n\n\n\nid\nval_x\nval_y\n\n\n\n\n0\n1\nlhs.1\nrhs.1\n\n\n1\n2\nlhs.2\nrhs.2\n\n\n2\n3\nlhs.3\nNaN\n\n\n\n\n\n\n\n\n\n\n12.4 anti join\nkeep data in left which not in right\n\n\nCode\nresult=lhs &gt;&gt; anti_join(_, rhs, on=\"id\")\nresult\n\n\n\n\n\n\n\n\n\n\nid\nval\n\n\n\n\n2\n3\nlhs.3\n\n\n\n\n\n\n\n\nkeep data in right which not in left\n\n\nCode\nresult=rhs &gt;&gt; anti_join(_, lhs, on=\"id\")\nresult\n\n\n\n\n\n\n\n\n\n\nid\nval\n\n\n\n\n2\n4\nrhs.3",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/4 siuba.html#reshape-tables",
    "href": "data manipulation/4 siuba.html#reshape-tables",
    "title": "Data manipulation with siuba",
    "section": "13 Reshape tables",
    "text": "13 Reshape tables\n\n\nCode\ncosts = pd.DataFrame({\n    'id': [1,2],\n    'price_x': [.1, .2],\n    'price_y': [.4, .5],\n    'price_z': [.7, .8]\n})\n\ncosts\n\n\n\n\n\n\n\n\n\n\nid\nprice_x\nprice_y\nprice_z\n\n\n\n\n0\n1\n0.1\n0.4\n0.7\n\n\n1\n2\n0.2\n0.5\n0.8\n\n\n\n\n\n\n\n\n\n13.1 Gather data long(wide to long)\nBelow 3 method will give same result\n\n\nCode\n# selecting each variable manually\ncosts &gt;&gt; gather('measure', 'value', _.price_x, _.price_y, _.price_z)\n\n\n\n\n\n\n\n\n\n\nid\nmeasure\nvalue\n\n\n\n\n0\n1\nprice_x\n0.1\n\n\n1\n2\nprice_x\n0.2\n\n\n2\n1\nprice_y\n0.4\n\n\n3\n2\nprice_y\n0.5\n\n\n4\n1\nprice_z\n0.7\n\n\n5\n2\nprice_z\n0.8\n\n\n\n\n\n\n\n\nother way:\n\n\nCode\n# selecting variables using a slice\ncosts &gt;&gt; gather('measure', 'value', _[\"price_x\":\"price_z\"])\n\n\n\n\n\n\n\n\n\n\nid\nmeasure\nvalue\n\n\n\n\n0\n1\nprice_x\n0.1\n\n\n1\n2\nprice_x\n0.2\n\n\n2\n1\nprice_y\n0.4\n\n\n3\n2\nprice_y\n0.5\n\n\n4\n1\nprice_z\n0.7\n\n\n5\n2\nprice_z\n0.8\n\n\n\n\n\n\n\n\nother way:\n\n\nCode\n# selecting by excluding id\ncosts &gt;&gt; gather('measure', 'value', -_.id)\n\n\n\n\n\n\n\n\n\n\nid\nmeasure\nvalue\n\n\n\n\n0\n1\nprice_x\n0.1\n\n\n1\n2\nprice_x\n0.2\n\n\n2\n1\nprice_y\n0.4\n\n\n3\n2\nprice_y\n0.5\n\n\n4\n1\nprice_z\n0.7\n\n\n5\n2\nprice_z\n0.8\n\n\n\n\n\n\n\n\n\n\n13.2 Spread data wide(long to wide)\n\n\nCode\ncosts_long= costs&gt;&gt; gather('measure', 'value', -_.id)\ncosts_long\n\n\n\n\n\n\n\n\n\n\nid\nmeasure\nvalue\n\n\n\n\n0\n1\nprice_x\n0.1\n\n\n1\n2\nprice_x\n0.2\n\n\n2\n1\nprice_y\n0.4\n\n\n3\n2\nprice_y\n0.5\n\n\n4\n1\nprice_z\n0.7\n\n\n5\n2\nprice_z\n0.8\n\n\n\n\n\n\n\n\n\n\nCode\ncosts_long&gt;&gt; spread('measure', 'value')\n\n\n\n\n\n\n\n\n\n\nid\nprice_x\nprice_y\nprice_z\n\n\n\n\n0\n1\n0.1\n0.4\n0.7\n\n\n1\n2\n0.2\n0.5\n0.8",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/4 siuba.html#string",
    "href": "data manipulation/4 siuba.html#string",
    "title": "Data manipulation with siuba",
    "section": "14 string",
    "text": "14 string\n\n\nCode\ndf = pd.DataFrame({'text': ['abc', 'DDD','1243c','aeEe'], 'num': [3, 4,7,8]})\n\ndf\n\n\n\n\n\n\n\n\n\n\ntext\nnum\n\n\n\n\n0\nabc\n3\n\n\n1\nDDD\n4\n\n\n2\n1243c\n7\n\n\n3\naeEe\n8\n\n\n\n\n\n\n\n\n\n14.1 upper case\n\n\nCode\ndf&gt;&gt; mutate(text_new=_.text.str.upper())\n\n\n\n\n\n\n\n\n\n\ntext\nnum\ntext_new\n\n\n\n\n0\nabc\n3\nABC\n\n\n1\nDDD\n4\nDDD\n\n\n2\n1243c\n7\n1243C\n\n\n3\naeEe\n8\nAEEE\n\n\n\n\n\n\n\n\n\n\n14.2 lower case\n\n\nCode\ndf&gt;&gt; mutate(text_new=_.text.str.lower())\n\n\n\n\n\n\n\n\n\n\ntext\nnum\ntext_new\n\n\n\n\n0\nabc\n3\nabc\n\n\n1\nDDD\n4\nddd\n\n\n2\n1243c\n7\n1243c\n\n\n3\naeEe\n8\naeee\n\n\n\n\n\n\n\n\n\n\n14.3 match\n\n\nCode\ndf&gt;&gt; mutate(text_new1=if_else(_.text== \"abc\",'T','F')\n            ,text_new2=if_else(_.text.str.startswith(\"a\"),'T','F')\n            ,text_new3=if_else(_.text.str.endswith(\"c\"),'T','F')\n            ,text_new4=if_else(_.text.str.contains(\"4\"),'T','F')\n\n)\n\n\n\n\n\n\n\n\n\n\ntext\nnum\ntext_new1\ntext_new2\ntext_new3\ntext_new4\n\n\n\n\n0\nabc\n3\nT\nT\nT\nF\n\n\n1\nDDD\n4\nF\nF\nF\nF\n\n\n2\n1243c\n7\nF\nF\nT\nT\n\n\n3\naeEe\n8\nF\nT\nF\nF\n\n\n\n\n\n\n\n\n\n\n14.4 concatenation\n\n\nCode\ndf&gt;&gt; mutate(text_new1=_.text+' is '+_.text\n)\n\n\n\n\n\n\n\n\n\n\ntext\nnum\ntext_new1\n\n\n\n\n0\nabc\n3\nabc is abc\n\n\n1\nDDD\n4\nDDD is DDD\n\n\n2\n1243c\n7\n1243c is 1243c\n\n\n3\naeEe\n8\naeEe is aeEe\n\n\n\n\n\n\n\n\n\n\n14.5 replace\nUse .str.replace(…, regex=True) with regular expressions to replace patterns in strings.\nFor example, the code below uses “p.”, where . is called a wildcard–which matches any character.\n\n\nCode\ndf&gt;&gt; mutate(text_new1=_.text.str.replace(\"a.\", \"XX\", regex=True)\n)\n\n\n\n\n\n\n\n\n\n\ntext\nnum\ntext_new1\n\n\n\n\n0\nabc\n3\nXXc\n\n\n1\nDDD\n4\nDDD\n\n\n2\n1243c\n7\n1243c\n\n\n3\naeEe\n8\nXXEe\n\n\n\n\n\n\n\n\n\n\n14.6 extract\nUse str.extract() with a regular expression to pull out a matching piece of text.\nFor example the regular expression “^(.*) ” contains the following pieces:\n\na matches the literal letter “a”\n.* has a . which matches anything, and * which modifies it to apply 0 or more times.\n\n\n\nCode\ndf&gt;&gt; mutate(text_new1=_.text.str.extract(\"a(.*)\")\n            ,text_new2=_.text.str.extract(\"(.*)c\")\n)\n\n\n\n\n\n\n\n\n\n\ntext\nnum\ntext_new1\ntext_new2\n\n\n\n\n0\nabc\n3\nbc\nab\n\n\n1\nDDD\n4\nNaN\nNaN\n\n\n2\n1243c\n7\nNaN\n1243\n\n\n3\naeEe\n8\neEe\nNaN",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/4 siuba.html#date",
    "href": "data manipulation/4 siuba.html#date",
    "title": "Data manipulation with siuba",
    "section": "15 date",
    "text": "15 date\n\n\nCode\ndf_dates = pd.DataFrame({\n    \"dates\": pd.to_datetime([\"2021-01-02\", \"2021-02-03\"]),\n    \"raw\": [\"2023-04-05 06:07:08\", \"2024-05-06 07:08:09\"],\n})\ndf_dates\n\n\n\n\n\n\n\n\n\n\ndates\nraw\n\n\n\n\n0\n2021-01-02\n2023-04-05 06:07:08\n\n\n1\n2021-02-03\n2024-05-06 07:08:09\n\n\n\n\n\n\n\n\n\n\nCode\nfrom datetime import datetime\n\ndf_date=df_dates&gt;&gt;mutate(month=_.dates.dt.month_name()\n                  ,date_format_raw = call(pd.to_datetime, _.raw)\n                  ,date_format_raw_year=_.date_format_raw.dt.year\n\n)\n\ndf_date\n\n\n\n\n\n\n\n\n\n\ndates\nraw\nmonth\ndate_format_raw\ndate_format_raw_year\n\n\n\n\n0\n2021-01-02\n2023-04-05 06:07:08\nJanuary\n2023-04-05 06:07:08\n2023\n\n\n1\n2021-02-03\n2024-05-06 07:08:09\nFebruary\n2024-05-06 07:08:09\n2024\n\n\n\n\n\n\n\n\n\n\nCode\ndf_date.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2 entries, 0 to 1\nData columns (total 5 columns):\n #   Column                Non-Null Count  Dtype         \n---  ------                --------------  -----         \n 0   dates                 2 non-null      datetime64[ns]\n 1   raw                   2 non-null      object        \n 2   month                 2 non-null      object        \n 3   date_format_raw       2 non-null      datetime64[ns]\n 4   date_format_raw_year  2 non-null      int32         \ndtypes: datetime64[ns](2), int32(1), object(2)\nmemory usage: 204.0+ bytes",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/4 siuba.html#using-siuba-with-database",
    "href": "data manipulation/4 siuba.html#using-siuba-with-database",
    "title": "Data manipulation with siuba",
    "section": "16 using siuba with database",
    "text": "16 using siuba with database\n\n16.1 set up a sqlite database, with an mtcars table.\n\n\nCode\nfrom sqlalchemy import create_engine\nfrom siuba.sql import LazyTbl\nfrom siuba import _, group_by, summarize, show_query, collect \nfrom siuba.data import mtcars\n\n# copy in to sqlite, using the pandas .to_sql() method\nengine = create_engine(\"sqlite:///:memory:\")\nmtcars.to_sql(\"mtcars\", engine, if_exists = \"replace\")\n\n\n32\n\n\n\n\n16.2 create table\n\n\nCode\n# Create a lazy SQL DataFrame\ntbl_mtcars = LazyTbl(engine, \"mtcars\")\ntbl_mtcars\n\n\n\n# Source: lazy query\n# DB Conn: Engine(sqlite:///:memory:)\n# Preview:\n\n\n\n\n\n\n\nindex\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\n0\n0\n21.0\n6\n160.0\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\n1\n1\n21.0\n6\n160.0\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\n2\n2\n22.8\n4\n108.0\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\n3\n3\n21.4\n6\n258.0\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\n4\n4\n18.7\n8\n360.0\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\n\n\n# .. may have more rows\n\n\n\n\n\n16.3 create query\n\n\nCode\n# connect with siuba\n\ntbl_query = (tbl_mtcars\n  &gt;&gt; group_by(_.mpg)\n  &gt;&gt; summarize(avg_hp = _.hp.mean())\n  )\n\ntbl_query\n\n\n\n# Source: lazy query\n# DB Conn: Engine(sqlite:///:memory:)\n# Preview:\n\n\n\n\n\n\n\nmpg\navg_hp\n\n\n\n\n0\n10.4\n210.0\n\n\n1\n13.3\n245.0\n\n\n2\n14.3\n245.0\n\n\n3\n14.7\n230.0\n\n\n4\n15.0\n335.0\n\n\n\n\n# .. may have more rows\n\n\n\n\n\n16.4 show query\n\n\nCode\n tbl_query &gt;&gt; show_query()\n\n\nSELECT mtcars.mpg, avg(mtcars.hp) AS avg_hp \nFROM mtcars GROUP BY mtcars.mpg\n\n\n\n# Source: lazy query\n# DB Conn: Engine(sqlite:///:memory:)\n# Preview:\n\n\n\n\n\n\n\nmpg\navg_hp\n\n\n\n\n0\n10.4\n210.0\n\n\n1\n13.3\n245.0\n\n\n2\n14.3\n245.0\n\n\n3\n14.7\n230.0\n\n\n4\n15.0\n335.0\n\n\n\n\n# .. may have more rows\n\n\n\n\n\n16.5 Collect to DataFrame\nbecause lazy expressions,the collect function is actually running the sql.\n\n\nCode\ndata=tbl_query &gt;&gt; collect()\nprint(data)\n\n\n     mpg  avg_hp\n0   10.4   210.0\n1   13.3   245.0\n2   14.3   245.0\n3   14.7   230.0\n4   15.0   335.0\n5   15.2   165.0\n6   15.5   150.0\n7   15.8   264.0\n8   16.4   180.0\n9   17.3   180.0\n10  17.8   123.0\n11  18.1   105.0\n12  18.7   175.0\n13  19.2   149.0\n14  19.7   175.0\n15  21.0   110.0\n16  21.4   109.5\n17  21.5    97.0\n18  22.8    94.0\n19  24.4    62.0\n20  26.0    91.0\n21  27.3    66.0\n22  30.4    82.5\n23  32.4    66.0\n24  33.9    65.0",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/4 siuba.html#reference",
    "href": "data manipulation/4 siuba.html#reference",
    "title": "Data manipulation with siuba",
    "section": "17 reference:",
    "text": "17 reference:\nhttps://siuba.org/",
    "crumbs": [
      "data manipulation",
      "Data manipulation with siuba"
    ]
  },
  {
    "objectID": "data manipulation/2 data structure in Python .html#numpy-array",
    "href": "data manipulation/2 data structure in Python .html#numpy-array",
    "title": "Data structure in Python",
    "section": "2.1 numpy Array",
    "text": "2.1 numpy Array\n\n\nCode\nimport numpy as np\n\nA2 = np.array([[1, 2, 3], [3, 4, 5]])\nprint(A2)\n\n\n[[1 2 3]\n [3 4 5]]\n\n\n\n\nCode\ntype(A2)\n\n\nnumpy.ndarray",
    "crumbs": [
      "data manipulation",
      "Data structure in Python"
    ]
  },
  {
    "objectID": "data manipulation/2 data structure in Python .html#shape",
    "href": "data manipulation/2 data structure in Python .html#shape",
    "title": "Data structure in Python",
    "section": "2.2 shape",
    "text": "2.2 shape\n\n\nCode\nA2.shape\n\n\n(2, 3)",
    "crumbs": [
      "data manipulation",
      "Data structure in Python"
    ]
  },
  {
    "objectID": "data manipulation/2 data structure in Python .html#row-number",
    "href": "data manipulation/2 data structure in Python .html#row-number",
    "title": "Data structure in Python",
    "section": "2.3 row number",
    "text": "2.3 row number\n\n\nCode\nlen(A2)\n\n\n2",
    "crumbs": [
      "data manipulation",
      "Data structure in Python"
    ]
  },
  {
    "objectID": "data manipulation/2 data structure in Python .html#total-elements",
    "href": "data manipulation/2 data structure in Python .html#total-elements",
    "title": "Data structure in Python",
    "section": "2.4 total elements",
    "text": "2.4 total elements\n\n\nCode\nA2.size\n\n\n6",
    "crumbs": [
      "data manipulation",
      "Data structure in Python"
    ]
  },
  {
    "objectID": "clustering/1 k mean  Clustering.html#download-data",
    "href": "clustering/1 k mean  Clustering.html#download-data",
    "title": "K mean Clustering",
    "section": "2.1 download data",
    "text": "2.1 download data\nhttps://www.kaggle.com/datasets/shwetabh123/mall-customer",
    "crumbs": [
      "Clustering",
      "K mean Clustering"
    ]
  },
  {
    "objectID": "clustering/1 k mean  Clustering.html#input-data",
    "href": "clustering/1 k mean  Clustering.html#input-data",
    "title": "K mean Clustering",
    "section": "2.2 input data",
    "text": "2.2 input data\n\n\nCode\n# Loading the data\ndataset = pd.read_csv('./data/Mall_Customers.csv')\n\n# Showing overview of the train dataset\ndataset.head()\n\n\n\n\n\n\n\n\n\n\nCustomerID\nGenre\nAge\nAnnual Income (k$)\nSpending Score (1-100)\n\n\n\n\n0\n1\nMale\n19\n15\n39\n\n\n1\n2\nMale\n21\n15\n81\n\n\n2\n3\nFemale\n20\n16\n6\n\n\n3\n4\nFemale\n23\n16\n77\n\n\n4\n5\nFemale\n31\n17\n40",
    "crumbs": [
      "Clustering",
      "K mean Clustering"
    ]
  },
  {
    "objectID": "clustering/1 k mean  Clustering.html#data-eda",
    "href": "clustering/1 k mean  Clustering.html#data-eda",
    "title": "K mean Clustering",
    "section": "2.3 data EDA",
    "text": "2.3 data EDA",
    "crumbs": [
      "Clustering",
      "K mean Clustering"
    ]
  },
  {
    "objectID": "clustering/1 k mean  Clustering.html#annual-income-k",
    "href": "clustering/1 k mean  Clustering.html#annual-income-k",
    "title": "K mean Clustering",
    "section": "5.1 ‘Annual Income (k$)’",
    "text": "5.1 ‘Annual Income (k$)’\n\n\nCode\nfrom plotnine import *\n\np=(\n)\n\np\n\n\n()",
    "crumbs": [
      "Clustering",
      "K mean Clustering"
    ]
  },
  {
    "objectID": "clustering/1 k mean  Clustering.html#spending-score-1-100",
    "href": "clustering/1 k mean  Clustering.html#spending-score-1-100",
    "title": "K mean Clustering",
    "section": "5.2 ‘Spending Score (1-100)’",
    "text": "5.2 ‘Spending Score (1-100)’\n\n\nCode\nfrom plotnine import *\n\np=(\n    ggplot(data=dataset)+aes(x='group_no',y='Spending Score (1-100)',fill=\"group_no\")+geom_boxplot()\n)\n\np",
    "crumbs": [
      "Clustering",
      "K mean Clustering"
    ]
  },
  {
    "objectID": "clustering/1 k mean  Clustering.html#age",
    "href": "clustering/1 k mean  Clustering.html#age",
    "title": "K mean Clustering",
    "section": "5.3 Age",
    "text": "5.3 Age\n\n\nCode\nfrom plotnine import *\n\np=(\n    ggplot(data=dataset)+aes(x='group_no',y='Age',fill=\"group_no\")+geom_boxplot()\n)\n\np",
    "crumbs": [
      "Clustering",
      "K mean Clustering"
    ]
  },
  {
    "objectID": "clustering/3 k mode Clustering.html",
    "href": "clustering/3 k mode Clustering.html",
    "title": "k mode Clustering",
    "section": "",
    "text": "Coming soon",
    "crumbs": [
      "Clustering",
      "k mode Clustering"
    ]
  },
  {
    "objectID": "clustering/3 k mode Clustering.html#download-data",
    "href": "clustering/3 k mode Clustering.html#download-data",
    "title": "k mode Clustering",
    "section": "2.1 download data",
    "text": "2.1 download data\nhttps://www.kaggle.com/datasets/shwetabh123/mall-customer",
    "crumbs": [
      "Clustering",
      "k mode Clustering"
    ]
  },
  {
    "objectID": "clustering/3 k mode Clustering.html#input-data",
    "href": "clustering/3 k mode Clustering.html#input-data",
    "title": "k mode Clustering",
    "section": "2.2 input data",
    "text": "2.2 input data\n\n\nCode\n# Loading the data\ndataset = pd.read_csv('./data/Mall_Customers.csv')\n\n# Showing overview of the train dataset\ndataset.head()\n\n\n\n\n\n\n\n\n\n\nCustomerID\nGenre\nAge\nAnnual Income (k$)\nSpending Score (1-100)\n\n\n\n\n0\n1\nMale\n19\n15\n39\n\n\n1\n2\nMale\n21\n15\n81\n\n\n2\n3\nFemale\n20\n16\n6\n\n\n3\n4\nFemale\n23\n16\n77\n\n\n4\n5\nFemale\n31\n17\n40",
    "crumbs": [
      "Clustering",
      "k mode Clustering"
    ]
  },
  {
    "objectID": "clustering/3 k mode Clustering.html#data-eda",
    "href": "clustering/3 k mode Clustering.html#data-eda",
    "title": "k mode Clustering",
    "section": "2.3 data EDA",
    "text": "2.3 data EDA",
    "crumbs": [
      "Clustering",
      "k mode Clustering"
    ]
  },
  {
    "objectID": "clustering/2 Hierarchical Clustering.html",
    "href": "clustering/2 Hierarchical Clustering.html",
    "title": "Hierarchical Clustering",
    "section": "",
    "text": "Coming soon",
    "crumbs": [
      "Clustering",
      "Hierarchical Clustering"
    ]
  },
  {
    "objectID": "clustering/2 Hierarchical Clustering.html#download-data",
    "href": "clustering/2 Hierarchical Clustering.html#download-data",
    "title": "Hierarchical Clustering",
    "section": "2.1 download data",
    "text": "2.1 download data\nhttps://www.kaggle.com/datasets/shwetabh123/mall-customer",
    "crumbs": [
      "Clustering",
      "Hierarchical Clustering"
    ]
  },
  {
    "objectID": "clustering/2 Hierarchical Clustering.html#input-data",
    "href": "clustering/2 Hierarchical Clustering.html#input-data",
    "title": "Hierarchical Clustering",
    "section": "2.2 input data",
    "text": "2.2 input data\n\n\nCode\n# Loading the data\ndataset = pd.read_csv('./data/Mall_Customers.csv')\n\n# Showing overview of the train dataset\ndataset.head()\n\n\n\n\n\n\n\n\n\n\nCustomerID\nGenre\nAge\nAnnual Income (k$)\nSpending Score (1-100)\n\n\n\n\n0\n1\nMale\n19\n15\n39\n\n\n1\n2\nMale\n21\n15\n81\n\n\n2\n3\nFemale\n20\n16\n6\n\n\n3\n4\nFemale\n23\n16\n77\n\n\n4\n5\nFemale\n31\n17\n40",
    "crumbs": [
      "Clustering",
      "Hierarchical Clustering"
    ]
  },
  {
    "objectID": "clustering/2 Hierarchical Clustering.html#data-eda",
    "href": "clustering/2 Hierarchical Clustering.html#data-eda",
    "title": "Hierarchical Clustering",
    "section": "2.3 data EDA",
    "text": "2.3 data EDA",
    "crumbs": [
      "Clustering",
      "Hierarchical Clustering"
    ]
  },
  {
    "objectID": "clustering/1 k mean Clustering.html#download-data",
    "href": "clustering/1 k mean Clustering.html#download-data",
    "title": "K mean Clustering",
    "section": "2.1 download data",
    "text": "2.1 download data\nhttps://www.kaggle.com/datasets/shwetabh123/mall-customer",
    "crumbs": [
      "Clustering",
      "K mean Clustering"
    ]
  },
  {
    "objectID": "clustering/1 k mean Clustering.html#input-data",
    "href": "clustering/1 k mean Clustering.html#input-data",
    "title": "K mean Clustering",
    "section": "2.2 input data",
    "text": "2.2 input data\n\n\nCode\n# Loading the data\ndataset = pd.read_csv('./data/Mall_Customers.csv')\n\n# Showing overview of the train dataset\ndataset.head()\n\n\n\n\n\n\n\n\n\n\nCustomerID\nGenre\nAge\nAnnual Income (k$)\nSpending Score (1-100)\n\n\n\n\n0\n1\nMale\n19\n15\n39\n\n\n1\n2\nMale\n21\n15\n81\n\n\n2\n3\nFemale\n20\n16\n6\n\n\n3\n4\nFemale\n23\n16\n77\n\n\n4\n5\nFemale\n31\n17\n40",
    "crumbs": [
      "Clustering",
      "K mean Clustering"
    ]
  },
  {
    "objectID": "clustering/1 k mean Clustering.html#data-eda",
    "href": "clustering/1 k mean Clustering.html#data-eda",
    "title": "K mean Clustering",
    "section": "2.3 data EDA",
    "text": "2.3 data EDA",
    "crumbs": [
      "Clustering",
      "K mean Clustering"
    ]
  },
  {
    "objectID": "clustering/1 k mean Clustering.html#annual-income-k",
    "href": "clustering/1 k mean Clustering.html#annual-income-k",
    "title": "K mean Clustering",
    "section": "5.1 ‘Annual Income (k$)’",
    "text": "5.1 ‘Annual Income (k$)’\n\n\nCode\nfrom plotnine import *\n\np=(\n)\n\np\n\n\n()",
    "crumbs": [
      "Clustering",
      "K mean Clustering"
    ]
  },
  {
    "objectID": "clustering/1 k mean Clustering.html#spending-score-1-100",
    "href": "clustering/1 k mean Clustering.html#spending-score-1-100",
    "title": "K mean Clustering",
    "section": "5.2 ‘Spending Score (1-100)’",
    "text": "5.2 ‘Spending Score (1-100)’\n\n\nCode\nfrom plotnine import *\n\np=(\n    ggplot(data=dataset)+aes(x='group_no',y='Spending Score (1-100)',fill=\"group_no\")+geom_boxplot()\n)\n\np",
    "crumbs": [
      "Clustering",
      "K mean Clustering"
    ]
  },
  {
    "objectID": "clustering/1 k mean Clustering.html#age",
    "href": "clustering/1 k mean Clustering.html#age",
    "title": "K mean Clustering",
    "section": "5.3 Age",
    "text": "5.3 Age\n\n\nCode\nfrom plotnine import *\n\np=(\n    ggplot(data=dataset)+aes(x='group_no',y='Age',fill=\"group_no\")+geom_boxplot()\n)\n\np",
    "crumbs": [
      "Clustering",
      "K mean Clustering"
    ]
  },
  {
    "objectID": "classification2/8 Multiple Models using Pipeline and tuning.html",
    "href": "classification2/8 Multiple Models using Pipeline and tuning.html",
    "title": "Multiple models using pipeline,fast tuning",
    "section": "",
    "text": "Code\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\n\nfrom siuba.siu import call\nfrom siuba import _, mutate, filter, group_by, summarize,show_query\n\nimport time\nfrom sklearn.experimental import enable_halving_search_cv  # noqa\nfrom sklearn.model_selection import HalvingGridSearchCV",
    "crumbs": [
      "Classification2",
      "Multiple models using pipeline,fast tuning"
    ]
  },
  {
    "objectID": "classification2/8 Multiple Models using Pipeline and tuning.html#download-data",
    "href": "classification2/8 Multiple Models using Pipeline and tuning.html#download-data",
    "title": "Multiple models using pipeline,fast tuning",
    "section": "2.1 download data",
    "text": "2.1 download data\n\n\nCode\nimport pandas as pd\n#url='https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv'\n#hotels=pd.read_csv(url)\n\nhotels=pd.read_csv('data/hotels.csv')\n\n\n\n\nCode\nhotels.head()\n\n\n\n\n\n\n\n\n\n\nhotel\nis_canceled\nlead_time\narrival_date_year\narrival_date_month\narrival_date_week_number\narrival_date_day_of_month\nstays_in_weekend_nights\nstays_in_week_nights\nadults\n...\ndeposit_type\nagent\ncompany\ndays_in_waiting_list\ncustomer_type\nadr\nrequired_car_parking_spaces\ntotal_of_special_requests\nreservation_status\nreservation_status_date\n\n\n\n\n0\nResort Hotel\n0\n342\n2015\nJuly\n27\n1\n0\n0\n2\n...\nNo Deposit\nNaN\nNaN\n0\nTransient\n0.0\n0\n0\nCheck-Out\n2015-07-01\n\n\n1\nResort Hotel\n0\n737\n2015\nJuly\n27\n1\n0\n0\n2\n...\nNo Deposit\nNaN\nNaN\n0\nTransient\n0.0\n0\n0\nCheck-Out\n2015-07-01\n\n\n2\nResort Hotel\n0\n7\n2015\nJuly\n27\n1\n0\n1\n1\n...\nNo Deposit\nNaN\nNaN\n0\nTransient\n75.0\n0\n0\nCheck-Out\n2015-07-02\n\n\n3\nResort Hotel\n0\n13\n2015\nJuly\n27\n1\n0\n1\n1\n...\nNo Deposit\n304.0\nNaN\n0\nTransient\n75.0\n0\n0\nCheck-Out\n2015-07-02\n\n\n4\nResort Hotel\n0\n14\n2015\nJuly\n27\n1\n0\n2\n2\n...\nNo Deposit\n240.0\nNaN\n0\nTransient\n98.0\n0\n1\nCheck-Out\n2015-07-03\n\n\n\n\n5 rows × 32 columns",
    "crumbs": [
      "Classification2",
      "Multiple models using pipeline,fast tuning"
    ]
  },
  {
    "objectID": "classification2/8 Multiple Models using Pipeline and tuning.html#data-eda",
    "href": "classification2/8 Multiple Models using Pipeline and tuning.html#data-eda",
    "title": "Multiple models using pipeline,fast tuning",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\nMissing Data\n\n\nCode\nhotels.isnull().sum()\n\n\nhotel                                  0\nis_canceled                            0\nlead_time                              0\narrival_date_year                      0\narrival_date_month                     0\narrival_date_week_number               0\narrival_date_day_of_month              0\nstays_in_weekend_nights                0\nstays_in_week_nights                   0\nadults                                 0\nchildren                               4\nbabies                                 0\nmeal                                   0\ncountry                              488\nmarket_segment                         0\ndistribution_channel                   0\nis_repeated_guest                      0\nprevious_cancellations                 0\nprevious_bookings_not_canceled         0\nreserved_room_type                     0\nassigned_room_type                     0\nbooking_changes                        0\ndeposit_type                           0\nagent                              16340\ncompany                           112593\ndays_in_waiting_list                   0\ncustomer_type                          0\nadr                                    0\nrequired_car_parking_spaces            0\ntotal_of_special_requests              0\nreservation_status                     0\nreservation_status_date                0\ndtype: int64\n\n\n\n\nCode\n#import math\n#hotels=hotels&gt;&gt; filter(math.isnan(_.children)==False)\nfrom siuba.siu import call\nfrom siuba import _, mutate, filter, group_by, summarize,show_query\nfrom siuba import *\n\nhotels=hotels.drop('company', axis=1)\n\nhotels &gt;&gt; group_by(_.children)  &gt;&gt; summarize(n = _.shape[0])\n\n\n\n\n\n\n\n\n\n\nchildren\nn\n\n\n\n\n0\n0.0\n110796\n\n\n1\n1.0\n4861\n\n\n2\n2.0\n3652\n\n\n3\n3.0\n76\n\n\n4\n10.0\n1\n\n\n5\nNaN\n4\n\n\n\n\n\n\n\n\n\n\nCode\n#import math\nhotels=hotels&gt;&gt;mutate(children=if_else(_.children &gt; 0, True, False))\n\n# Create a boolean mask and apply it\nmask = pd.notna(hotels['children'])\nhotels = hotels[mask]",
    "crumbs": [
      "Classification2",
      "Multiple models using pipeline,fast tuning"
    ]
  },
  {
    "objectID": "classification2/8 Multiple Models using Pipeline and tuning.html#data-wrangling",
    "href": "classification2/8 Multiple Models using Pipeline and tuning.html#data-wrangling",
    "title": "Multiple models using pipeline,fast tuning",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nchildren_train = hotels .children\n\n# Concatenate training and test sets\ndata  = hotels",
    "crumbs": [
      "Classification2",
      "Multiple models using pipeline,fast tuning"
    ]
  },
  {
    "objectID": "classification2/8 Multiple Models using Pipeline and tuning.html#categorical_cols-and-numerical_cols",
    "href": "classification2/8 Multiple Models using Pipeline and tuning.html#categorical_cols-and-numerical_cols",
    "title": "Multiple models using pipeline,fast tuning",
    "section": "2.4 categorical_cols and numerical_cols",
    "text": "2.4 categorical_cols and numerical_cols\n\n\nCode\ncategorical_cols = [cname for cname in data \n                    if data[cname].nunique() &lt; 10 and data[cname].dtype == \"object\"]\n                    \n                    \nnumerical_cols = numerical_cols = [cname for cname in data.columns \n                    if data[cname].dtype in ['int64', 'float64']]\n\n\n\n\nCode\nprint(\"The total number of categorical columns:\", len(categorical_cols))\nprint(\"The total number of numerical columns:\", len(numerical_cols))\n\n\nThe total number of categorical columns: 7\nThe total number of numerical columns: 18\n\n\n\n\nCode\ndata &gt;&gt; group_by(_.children)  &gt;&gt; summarize(n = _.shape[0])\n\n\n\n\n\n\n\n\n\n\nchildren\nn\n\n\n\n\n0\nFalse\n110800\n\n\n1\nTrue\n8590",
    "crumbs": [
      "Classification2",
      "Multiple models using pipeline,fast tuning"
    ]
  },
  {
    "objectID": "classification2/8 Multiple Models using Pipeline and tuning.html#split-data",
    "href": "classification2/8 Multiple Models using Pipeline and tuning.html#split-data",
    "title": "Multiple models using pipeline,fast tuning",
    "section": "2.5 split data",
    "text": "2.5 split data\n80% training / 10% validation/ 10% testing\n\n\n\nCode\nY=data['children']\nX=data.drop('children', axis=1)\n\ntraining_size=0.8\nvalidation_size=0.1\ntesting_size=0.1\n\n\nX_train, X_val, Y_train, Y_val= train_test_split(X, Y, test_size=validation_size, random_state=1)\n\n\nX_train, X_test, Y_train, Y_test= train_test_split(X_train, Y_train, test_size=testing_size/training_size, random_state=1) \n\n\n\n\nCode\nlen(X_train)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.7874947650556998\n\n\n\n\nCode\nlen(X_val)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.1\n\n\n\n\nCode\nlen(X_test)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.11250523494430019",
    "crumbs": [
      "Classification2",
      "Multiple models using pipeline,fast tuning"
    ]
  },
  {
    "objectID": "classification2/8 Multiple Models using Pipeline and tuning.html#categorical_cols-and-numerical_cols-1",
    "href": "classification2/8 Multiple Models using Pipeline and tuning.html#categorical_cols-and-numerical_cols-1",
    "title": "Multiple models using pipeline,fast tuning",
    "section": "2.6 categorical_cols and numerical_cols",
    "text": "2.6 categorical_cols and numerical_cols\n\n\nCode\ncategorical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].nunique() &lt; 10 and X_train[cname].dtype == \"object\"]\n                    \n                    \nnumerical_cols = numerical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].dtype in ['int64', 'float64']]\n\n\n\n\nCode\nprint(\"The total number of categorical columns:\", len(categorical_cols))\nprint(\"The total number of numerical columns:\", len(numerical_cols))\n\n\nThe total number of categorical columns: 7\nThe total number of numerical columns: 18\n\n\n\n\nCode\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train[my_cols].copy()\nX_val = X_val[my_cols].copy()\nX_test= X_test[my_cols].copy()\n\nmy_cols\n#X_final = df_test[my_cols].copy()\n\n\n['hotel',\n 'meal',\n 'market_segment',\n 'distribution_channel',\n 'deposit_type',\n 'customer_type',\n 'reservation_status',\n 'is_canceled',\n 'lead_time',\n 'arrival_date_year',\n 'arrival_date_week_number',\n 'arrival_date_day_of_month',\n 'stays_in_weekend_nights',\n 'stays_in_week_nights',\n 'adults',\n 'babies',\n 'is_repeated_guest',\n 'previous_cancellations',\n 'previous_bookings_not_canceled',\n 'booking_changes',\n 'agent',\n 'days_in_waiting_list',\n 'adr',\n 'required_car_parking_spaces',\n 'total_of_special_requests']",
    "crumbs": [
      "Classification2",
      "Multiple models using pipeline,fast tuning"
    ]
  },
  {
    "objectID": "classification2/8 Multiple Models using Pipeline and tuning.html#pipelines-for-data-preprocessing",
    "href": "classification2/8 Multiple Models using Pipeline and tuning.html#pipelines-for-data-preprocessing",
    "title": "Multiple models using pipeline,fast tuning",
    "section": "2.7 Pipelines for Data Preprocessing",
    "text": "2.7 Pipelines for Data Preprocessing\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\nCode\nnumerical_transformer = Pipeline(steps=[\n    ('imputer_num', SimpleImputer(strategy='median')), \n    ('scaler', StandardScaler())\n])\n\n\n\n\nCode\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n\n\nCode\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)])",
    "crumbs": [
      "Classification2",
      "Multiple models using pipeline,fast tuning"
    ]
  },
  {
    "objectID": "classification2/8 Multiple Models using Pipeline and tuning.html#define-model",
    "href": "classification2/8 Multiple Models using Pipeline and tuning.html#define-model",
    "title": "Multiple models using pipeline,fast tuning",
    "section": "3.1 define model",
    "text": "3.1 define model\n\n3.1.1 XGB model\n\n\nCode\nimport xgboost\nprint(xgboost.__version__)\n\n\n2.0.3\n\n\n\n\nCode\nfrom xgboost import XGBClassifier\nxgb_model = XGBClassifier()\nxgb_model\n\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. XGBClassifieriNot fittedXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...) \n\n\n\n\n3.1.2 Random Forest model\n\n\nCode\nfrom sklearn.ensemble import RandomForestClassifier\nrandom_forest_model = RandomForestClassifier()\nrandom_forest_model\n\n\nRandomForestClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriNot fittedRandomForestClassifier() \n\n\n\n\n3.1.3 Logistic Regression model\n\n\nCode\nfrom sklearn.linear_model import LogisticRegression\nLogisticRegression_model = LogisticRegression(solver='liblinear')\nLogisticRegression_model\n\n\nLogisticRegression(solver='liblinear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniNot fittedLogisticRegression(solver='liblinear')",
    "crumbs": [
      "Classification2",
      "Multiple models using pipeline,fast tuning"
    ]
  },
  {
    "objectID": "classification2/8 Multiple Models using Pipeline and tuning.html#define-pipline",
    "href": "classification2/8 Multiple Models using Pipeline and tuning.html#define-pipline",
    "title": "Multiple models using pipeline,fast tuning",
    "section": "3.2 define pipline",
    "text": "3.2 define pipline\n\n\nCode\npipeline_xgb = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', xgb_model)\n         ]\n)\n\npipeline_rf = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', random_forest_model)\n         ]\n)\n\npipeline_lr = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', LogisticRegression_model)\n         ]\n)",
    "crumbs": [
      "Classification2",
      "Multiple models using pipeline,fast tuning"
    ]
  },
  {
    "objectID": "classification2/8 Multiple Models using Pipeline and tuning.html#define-gridsearch",
    "href": "classification2/8 Multiple Models using Pipeline and tuning.html#define-gridsearch",
    "title": "Multiple models using pipeline,fast tuning",
    "section": "3.3 define GridSearch",
    "text": "3.3 define GridSearch\n\n\nCode\nparameters_xgb= {\n        'model__learning_rate': [0.01, 0.02,0.08,0.1],\n        'model__max_depth': [3, 5, 7,8,9,10],\n        'model__min_child_weight': [1, 3,5,8],\n        'model__subsample': [0.5, 0.7,0.9],\n        \n       # 'model__colsample__bytree': [0.5, 0.7],\n       \n        'model__n_estimators' : [100, 200],\n        'model__objective': ['reg:squarederror']\n    }\n\n\nGrid_xgb = HalvingGridSearchCV(pipeline_xgb\n                ,parameters_xgb \n                ,scoring='accuracy'\n                ,max_resources=50\n                , cv=10, n_jobs=-1)\n                \n                \nparameters_rf = {'model__max_depth':[20,30,40],\n                 'model__n_estimators':[200,250],\n                 'model__min_samples_leaf':[1,2,3]\n                 }                \n                \n\nGrid_rf = HalvingGridSearchCV(pipeline_rf\n                ,parameters_rf\n                ,scoring='accuracy'\n                ,max_resources=50\n                , cv=10, n_jobs=-1)",
    "crumbs": [
      "Classification2",
      "Multiple models using pipeline,fast tuning"
    ]
  },
  {
    "objectID": "classification2/8 Multiple Models using Pipeline and tuning.html#train-model",
    "href": "classification2/8 Multiple Models using Pipeline and tuning.html#train-model",
    "title": "Multiple models using pipeline,fast tuning",
    "section": "3.4 train model",
    "text": "3.4 train model\n\n\nCode\nstart_time = time.time()\n\n\nGrids = [Grid_xgb, Grid_rf,pipeline_xgb,pipeline_rf,pipeline_lr]\nfor Grid in Grids:\n    Grid.fit(X_train,Y_train)\n\n\nend_time = time.time()\nduration = end_time - start_time\nduration\n\nprint(\"trainning time:\",duration)\n\n\ntrainning time: 235.7956202030182",
    "crumbs": [
      "Classification2",
      "Multiple models using pipeline,fast tuning"
    ]
  },
  {
    "objectID": "classification2/8 Multiple Models using Pipeline and tuning.html#preformance",
    "href": "classification2/8 Multiple Models using Pipeline and tuning.html#preformance",
    "title": "Multiple models using pipeline,fast tuning",
    "section": "3.5 Preformance",
    "text": "3.5 Preformance\n\n\nCode\ngrid_dict = {0: 'XGB', 1: 'random forest', 2: 'XGB non tune',3: 'ramdon forest non tune',4:'Logistic regression non tune' }\n\nfor i, model in enumerate(Grids):\n    print('{} Test Accuracy: {}'.format(grid_dict[i],\n    model.score(X_test,Y_test)))\n    #print('{} Best Params: {}'.format(grid_dict[i], model.best_params_))\n\n\nXGB Test Accuracy: 0.928752233472305\nrandom forest Test Accuracy: 0.944237641453246\nXGB non tune Test Accuracy: 0.9532459797498511\nramdon forest non tune Test Accuracy: 0.949746873138773\nLogistic regression non tune Test Accuracy: 0.9321768910065515\n\n\n\n\nCode\nbest_ml=Grid_xgb.best_estimator_\n\n\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = best_ml.predict(X_test) #always gets x and retuns y\nY_pred_dt\n\n\narray([0, 0, 0, ..., 0, 0, 0])\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_test,Y_pred_dt)  \naccuracy\n\n\n0.928752233472305\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_test,Y_pred_dt)  \nprecision_dt\n\n\n0.0\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_test,Y_pred_dt)  \nrecall_dt\n\n\n0.0\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_test,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[12475,     0],\n       [  957,     0]])\n\n\n\n\nCode\nfrom sklearn.metrics import ConfusionMatrixDisplay\nConfusionMatrixDisplay.from_estimator(best_ml, X_test, Y_test)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_test, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.5\n\n\n\n\nCode\nfpr, tpr, thresholds = metrics.roc_curve(Y_test, Y_pred_dt)\nroc_auc = metrics.auc(fpr, tpr)\n\ndisplay = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,\n                                 estimator_name='example estimator')\n\ndisplay.plot()\n\nplt.show()",
    "crumbs": [
      "Classification2",
      "Multiple models using pipeline,fast tuning"
    ]
  },
  {
    "objectID": "classification2/8 Multiple Models using Pipeline and tuning.html#k-fold-cross-validation",
    "href": "classification2/8 Multiple Models using Pipeline and tuning.html#k-fold-cross-validation",
    "title": "Multiple models using pipeline,fast tuning",
    "section": "3.6 k-Fold Cross-Validation",
    "text": "3.6 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(pipeline_xgb, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.9532328670096966",
    "crumbs": [
      "Classification2",
      "Multiple models using pipeline,fast tuning"
    ]
  },
  {
    "objectID": "classification2/8 Multiple Models using Pipeline and tuning.html#save-model",
    "href": "classification2/8 Multiple Models using Pipeline and tuning.html#save-model",
    "title": "Multiple models using pipeline,fast tuning",
    "section": "3.7 save model",
    "text": "3.7 save model\n\n\nCode\nfrom joblib import dump, load\ndump(Grid_xgb, 'trained_grid_8.joblib', compress=True)  \n\n\n['trained_grid_8.joblib']",
    "crumbs": [
      "Classification2",
      "Multiple models using pipeline,fast tuning"
    ]
  },
  {
    "objectID": "classification2/8 Multiple Models using Pipeline and tuning.html#load-model",
    "href": "classification2/8 Multiple Models using Pipeline and tuning.html#load-model",
    "title": "Multiple models using pipeline,fast tuning",
    "section": "3.8 load model",
    "text": "3.8 load model\n\n\nCode\nmodel_reload = load('trained_grid_8.joblib') \n\n\n\n\nCode\nbest_ml=model_reload.best_estimator_",
    "crumbs": [
      "Classification2",
      "Multiple models using pipeline,fast tuning"
    ]
  },
  {
    "objectID": "classification2/8 Multiple Models using Pipeline and tuning.html#final-prediction",
    "href": "classification2/8 Multiple Models using Pipeline and tuning.html#final-prediction",
    "title": "Multiple models using pipeline,fast tuning",
    "section": "3.9 final prediction",
    "text": "3.9 final prediction\n\n\nCode\nY_pred_dt_final =best_ml.predict(X_val) #always gets x and retuns y\n\nY_pred_dt_final[0:5]\n\n\narray([0, 0, 0, 0, 0])\n\n\n\n\nCode\nimport collections, numpy\ncollections.Counter(Y_pred_dt_final)\n\n\nCounter({0: 11939})",
    "crumbs": [
      "Classification2",
      "Multiple models using pipeline,fast tuning"
    ]
  },
  {
    "objectID": "classification2/8.2 Multiple Models using Pipeline and tuning.html",
    "href": "classification2/8.2 Multiple Models using Pipeline and tuning.html",
    "title": "Multiple models using downsample,pipeline,fast tuning",
    "section": "",
    "text": "Since we have enough rare class data(at least 1K).Let handle imbalanced data with down sample before training.",
    "crumbs": [
      "Classification2",
      "Multiple models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "classification2/8.2 Multiple Models using Pipeline and tuning.html#download-data",
    "href": "classification2/8.2 Multiple Models using Pipeline and tuning.html#download-data",
    "title": "Multiple models using downsample,pipeline,fast tuning",
    "section": "2.1 download data",
    "text": "2.1 download data\n\n\nCode\nimport pandas as pd\n#url='https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv'\n#hotels=pd.read_csv(url)\n\nhotels=pd.read_csv('data/hotels.csv')\n\n\n\n\nCode\nhotels.head()\n\n\n\n\n\n\n\n\n\n\nhotel\nis_canceled\nlead_time\narrival_date_year\narrival_date_month\narrival_date_week_number\narrival_date_day_of_month\nstays_in_weekend_nights\nstays_in_week_nights\nadults\n...\ndeposit_type\nagent\ncompany\ndays_in_waiting_list\ncustomer_type\nadr\nrequired_car_parking_spaces\ntotal_of_special_requests\nreservation_status\nreservation_status_date\n\n\n\n\n0\nResort Hotel\n0\n342\n2015\nJuly\n27\n1\n0\n0\n2\n...\nNo Deposit\nNaN\nNaN\n0\nTransient\n0.0\n0\n0\nCheck-Out\n2015-07-01\n\n\n1\nResort Hotel\n0\n737\n2015\nJuly\n27\n1\n0\n0\n2\n...\nNo Deposit\nNaN\nNaN\n0\nTransient\n0.0\n0\n0\nCheck-Out\n2015-07-01\n\n\n2\nResort Hotel\n0\n7\n2015\nJuly\n27\n1\n0\n1\n1\n...\nNo Deposit\nNaN\nNaN\n0\nTransient\n75.0\n0\n0\nCheck-Out\n2015-07-02\n\n\n3\nResort Hotel\n0\n13\n2015\nJuly\n27\n1\n0\n1\n1\n...\nNo Deposit\n304.0\nNaN\n0\nTransient\n75.0\n0\n0\nCheck-Out\n2015-07-02\n\n\n4\nResort Hotel\n0\n14\n2015\nJuly\n27\n1\n0\n2\n2\n...\nNo Deposit\n240.0\nNaN\n0\nTransient\n98.0\n0\n1\nCheck-Out\n2015-07-03\n\n\n\n\n5 rows × 32 columns",
    "crumbs": [
      "Classification2",
      "Multiple models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "classification2/8.2 Multiple Models using Pipeline and tuning.html#data-eda",
    "href": "classification2/8.2 Multiple Models using Pipeline and tuning.html#data-eda",
    "title": "Multiple models using downsample,pipeline,fast tuning",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\nMissing Data\n\n\nCode\nhotels.isnull().sum()\n\n\nhotel                                  0\nis_canceled                            0\nlead_time                              0\narrival_date_year                      0\narrival_date_month                     0\narrival_date_week_number               0\narrival_date_day_of_month              0\nstays_in_weekend_nights                0\nstays_in_week_nights                   0\nadults                                 0\nchildren                               4\nbabies                                 0\nmeal                                   0\ncountry                              488\nmarket_segment                         0\ndistribution_channel                   0\nis_repeated_guest                      0\nprevious_cancellations                 0\nprevious_bookings_not_canceled         0\nreserved_room_type                     0\nassigned_room_type                     0\nbooking_changes                        0\ndeposit_type                           0\nagent                              16340\ncompany                           112593\ndays_in_waiting_list                   0\ncustomer_type                          0\nadr                                    0\nrequired_car_parking_spaces            0\ntotal_of_special_requests              0\nreservation_status                     0\nreservation_status_date                0\ndtype: int64\n\n\n\n\nCode\n#import math\n#hotels=hotels&gt;&gt; filter(math.isnan(_.children)==False)\nfrom siuba.siu import call\nfrom siuba import _, mutate, filter, group_by, summarize,show_query\nfrom siuba import *\n\nhotels=hotels.drop('company', axis=1)\n\nhotels &gt;&gt; group_by(_.children)  &gt;&gt; summarize(n = _.shape[0])\n\n\n\n\n\n\n\n\n\n\nchildren\nn\n\n\n\n\n0\n0.0\n110796\n\n\n1\n1.0\n4861\n\n\n2\n2.0\n3652\n\n\n3\n3.0\n76\n\n\n4\n10.0\n1\n\n\n5\nNaN\n4\n\n\n\n\n\n\n\n\n\n\nCode\n#import math\nhotels=hotels&gt;&gt;mutate(children=if_else(_.children &gt; 0, True, False))\n\n# Create a boolean mask and apply it\nmask = pd.notna(hotels['children'])\nhotels = hotels[mask]",
    "crumbs": [
      "Classification2",
      "Multiple models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "classification2/8.2 Multiple Models using Pipeline and tuning.html#data-wrangling",
    "href": "classification2/8.2 Multiple Models using Pipeline and tuning.html#data-wrangling",
    "title": "Multiple models using downsample,pipeline,fast tuning",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\n# Store target variable of training data in a safe place\nchildren_train = hotels .children\n\n# Concatenate training and test sets\ndata  = hotels",
    "crumbs": [
      "Classification2",
      "Multiple models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "classification2/8.2 Multiple Models using Pipeline and tuning.html#categorical_cols-and-numerical_cols",
    "href": "classification2/8.2 Multiple Models using Pipeline and tuning.html#categorical_cols-and-numerical_cols",
    "title": "Multiple models using downsample,pipeline,fast tuning",
    "section": "2.4 categorical_cols and numerical_cols",
    "text": "2.4 categorical_cols and numerical_cols\n\n\nCode\ncategorical_cols = [cname for cname in data \n                    if data[cname].nunique() &lt; 10 and data[cname].dtype == \"object\"]\n                    \n                    \nnumerical_cols = numerical_cols = [cname for cname in data.columns \n                    if data[cname].dtype in ['int64', 'float64']]\n\n\n\n\nCode\nprint(\"The total number of categorical columns:\", len(categorical_cols))\nprint(\"The total number of numerical columns:\", len(numerical_cols))\n\n\nThe total number of categorical columns: 7\nThe total number of numerical columns: 18\n\n\n\n\nCode\ndata &gt;&gt; group_by(_.children)  &gt;&gt; summarize(n = _.shape[0])\n\n\n\n\n\n\n\n\n\n\nchildren\nn\n\n\n\n\n0\nFalse\n110800\n\n\n1\nTrue\n8590",
    "crumbs": [
      "Classification2",
      "Multiple models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "classification2/8.2 Multiple Models using Pipeline and tuning.html#split-data",
    "href": "classification2/8.2 Multiple Models using Pipeline and tuning.html#split-data",
    "title": "Multiple models using downsample,pipeline,fast tuning",
    "section": "2.5 split data",
    "text": "2.5 split data\n80% training / 10% validation/ 10% testing\n\n\n\nCode\nY=data['children']\nX=data.drop('children', axis=1)\n\ntraining_size=0.8\nvalidation_size=0.1\ntesting_size=0.1\n\n\nX_train, X_val, Y_train, Y_val= train_test_split(X, Y, test_size=validation_size, random_state=1)\n\nX_train, X_test, Y_train, Y_test= train_test_split(X_train, Y_train, test_size=testing_size/training_size, random_state=1) \n\n\n\n\nCode\nfrom imblearn.under_sampling import RandomUnderSampler\n\nros=RandomUnderSampler(random_state=0)\n\nX_train_resample,Y_train_resample=ros.fit_resample(X_train,Y_train)\n\n\n\n\nCode\nX_train_resample.shape\n\n\n(13552, 30)\n\n\n\n\nCode\nY_train_resample.shape\n\n\n(13552,)\n\n\n\n\nCode\nlen(X_train)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.7874947650556998\n\n\n\n\nCode\nlen(X_val)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.1\n\n\n\n\nCode\nlen(X_test)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.11250523494430019\n\n\n\n\nCode\nY_train.value_counts()\n\n\nchildren\nFalse    87243\nTrue      6776\nName: count, dtype: int64\n\n\n\n\nCode\nY_train_resample.value_counts()\n\n\nchildren\nFalse    6776\nTrue     6776\nName: count, dtype: int64\n\n\n\n\nCode\nY_test.value_counts()\n\n\nchildren\nFalse    12475\nTrue       957\nName: count, dtype: int64\n\n\n\n\nCode\nY_val.value_counts()\n\n\nchildren\nFalse    11082\nTrue       857\nName: count, dtype: int64",
    "crumbs": [
      "Classification2",
      "Multiple models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "classification2/8.2 Multiple Models using Pipeline and tuning.html#categorical_cols-and-numerical_cols-1",
    "href": "classification2/8.2 Multiple Models using Pipeline and tuning.html#categorical_cols-and-numerical_cols-1",
    "title": "Multiple models using downsample,pipeline,fast tuning",
    "section": "2.6 categorical_cols and numerical_cols",
    "text": "2.6 categorical_cols and numerical_cols\n\n\nCode\ncategorical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].nunique() &lt; 10 and X_train[cname].dtype == \"object\"]\n                    \n                    \nnumerical_cols = numerical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].dtype in ['int64', 'float64']]\n\n\n\n\nCode\nprint(\"The total number of categorical columns:\", len(categorical_cols))\nprint(\"The total number of numerical columns:\", len(numerical_cols))\n\n\nThe total number of categorical columns: 7\nThe total number of numerical columns: 18\n\n\n\n\nCode\nmy_cols = categorical_cols + numerical_cols\nX_train_resample = X_train_resample[my_cols].copy()\nX_val = X_val[my_cols].copy()\nX_test= X_test[my_cols].copy()\n\nmy_cols\n#X_final = df_test[my_cols].copy()\n\n\n['hotel',\n 'meal',\n 'market_segment',\n 'distribution_channel',\n 'deposit_type',\n 'customer_type',\n 'reservation_status',\n 'is_canceled',\n 'lead_time',\n 'arrival_date_year',\n 'arrival_date_week_number',\n 'arrival_date_day_of_month',\n 'stays_in_weekend_nights',\n 'stays_in_week_nights',\n 'adults',\n 'babies',\n 'is_repeated_guest',\n 'previous_cancellations',\n 'previous_bookings_not_canceled',\n 'booking_changes',\n 'agent',\n 'days_in_waiting_list',\n 'adr',\n 'required_car_parking_spaces',\n 'total_of_special_requests']",
    "crumbs": [
      "Classification2",
      "Multiple models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "classification2/8.2 Multiple Models using Pipeline and tuning.html#pipelines-for-data-preprocessing",
    "href": "classification2/8.2 Multiple Models using Pipeline and tuning.html#pipelines-for-data-preprocessing",
    "title": "Multiple models using downsample,pipeline,fast tuning",
    "section": "2.7 Pipelines for Data Preprocessing",
    "text": "2.7 Pipelines for Data Preprocessing\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\nCode\nnumerical_transformer = Pipeline(steps=[\n    ('imputer_num', SimpleImputer(strategy='median')), \n    ('scaler', StandardScaler())\n])\n\n\n\n\nCode\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n\n\nCode\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)])",
    "crumbs": [
      "Classification2",
      "Multiple models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "classification2/8.2 Multiple Models using Pipeline and tuning.html#define-model",
    "href": "classification2/8.2 Multiple Models using Pipeline and tuning.html#define-model",
    "title": "Multiple models using downsample,pipeline,fast tuning",
    "section": "3.1 define model",
    "text": "3.1 define model\n\n3.1.1 XGB model\n\n\nCode\nimport xgboost\nprint(xgboost.__version__)\n\n\n2.0.3\n\n\n\n\nCode\nfrom xgboost import XGBClassifier\nxgb_model = XGBClassifier()\nxgb_model\n\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. XGBClassifieriNot fittedXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None,\n              num_parallel_tree=None, random_state=None, ...) \n\n\n\n\n3.1.2 Random Forest model\n\n\nCode\nfrom sklearn.ensemble import RandomForestClassifier\nrandom_forest_model = RandomForestClassifier()\nrandom_forest_model\n\n\nRandomForestClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriNot fittedRandomForestClassifier() \n\n\n\n\n3.1.3 Logistic Regression model\n\n\nCode\nfrom sklearn.linear_model import LogisticRegression\nLogisticRegression_model = LogisticRegression(solver='liblinear')\nLogisticRegression_model\n\n\nLogisticRegression(solver='liblinear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniNot fittedLogisticRegression(solver='liblinear')",
    "crumbs": [
      "Classification2",
      "Multiple models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "classification2/8.2 Multiple Models using Pipeline and tuning.html#define-pipline",
    "href": "classification2/8.2 Multiple Models using Pipeline and tuning.html#define-pipline",
    "title": "Multiple models using downsample,pipeline,fast tuning",
    "section": "3.2 define pipline",
    "text": "3.2 define pipline\n\n\nCode\npipeline_xgb = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', xgb_model)\n         ]\n)\n\npipeline_rf = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', random_forest_model)\n         ]\n)\n\npipeline_lr = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', LogisticRegression_model)\n         ]\n)",
    "crumbs": [
      "Classification2",
      "Multiple models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "classification2/8.2 Multiple Models using Pipeline and tuning.html#define-gridsearch",
    "href": "classification2/8.2 Multiple Models using Pipeline and tuning.html#define-gridsearch",
    "title": "Multiple models using downsample,pipeline,fast tuning",
    "section": "3.3 define GridSearch",
    "text": "3.3 define GridSearch\n\n\nCode\nparameters_xgb= {\n        'model__learning_rate': [0.01, 0.02,0.08,0.1],\n        'model__max_depth': [3, 5, 7,8,9,10],\n        'model__min_child_weight': [1, 3,5,8],\n        'model__subsample': [0.5, 0.7,0.9],\n        \n       # 'model__colsample__bytree': [0.5, 0.7],\n       \n        'model__n_estimators' : [100, 200],\n        'model__objective': ['reg:squarederror']\n    }\n\n\nGrid_xgb = HalvingGridSearchCV(pipeline_xgb\n                ,parameters_xgb \n                ,scoring='accuracy'\n                ,max_resources=50\n                , cv=10, n_jobs=-1)\n                \n                \nparameters_rf = {'model__max_depth':[20,30,40],\n                 'model__n_estimators':[200,250],\n                 'model__min_samples_leaf':[1,2,3]\n                 }                \n                \n\nGrid_rf = HalvingGridSearchCV(pipeline_rf\n                ,parameters_rf\n                ,scoring='accuracy'\n                ,max_resources=50\n                , cv=10, n_jobs=-1)",
    "crumbs": [
      "Classification2",
      "Multiple models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "classification2/8.2 Multiple Models using Pipeline and tuning.html#train-model",
    "href": "classification2/8.2 Multiple Models using Pipeline and tuning.html#train-model",
    "title": "Multiple models using downsample,pipeline,fast tuning",
    "section": "3.4 train model",
    "text": "3.4 train model\n\n\nCode\nstart_time = time.time()\n\n\nGrids = [Grid_xgb, Grid_rf,pipeline_xgb,pipeline_rf,pipeline_lr]\nfor Grid in Grids:\n    Grid.fit(X_train_resample,Y_train_resample)\n\n\nend_time = time.time()\nduration = end_time - start_time\nduration\n\nprint(\"trainning time:\",duration)\n\n\ntrainning time: 62.23781609535217",
    "crumbs": [
      "Classification2",
      "Multiple models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "classification2/8.2 Multiple Models using Pipeline and tuning.html#preformance",
    "href": "classification2/8.2 Multiple Models using Pipeline and tuning.html#preformance",
    "title": "Multiple models using downsample,pipeline,fast tuning",
    "section": "3.5 Preformance",
    "text": "3.5 Preformance\n\n\nCode\ngrid_dict = {0: 'XGB', 1: 'random forest', 2: 'XGB non tune',3: 'ramdon forest non tune',4:'Logistic regression non tune' }\n\nfor i, model in enumerate(Grids):\n    print('{} Test Accuracy: {}'.format(grid_dict[i],\n    model.score(X_test,Y_test)))\n    #print('{} Best Params: {}'.format(grid_dict[i], model.best_params_))\n\n\nXGB Test Accuracy: 0.7948927933293627\nrandom forest Test Accuracy: 0.8039011316259679\nXGB non tune Test Accuracy: 0.8515485407980941\nramdon forest non tune Test Accuracy: 0.829883859440143\nLogistic regression non tune Test Accuracy: 0.740842763549732\n\n\n\n\nCode\nfrom sklearn import metrics\nfor i, model in enumerate(Grids):\n    Y_pred_dt = model.predict(X_test)\n    print('{} Test auc: {}'.format(grid_dict[i],\n    metrics.roc_auc_score(Y_test, Y_pred_dt)))\n\n\nXGB Test auc: 0.7897252812835702\nrandom forest Test auc: 0.8404016392241118\nXGB non tune Test auc: 0.8650881700705486\nramdon forest non tune Test auc: 0.8505305281409213\nLogistic regression non tune Test auc: 0.7779929765487088\n\n\n\n\nCode\nbest_ml=Grid_xgb.best_estimator_\n\n\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = best_ml.predict(X_test) #always gets x and retuns y\nY_pred_dt\n\n\narray([1, 1, 0, ..., 0, 0, 1])\n\n\n\nAccuracy\n\n\n\nCode\n# Accuracy = true negatives + true positives / true positives + false positives + true negatives + false negatives\n# Here is another way to find the accuracy score\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(Y_test,Y_pred_dt)  \naccuracy\n\n\n0.7948927933293627\n\n\n\nPrecision\n\n\n\nCode\n# Precision = true positive / true positive + false positive\nprecision_dt = metrics.precision_score(Y_test,Y_pred_dt)  \nprecision_dt\n\n\n0.22741055184960582\n\n\n\nRecall\n\n\n\nCode\n# Recall = true positive / true positive + false negative\nrecall_dt = metrics.recall_score(Y_test,Y_pred_dt)  \nrecall_dt\n\n\n0.7836990595611285\n\n\n\nConfusion matrix\n\n\n\nCode\nimport seaborn as sns\nconfusion_matrix_dt = metrics.confusion_matrix(Y_test,Y_pred_dt)\nconfusion_matrix_dt\n\n\narray([[9927, 2548],\n       [ 207,  750]])\n\n\n\n\nCode\nfrom sklearn.metrics import ConfusionMatrixDisplay\nConfusionMatrixDisplay.from_estimator(best_ml, X_test, Y_test)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nAUC - ROC Curve\n\n\n\nCode\nauc_dt = metrics.roc_auc_score(Y_test, Y_pred_dt) # as the documentation explain, the main parameters are: y_true and y_score\nauc_dt\n\n\n0.7897252812835702\n\n\n\n\nCode\nfpr, tpr, thresholds = metrics.roc_curve(Y_test, Y_pred_dt)\nroc_auc = metrics.auc(fpr, tpr)\n\ndisplay = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,\n                                 estimator_name='example estimator')\n\ndisplay.plot()\n\nplt.show()",
    "crumbs": [
      "Classification2",
      "Multiple models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "classification2/8.2 Multiple Models using Pipeline and tuning.html#k-fold-cross-validation",
    "href": "classification2/8.2 Multiple Models using Pipeline and tuning.html#k-fold-cross-validation",
    "title": "Multiple models using downsample,pipeline,fast tuning",
    "section": "3.6 k-Fold Cross-Validation",
    "text": "3.6 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(pipeline_xgb, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)\n\n\n0.9531690383559412",
    "crumbs": [
      "Classification2",
      "Multiple models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "classification2/8.2 Multiple Models using Pipeline and tuning.html#save-model",
    "href": "classification2/8.2 Multiple Models using Pipeline and tuning.html#save-model",
    "title": "Multiple models using downsample,pipeline,fast tuning",
    "section": "3.7 save model",
    "text": "3.7 save model\n\n\nCode\nfrom joblib import dump, load\ndump(Grid_xgb, 'trained_grid_8_2.joblib', compress=True)  \n\n\n['trained_grid_8_2.joblib']",
    "crumbs": [
      "Classification2",
      "Multiple models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "classification2/8.2 Multiple Models using Pipeline and tuning.html#load-model",
    "href": "classification2/8.2 Multiple Models using Pipeline and tuning.html#load-model",
    "title": "Multiple models using downsample,pipeline,fast tuning",
    "section": "3.8 load model",
    "text": "3.8 load model\n\n\nCode\nmodel_reload = load('trained_grid_8_2.joblib') \n\n\n\n\nCode\nbest_ml=model_reload.best_estimator_",
    "crumbs": [
      "Classification2",
      "Multiple models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "classification2/8.2 Multiple Models using Pipeline and tuning.html#final-prediction",
    "href": "classification2/8.2 Multiple Models using Pipeline and tuning.html#final-prediction",
    "title": "Multiple models using downsample,pipeline,fast tuning",
    "section": "3.9 final prediction",
    "text": "3.9 final prediction\n\n\nCode\nY_pred_dt_final =best_ml.predict(X_val) #always gets x and retuns y\n\nY_pred_dt_final[0:5]\n\n\narray([0, 1, 0, 0, 0])\n\n\n\n\nCode\nimport collections, numpy\ncollections.Counter(Y_pred_dt_final)\n\n\nCounter({0: 8969, 1: 2970})",
    "crumbs": [
      "Classification2",
      "Multiple models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "clustering/1.2 k mean Clustering.html",
    "href": "clustering/1.2 k mean Clustering.html",
    "title": "K mean Clustering with image",
    "section": "",
    "text": "Using K mean Clustering to replace colow on below picture",
    "crumbs": [
      "Clustering",
      "K mean Clustering with image"
    ]
  },
  {
    "objectID": "clustering/1.2 k mean Clustering.html#input-data",
    "href": "clustering/1.2 k mean Clustering.html#input-data",
    "title": "K mean Clustering with image",
    "section": "2.1 input data",
    "text": "2.1 input data\n\n\nCode\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom skimage.io import imread, imsave\n\nsample_img = imread('images/python logo.jpg')\nw,h,c = sample_img.shape\n\n\n\n\nCode\nw\nh\nc\n\n\n3\n\n\n\n\nCode\nsample_img.shape\n\n\n(235, 214, 3)\n\n\n\n\nCode\nsample_img = sample_img.reshape(w*h,3)\n\n\n\n\nCode\nsample_img.shape\n\n\n(50290, 3)\n\n\n\n\nCode\n#test=sample_img.tolist()\n#np.unique(test)\n\n\nData Normalization\nSince the dataset contains a range of values from 0 to 255, the dataset has to be normalized. Data Normalization is an important preprocessing step which ensures that each input parameter (pixel, in this case) has a similar data distribution. This fastens the process of covergence while training the model. Also Normalization makes sure no one particular parameter influences the output significantly.\n\n\nCode\nx=sample_img/255\n\n\n\n\nCode\n#test=x.tolist()\n#np.unique(test)",
    "crumbs": [
      "Clustering",
      "K mean Clustering with image"
    ]
  },
  {
    "objectID": "data manipulation/2 data structure in Python .html#count-numpy.ndarray",
    "href": "data manipulation/2 data structure in Python .html#count-numpy.ndarray",
    "title": "Data structure in Python",
    "section": "2.6 count numpy.ndarray",
    "text": "2.6 count numpy.ndarray\n\n\nCode\nimport collections, numpy\na = numpy.array([0, 3, 0, 4])\ncollections.Counter(a)\n\n\nCounter({0: 2, 3: 1, 4: 1})\n\n\n\n2.6.1 convert list into numpy array\n\n\nCode\nA = [\n  [1, 4, 5, 12], \n  [-5, 8, 9, 0],\n  [-6, 7, 11, 19],\n  [1, 4, 5, 12], \n  [-5, 8, 9, 0],\n  [-6, 7, 11, 19],\n  [1, 4, 5, 12], \n  [-5, 8, 9, 0],\n  [-6, 7, 11, 19]\n  ]\n    \nA3 = np.array(A)\n\nprint(A3)\n\n\n[[ 1  4  5 12]\n [-5  8  9  0]\n [-6  7 11 19]\n [ 1  4  5 12]\n [-5  8  9  0]\n [-6  7 11 19]\n [ 1  4  5 12]\n [-5  8  9  0]\n [-6  7 11 19]]\n\n\n\n\n2.6.2 selection\n\n2.6.2.1 first 5 row\n\n\nCode\nA[:5]\n\n\n[[1, 4, 5, 12], [-5, 8, 9, 0], [-6, 7, 11, 19], [1, 4, 5, 12], [-5, 8, 9, 0]]\n\n\n\n\n2.6.2.2 lst 5 row\n\n\nCode\nA[:-5]\n\n\n[[1, 4, 5, 12], [-5, 8, 9, 0], [-6, 7, 11, 19], [1, 4, 5, 12]]\n\n\n\n\n2.6.2.3 first row\n\n\nCode\nA[0]\n\n\n[1, 4, 5, 12]\n\n\n\n\n2.6.2.4 first column\n\n\nCode\nA2[:,0]\n\n\narray([1, 3])\n\n\n\n\n2.6.2.5 first row and first column element\n\n\nCode\nA2[0,0]\n\n\n1\n\n\n\n\nCode\nA2.dtype\n\n\ndtype('int64')\n\n\n\n\n2.6.2.6 2 row and 3 column\n\n\nCode\nA2[1,2]\n\n\n5\n\n\n\n\n2.6.2.7 filter\n\n2.6.2.7.1 filter all\n\n\nCode\nprint(A3)\n\n\n[[ 1  4  5 12]\n [-5  8  9  0]\n [-6  7 11 19]\n [ 1  4  5 12]\n [-5  8  9  0]\n [-6  7 11 19]\n [ 1  4  5 12]\n [-5  8  9  0]\n [-6  7 11 19]]\n\n\n\n\nCode\nA3&gt;4\n\n\narray([[False, False,  True,  True],\n       [False,  True,  True, False],\n       [False,  True,  True,  True],\n       [False, False,  True,  True],\n       [False,  True,  True, False],\n       [False,  True,  True,  True],\n       [False, False,  True,  True],\n       [False,  True,  True, False],\n       [False,  True,  True,  True]])\n\n\n\n\nCode\nA3[A3&gt;4]\n\n\narray([ 5, 12,  8,  9,  7, 11, 19,  5, 12,  8,  9,  7, 11, 19,  5, 12,  8,\n        9,  7, 11, 19])\n\n\n\n\n2.6.2.7.2 filter row\n\n\nCode\nA3\n\n\narray([[ 1,  4,  5, 12],\n       [-5,  8,  9,  0],\n       [-6,  7, 11, 19],\n       [ 1,  4,  5, 12],\n       [-5,  8,  9,  0],\n       [-6,  7, 11, 19],\n       [ 1,  4,  5, 12],\n       [-5,  8,  9,  0],\n       [-6,  7, 11, 19]])\n\n\nfilter secound column&gt;5\n\n\nCode\nfilter_val=(A3&gt;5)[:,2]\n\n\nwhich only keep 2,3 row.\n\n\nCode\nA3[filter_val,0:]\n\n\narray([[-5,  8,  9,  0],\n       [-6,  7, 11, 19],\n       [-5,  8,  9,  0],\n       [-6,  7, 11, 19],\n       [-5,  8,  9,  0],\n       [-6,  7, 11, 19]])\n\n\n\n\n\n\n2.6.3 create numpy array\n\n2.6.3.1 create eye\n\n\nCode\nnp.eye(3)\n\n\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])\n\n\n\n\n2.6.3.2 create zero\n\n\nCode\nnp.zeros((2,3))\n\n\narray([[0., 0., 0.],\n       [0., 0., 0.]])\n\n\n\n\n2.6.3.3 create ones\n\n\nCode\nnp.ones((2,3))\n\n\narray([[1., 1., 1.],\n       [1., 1., 1.]])\n\n\n\n\n\n2.6.4 compare\n\n\nCode\n# Creating Array\na = np.array([1,2,3,4]) \nb = np.array([3,2,5,6])\n\n\n\n\nCode\n# Comparing two arrays\nnp.greater(a, b)\n\n\narray([False, False, False, False])\n\n\n\n\nCode\na &gt;= b\n\n\narray([False,  True, False, False])\n\n\n\n\nCode\n# Comparing two arrays\nnp.less(a, b)\n\n\narray([ True, False,  True,  True])\n\n\n\n\nCode\n# Comparing two arrays\nnp.equal(a, b)\n\n\narray([False,  True, False, False])\n\n\n\n\n2.6.5 reshape\n\n\nCode\na=np.arange(9).reshape(3, 3)\n\na\n\n\narray([[0, 1, 2],\n       [3, 4, 5],\n       [6, 7, 8]])\n\n\n\n\n2.6.6 calculation\n\n\nCode\nb=a*a\nb\n\n\narray([[ 0,  1,  4],\n       [ 9, 16, 25],\n       [36, 49, 64]])\n\n\n\n\nCode\nb=a+a\nb\n\n\narray([[ 0,  2,  4],\n       [ 6,  8, 10],\n       [12, 14, 16]])\n\n\n\n\n2.6.7 numpy array to dataframe\n\n\nCode\nimport pandas as pd\ndf = pd.DataFrame(b, columns=['Column_A', 'Column_B', 'Column_C'])\n\ndf\n\n\n\n\n\n\n\n\n\n\nColumn_A\nColumn_B\nColumn_C\n\n\n\n\n0\n0\n2\n4\n\n\n1\n6\n8\n10\n\n\n2\n12\n14\n16",
    "crumbs": [
      "data manipulation",
      "Data structure in Python"
    ]
  },
  {
    "objectID": "intro/1 Book.html",
    "href": "intro/1 Book.html",
    "title": "Boook",
    "section": "",
    "text": "1 Python for Data Analysis\nby William McKinney\n\n\n\n2 Introduction to Machine Learning with Python\nby Andreas C. Müller, Sarah Guido\n\n\n\n3 Machine Learning with Python Cookbook\nby Chris Albon\n\n\n\n4 Data Wrangling with Python\nby Jacqueline Kazil (Author), Katharine Jarmul (Author)\n\n\n\n5 A Course in Machine Learning\nby Hal Daumé\n\n\n\n6 Think Stats\nby Allen B. Downey\n\n\n\n7 Think Bayes\nby Allen B. Downey\n\n\n\n8 Think Like a Data Scientist\nby Brian Godsey\n\n\n\n9 Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\nby Aurélien Géron\n\n\n\n10 Deep Learning with Python\nBy Francois Chollet",
    "crumbs": [
      "Intro",
      "Boook"
    ]
  },
  {
    "objectID": "data manipulation/3 Pandas.html#dataframe-to-numpy-array",
    "href": "data manipulation/3 Pandas.html#dataframe-to-numpy-array",
    "title": "Data manipulation with Pandas",
    "section": "0.14 dataframe to numpy array",
    "text": "0.14 dataframe to numpy array\n\n\nCode\ndf_date.to_numpy()\n\n\narray([[Timestamp('2021-01-02 00:00:00'), '2023-04-05 06:07:08',\n        'January', Timestamp('2023-04-05 06:07:08'), 2023],\n       [Timestamp('2021-02-03 00:00:00'), '2024-05-06 07:08:09',\n        'February', Timestamp('2024-05-06 07:08:09'), 2024]], dtype=object)",
    "crumbs": [
      "data manipulation",
      "Data manipulation with Pandas"
    ]
  },
  {
    "objectID": "intro/1 Python Book.html",
    "href": "intro/1 Python Book.html",
    "title": "Python Boook",
    "section": "",
    "text": "1 Introducing Python\nby Bill Lubanovic\n\n\n\n2 Fluent Python\nby Luciano Ramalho\n\n\n\n3 Python Crash Course\nby Eric Matthes\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Intro",
      "Python Boook"
    ]
  },
  {
    "objectID": "intro/2 Data Book.html",
    "href": "intro/2 Data Book.html",
    "title": "Data analytic in Python Boook",
    "section": "",
    "text": "1 Python for Data Analysis\nby William McKinney\n\n\n\n2 Introduction to Machine Learning with Python\nby Andreas C. Müller, Sarah Guido\n\n\n\n3 Machine Learning with Python Cookbook\nby Chris Albon\n\n\n\n4 Data Wrangling with Python\nby Jacqueline Kazil (Author), Katharine Jarmul (Author)\n\n\n\n5 A Course in Machine Learning\nby Hal Daumé\n\n\n\n6 Think Stats\nby Allen B. Downey\n\n\n\n7 Think Bayes\nby Allen B. Downey\n\n\n\n8 Think Like a Data Scientist\nby Brian Godsey\n\n\n\n9 Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\nby Aurélien Géron\n\n\n\n10 Deep Learning with Python\nBy Francois Chollet\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Intro",
      "Data analytic in Python Boook"
    ]
  },
  {
    "objectID": "intro/3 Basic python.html#for-loop",
    "href": "intro/3 Basic python.html#for-loop",
    "title": "Basic Python",
    "section": "2.1 for Loop",
    "text": "2.1 for Loop\n\n\nCode\nfruits = [\"apple\", \"banana\", \"cherry\"]\nfor x in fruits:\n  print(x)\n\n\napple\nbanana\ncherry",
    "crumbs": [
      "Intro",
      "Basic Python"
    ]
  },
  {
    "objectID": "intro/3 Basic python.html#while-loop",
    "href": "intro/3 Basic python.html#while-loop",
    "title": "Basic Python",
    "section": "2.2 while Loop",
    "text": "2.2 while Loop\n\n\nCode\ni = 1\nwhile i &lt; 4:\n  print(i)\n  i += 1\n\n\n1\n2\n3\n\n\nwith break statement\n\n\nCode\ni = 1\nwhile i &lt; 6:\n  print(i)\n  if i == 3:\n    break\n  i += 1\n\n\n1\n2\n3",
    "crumbs": [
      "Intro",
      "Basic Python"
    ]
  },
  {
    "objectID": "intro/3 Basic python.html#install-package",
    "href": "intro/3 Basic python.html#install-package",
    "title": "Basic Python",
    "section": "5.2 install package",
    "text": "5.2 install package\n\n\nCode\nimport os\nos.system('pip install scikit-learn')",
    "crumbs": [
      "Intro",
      "Basic Python"
    ]
  },
  {
    "objectID": "intro/3 Basic python.html#check-one-package-version",
    "href": "intro/3 Basic python.html#check-one-package-version",
    "title": "Basic Python",
    "section": "5.3 check one package version",
    "text": "5.3 check one package version\n\n\nCode\nimport os\nos.system('pip show scikit-learn')\n\n\nName: scikit-learn\nVersion: 1.4.1.post1\nSummary: A set of python modules for machine learning and data mining\nHome-page: https://scikit-learn.org\nAuthor: \nAuthor-email: \nLicense: new BSD\nLocation: /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages\nRequires: joblib, numpy, scipy, threadpoolctl\nRequired-by: imbalanced-learn, librosa, tune-sklearn\n\n\n0",
    "crumbs": [
      "Intro",
      "Basic Python"
    ]
  },
  {
    "objectID": "intro/3 Basic python.html#check-all-package-version",
    "href": "intro/3 Basic python.html#check-all-package-version",
    "title": "Basic Python",
    "section": "5.4 check all package version",
    "text": "5.4 check all package version\n\n\nCode\nimport os\nos.system('pip list')\n\n\nPackage                      Version\n---------------------------- ------------\nabsl-py                      1.4.0\naioquic-mitmproxy            0.9.21.1\naiosignal                    1.3.1\nannotated-types              0.6.0\nanyio                        4.0.0\nappdirs                      1.4.4\nappnope                      0.1.3\nargon2-cffi                  23.1.0\nargon2-cffi-bindings         21.2.0\narrow                        1.3.0\nasgiref                      3.7.2\nasttokens                    2.4.0\nastunparse                   1.6.3\nasync-lru                    2.0.4\nattrs                        23.1.0\naudioread                    3.0.1\nBabel                        2.13.0\nbackcall                     0.2.0\nbeautifulsoup4               4.12.2\nbleach                       6.1.0\nblinker                      1.7.0\nBrotli                       1.1.0\ncachetools                   5.3.1\ncelluloid                    0.2.0\ncertifi                      2023.7.22\ncffi                         1.16.0\ncharset-normalizer           3.3.0\nclick                        8.1.7\ncomm                         0.1.4\ncontourpy                    1.1.1\ncryptography                 41.0.7\ncycler                       0.12.1\ndacite                       1.8.1\ndebugpy                      1.8.0\ndecorator                    5.1.1\ndefusedxml                   0.7.1\ndill                         0.3.8\ndm-tree                      0.1.8\net-xmlfile                   1.1.0\netils                        1.8.0\nexecuting                    2.0.0\nfastjsonschema               2.18.1\nfilelock                     3.12.4\nFlask                        3.0.0\nflatbuffers                  23.5.26\nfonttools                    4.43.1\nfqdn                         1.5.1\nfrozenlist                   1.4.1\nfsspec                       2023.9.2\nfuture                       1.0.0\ngast                         0.5.4\ngoogle-auth                  2.23.3\ngoogle-auth-oauthlib         1.0.0\ngoogle-pasta                 0.2.0\ngoogleapis-common-protos     1.63.0\ngrpcio                       1.59.0\nh11                          0.14.0\nh2                           4.1.0\nh5py                         3.10.0\nhpack                        4.0.0\nhtmlmin                      0.1.12\nhtmltools                    0.5.1\nhuggingface-hub              0.17.3\nhyperframe                   6.0.1\nidna                         3.4\nImageHash                    4.3.1\nimageio                      2.34.0\nimbalanced-learn             0.12.2\nimblearn                     0.0\nimportlib_resources          6.4.0\ninstall                      1.3.5\nipykernel                    6.25.2\nipython                      8.16.1\nipython-genutils             0.2.0\nipywidgets                   8.1.1\nisoduration                  20.11.0\nitsdangerous                 2.1.2\njedi                         0.19.1\nJinja2                       3.1.2\njoblib                       1.3.2\njson5                        0.9.14\njsonpointer                  2.4\njsonschema                   4.19.1\njsonschema-specifications    2023.7.1\njupyter                      1.0.0\njupyter_client               8.3.1\njupyter-console              6.6.3\njupyter_core                 5.3.2\njupyter-events               0.7.0\njupyter-lsp                  2.2.0\njupyter_server               2.7.3\njupyter_server_terminals     0.4.4\njupyterlab                   4.0.6\njupyterlab-pygments          0.2.2\njupyterlab_server            2.25.0\njupyterlab-widgets           3.0.9\nkaggle                       1.5.16\nkaitaistruct                 0.10\nkaleido                      0.2.1\nkeras                        3.1.1\nKeras-Preprocessing          1.1.2\nkiwisolver                   1.4.5\nlazy_loader                  0.3\nldap3                        2.9.1\nlibclang                     16.0.6\nlibrosa                      0.10.1\nlinkify-it-py                2.0.3\nllvmlite                     0.41.0\nlxml                         4.9.3\nlzstring                     1.0.4\nMarkdown                     3.5\nmarkdown-it-py               3.0.0\nMarkupSafe                   2.1.3\nmatplotlib                   3.8.3\nmatplotlib-inline            0.1.6\nmdit-py-plugins              0.4.0\nmdurl                        0.1.2\nmissingno                    0.5.2\nmistune                      3.0.2\nmitmproxy                    10.1.5\nmitmproxy-macos              0.4.1\nmitmproxy_rs                 0.4.1\nmizani                       0.11.1\nml-dtypes                    0.3.2\nmpmath                       1.3.0\nmsgpack                      1.0.7\nmultimethod                  1.10\nmutagen                      1.47.0\nnamex                        0.0.7\nnbclient                     0.8.0\nnbconvert                    7.9.2\nnbformat                     5.9.2\nnest-asyncio                 1.5.8\nnetworkx                     3.1\nnltk                         3.8.1\nnotebook                     7.0.4\nnotebook_shim                0.2.3\nnumba                        0.58.0\nnumpy                        1.26.4\noauthlib                     3.2.2\nopencv-python                4.8.1.78\nopendatasets                 0.1.22\nopenpyxl                     3.1.2\nopt-einsum                   3.3.0\noptree                       0.10.0\noutcome                      1.3.0.post0\noverrides                    7.4.0\npackaging                    23.2\npandas                       2.2.1\npandas-profiling             3.2.0\npandocfilters                1.5.0\nparso                        0.8.3\npasslib                      1.7.4\npatchworklib                 0.6.4\npatsy                        0.5.3\npexpect                      4.8.0\nphik                         0.12.3\npickleshare                  0.7.5\nPillow                       10.0.1\npip                          24.0\nplatformdirs                 3.11.0\nplotly                       5.20.0\nplotnine                     0.13.3\npooch                        1.7.0\nprometheus-client            0.17.1\npromise                      2.3\nprompt-toolkit               3.0.36\nprotobuf                     3.20.3\npsutil                       5.9.5\nptyprocess                   0.7.0\npublicsuffix2                2.20191221\npure-eval                    0.2.2\npyarrow                      15.0.2\npyasn1                       0.5.0\npyasn1-modules               0.3.0\npycparser                    2.21\npycryptodomex                3.20.0\npydantic                     1.10.13\npydantic_core                2.10.1\npydantic-settings            2.0.3\nPygments                     2.16.1\npylsqpack                    0.3.18\npyOpenSSL                    23.3.0\npyparsing                    3.1.1\npyperclip                    1.8.2\npypi-latest                  0.1.2\npyplot-themes                0.2.2\nPySocks                      1.7.1\npython-dateutil              2.8.2\npython-dotenv                1.0.0\npython-json-logger           2.0.7\npython-multipart             0.0.9\npython-slugify               8.0.1\npytz                         2023.3.post1\nPyWavelets                   1.4.1\nPyYAML                       6.0.1\npyzmq                        25.1.1\nqtconsole                    5.4.4\nQtPy                         2.4.0\nquestionary                  2.0.1\nray                          2.10.0\nreferencing                  0.30.2\nregex                        2023.10.3\nrequests                     2.31.0\nrequests-oauthlib            1.3.1\nrfc3339-validator            0.1.4\nrfc3986-validator            0.1.1\nrich                         13.7.1\nrpds-py                      0.10.4\nrsa                          4.9\nruamel.yaml                  0.18.5\nruamel.yaml.clib             0.2.8\nsafetensors                  0.4.0\nscikit-image                 0.22.0\nscikit-learn                 1.4.1.post1\nscipy                        1.12.0\nseaborn                      0.12.2\nselenium                     4.15.2\nSend2Trash                   1.8.2\nservice-identity             23.1.0\nsetuptools                   65.5.0\nshiny                        0.7.1\nshinylive                    0.2.2\nsiuba                        0.4.4\nsix                          1.16.0\nsniffio                      1.3.0\nsortedcontainers             2.4.0\nsoundfile                    0.12.1\nsoupsieve                    2.5\nsoxr                         0.3.7\nSQLAlchemy                   2.0.22\nstack-data                   0.6.3\nstarlette                    0.34.0\nstatsmodels                  0.14.0\nsweetviz                     2.3.1\nsympy                        1.12\ntangled-up-in-unicode        0.2.0\ntenacity                     8.2.3\ntensorboard                  2.16.2\ntensorboard-data-server      0.7.1\ntensorboardX                 2.6.2.2\ntensorflow                   2.16.1\ntensorflow-datasets          4.9.4\ntensorflow_decision_forests  1.9.0\ntensorflow-estimator         2.14.0\ntensorflow-io-gcs-filesystem 0.34.0\ntensorflow-macos             2.14.0\ntensorflow-metadata          1.14.0\ntermcolor                    2.3.0\nterminado                    0.17.1\ntext-unidecode               1.3\ntf_keras                     2.16.0\nthreadpoolctl                3.2.0\ntifffile                     2024.2.12\ntinycss2                     1.2.1\ntokenizers                   0.14.1\ntoml                         0.10.2\ntorch                        2.1.0\ntorchvision                  0.16.0\ntornado                      6.3.3\ntqdm                         4.66.1\ntraitlets                    5.11.2\ntransformers                 4.34.0\ntrio                         0.23.1\ntrio-websocket               0.11.1\ntune-sklearn                 0.5.0\ntypeguard                    4.1.5\ntypes-python-dateutil        2.8.19.14\ntyping_extensions            4.8.0\ntzdata                       2023.3\nuc-micro-py                  1.0.3\nuri-template                 1.3.0\nurllib3                      2.0.6\nurwid-mitmproxy              2.1.2.1\nuvicorn                      0.27.1\nvisions                      0.7.5\nwatchfiles                   0.21.0\nwcwidth                      0.2.8\nwebcolors                    1.13\nwebencodings                 0.5.1\nwebsocket-client             1.6.4\nwebsockets                   12.0\nWerkzeug                     3.0.0\nwheel                        0.41.2\nwidgetsnbextension           4.0.9\nwordcloud                    1.9.2\nwrapt                        1.14.1\nwsproto                      1.2.0\nwurlitzer                    3.0.3\nxgboost                      2.0.3\nydata-profiling              4.6.0\nyt-dlp                       2023.12.30\nzipp                         3.18.1\nzstandard                    0.22.0\n\n\n0",
    "crumbs": [
      "Intro",
      "Basic Python"
    ]
  },
  {
    "objectID": "intro/3 Basic python.html",
    "href": "intro/3 Basic python.html",
    "title": "Basic Python",
    "section": "",
    "text": "# if/elif/else\nCode\na = 200\nb = 33\nif b &gt; a:\n  print(\"b is greater than a\")\nelif a == b:\n  print(\"a and b are equal\")\nelse:\n  print(\"a is greater than b\")\n\n\na is greater than b",
    "crumbs": [
      "Intro",
      "Basic Python"
    ]
  },
  {
    "objectID": "intro/3 Basic python.html#without-arguments",
    "href": "intro/3 Basic python.html#without-arguments",
    "title": "Basic Python",
    "section": "3.1 without Arguments",
    "text": "3.1 without Arguments\n\n\nCode\ndef my_function():\n  print(\"Hello from a function\")\n\n\n\n\nCode\nmy_function()\n\n\nHello from a function",
    "crumbs": [
      "Intro",
      "Basic Python"
    ]
  },
  {
    "objectID": "intro/3 Basic python.html#with-arguments",
    "href": "intro/3 Basic python.html#with-arguments",
    "title": "Basic Python",
    "section": "3.2 with Arguments",
    "text": "3.2 with Arguments\n\n\nCode\ndef my_function(x):\n  print(x + \" !!!!!!!!!!!!!!!!!\")\n\n\n\n\nCode\nmy_function('hello')\n\n\nhello !!!!!!!!!!!!!!!!!",
    "crumbs": [
      "Intro",
      "Basic Python"
    ]
  },
  {
    "objectID": "intro/3 Basic python.html#return-result",
    "href": "intro/3 Basic python.html#return-result",
    "title": "Basic Python",
    "section": "3.3 return result",
    "text": "3.3 return result\n\n\nCode\ndef adding_ten(x):\n  a=x+10\n  return(a)\n\n\n\n\nCode\nresult=adding_ten(3)\nresult\n\n\n13",
    "crumbs": [
      "Intro",
      "Basic Python"
    ]
  },
  {
    "objectID": "intro/3 Basic python.html#check-python-version",
    "href": "intro/3 Basic python.html#check-python-version",
    "title": "Basic Python",
    "section": "5.1 check python version",
    "text": "5.1 check python version\n\n\nCode\nimport sys\nprint(sys.version)\n\n\n3.11.4 (v3.11.4:d2340ef257, Jun  6 2023, 19:15:51) [Clang 13.0.0 (clang-1300.0.29.30)]",
    "crumbs": [
      "Intro",
      "Basic Python"
    ]
  },
  {
    "objectID": "intro/3 statistic Book.html",
    "href": "intro/3 statistic Book.html",
    "title": "Statistic Boook",
    "section": "",
    "text": "1 Naked Statistics\nby Charles Wheelan\n\n\n\n2 Storytelling with Data\nby Cole Nussbaumer Knaflic\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Intro",
      "Statistic Boook"
    ]
  },
  {
    "objectID": "intro/0 Basic python.html#for-loop",
    "href": "intro/0 Basic python.html#for-loop",
    "title": "Basic Python",
    "section": "3.1 for Loop",
    "text": "3.1 for Loop\n\n\nCode\nfruits = [\"apple\", \"banana\", \"cherry\"]\nfor x in fruits:\n  print(x)\n\n\napple\nbanana\ncherry",
    "crumbs": [
      "Intro",
      "Basic Python"
    ]
  },
  {
    "objectID": "intro/0 Basic python.html#while-loop",
    "href": "intro/0 Basic python.html#while-loop",
    "title": "Basic Python",
    "section": "3.3 while Loop",
    "text": "3.3 while Loop\n\n\nCode\ni = 1\nwhile i &lt; 4:\n  print(i)\n  i += 1\n\n\n1\n2\n3\n\n\nwith break statement\n\n\nCode\ni = 1\nwhile i &lt; 6:\n  print(i)\n  if i == 3:\n    break\n  i += 1\n\n\n1\n2\n3",
    "crumbs": [
      "Intro",
      "Basic Python"
    ]
  },
  {
    "objectID": "intro/0 Basic python.html#without-arguments",
    "href": "intro/0 Basic python.html#without-arguments",
    "title": "Basic Python",
    "section": "4.1 without Arguments",
    "text": "4.1 without Arguments\n\n\nCode\ndef my_function():\n  print(\"Hello from a function\")\n\n\n\n\nCode\nmy_function()\n\n\nHello from a function",
    "crumbs": [
      "Intro",
      "Basic Python"
    ]
  },
  {
    "objectID": "intro/0 Basic python.html#with-arguments",
    "href": "intro/0 Basic python.html#with-arguments",
    "title": "Basic Python",
    "section": "4.2 with Arguments",
    "text": "4.2 with Arguments\n\n\nCode\ndef my_function(x):\n  print(x + \" !!!!!!!!!!!!!!!!!\")\n\n\n\n\nCode\nmy_function('hello')\n\n\nhello !!!!!!!!!!!!!!!!!",
    "crumbs": [
      "Intro",
      "Basic Python"
    ]
  },
  {
    "objectID": "intro/0 Basic python.html#return-result",
    "href": "intro/0 Basic python.html#return-result",
    "title": "Basic Python",
    "section": "4.3 return result",
    "text": "4.3 return result\n\n\nCode\ndef adding_ten(x):\n  a=x+10\n  return(a)\n\n\n\n\nCode\nresult=adding_ten(3)\nresult\n\n\n13",
    "crumbs": [
      "Intro",
      "Basic Python"
    ]
  },
  {
    "objectID": "intro/0 Basic python.html#check-python-version",
    "href": "intro/0 Basic python.html#check-python-version",
    "title": "Basic Python",
    "section": "6.1 check python version",
    "text": "6.1 check python version\n\n\nCode\nimport sys\nprint(sys.version)\n\n\n3.11.4 (v3.11.4:d2340ef257, Jun  6 2023, 19:15:51) [Clang 13.0.0 (clang-1300.0.29.30)]",
    "crumbs": [
      "Intro",
      "Basic Python"
    ]
  },
  {
    "objectID": "intro/0 Basic python.html#install-package",
    "href": "intro/0 Basic python.html#install-package",
    "title": "Basic Python",
    "section": "6.2 install package",
    "text": "6.2 install package\nThe Python Package Index (PyPI).https://pypi.org//\n\n\n\nCode\nimport os\nos.system('pip install scikit-learn')",
    "crumbs": [
      "Intro",
      "Basic Python"
    ]
  },
  {
    "objectID": "intro/0 Basic python.html#check-one-package-version",
    "href": "intro/0 Basic python.html#check-one-package-version",
    "title": "Basic Python",
    "section": "6.3 check one package version",
    "text": "6.3 check one package version\n\n\nCode\nimport os\nos.system('pip show scikit-learn')\n\n\nName: scikit-learn\nVersion: 1.4.1.post1\nSummary: A set of python modules for machine learning and data mining\nHome-page: https://scikit-learn.org\nAuthor: \nAuthor-email: \nLicense: new BSD\nLocation: /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages\nRequires: joblib, numpy, scipy, threadpoolctl\nRequired-by: imbalanced-learn, librosa, tune-sklearn\n\n\n0",
    "crumbs": [
      "Intro",
      "Basic Python"
    ]
  },
  {
    "objectID": "intro/0 Basic python.html#check-all-package-version",
    "href": "intro/0 Basic python.html#check-all-package-version",
    "title": "Basic Python",
    "section": "6.4 check all package version",
    "text": "6.4 check all package version\n\n\nCode\nimport os\nos.system('pip list')\n\n\nPackage                      Version\n---------------------------- ------------\nabsl-py                      1.4.0\naioquic-mitmproxy            0.9.21.1\naiosignal                    1.3.1\nannotated-types              0.6.0\nanyio                        4.0.0\nappdirs                      1.4.4\nappnope                      0.1.3\nargon2-cffi                  23.1.0\nargon2-cffi-bindings         21.2.0\narrow                        1.3.0\nasgiref                      3.7.2\nasttokens                    2.4.0\nastunparse                   1.6.3\nasync-lru                    2.0.4\nattrs                        23.1.0\naudioread                    3.0.1\nBabel                        2.13.0\nbackcall                     0.2.0\nbeautifulsoup4               4.12.2\nbleach                       6.1.0\nblinker                      1.7.0\nBrotli                       1.1.0\ncachetools                   5.3.1\ncelluloid                    0.2.0\ncertifi                      2023.7.22\ncffi                         1.16.0\ncharset-normalizer           3.3.0\nclick                        8.1.7\ncomm                         0.1.4\ncontourpy                    1.1.1\ncryptography                 41.0.7\ncycler                       0.12.1\ndacite                       1.8.1\ndebugpy                      1.8.0\ndecorator                    5.1.1\ndefusedxml                   0.7.1\ndill                         0.3.8\ndm-tree                      0.1.8\net-xmlfile                   1.1.0\netils                        1.8.0\nexecuting                    2.0.0\nfastjsonschema               2.18.1\nfilelock                     3.12.4\nFlask                        3.0.0\nflatbuffers                  23.5.26\nfonttools                    4.43.1\nfqdn                         1.5.1\nfrozenlist                   1.4.1\nfsspec                       2023.9.2\nfuture                       1.0.0\ngast                         0.5.4\ngoogle-auth                  2.23.3\ngoogle-auth-oauthlib         1.0.0\ngoogle-pasta                 0.2.0\ngoogleapis-common-protos     1.63.0\ngrpcio                       1.59.0\nh11                          0.14.0\nh2                           4.1.0\nh5py                         3.10.0\nhpack                        4.0.0\nhtmlmin                      0.1.12\nhtmltools                    0.5.1\nhuggingface-hub              0.17.3\nhyperframe                   6.0.1\nidna                         3.4\nImageHash                    4.3.1\nimageio                      2.34.0\nimbalanced-learn             0.12.2\nimblearn                     0.0\nimportlib_resources          6.4.0\ninstall                      1.3.5\nipykernel                    6.25.2\nipython                      8.16.1\nipython-genutils             0.2.0\nipywidgets                   8.1.1\nisoduration                  20.11.0\nitsdangerous                 2.1.2\njedi                         0.19.1\nJinja2                       3.1.2\njoblib                       1.3.2\njson5                        0.9.14\njsonpointer                  2.4\njsonschema                   4.19.1\njsonschema-specifications    2023.7.1\njupyter                      1.0.0\njupyter_client               8.3.1\njupyter-console              6.6.3\njupyter_core                 5.3.2\njupyter-events               0.7.0\njupyter-lsp                  2.2.0\njupyter_server               2.7.3\njupyter_server_terminals     0.4.4\njupyterlab                   4.0.6\njupyterlab-pygments          0.2.2\njupyterlab_server            2.25.0\njupyterlab-widgets           3.0.9\nkaggle                       1.5.16\nkaitaistruct                 0.10\nkaleido                      0.2.1\nkeras                        3.1.1\nKeras-Preprocessing          1.1.2\nkiwisolver                   1.4.5\nlazy_loader                  0.3\nldap3                        2.9.1\nlibclang                     16.0.6\nlibrosa                      0.10.1\nlinkify-it-py                2.0.3\nllvmlite                     0.41.0\nlxml                         4.9.3\nlzstring                     1.0.4\nMarkdown                     3.5\nmarkdown-it-py               3.0.0\nMarkupSafe                   2.1.3\nmatplotlib                   3.8.3\nmatplotlib-inline            0.1.6\nmdit-py-plugins              0.4.0\nmdurl                        0.1.2\nmissingno                    0.5.2\nmistune                      3.0.2\nmitmproxy                    10.1.5\nmitmproxy-macos              0.4.1\nmitmproxy_rs                 0.4.1\nmizani                       0.11.1\nml-dtypes                    0.3.2\nmpmath                       1.3.0\nmsgpack                      1.0.7\nmultimethod                  1.10\nmutagen                      1.47.0\nnamex                        0.0.7\nnbclient                     0.8.0\nnbconvert                    7.9.2\nnbformat                     5.9.2\nnest-asyncio                 1.5.8\nnetworkx                     3.1\nnltk                         3.8.1\nnotebook                     7.0.4\nnotebook_shim                0.2.3\nnumba                        0.58.0\nnumpy                        1.26.4\noauthlib                     3.2.2\nopencv-python                4.8.1.78\nopendatasets                 0.1.22\nopenpyxl                     3.1.2\nopt-einsum                   3.3.0\noptree                       0.10.0\noutcome                      1.3.0.post0\noverrides                    7.4.0\npackaging                    23.2\npandas                       2.2.1\npandas-profiling             3.2.0\npandocfilters                1.5.0\nparso                        0.8.3\npasslib                      1.7.4\npatchworklib                 0.6.4\npatsy                        0.5.3\npexpect                      4.8.0\nphik                         0.12.3\npickleshare                  0.7.5\nPillow                       10.0.1\npip                          24.0\nplatformdirs                 3.11.0\nplotly                       5.20.0\nplotnine                     0.13.3\npooch                        1.7.0\nprometheus-client            0.17.1\npromise                      2.3\nprompt-toolkit               3.0.36\nprotobuf                     3.20.3\npsutil                       5.9.5\nptyprocess                   0.7.0\npublicsuffix2                2.20191221\npure-eval                    0.2.2\npyarrow                      15.0.2\npyasn1                       0.5.0\npyasn1-modules               0.3.0\npycparser                    2.21\npycryptodomex                3.20.0\npydantic                     1.10.13\npydantic_core                2.10.1\npydantic-settings            2.0.3\nPygments                     2.16.1\npylsqpack                    0.3.18\npyOpenSSL                    23.3.0\npyparsing                    3.1.1\npyperclip                    1.8.2\npypi-latest                  0.1.2\npyplot-themes                0.2.2\nPySocks                      1.7.1\npython-dateutil              2.8.2\npython-dotenv                1.0.0\npython-json-logger           2.0.7\npython-multipart             0.0.9\npython-slugify               8.0.1\npytz                         2023.3.post1\nPyWavelets                   1.4.1\nPyYAML                       6.0.1\npyzmq                        25.1.1\nqtconsole                    5.4.4\nQtPy                         2.4.0\nquestionary                  2.0.1\nray                          2.10.0\nreferencing                  0.30.2\nregex                        2023.10.3\nrequests                     2.31.0\nrequests-oauthlib            1.3.1\nrfc3339-validator            0.1.4\nrfc3986-validator            0.1.1\nrich                         13.7.1\nrpds-py                      0.10.4\nrsa                          4.9\nruamel.yaml                  0.18.5\nruamel.yaml.clib             0.2.8\nsafetensors                  0.4.0\nscikit-image                 0.22.0\nscikit-learn                 1.4.1.post1\nscipy                        1.12.0\nseaborn                      0.12.2\nselenium                     4.15.2\nSend2Trash                   1.8.2\nservice-identity             23.1.0\nsetuptools                   65.5.0\nshiny                        0.7.1\nshinylive                    0.2.2\nsiuba                        0.4.4\nsix                          1.16.0\nsniffio                      1.3.0\nsortedcontainers             2.4.0\nsoundfile                    0.12.1\nsoupsieve                    2.5\nsoxr                         0.3.7\nSQLAlchemy                   2.0.22\nstack-data                   0.6.3\nstarlette                    0.34.0\nstatsmodels                  0.14.0\nsweetviz                     2.3.1\nsympy                        1.12\ntangled-up-in-unicode        0.2.0\ntenacity                     8.2.3\ntensorboard                  2.16.2\ntensorboard-data-server      0.7.1\ntensorboardX                 2.6.2.2\ntensorflow                   2.16.1\ntensorflow-datasets          4.9.4\ntensorflow_decision_forests  1.9.0\ntensorflow-estimator         2.14.0\ntensorflow-io-gcs-filesystem 0.34.0\ntensorflow-macos             2.14.0\ntensorflow-metadata          1.14.0\ntermcolor                    2.3.0\nterminado                    0.17.1\ntext-unidecode               1.3\ntf_keras                     2.16.0\nthreadpoolctl                3.2.0\ntifffile                     2024.2.12\ntinycss2                     1.2.1\ntokenizers                   0.14.1\ntoml                         0.10.2\ntorch                        2.1.0\ntorchvision                  0.16.0\ntornado                      6.3.3\ntqdm                         4.66.1\ntraitlets                    5.11.2\ntransformers                 4.34.0\ntrio                         0.23.1\ntrio-websocket               0.11.1\ntune-sklearn                 0.5.0\ntypeguard                    4.1.5\ntypes-python-dateutil        2.8.19.14\ntyping_extensions            4.8.0\ntzdata                       2023.3\nuc-micro-py                  1.0.3\nuri-template                 1.3.0\nurllib3                      2.0.6\nurwid-mitmproxy              2.1.2.1\nuvicorn                      0.27.1\nvisions                      0.7.5\nwatchfiles                   0.21.0\nwcwidth                      0.2.8\nwebcolors                    1.13\nwebencodings                 0.5.1\nwebsocket-client             1.6.4\nwebsockets                   12.0\nWerkzeug                     3.0.0\nwheel                        0.41.2\nwidgetsnbextension           4.0.9\nwordcloud                    1.9.2\nwrapt                        1.14.1\nwsproto                      1.2.0\nwurlitzer                    3.0.3\nxgboost                      2.0.3\nydata-profiling              4.6.0\nyt-dlp                       2023.12.30\nzipp                         3.18.1\nzstandard                    0.22.0\n\n\n0",
    "crumbs": [
      "Intro",
      "Basic Python"
    ]
  },
  {
    "objectID": "data manipulation/2 data structure in Python .html#dimension",
    "href": "data manipulation/2 data structure in Python .html#dimension",
    "title": "Data structure in Python",
    "section": "2.5 dimension",
    "text": "2.5 dimension\n\n\nCode\nA2.ndim\n\n\n2",
    "crumbs": [
      "data manipulation",
      "Data structure in Python"
    ]
  },
  {
    "objectID": "classification/9 break line.html",
    "href": "classification/9 break line.html",
    "title": "———————————————————",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Classification",
      "---------------------------------------------------------"
    ]
  },
  {
    "objectID": "clustering/4 Principal component analysis .html",
    "href": "clustering/4 Principal component analysis .html",
    "title": "Principal component analysis",
    "section": "",
    "text": "Principal component analysis (PCA) is a method of reducing the dimensionality of data and is used to improve data visualization and speed up machine learning model training.",
    "crumbs": [
      "Clustering",
      "Principal component analysis"
    ]
  },
  {
    "objectID": "clustering/4 Principal component analysis .html#download-data",
    "href": "clustering/4 Principal component analysis .html#download-data",
    "title": "Principal component analysis",
    "section": "2.1 download data",
    "text": "2.1 download data\nhttps://www.kaggle.com/datasets/shwetabh123/mall-customer",
    "crumbs": [
      "Clustering",
      "Principal component analysis"
    ]
  },
  {
    "objectID": "clustering/4 Principal component analysis .html#input-data",
    "href": "clustering/4 Principal component analysis .html#input-data",
    "title": "Principal component analysis",
    "section": "2.2 input data",
    "text": "2.2 input data\n\n\nCode\nimport pandas as pd\n\nurl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n\n# load dataset into Pandas DataFrame\ndf = pd.read_csv(url, names=['sepal length','sepal width','petal length','petal width','target'])\n\n# Showing overview of the train dataset\ndf.head()\n\n\n\n\n\n\n\n\n\n\nsepal length\nsepal width\npetal length\npetal width\ntarget\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nIris-setosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nIris-setosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nIris-setosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nIris-setosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nIris-setosa",
    "crumbs": [
      "Clustering",
      "Principal component analysis"
    ]
  },
  {
    "objectID": "clustering/4 Principal component analysis .html#standardize-the-data",
    "href": "clustering/4 Principal component analysis .html#standardize-the-data",
    "title": "Principal component analysis",
    "section": "2.3 STANDARDIZE THE DATA",
    "text": "2.3 STANDARDIZE THE DATA\nPCA is affected by scale, so you need to scale the features in your data before applying PCA. Use StandardScaler to help you standardize the data set’s features onto unit scale (mean = 0 and variance = 1), which is a requirement for the optimal performance of many machine learning algorithms. If you don’t scale your data, it can have a negative effect on your algorithm.\n\n\nCode\nfrom sklearn.preprocessing import StandardScaler\n\nfeatures = ['sepal length', 'sepal width', 'petal length', 'petal width']\n\n# Separating out the features， dataframe to numpy array\nx_raw = df.loc[:, features].values\n\n# Separating out the target, dataframe to numpy array\ny = df.loc[:,['target']].values\n\n# Standardizing the features\nscaler = StandardScaler().fit(x_raw)\nx = scaler.fit_transform(x_raw)",
    "crumbs": [
      "Clustering",
      "Principal component analysis"
    ]
  },
  {
    "objectID": "clustering/4 Principal component analysis .html#before-standardize",
    "href": "clustering/4 Principal component analysis .html#before-standardize",
    "title": "Principal component analysis",
    "section": "2.4 Before STANDARDIZE",
    "text": "2.4 Before STANDARDIZE\n\n\nCode\nx_raw[:3]\n\n\narray([[5.1, 3.5, 1.4, 0.2],\n       [4.9, 3. , 1.4, 0.2],\n       [4.7, 3.2, 1.3, 0.2]])\n\n\n\n\nCode\nnp.mean(x_raw, axis=0)\n\n\narray([5.84333333, 3.054     , 3.75866667, 1.19866667])\n\n\n\n\nCode\nnp.std(x_raw, axis=0)\n\n\narray([0.82530129, 0.43214658, 1.75852918, 0.76061262])\n\n\n\n\nCode\nx_raw_df=pd.DataFrame(x_raw, columns=features)\n\n\n\n\nCode\np=(\n    ggplot(data=x_raw_df)+aes(x='sepal length')+ geom_histogram()\n)\n\np",
    "crumbs": [
      "Clustering",
      "Principal component analysis"
    ]
  },
  {
    "objectID": "clustering/4 Principal component analysis .html#after-standardize",
    "href": "clustering/4 Principal component analysis .html#after-standardize",
    "title": "Principal component analysis",
    "section": "2.5 After STANDARDIZE",
    "text": "2.5 After STANDARDIZE\n\n\nCode\nx[:3]\n\n\narray([[-0.90068117,  1.03205722, -1.3412724 , -1.31297673],\n       [-1.14301691, -0.1249576 , -1.3412724 , -1.31297673],\n       [-1.38535265,  0.33784833, -1.39813811, -1.31297673]])\n\n\n\n\nCode\nnp.mean(x, axis=0)\n\n\narray([-4.73695157e-16, -6.63173220e-16,  3.31586610e-16, -2.84217094e-16])\n\n\n\n\nCode\nnp.std(x, axis=0)\n\n\narray([1., 1., 1., 1.])\n\n\n\n\nCode\nx_df=pd.DataFrame(x, columns=features)\n\n\n\n\nCode\np=(\n    ggplot(data=x_df)+aes(x='sepal length')+ geom_histogram()\n)\n\np",
    "crumbs": [
      "Clustering",
      "Principal component analysis"
    ]
  },
  {
    "objectID": "clustering/4 Principal component analysis .html#inverse-standardize.for-testing-purpose-only-have-no-impact-on-pca.",
    "href": "clustering/4 Principal component analysis .html#inverse-standardize.for-testing-purpose-only-have-no-impact-on-pca.",
    "title": "Principal component analysis",
    "section": "2.6 inverse STANDARDIZE.for testing purpose only, have no impact on PCA.",
    "text": "2.6 inverse STANDARDIZE.for testing purpose only, have no impact on PCA.\nfor testing purpose only.\n\n\nCode\nx_inverse=scaler.inverse_transform(x)\n\n\n\n\nCode\nnp.mean(x_inverse, axis=0)\n\n\narray([5.84333333, 3.054     , 3.75866667, 1.19866667])\n\n\n\n\nCode\nnp.std(x_inverse, axis=0)\n\n\narray([0.82530129, 0.43214658, 1.75852918, 0.76061262])",
    "crumbs": [
      "Clustering",
      "Principal component analysis"
    ]
  },
  {
    "objectID": "intro/0 Basic python.html#get-current-directory",
    "href": "intro/0 Basic python.html#get-current-directory",
    "title": "Basic Python",
    "section": "1.1 get current directory",
    "text": "1.1 get current directory\n\n\nCode\nimport os\nos.getcwd()",
    "crumbs": [
      "Intro",
      "Basic Python"
    ]
  },
  {
    "objectID": "intro/0 Basic python.html#get-all-file-name-under-current-directory",
    "href": "intro/0 Basic python.html#get-all-file-name-under-current-directory",
    "title": "Basic Python",
    "section": "1.2 get all file name under current directory",
    "text": "1.2 get all file name under current directory\n\n\nCode\nimport os\nos.listdir(os.getcwd()) \n\n\n['.Rhistory',\n 'test_folder',\n '.DS_Store',\n 'images',\n '0 Basic python.ipynb',\n '0 Basic python.qmd',\n 'hotels.csv',\n '.RData',\n '3 statistic Book.qmd',\n '1 Python Book.qmd',\n '2 Data Book.qmd',\n 'data']",
    "crumbs": [
      "Intro",
      "Basic Python"
    ]
  },
  {
    "objectID": "intro/0 Basic python.html#get-file-info",
    "href": "intro/0 Basic python.html#get-file-info",
    "title": "Basic Python",
    "section": "1.3 get file info",
    "text": "1.3 get file info\n\n\nCode\na=os.stat('3 statistic Book.qmd')\na\n\n\nos.stat_result(st_mode=33188, st_ino=29449448, st_dev=16777233, st_nlink=1, st_uid=501, st_gid=20, st_size=453, st_atime=1712419160, st_mtime=1712419159, st_ctime=1712419159)\n\n\nshow st_atime\n\n\nCode\nimport datetime as dt\n#a.st_atime\ndt.date.fromtimestamp(a.st_atime).strftime('%Y%m%d')\n\n\n'20240406'",
    "crumbs": [
      "Intro",
      "Basic Python"
    ]
  },
  {
    "objectID": "intro/0 Basic python.html#create-folder",
    "href": "intro/0 Basic python.html#create-folder",
    "title": "Basic Python",
    "section": "1.4 create folder",
    "text": "1.4 create folder\ncreate it if not exist\n\n\nCode\nif not os.path.exists('test_folder'): \n  os.mkdir('test_folder')",
    "crumbs": [
      "Intro",
      "Basic Python"
    ]
  },
  {
    "objectID": "intro/0 Basic python.html#delete-folder",
    "href": "intro/0 Basic python.html#delete-folder",
    "title": "Basic Python",
    "section": "1.5 delete folder",
    "text": "1.5 delete folder\n\n\nCode\nos.rmdir('test_folder')",
    "crumbs": [
      "Intro",
      "Basic Python"
    ]
  },
  {
    "objectID": "intro/0 Basic python.html#delete-file",
    "href": "intro/0 Basic python.html#delete-file",
    "title": "Basic Python",
    "section": "1.6 delete file",
    "text": "1.6 delete file\n\n\nCode\nos.remove('test.csv')",
    "crumbs": [
      "Intro",
      "Basic Python"
    ]
  },
  {
    "objectID": "intro/0 Basic python.html#copy-file",
    "href": "intro/0 Basic python.html#copy-file",
    "title": "Basic Python",
    "section": "1.7 copy file",
    "text": "1.7 copy file\n\n\nCode\nimport shutil\n\nshutil.copyfile('test.csv', 'test2.csv')",
    "crumbs": [
      "Intro",
      "Basic Python"
    ]
  },
  {
    "objectID": "intro/0 Basic python.html#downlaod-file-online",
    "href": "intro/0 Basic python.html#downlaod-file-online",
    "title": "Basic Python",
    "section": "1.8 downlaod file online",
    "text": "1.8 downlaod file online\n\n\nCode\nimport urllib.request\n\nurl=\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv\"\n\nurllib.request.urlretrieve(url, \"hotels.csv\")",
    "crumbs": [
      "Intro",
      "Basic Python"
    ]
  },
  {
    "objectID": "intro/0 Basic python.html#check-package-install-loaciton",
    "href": "intro/0 Basic python.html#check-package-install-loaciton",
    "title": "Basic Python",
    "section": "6.5 check package install loaciton",
    "text": "6.5 check package install loaciton\n\n\nCode\nimport site; site.getsitepackages()",
    "crumbs": [
      "Intro",
      "Basic Python"
    ]
  },
  {
    "objectID": "intro/0 Basic python.html#for-enumerate-loop",
    "href": "intro/0 Basic python.html#for-enumerate-loop",
    "title": "Basic Python",
    "section": "3.2 for enumerate Loop",
    "text": "3.2 for enumerate Loop\nit will add index\n\n\nCode\nfruits = [\"apple\", \"banana\", \"cherry\"]\n\nlist(enumerate(fruits))\n\n\n[(0, 'apple'), (1, 'banana'), (2, 'cherry')]\n\n\n\n\nCode\nfor index,i in enumerate(fruits):\n  print(\"The index is:\",index)\n  print(\"The element is:\",i)\n\n\nThe index is: 0\nThe element is: apple\nThe index is: 1\nThe element is: banana\nThe index is: 2\nThe element is: cherry",
    "crumbs": [
      "Intro",
      "Basic Python"
    ]
  },
  {
    "objectID": "intro/0 Basic python.html#lambda-function",
    "href": "intro/0 Basic python.html#lambda-function",
    "title": "Basic Python",
    "section": "4.4 lambda function",
    "text": "4.4 lambda function\nit a faster way to do function\n\n\nCode\nlambda_adding_ten=lambda x:x+10\n\n\n\n\nCode\nlambda_adding_ten(4)\n\n\n14",
    "crumbs": [
      "Intro",
      "Basic Python"
    ]
  },
  {
    "objectID": "data manipulation/5 SQL database.html",
    "href": "data manipulation/5 SQL database.html",
    "title": "SQL database with python",
    "section": "",
    "text": "Code\nimport seaborn as sns\nimport pandas as pd\nimport sqlite3\n\ntips = sns.load_dataset(\"tips\")\nplanets=sns.load_dataset('planets')\n\n\n\n\n\n\nCode\nimport sqlite3\n# Create your connection.\ndb = sqlite3.connect('pythonsqlite.db')\n\ntips.to_sql(name='tips', con=db,if_exists='replace')\nplanets.to_sql(name='planets', con=db,if_exists='replace')\n\n\ndb.close()\n\n\n\n\n\n\n\nCode\ndb = sqlite3.connect('pythonsqlite.db')\ntable = pd.read_sql_query(\"SELECT name FROM sqlite_master WHERE type='table';\", db)\ntable\n\n\n\n\n\n\n\n\n\n\nname\n\n\n\n\n0\ntips\n\n\n1\nplanets",
    "crumbs": [
      "data manipulation",
      "SQL database with python"
    ]
  },
  {
    "objectID": "data manipulation/5 SQL database.html#create-database-file-pythonsqlite.db-and-copy-tips-data-and-planets-data-into-database",
    "href": "data manipulation/5 SQL database.html#create-database-file-pythonsqlite.db-and-copy-tips-data-and-planets-data-into-database",
    "title": "SQL database with python",
    "section": "1.1 create database file pythonsqlite.db and copy tips data and planets data into database",
    "text": "1.1 create database file pythonsqlite.db and copy tips data and planets data into database\n\n\nCode\nimport sqlite3\n# Create your connection.\ndb = sqlite3.connect('pythonsqlite.db')\n\ntips.to_sql(name='tips', con=db,if_exists='replace')\nplanets.to_sql(name='planets', con=db,if_exists='replace')\n\n\ndb.close()",
    "crumbs": [
      "data manipulation",
      "SQL database with python"
    ]
  },
  {
    "objectID": "data manipulation/5 SQL database.html#check-all-table-in-database",
    "href": "data manipulation/5 SQL database.html#check-all-table-in-database",
    "title": "SQL database with python",
    "section": "1.2 check all table in database",
    "text": "1.2 check all table in database\n\n\nCode\ndb = sqlite3.connect('pythonsqlite.db')\ntable = pd.read_sql_query(\"SELECT name FROM sqlite_master WHERE type='table';\", db)\ntable\n\n\n\n\n\n\n\n\n\n\nname\n\n\n\n\n0\ntips\n\n\n1\nplanets",
    "crumbs": [
      "data manipulation",
      "SQL database with python"
    ]
  },
  {
    "objectID": "data manipulation/5 SQL database.html#select",
    "href": "data manipulation/5 SQL database.html#select",
    "title": "SQL database with python",
    "section": "2.1 select",
    "text": "2.1 select\n\n\nCode\ndb = sqlite3.connect('pythonsqlite.db')\nsql=\"select * from tips LIMIT 3;\"\ntable = pd.read_sql_query(sql,db)\ntable\n\n\n\n\n\n\n\n\n\n\nindex\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3",
    "crumbs": [
      "data manipulation",
      "SQL database with python"
    ]
  },
  {
    "objectID": "data manipulation/5 SQL database.html#renaming-column",
    "href": "data manipulation/5 SQL database.html#renaming-column",
    "title": "SQL database with python",
    "section": "2.2 Renaming column",
    "text": "2.2 Renaming column\n\n\nCode\nsql=\"select total_bill as new_total_bill from tips\"\ntable = pd.read_sql_query(sql,db)\ntable.head()\n\n\n\n\n\n\n\n\n\n\nnew_total_bill\n\n\n\n\n0\n16.99\n\n\n1\n10.34\n\n\n2\n21.01\n\n\n3\n23.68\n\n\n4\n24.59",
    "crumbs": [
      "data manipulation",
      "SQL database with python"
    ]
  },
  {
    "objectID": "data manipulation/5 SQL database.html#create-column",
    "href": "data manipulation/5 SQL database.html#create-column",
    "title": "SQL database with python",
    "section": "2.3 create column",
    "text": "2.3 create column\n\n\nCode\nsql=\"select total_bill as new_total_bill,total_bill from tips\"\ntable = pd.read_sql_query(sql,db)\ntable.head()\n\n\n\n\n\n\n\n\n\n\nnew_total_bill\ntotal_bill\n\n\n\n\n0\n16.99\n16.99\n\n\n1\n10.34\n10.34\n\n\n2\n21.01\n21.01\n\n\n3\n23.68\n23.68\n\n\n4\n24.59\n24.59",
    "crumbs": [
      "data manipulation",
      "SQL database with python"
    ]
  },
  {
    "objectID": "data manipulation/5 SQL database.html#filter-rows",
    "href": "data manipulation/5 SQL database.html#filter-rows",
    "title": "SQL database with python",
    "section": "2.4 Filter rows",
    "text": "2.4 Filter rows\n\n\nCode\nsql=\"select * from tips where sex='Male'\"\ntable = pd.read_sql_query(sql,db)\ntable.head()\n\n\n\n\n\n\n\n\n\n\nindex\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n1\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n2\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n3\n5\n25.29\n4.71\nMale\nNo\nSun\nDinner\n4\n\n\n4\n6\n8.77\n2.00\nMale\nNo\nSun\nDinner\n2\n\n\n\n\n\n\n\n\n\n2.4.1 Filters with AND conditions\n\n\nCode\nsql=\"select * from tips where sex='Male' and size&gt;3\"\ntable = pd.read_sql_query(sql,db)\ntable.head()\n\n\n\n\n\n\n\n\n\n\nindex\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n5\n25.29\n4.71\nMale\nNo\nSun\nDinner\n4\n\n\n1\n7\n26.88\n3.12\nMale\nNo\nSun\nDinner\n4\n\n\n2\n13\n18.43\n3.00\nMale\nNo\nSun\nDinner\n4\n\n\n3\n23\n39.42\n7.58\nMale\nNo\nSat\nDinner\n4\n\n\n4\n25\n17.81\n2.34\nMale\nNo\nSat\nDinner\n4\n\n\n\n\n\n\n\n\n\n\n2.4.2 Filters with or conditions\n\n\nCode\nsql=\"select * from tips where sex='Male' or size&gt;3\"\ntable = pd.read_sql_query(sql,db)\ntable.shape\n\n\n(169, 8)",
    "crumbs": [
      "data manipulation",
      "SQL database with python"
    ]
  },
  {
    "objectID": "data manipulation/5 SQL database.html#append",
    "href": "data manipulation/5 SQL database.html#append",
    "title": "SQL database with python",
    "section": "2.5 Append",
    "text": "2.5 Append\n\n2.5.1 append by row\n\n\nCode\nsql=\"select * from tips UNION all select * from tips\"\ntable = pd.read_sql_query(sql,db)\ntable.shape\n\n\n(488, 8)\n\n\n\n\n2.5.2 append by column\n\n\n2.5.3 Dropping NA values\n\n\n2.5.4 keep NA values",
    "crumbs": [
      "data manipulation",
      "SQL database with python"
    ]
  },
  {
    "objectID": "data manipulation/5 SQL database.html#group-by",
    "href": "data manipulation/5 SQL database.html#group-by",
    "title": "SQL database with python",
    "section": "2.6 group by",
    "text": "2.6 group by\n\n2.6.1 average,min,max,sum\n\n\nCode\nsql=\"select AVG(total_bill),min(total_bill),max(total_bill),sum(total_bill) from tips group by sex\"\ntable = pd.read_sql_query(sql,db)\ntable\n\n\n\n\n\n\n\n\n\n\nAVG(total_bill)\nmin(total_bill)\nmax(total_bill)\nsum(total_bill)\n\n\n\n\n0\n18.056897\n3.07\n44.30\n1570.95\n\n\n1\n20.744076\n7.25\n50.81\n3256.82\n\n\n\n\n\n\n\n\n\n\n2.6.2 count\n\n\nCode\nsql=\"select sex,count(*) from tips group by 1\"\ntable = pd.read_sql_query(sql,db)\ntable\n\n\n\n\n\n\n\n\n\n\nsex\ncount(*)\n\n\n\n\n0\nFemale\n87\n\n\n1\nMale\n157",
    "crumbs": [
      "data manipulation",
      "SQL database with python"
    ]
  },
  {
    "objectID": "data manipulation/5 SQL database.html#order-rows",
    "href": "data manipulation/5 SQL database.html#order-rows",
    "title": "SQL database with python",
    "section": "2.7 order rows",
    "text": "2.7 order rows\n\n\nCode\nsql=\"select * from tips order by total_bill\"\ntable = pd.read_sql_query(sql,db)\ntable.head()\n\n\n\n\n\n\n\n\n\n\nindex\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n67\n3.07\n1.00\nFemale\nYes\nSat\nDinner\n1\n\n\n1\n92\n5.75\n1.00\nFemale\nYes\nFri\nDinner\n2\n\n\n2\n111\n7.25\n1.00\nFemale\nNo\nSat\nDinner\n1\n\n\n3\n172\n7.25\n5.15\nMale\nYes\nSun\nDinner\n2\n\n\n4\n149\n7.51\n2.00\nMale\nNo\nThur\nLunch\n2\n\n\n\n\n\n\n\n\n\n2.7.1 Sort in descending order\n\n\nCode\nsql=\"select * from tips order by total_bill desc\"\ntable = pd.read_sql_query(sql,db)\ntable.head()\n\n\n\n\n\n\n\n\n\n\nindex\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n170\n50.81\n10.00\nMale\nYes\nSat\nDinner\n3\n\n\n1\n212\n48.33\n9.00\nMale\nNo\nSat\nDinner\n4\n\n\n2\n59\n48.27\n6.73\nMale\nNo\nSat\nDinner\n4\n\n\n3\n156\n48.17\n5.00\nMale\nNo\nSun\nDinner\n6\n\n\n4\n182\n45.35\n3.50\nMale\nYes\nSun\nDinner\n3\n\n\n\n\n\n\n\n\n\n\n2.7.2 Arrange by multiple variables\n\n\nCode\nsql=\"select * from tips order by total_bill,tip\"\ntable = pd.read_sql_query(sql,db)\ntable.head()\n\n\n\n\n\n\n\n\n\n\nindex\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n67\n3.07\n1.00\nFemale\nYes\nSat\nDinner\n1\n\n\n1\n92\n5.75\n1.00\nFemale\nYes\nFri\nDinner\n2\n\n\n2\n111\n7.25\n1.00\nFemale\nNo\nSat\nDinner\n1\n\n\n3\n172\n7.25\n5.15\nMale\nYes\nSun\nDinner\n2\n\n\n4\n149\n7.51\n2.00\nMale\nNo\nThur\nLunch\n2",
    "crumbs": [
      "data manipulation",
      "SQL database with python"
    ]
  },
  {
    "objectID": "data manipulation/5 SQL database.html#join",
    "href": "data manipulation/5 SQL database.html#join",
    "title": "SQL database with python",
    "section": "2.8 join",
    "text": "2.8 join\n\n2.8.1 inner_join\n\n\n2.8.2 full join\n\n\n2.8.3 left join\n\n\n2.8.4 anti join",
    "crumbs": [
      "data manipulation",
      "SQL database with python"
    ]
  },
  {
    "objectID": "data manipulation/5 SQL database.html#reshape-tables",
    "href": "data manipulation/5 SQL database.html#reshape-tables",
    "title": "SQL database with python",
    "section": "2.9 Reshape tables",
    "text": "2.9 Reshape tables\n\n2.9.1 Gather data long(wide to long)\n\n\n2.9.2 Spread data wide (long to wide)",
    "crumbs": [
      "data manipulation",
      "SQL database with python"
    ]
  },
  {
    "objectID": "data manipulation/5 SQL database.html#string",
    "href": "data manipulation/5 SQL database.html#string",
    "title": "SQL database with python",
    "section": "2.10 string",
    "text": "2.10 string\n\n2.10.1 upper case\n\n\n2.10.2 lower case\n\n\n2.10.3 match\n\n\n2.10.4 concatenation\n\n\n2.10.5 replace\n\n\n2.10.6 extract",
    "crumbs": [
      "data manipulation",
      "SQL database with python"
    ]
  },
  {
    "objectID": "data manipulation/5 SQL database.html#date",
    "href": "data manipulation/5 SQL database.html#date",
    "title": "SQL database with python",
    "section": "2.11 date",
    "text": "2.11 date",
    "crumbs": [
      "data manipulation",
      "SQL database with python"
    ]
  },
  {
    "objectID": "data manipulation/5 SQL database.html#create-table-into-database",
    "href": "data manipulation/5 SQL database.html#create-table-into-database",
    "title": "SQL database with python",
    "section": "2.12 create table into database",
    "text": "2.12 create table into database\n\n\nCode\nsql=\"create table if not exists new_tips as select * from tips\"\ndb.execute(sql)\n\ntable = pd.read_sql_query(\"SELECT name FROM sqlite_master WHERE type='table';\", db)\ntable\n\n\n\n\n\n\n\n\n\n\nname\n\n\n\n\n0\ntips\n\n\n1\nplanets\n\n\n2\nnew_tips",
    "crumbs": [
      "data manipulation",
      "SQL database with python"
    ]
  },
  {
    "objectID": "data manipulation/5 SQL database.html#delete-table-in-database",
    "href": "data manipulation/5 SQL database.html#delete-table-in-database",
    "title": "SQL database with python",
    "section": "2.13 delete table in database",
    "text": "2.13 delete table in database\n\n\nCode\nsql=\"drop table if  exists new_tips\"\ndb.execute(sql)\n\ntable = pd.read_sql_query(\"SELECT name FROM sqlite_master WHERE type='table';\", db)\ntable\n\n\n\n\n\n\n\n\n\n\nname\n\n\n\n\n0\ntips\n\n\n1\nplanets",
    "crumbs": [
      "data manipulation",
      "SQL database with python"
    ]
  },
  {
    "objectID": "data manipulation/5 SQL database.html#edit-table-in-database",
    "href": "data manipulation/5 SQL database.html#edit-table-in-database",
    "title": "SQL database with python",
    "section": "2.14 edit table in database",
    "text": "2.14 edit table in database\n\n\nCode\n# close database connection\ndb.close()",
    "crumbs": [
      "data manipulation",
      "SQL database with python"
    ]
  },
  {
    "objectID": "intro/0 Basic python.html#check-package-install-location",
    "href": "intro/0 Basic python.html#check-package-install-location",
    "title": "Basic Python",
    "section": "6.5 check package install location",
    "text": "6.5 check package install location\n\n\nCode\nimport site; site.getsitepackages()",
    "crumbs": [
      "Intro",
      "Basic Python"
    ]
  },
  {
    "objectID": "multi class classification/0 Customer Segmentation data.html#download-data",
    "href": "multi class classification/0 Customer Segmentation data.html#download-data",
    "title": "Customer Segmentation Dataset",
    "section": "2.1 download data",
    "text": "2.1 download data\nhttps://www.kaggle.com/datasets/vetrirah/customer\n\n\nCode\nimport pandas as pd\n\ndf_train=pd.read_csv('data/Train.csv')\n\ndf_test=pd.read_csv('data/Test.csv')\n\n\n\n\nCode\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nID\nGender\nEver_Married\nAge\nGraduated\nProfession\nWork_Experience\nSpending_Score\nFamily_Size\nVar_1\nSegmentation\n\n\n\n\n0\n462809\nMale\nNo\n22\nNo\nHealthcare\n1.0\nLow\n4.0\nCat_4\nD\n\n\n1\n462643\nFemale\nYes\n38\nYes\nEngineer\nNaN\nAverage\n3.0\nCat_4\nA\n\n\n2\n466315\nFemale\nYes\n67\nYes\nEngineer\n1.0\nLow\n1.0\nCat_6\nB\n\n\n3\n461735\nMale\nYes\n67\nYes\nLawyer\n0.0\nHigh\n2.0\nCat_6\nB\n\n\n4\n462669\nFemale\nYes\n40\nYes\nEntertainment\nNaN\nHigh\n6.0\nCat_6\nA",
    "crumbs": [
      "multi class classification",
      "Customer Segmentation Dataset"
    ]
  },
  {
    "objectID": "multi class classification/0 Customer Segmentation data.html#data-eda",
    "href": "multi class classification/0 Customer Segmentation data.html#data-eda",
    "title": "Customer Segmentation Dataset",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA\n\n\nCode\ndf_train.describe()\n\n\n\n\n\n\n\n\n\n\nID\nAge\nWork_Experience\nFamily_Size\n\n\n\n\ncount\n8068.000000\n8068.000000\n7239.000000\n7733.000000\n\n\nmean\n463479.214551\n43.466906\n2.641663\n2.850123\n\n\nstd\n2595.381232\n16.711696\n3.406763\n1.531413\n\n\nmin\n458982.000000\n18.000000\n0.000000\n1.000000\n\n\n25%\n461240.750000\n30.000000\n0.000000\n2.000000\n\n\n50%\n463472.500000\n40.000000\n1.000000\n3.000000\n\n\n75%\n465744.250000\n53.000000\n4.000000\n4.000000\n\n\nmax\n467974.000000\n89.000000\n14.000000\n9.000000\n\n\n\n\n\n\n\n\n\n\nCode\ndf_train.describe(include=[object])\n\n\n\n\n\n\n\n\n\n\nGender\nEver_Married\nGraduated\nProfession\nSpending_Score\nVar_1\nSegmentation\n\n\n\n\ncount\n8068\n7928\n7990\n7944\n8068\n7992\n8068\n\n\nunique\n2\n2\n2\n9\n3\n7\n4\n\n\ntop\nMale\nYes\nYes\nArtist\nLow\nCat_6\nD\n\n\nfreq\n4417\n4643\n4968\n2516\n4878\n5238\n2268\n\n\n\n\n\n\n\n\nMissing Data\n\n\nCode\ndf_train.isnull().sum()\n\n\nID                   0\nGender               0\nEver_Married       140\nAge                  0\nGraduated           78\nProfession         124\nWork_Experience    829\nSpending_Score       0\nFamily_Size        335\nVar_1               76\nSegmentation         0\ndtype: int64\n\n\n\n\nCode\nimport math\n\nfrom siuba.siu import call\nfrom siuba import _, mutate, filter, group_by, summarize,show_query\nfrom siuba import *\n\ndf_train &gt;&gt; group_by(_.Segmentation)  &gt;&gt; summarize(n = _.shape[0])\n\n\n\n\n\n\n\n\n\n\nSegmentation\nn\n\n\n\n\n0\nA\n1972\n\n\n1\nB\n1858\n\n\n2\nC\n1970\n\n\n3\nD\n2268\n\n\n\n\n\n\n\n\n\n\nCode\nimport sweetviz as sv\nmy_report = sv.analyze(df_train)\n\n\n\n\n\n\n\nCode\nmy_report.show_notebook()",
    "crumbs": [
      "multi class classification",
      "Customer Segmentation Dataset"
    ]
  },
  {
    "objectID": "multi class classification/0 Customer Segmentation data.html#feature-vs-target",
    "href": "multi class classification/0 Customer Segmentation data.html#feature-vs-target",
    "title": "Customer Segmentation Dataset",
    "section": "2.3 feature vs target",
    "text": "2.3 feature vs target\n\n\nCode\nsns.countplot(data=df_train,x='Segmentation',hue='Gender')\n\n\n\n\n\n\n\n\n\n\n\nCode\npd.crosstab(df_train['Segmentation'],df_train['Gender'], normalize='index').plot.bar(stacked=True)",
    "crumbs": [
      "multi class classification",
      "Customer Segmentation Dataset"
    ]
  },
  {
    "objectID": "multi class classification/0 Customer Segmentation data.html#compare-train-data-and-test-data",
    "href": "multi class classification/0 Customer Segmentation data.html#compare-train-data-and-test-data",
    "title": "Customer Segmentation Dataset",
    "section": "2.4 compare train data and test data",
    "text": "2.4 compare train data and test data\n\n\nCode\ncompare = sv.compare(source=df_train, compare=df_test)\n\n\n\n\n\n\n\nCode\ncompare.show_notebook()",
    "crumbs": [
      "multi class classification",
      "Customer Segmentation Dataset"
    ]
  },
  {
    "objectID": "multi class classification/0 Customer Segmentation data.html#data-dictionary",
    "href": "multi class classification/0 Customer Segmentation data.html#data-dictionary",
    "title": "Customer Segmentation Dataset",
    "section": "2.5 data dictionary",
    "text": "2.5 data dictionary",
    "crumbs": [
      "multi class classification",
      "Customer Segmentation Dataset"
    ]
  },
  {
    "objectID": "multi class classification/multi class classification.html",
    "href": "multi class classification/multi class classification.html",
    "title": "Multi Models using downsample,pipeline,fast tuning",
    "section": "",
    "text": "Since we have enough rare class data(at least 1K).Let handle imbalanced data with down sample before training.",
    "crumbs": [
      "multi class classification",
      "Multi Models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "multi class classification/multi class classification.html#download-data",
    "href": "multi class classification/multi class classification.html#download-data",
    "title": "Multi Models using downsample,pipeline,fast tuning",
    "section": "2.1 download data",
    "text": "2.1 download data\nhttps://www.kaggle.com/datasets/vetrirah/customer\n\n\nCode\nimport pandas as pd\n\ndf_tran_raw=pd.read_csv('data/Train.csv')\n\ndf_test_raw=pd.read_csv('data/Test.csv')",
    "crumbs": [
      "multi class classification",
      "Multi Models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "multi class classification/multi class classification.html#data-eda",
    "href": "multi class classification/multi class classification.html#data-eda",
    "title": "Multi Models using downsample,pipeline,fast tuning",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA",
    "crumbs": [
      "multi class classification",
      "Multi Models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "multi class classification/multi class classification.html#data-wrangling",
    "href": "multi class classification/multi class classification.html#data-wrangling",
    "title": "Multi Models using downsample,pipeline,fast tuning",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\ndf_tran_raw.head()\n\n\n\n\nCode\ndf_train=df_tran_raw.drop('ID', axis=1)\n\n\n\n\nCode\ndf_test=df_test_raw.drop('ID', axis=1)\n\n\n\n\nCode\ndf_train.head()\n\n\n\n\nCode\ndf_train &gt;&gt; group_by(_.Segmentation)  &gt;&gt; summarize(n = _.shape[0])",
    "crumbs": [
      "multi class classification",
      "Multi Models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "multi class classification/multi class classification.html#categorical_cols-and-numerical_cols",
    "href": "multi class classification/multi class classification.html#categorical_cols-and-numerical_cols",
    "title": "Multi Models using downsample,pipeline,fast tuning",
    "section": "2.5 categorical_cols and numerical_cols",
    "text": "2.5 categorical_cols and numerical_cols\n\n\nCode\ncategorical_cols = [cname for cname in X_train \n                    if X_train[cname].nunique() &lt; 10 and X_train[cname].dtype == \"object\"]\n                    \n                    \nnumerical_cols = numerical_cols = [cname for cname in X_train \n                    if X_train[cname].dtype in ['int64', 'float64']]\n\n\n\n\nCode\nprint(\"The total number of categorical columns:\", len(categorical_cols))\nprint(\"The total number of numerical columns:\", len(numerical_cols))\n\n\n\n\nCode\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train[my_cols].copy()\nX_val = X_val[my_cols].copy()\nX_test= X_test[my_cols].copy()\n\nmy_cols\n#X_final = df_test[my_cols].copy()",
    "crumbs": [
      "multi class classification",
      "Multi Models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "multi class classification/multi class classification.html#split-data",
    "href": "multi class classification/multi class classification.html#split-data",
    "title": "Multi Models using downsample,pipeline,fast tuning",
    "section": "2.4 split data",
    "text": "2.4 split data\n80% training / 10% validation/ 10% testing\n\n\n\nCode\nY=df_train['Segmentation']\nX=df_train.drop('Segmentation', axis=1)\n\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nY = le.fit_transform(Y)\n\n\n\n\nCode\nimport collections, numpy\ncollections.Counter(Y)\n\n\ninverse_transform for testing only\n\n\nCode\ninv = le.inverse_transform(Y)\n\n\n\n\nCode\nimport collections, numpy\ncollections.Counter(inv)\n\n\n\n\nCode\ntraining_size=0.8\nvalidation_size=0.1\ntesting_size=0.1\n\n\n\nX_train, X_val, Y_train, Y_val= train_test_split(X, Y, test_size=validation_size, random_state=1)\n\nX_train, X_test, Y_train, Y_test= train_test_split(X_train, Y_train, test_size=testing_size/training_size, random_state=1) \n\n\n\n\nCode\nX_train.shape\n\n\n\n\nCode\nY_train.shape\n\n\n\n\nCode\nlen(X_train)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n\n\nCode\nlen(X_val)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n\n\nCode\nlen(X_test)/(len(X_train) +len(X_val) +len(X_test) )",
    "crumbs": [
      "multi class classification",
      "Multi Models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "multi class classification/multi class classification.html#categorical_cols-and-numerical_cols-1",
    "href": "multi class classification/multi class classification.html#categorical_cols-and-numerical_cols-1",
    "title": "Multi Models using downsample,pipeline,fast tuning",
    "section": "2.6 categorical_cols and numerical_cols",
    "text": "2.6 categorical_cols and numerical_cols\n\n\nCode\ncategorical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].nunique() &lt; 10 and X_train[cname].dtype == \"object\"]\n                    \n                    \nnumerical_cols = numerical_cols = [cname for cname in X_train.columns \n                    if X_train[cname].dtype in ['int64', 'float64']]\n\n\n\n\nCode\nprint(\"The total number of categorical columns:\", len(categorical_cols))\nprint(\"The total number of numerical columns:\", len(numerical_cols))\n\n\n\n\nCode\nmy_cols = categorical_cols + numerical_cols\nX_train_resample = X_train_resample[my_cols].copy()\nX_val = X_val[my_cols].copy()\nX_test= X_test[my_cols].copy()\n\nmy_cols\n#X_final = df_test[my_cols].copy()",
    "crumbs": [
      "multi class classification",
      "Multi Models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "multi class classification/multi class classification.html#pipelines-for-data-preprocessing",
    "href": "multi class classification/multi class classification.html#pipelines-for-data-preprocessing",
    "title": "Multi Models using downsample,pipeline,fast tuning",
    "section": "2.6 Pipelines for Data Preprocessing",
    "text": "2.6 Pipelines for Data Preprocessing\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n2.6.1 numerical_transformer\n\n\nCode\nnumerical_transformer = Pipeline(steps=[\n    ('imputer_num', SimpleImputer(strategy='median')), \n    ('scaler', StandardScaler())\n])\n\n\n\n\n2.6.2 categorical_transformer\n\n\nCode\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n\n\nCode\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)])",
    "crumbs": [
      "multi class classification",
      "Multi Models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "multi class classification/multi class classification.html#define-model",
    "href": "multi class classification/multi class classification.html#define-model",
    "title": "Multi Models using downsample,pipeline,fast tuning",
    "section": "3.1 define model",
    "text": "3.1 define model\n\n3.1.1 XGB model\n\n\nCode\nimport xgboost\nprint(xgboost.__version__)\n\n\nSince its multi calss classification so add objective='multi:softmax'\n\n\nCode\nfrom xgboost import XGBClassifier\nxgb_model = XGBClassifier(objective='multi:softprob',num_class=4)\nxgb_model\n\n\n\n\n3.1.2 Random Forest model\n\n\nCode\nfrom sklearn.ensemble import RandomForestClassifier\nrandom_forest_model = RandomForestClassifier()\nrandom_forest_model\n\n\n\n\n3.1.3 Logistic Regression model\n\n\nCode\nfrom sklearn.linear_model import LogisticRegression\nLogisticRegression_model = LogisticRegression(solver='liblinear')\nLogisticRegression_model",
    "crumbs": [
      "multi class classification",
      "Multi Models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "multi class classification/multi class classification.html#define-pipline",
    "href": "multi class classification/multi class classification.html#define-pipline",
    "title": "Multi Models using downsample,pipeline,fast tuning",
    "section": "3.2 define pipline",
    "text": "3.2 define pipline\n\n\nCode\npipeline_xgb = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', xgb_model)\n         ]\n)\n\npipeline_rf = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', random_forest_model)\n         ]\n)\n\npipeline_lr = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', LogisticRegression_model)\n         ]\n)",
    "crumbs": [
      "multi class classification",
      "Multi Models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "multi class classification/multi class classification.html#define-gridsearch",
    "href": "multi class classification/multi class classification.html#define-gridsearch",
    "title": "Multi Models using downsample,pipeline,fast tuning",
    "section": "3.3 define GridSearch",
    "text": "3.3 define GridSearch\n\n\nCode\nparameters_xgb= {\n        'model__learning_rate': [0.01, 0.02,0.08,0.1],\n        'model__max_depth': [3, 5, 7,8,9,10],\n        'model__min_child_weight': [1, 3,5,8],\n        'model__subsample': [0.5, 0.7,0.9],\n        \n       # 'model__colsample__bytree': [0.5, 0.7],\n       \n        'model__n_estimators' : [100, 200],\n        'model__objective': ['reg:squarederror']\n    }\n\n\nGrid_xgb = HalvingGridSearchCV(pipeline_xgb\n                ,parameters_xgb \n                #,scoring='merror'\n                ,max_resources=100\n                , cv=10, n_jobs=-1)\n                \n                \nparameters_rf = {'model__max_depth':[20,30,40],\n                 'model__n_estimators':[200,250],\n                 'model__min_samples_leaf':[1,2,3]\n                 }                \n                \n\nGrid_rf = HalvingGridSearchCV(pipeline_rf\n                ,parameters_rf\n                #,scoring='merror'\n                ,max_resources=100\n                , cv=10, n_jobs=-1)",
    "crumbs": [
      "multi class classification",
      "Multi Models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "multi class classification/multi class classification.html#train-model",
    "href": "multi class classification/multi class classification.html#train-model",
    "title": "Multi Models using downsample,pipeline,fast tuning",
    "section": "3.4 train model",
    "text": "3.4 train model\n\n\nCode\nstart_time = time.time()\n\n\nGrids = [Grid_xgb, Grid_rf,pipeline_xgb,pipeline_rf,pipeline_lr]\n\n\n\nfor Grid in Grids:\n    Grid.fit(X_train,Y_train)\n\n\nend_time = time.time()\nduration = end_time - start_time\nduration\n\nprint(\"trainning time:\",duration)",
    "crumbs": [
      "multi class classification",
      "Multi Models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "multi class classification/multi class classification.html#preformance",
    "href": "multi class classification/multi class classification.html#preformance",
    "title": "Multi Models using downsample,pipeline,fast tuning",
    "section": "3.5 Preformance",
    "text": "3.5 Preformance\n\n3.5.1 dummy Preformance\n\n3.5.1.1 dummy 1 : there are 3 class, so each class random 33.33%\n\n\nCode\nimport random\nfrom array import array\n\nmylist = []\n\nfor i in range(0,len(Y_test)):\n    x = random.randrange(0, 4)\n    mylist.append(x)\n    \nY_dummy1=numpy.array(mylist)\n\n\n\n\nCode\nimport collections, numpy\nc=collections.Counter(Y_dummy1)\ndf = pd.DataFrame.from_records(list(dict(c).items()), columns=['class','count']).sort_values('class')\ndf\n\n\n\n\nCode\nprint(classification_report(le.inverse_transform(Y_test), le.inverse_transform(Y_dummy1)))\n\n\n\n\n3.5.1.2 dummy 2 : there are 3 class, random assign to each share:\neach class in Y_test\n\n\nCode\nimport collections, numpy\nc=collections.Counter(Y_test)\ndf = pd.DataFrame.from_records(list(dict(c).items()), columns=['class','count']).sort_values('class')\ndf\n\n\n\n\nCode\nimport random\nY_dummy2=numpy.array(random.choices([0, 1, 2,3], [218/sum(Y_test), 208/sum(Y_test), 218/sum(Y_test),264/sum(Y_test)], k=len(Y_test)))\n\n\n\n\nCode\nimport collections, numpy\nc=collections.Counter(Y_dummy2)\ndf = pd.DataFrame.from_records(list(dict(c).items()), columns=['class','count']).sort_values('class')\ndf\n\n\n\n\nCode\nprint(classification_report(le.inverse_transform(Y_test), le.inverse_transform(Y_dummy2)))\n\n\n\n\n\n3.5.2 model Preformance\n\n\nCode\ngrid_dict = {0: 'XGB', 1: 'random forest', 2: 'XGB non tune',3: 'ramdon forest non tune',4:'Logistic regression non tune' }\n\nfor i, model in enumerate(Grids):\n    print('{} Test Accuracy: {}'.format(grid_dict[i],\n    model.score(X_test,Y_test)))\n    #print('{} Best Params: {}'.format(grid_dict[i], model.best_params_))\n\n\n\n\nCode\nbest_ml=Grid_xgb.best_estimator_\n\n\n\n\nCode\ntype(best_ml)\n\n\n\n\n3.5.3 model feature importances\n\n\nCode\nvi=best_ml.steps[1][1].feature_importances_\nvi\n\n\n\n\nCode\n## get pipeline feature names\nvar_name=best_ml[:-1].get_feature_names_out()\n\n\n\n\nCode\ndataset = pd.DataFrame({'feature': var_name, 'importances': vi}, columns=['feature', 'importances']).sort_values('importances',ascending=False)\ndataset\n\n\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = best_ml.predict(X_test) #always gets x and retuns y\n#Y_pred_dt\n\n\n\n\nCode\nfrom sklearn.metrics import classification_report\nprint(classification_report(Y_test, Y_pred_dt))\n\n\nreverse target label back to A B C D\n\n\nCode\nfrom sklearn.metrics import classification_report\nprint(classification_report(le.inverse_transform(Y_test), le.inverse_transform(Y_pred_dt)))",
    "crumbs": [
      "multi class classification",
      "Multi Models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "multi class classification/multi class classification.html#k-fold-cross-validation",
    "href": "multi class classification/multi class classification.html#k-fold-cross-validation",
    "title": "Multi Models using downsample,pipeline,fast tuning",
    "section": "3.6 k-Fold Cross-Validation",
    "text": "3.6 k-Fold Cross-Validation\n\n\nCode\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\n\n\nCode\nkf_dt = KFold(n_splits=5,shuffle=True)  \ncv_dt = cross_val_score(pipeline_xgb, X_train, Y_train, cv=kf_dt)\nnp.mean(cv_dt)",
    "crumbs": [
      "multi class classification",
      "Multi Models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "multi class classification/multi class classification.html#save-model",
    "href": "multi class classification/multi class classification.html#save-model",
    "title": "Multi Models using downsample,pipeline,fast tuning",
    "section": "3.6 save model",
    "text": "3.6 save model\n\n\nCode\nfrom joblib import dump, load\ndump(Grid_xgb, 'trained_grid_multi_calss.joblib', compress=True)",
    "crumbs": [
      "multi class classification",
      "Multi Models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "multi class classification/multi class classification.html#load-model",
    "href": "multi class classification/multi class classification.html#load-model",
    "title": "Multi Models using downsample,pipeline,fast tuning",
    "section": "3.7 load model",
    "text": "3.7 load model\n\n\nCode\nmodel_reload = load('trained_grid_multi_calss.joblib') \n\n\n\n\nCode\nbest_ml=model_reload.best_estimator_",
    "crumbs": [
      "multi class classification",
      "Multi Models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "multi class classification/multi class classification.html#final-prediction",
    "href": "multi class classification/multi class classification.html#final-prediction",
    "title": "Multi Models using downsample,pipeline,fast tuning",
    "section": "3.8 final prediction",
    "text": "3.8 final prediction\n\n\nCode\nY_pred_dt_final =best_ml.predict(X_val) #always gets x and retuns y\n\n\nY_pred_dt_final_lable=le.inverse_transform(Y_pred_dt_final)\nY_pred_dt_final_lable[0:5]\n\n\n\n\nCode\nimport collections, numpy\nc=collections.Counter(Y_pred_dt_final_lable)\ndf = pd.DataFrame.from_records(list(dict(c).items()), columns=['class','count']).sort_values('class')\ndf",
    "crumbs": [
      "multi class classification",
      "Multi Models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "multi class classification/2 multi class classification.html",
    "href": "multi class classification/2 multi class classification.html",
    "title": "Multi Models using downsample,pipeline,fast tuning",
    "section": "",
    "text": "Since we have enough rare class data(at least 1K).Let handle imbalanced data with down sample before training.",
    "crumbs": [
      "multi class classification",
      "Multi Models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "multi class classification/2 multi class classification.html#download-data",
    "href": "multi class classification/2 multi class classification.html#download-data",
    "title": "Multi Models using downsample,pipeline,fast tuning",
    "section": "2.1 download data",
    "text": "2.1 download data\nhttps://www.kaggle.com/datasets/vetrirah/customer\n\n\nCode\nimport pandas as pd\n\ndf_tran_raw=pd.read_csv('data/Train.csv')\n\ndf_test_raw=pd.read_csv('data/Test.csv')",
    "crumbs": [
      "multi class classification",
      "Multi Models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "multi class classification/2 multi class classification.html#data-eda",
    "href": "multi class classification/2 multi class classification.html#data-eda",
    "title": "Multi Models using downsample,pipeline,fast tuning",
    "section": "2.2 data EDA",
    "text": "2.2 data EDA",
    "crumbs": [
      "multi class classification",
      "Multi Models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "multi class classification/2 multi class classification.html#data-wrangling",
    "href": "multi class classification/2 multi class classification.html#data-wrangling",
    "title": "Multi Models using downsample,pipeline,fast tuning",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\n\nCode\ndf_tran_raw.head()\n\n\n\n\n\n\n\n\n\n\nID\nGender\nEver_Married\nAge\nGraduated\nProfession\nWork_Experience\nSpending_Score\nFamily_Size\nVar_1\nSegmentation\n\n\n\n\n0\n462809\nMale\nNo\n22\nNo\nHealthcare\n1.0\nLow\n4.0\nCat_4\nD\n\n\n1\n462643\nFemale\nYes\n38\nYes\nEngineer\nNaN\nAverage\n3.0\nCat_4\nA\n\n\n2\n466315\nFemale\nYes\n67\nYes\nEngineer\n1.0\nLow\n1.0\nCat_6\nB\n\n\n3\n461735\nMale\nYes\n67\nYes\nLawyer\n0.0\nHigh\n2.0\nCat_6\nB\n\n\n4\n462669\nFemale\nYes\n40\nYes\nEntertainment\nNaN\nHigh\n6.0\nCat_6\nA\n\n\n\n\n\n\n\n\n\n\nCode\ndf_train=df_tran_raw.drop('ID', axis=1)\n\n\n\n\nCode\ndf_test=df_test_raw.drop('ID', axis=1)\n\n\n\n\nCode\ndf_train.head()\n\n\n\n\n\n\n\n\n\n\nGender\nEver_Married\nAge\nGraduated\nProfession\nWork_Experience\nSpending_Score\nFamily_Size\nVar_1\nSegmentation\n\n\n\n\n0\nMale\nNo\n22\nNo\nHealthcare\n1.0\nLow\n4.0\nCat_4\nD\n\n\n1\nFemale\nYes\n38\nYes\nEngineer\nNaN\nAverage\n3.0\nCat_4\nA\n\n\n2\nFemale\nYes\n67\nYes\nEngineer\n1.0\nLow\n1.0\nCat_6\nB\n\n\n3\nMale\nYes\n67\nYes\nLawyer\n0.0\nHigh\n2.0\nCat_6\nB\n\n\n4\nFemale\nYes\n40\nYes\nEntertainment\nNaN\nHigh\n6.0\nCat_6\nA\n\n\n\n\n\n\n\n\n\n\nCode\ndf_train &gt;&gt; group_by(_.Segmentation)  &gt;&gt; summarize(n = _.shape[0])\n\n\n\n\n\n\n\n\n\n\nSegmentation\nn\n\n\n\n\n0\nA\n1972\n\n\n1\nB\n1858\n\n\n2\nC\n1970\n\n\n3\nD\n2268",
    "crumbs": [
      "multi class classification",
      "Multi Models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "multi class classification/2 multi class classification.html#split-data",
    "href": "multi class classification/2 multi class classification.html#split-data",
    "title": "Multi Models using downsample,pipeline,fast tuning",
    "section": "2.4 split data",
    "text": "2.4 split data\n80% training / 10% validation/ 10% testing\n\n\n\nCode\nY=df_train['Segmentation']\nX=df_train.drop('Segmentation', axis=1)\n\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nY = le.fit_transform(Y)\n\n\n\n\nCode\nimport collections, numpy\ncollections.Counter(Y)\n\n\nCounter({3: 2268, 0: 1972, 2: 1970, 1: 1858})\n\n\ninverse_transform for testing only\n\n\nCode\ninv = le.inverse_transform(Y)\n\n\n\n\nCode\nimport collections, numpy\ncollections.Counter(inv)\n\n\nCounter({'D': 2268, 'A': 1972, 'C': 1970, 'B': 1858})\n\n\n\n\nCode\ntraining_size=0.8\nvalidation_size=0.1\ntesting_size=0.1\n\n\n\nX_train, X_val, Y_train, Y_val= train_test_split(X, Y, test_size=validation_size, random_state=1)\n\nX_train, X_test, Y_train, Y_test= train_test_split(X_train, Y_train, test_size=testing_size/training_size, random_state=1) \n\n\n\n\nCode\nX_train.shape\n\n\n(6353, 9)\n\n\n\n\nCode\nY_train.shape\n\n\n(6353,)\n\n\n\n\nCode\nlen(X_train)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.7874318294496777\n\n\n\n\nCode\nlen(X_val)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.10002478929102628\n\n\n\n\nCode\nlen(X_test)/(len(X_train) +len(X_val) +len(X_test) )\n\n\n0.11254338125929599",
    "crumbs": [
      "multi class classification",
      "Multi Models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "multi class classification/2 multi class classification.html#categorical_cols-and-numerical_cols",
    "href": "multi class classification/2 multi class classification.html#categorical_cols-and-numerical_cols",
    "title": "Multi Models using downsample,pipeline,fast tuning",
    "section": "2.5 categorical_cols and numerical_cols",
    "text": "2.5 categorical_cols and numerical_cols\n\n\nCode\ncategorical_cols = [cname for cname in X_train \n                    if X_train[cname].nunique() &lt; 10 and X_train[cname].dtype == \"object\"]\n                    \n                    \nnumerical_cols = numerical_cols = [cname for cname in X_train \n                    if X_train[cname].dtype in ['int64', 'float64']]\n\n\n\n\nCode\nprint(\"The total number of categorical columns:\", len(categorical_cols))\nprint(\"The total number of numerical columns:\", len(numerical_cols))\n\n\nThe total number of categorical columns: 6\nThe total number of numerical columns: 3\n\n\n\n\nCode\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train[my_cols].copy()\nX_val = X_val[my_cols].copy()\nX_test= X_test[my_cols].copy()\n\nmy_cols\n#X_final = df_test[my_cols].copy()\n\n\n['Gender',\n 'Ever_Married',\n 'Graduated',\n 'Profession',\n 'Spending_Score',\n 'Var_1',\n 'Age',\n 'Work_Experience',\n 'Family_Size']",
    "crumbs": [
      "multi class classification",
      "Multi Models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "multi class classification/2 multi class classification.html#pipelines-for-data-preprocessing",
    "href": "multi class classification/2 multi class classification.html#pipelines-for-data-preprocessing",
    "title": "Multi Models using downsample,pipeline,fast tuning",
    "section": "2.6 Pipelines for Data Preprocessing",
    "text": "2.6 Pipelines for Data Preprocessing\n\n\nCode\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n2.6.1 numerical_transformer\n\n\nCode\nnumerical_transformer = Pipeline(steps=[\n    ('imputer_num', SimpleImputer(strategy='median')), \n    ('scaler', StandardScaler())\n])\n\n\n\n\n2.6.2 categorical_transformer\n\n\nCode\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\n\n\nCode\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)])",
    "crumbs": [
      "multi class classification",
      "Multi Models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "multi class classification/2 multi class classification.html#define-model",
    "href": "multi class classification/2 multi class classification.html#define-model",
    "title": "Multi Models using downsample,pipeline,fast tuning",
    "section": "3.1 define model",
    "text": "3.1 define model\n\n3.1.1 XGB model\n\n\nCode\nimport xgboost\nprint(xgboost.__version__)\n\n\n2.0.3\n\n\nSince its multi calss classification so add objective='multi:softmax'\n\n\nCode\nfrom xgboost import XGBClassifier\nxgb_model = XGBClassifier(objective='multi:softprob',num_class=4)\nxgb_model\n\n\nXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None, num_class=4,\n              num_parallel_tree=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. XGBClassifieriNot fittedXGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=None, n_jobs=None, num_class=4,\n              num_parallel_tree=None, ...) \n\n\n\n\n3.1.2 Random Forest model\n\n\nCode\nfrom sklearn.ensemble import RandomForestClassifier\nrandom_forest_model = RandomForestClassifier()\nrandom_forest_model\n\n\nRandomForestClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriNot fittedRandomForestClassifier() \n\n\n\n\n3.1.3 Logistic Regression model\n\n\nCode\nfrom sklearn.linear_model import LogisticRegression\nLogisticRegression_model = LogisticRegression(solver='liblinear')\nLogisticRegression_model\n\n\nLogisticRegression(solver='liblinear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniNot fittedLogisticRegression(solver='liblinear')",
    "crumbs": [
      "multi class classification",
      "Multi Models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "multi class classification/2 multi class classification.html#define-pipline",
    "href": "multi class classification/2 multi class classification.html#define-pipline",
    "title": "Multi Models using downsample,pipeline,fast tuning",
    "section": "3.2 define pipline",
    "text": "3.2 define pipline\n\n\nCode\npipeline_xgb = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', xgb_model)\n         ]\n)\n\npipeline_rf = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', random_forest_model)\n         ]\n)\n\npipeline_lr = Pipeline(\n  steps=[\n         ('preprocessor', preprocessor), \n         ('model', LogisticRegression_model)\n         ]\n)",
    "crumbs": [
      "multi class classification",
      "Multi Models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "multi class classification/2 multi class classification.html#define-gridsearch",
    "href": "multi class classification/2 multi class classification.html#define-gridsearch",
    "title": "Multi Models using downsample,pipeline,fast tuning",
    "section": "3.3 define GridSearch",
    "text": "3.3 define GridSearch\n\n\nCode\nparameters_xgb= {\n        'model__learning_rate': [0.01, 0.02,0.08,0.1],\n        'model__max_depth': [3, 5, 7,8,9,10],\n        'model__min_child_weight': [1, 3,5,8],\n        'model__subsample': [0.5, 0.7,0.9],\n        \n       # 'model__colsample__bytree': [0.5, 0.7],\n       \n        'model__n_estimators' : [100, 200],\n        'model__objective': ['reg:squarederror']\n    }\n\n\nGrid_xgb = HalvingGridSearchCV(pipeline_xgb\n                ,parameters_xgb \n                #,scoring='merror'\n                ,max_resources=100\n                , cv=10, n_jobs=-1)\n                \n                \nparameters_rf = {'model__max_depth':[20,30,40],\n                 'model__n_estimators':[200,250],\n                 'model__min_samples_leaf':[1,2,3]\n                 }                \n                \n\nGrid_rf = HalvingGridSearchCV(pipeline_rf\n                ,parameters_rf\n                #,scoring='merror'\n                ,max_resources=100\n                , cv=10, n_jobs=-1)",
    "crumbs": [
      "multi class classification",
      "Multi Models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "multi class classification/2 multi class classification.html#train-model",
    "href": "multi class classification/2 multi class classification.html#train-model",
    "title": "Multi Models using downsample,pipeline,fast tuning",
    "section": "3.4 train model",
    "text": "3.4 train model\n\n\nCode\nstart_time = time.time()\n\n\nGrids = [Grid_xgb, Grid_rf,pipeline_xgb,pipeline_rf,pipeline_lr]\n\n\n\nfor Grid in Grids:\n    Grid.fit(X_train,Y_train)\n\n\nend_time = time.time()\nduration = end_time - start_time\nduration\n\nprint(\"trainning time:\",duration)\n\n\ntrainning time: 38.49195981025696",
    "crumbs": [
      "multi class classification",
      "Multi Models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "multi class classification/2 multi class classification.html#preformance",
    "href": "multi class classification/2 multi class classification.html#preformance",
    "title": "Multi Models using downsample,pipeline,fast tuning",
    "section": "3.5 Preformance",
    "text": "3.5 Preformance\n\n3.5.1 dummy Preformance\n\n3.5.1.1 dummy 1 : there are 3 class, so each class random 33.33%\n\n\nCode\nimport random\nfrom array import array\n\nmylist = []\n\nfor i in range(0,len(Y_test)):\n    x = random.randrange(0, 4)\n    mylist.append(x)\n    \nY_dummy1=numpy.array(mylist)\n\n\n\n\nCode\nimport collections, numpy\nc=collections.Counter(Y_dummy1)\ndf = pd.DataFrame.from_records(list(dict(c).items()), columns=['class','count']).sort_values('class')\ndf\n\n\n\n\n\n\n\n\n\n\nclass\ncount\n\n\n\n\n1\n0\n213\n\n\n0\n1\n227\n\n\n2\n2\n232\n\n\n3\n3\n236\n\n\n\n\n\n\n\n\n\n\nCode\nprint(classification_report(le.inverse_transform(Y_test), le.inverse_transform(Y_dummy1)))\n\n\n              precision    recall  f1-score   support\n\n           A       0.25      0.24      0.25       218\n           B       0.22      0.24      0.23       208\n           C       0.23      0.24      0.24       218\n           D       0.28      0.25      0.26       264\n\n    accuracy                           0.24       908\n   macro avg       0.24      0.24      0.24       908\nweighted avg       0.25      0.24      0.24       908\n\n\n\n\n\n3.5.1.2 dummy 2 : there are 3 class, random assign to each share:\neach class in Y_test\n\n\nCode\nimport collections, numpy\nc=collections.Counter(Y_test)\ndf = pd.DataFrame.from_records(list(dict(c).items()), columns=['class','count']).sort_values('class')\ndf\n\n\n\n\n\n\n\n\n\n\nclass\ncount\n\n\n\n\n3\n0\n218\n\n\n2\n1\n208\n\n\n0\n2\n218\n\n\n1\n3\n264\n\n\n\n\n\n\n\n\n\n\nCode\nimport random\nY_dummy2=numpy.array(random.choices([0, 1, 2,3], [218/sum(Y_test), 208/sum(Y_test), 218/sum(Y_test),264/sum(Y_test)], k=len(Y_test)))\n\n\n\n\nCode\nimport collections, numpy\nc=collections.Counter(Y_dummy2)\ndf = pd.DataFrame.from_records(list(dict(c).items()), columns=['class','count']).sort_values('class')\ndf\n\n\n\n\n\n\n\n\n\n\nclass\ncount\n\n\n\n\n3\n0\n232\n\n\n1\n1\n226\n\n\n2\n2\n216\n\n\n0\n3\n234\n\n\n\n\n\n\n\n\n\n\nCode\nprint(classification_report(le.inverse_transform(Y_test), le.inverse_transform(Y_dummy2)))\n\n\n              precision    recall  f1-score   support\n\n           A       0.26      0.28      0.27       218\n           B       0.24      0.26      0.25       208\n           C       0.23      0.23      0.23       218\n           D       0.26      0.23      0.24       264\n\n    accuracy                           0.25       908\n   macro avg       0.25      0.25      0.25       908\nweighted avg       0.25      0.25      0.25       908\n\n\n\n\n\n\n3.5.2 model Preformance\n\n\nCode\ngrid_dict = {0: 'XGB', 1: 'random forest', 2: 'XGB non tune',3: 'ramdon forest non tune',4:'Logistic regression non tune' }\n\nfor i, model in enumerate(Grids):\n    print('{} Test Accuracy: {}'.format(grid_dict[i],\n    model.score(X_test,Y_test)))\n    #print('{} Best Params: {}'.format(grid_dict[i], model.best_params_))\n\n\nXGB Test Accuracy: 0.5231277533039648\nrandom forest Test Accuracy: 0.5220264317180616\nXGB non tune Test Accuracy: 0.4900881057268722\nramdon forest non tune Test Accuracy: 0.47687224669603523\nLogistic regression non tune Test Accuracy: 0.513215859030837\n\n\n\n\nCode\nbest_ml=Grid_xgb.best_estimator_\n\n\n\n\nCode\ntype(best_ml)\n\n\nsklearn.pipeline.Pipeline\n\n\n\n\n3.5.3 model feature importances\n\n\nCode\nvi=best_ml.steps[1][1].feature_importances_\nvi\n\n\narray([0.15059087, 0.01543647, 0.02962219, 0.01859025, 0.        ,\n       0.04503193, 0.        , 0.08123998, 0.        , 0.07617344,\n       0.01314482, 0.03293596, 0.05356803, 0.01554592, 0.07302563,\n       0.00575411, 0.01688441, 0.06012005, 0.04231628, 0.00646925,\n       0.16279697, 0.        , 0.01408694, 0.00926084, 0.03452247,\n       0.00613631, 0.02880599, 0.00794088], dtype=float32)\n\n\n\n\nCode\n## get pipeline feature names\nvar_name=best_ml[:-1].get_feature_names_out()\n\n\n\n\nCode\ndataset = pd.DataFrame({'feature': var_name, 'importances': vi}, columns=['feature', 'importances']).sort_values('importances',ascending=False)\ndataset\n\n\n\n\n\n\n\n\n\n\nfeature\nimportances\n\n\n\n\n20\ncat__Spending_Score_Low\n0.162797\n\n\n0\nnum__Age\n0.150591\n\n\n7\ncat__Graduated_No\n0.081240\n\n\n9\ncat__Profession_Artist\n0.076173\n\n\n14\ncat__Profession_Healthcare\n0.073026\n\n\n17\ncat__Profession_Marketing\n0.060120\n\n\n12\ncat__Profession_Entertainment\n0.053568\n\n\n5\ncat__Ever_Married_No\n0.045032\n\n\n18\ncat__Spending_Score_Average\n0.042316\n\n\n24\ncat__Var_1_Cat_4\n0.034522\n\n\n11\ncat__Profession_Engineer\n0.032936\n\n\n2\nnum__Family_Size\n0.029622\n\n\n26\ncat__Var_1_Cat_6\n0.028806\n\n\n3\ncat__Gender_Female\n0.018590\n\n\n16\ncat__Profession_Lawyer\n0.016884\n\n\n13\ncat__Profession_Executive\n0.015546\n\n\n1\nnum__Work_Experience\n0.015436\n\n\n22\ncat__Var_1_Cat_2\n0.014087\n\n\n10\ncat__Profession_Doctor\n0.013145\n\n\n23\ncat__Var_1_Cat_3\n0.009261\n\n\n27\ncat__Var_1_Cat_7\n0.007941\n\n\n19\ncat__Spending_Score_High\n0.006469\n\n\n25\ncat__Var_1_Cat_5\n0.006136\n\n\n15\ncat__Profession_Homemaker\n0.005754\n\n\n6\ncat__Ever_Married_Yes\n0.000000\n\n\n21\ncat__Var_1_Cat_1\n0.000000\n\n\n8\ncat__Graduated_Yes\n0.000000\n\n\n4\ncat__Gender_Male\n0.000000\n\n\n\n\n\n\n\n\n\n\nCode\n#Using predict method to test the model\nY_pred_dt = best_ml.predict(X_test) #always gets x and retuns y\n#Y_pred_dt\n\n\n\n\nCode\nfrom sklearn.metrics import classification_report\nprint(classification_report(Y_test, Y_pred_dt))\n\n\n              precision    recall  f1-score   support\n\n           0       0.40      0.43      0.42       218\n           1       0.38      0.34      0.36       208\n           2       0.58      0.56      0.57       218\n           3       0.67      0.72      0.69       264\n\n    accuracy                           0.52       908\n   macro avg       0.51      0.51      0.51       908\nweighted avg       0.52      0.52      0.52       908\n\n\n\nreverse target label back to A B C D\n\n\nCode\nfrom sklearn.metrics import classification_report\nprint(classification_report(le.inverse_transform(Y_test), le.inverse_transform(Y_pred_dt)))\n\n\n              precision    recall  f1-score   support\n\n           A       0.40      0.43      0.42       218\n           B       0.38      0.34      0.36       208\n           C       0.58      0.56      0.57       218\n           D       0.67      0.72      0.69       264\n\n    accuracy                           0.52       908\n   macro avg       0.51      0.51      0.51       908\nweighted avg       0.52      0.52      0.52       908",
    "crumbs": [
      "multi class classification",
      "Multi Models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "multi class classification/2 multi class classification.html#save-model",
    "href": "multi class classification/2 multi class classification.html#save-model",
    "title": "Multi Models using downsample,pipeline,fast tuning",
    "section": "3.6 save model",
    "text": "3.6 save model\n\n\nCode\nfrom joblib import dump, load\ndump(Grid_xgb, 'trained_grid_multi_calss.joblib', compress=True)  \n\n\n['trained_grid_multi_calss.joblib']",
    "crumbs": [
      "multi class classification",
      "Multi Models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "multi class classification/2 multi class classification.html#load-model",
    "href": "multi class classification/2 multi class classification.html#load-model",
    "title": "Multi Models using downsample,pipeline,fast tuning",
    "section": "3.7 load model",
    "text": "3.7 load model\n\n\nCode\nmodel_reload = load('trained_grid_multi_calss.joblib') \n\n\n\n\nCode\nbest_ml=model_reload.best_estimator_",
    "crumbs": [
      "multi class classification",
      "Multi Models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "multi class classification/2 multi class classification.html#final-prediction",
    "href": "multi class classification/2 multi class classification.html#final-prediction",
    "title": "Multi Models using downsample,pipeline,fast tuning",
    "section": "3.8 final prediction",
    "text": "3.8 final prediction\n\n\nCode\nY_pred_dt_final =best_ml.predict(X_val) #always gets x and retuns y\n\n\nY_pred_dt_final_lable=le.inverse_transform(Y_pred_dt_final)\nY_pred_dt_final_lable[0:5]\n\n\narray(['C', 'D', 'B', 'D', 'D'], dtype=object)\n\n\n\n\nCode\nimport collections, numpy\nc=collections.Counter(Y_pred_dt_final_lable)\ndf = pd.DataFrame.from_records(list(dict(c).items()), columns=['class','count']).sort_values('class')\ndf\n\n\n\n\n\n\n\n\n\n\nclass\ncount\n\n\n\n\n3\nA\n197\n\n\n2\nB\n162\n\n\n0\nC\n204\n\n\n1\nD\n244",
    "crumbs": [
      "multi class classification",
      "Multi Models using downsample,pipeline,fast tuning"
    ]
  },
  {
    "objectID": "other/1 Web scraping on www.whiskynotes.be/2 whiskynote one page.html",
    "href": "other/1 Web scraping on www.whiskynotes.be/2 whiskynote one page.html",
    "title": "One page reveiw",
    "section": "",
    "text": "Code\nimport requests\nimport os\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport time\n\n\n\n\nCode\nos.system('pip show beautifulsoup4')\n\n\nName: beautifulsoup4\nVersion: 4.12.2\nSummary: Screen-scraping library\nHome-page: \nAuthor: \nAuthor-email: Leonard Richardson &lt;leonardr@segfault.org&gt;\nLicense: \nLocation: /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages\nRequires: soupsieve\nRequired-by: nbconvert\n\n\n0\n\n\nWeb scraping on www.whiskynotes.be\n\n1 review page\n\n\n\nCode\nreview_url='https://www.whiskynotes.be/2024/ardbeg/spheric-spirits-springbank-blended-malt-islay-malt/'\n\n\n\n\n2 read in html\n\n\nCode\n# Send an HTTP GET request to the website\nheaders = {'User-Agent': 'My User Agent'}\nresponse = requests.get(review_url,headers=headers)\n\n\n\n\nCode\n# success code - 200 \nprint(response) \n\n\n&lt;Response [200]&gt;\n\n\n\n\nCode\n#print(response.content)\n\n\n\n\nCode\n# Parse the HTML code using BeautifulSoup\nsoup = BeautifulSoup(response.content, 'html.parser')\n\n\n\n\nCode\n#print(soup.prettify())\n\n\n\n\nCode\nsoup.title\n\n\n&lt;title&gt;Spheric Spirits: Springbank / Blended Malt / Islay Malt&lt;/title&gt;\n\n\n\n\nCode\nsoup.p\n\n\n&lt;p&gt;&lt;strong&gt;Spheric Spirits&lt;/strong&gt; is a young indie bottler from Germany, started by Benedict and Claudio. They have a slightly edgy / flashy branding (check &lt;a href=\"https://sphericspirits.com/\" rel=\"noopener\" target=\"_blank\"&gt;the website&lt;/a&gt;) – I like the vibrant colours on the labels but apparently not everything agrees. Besides whisky, they’re also looking at armagnac and &lt;em&gt;destilado de agave&lt;/em&gt;. They’re not just bottlers, by the way, they like to get involved at the beginning of production.&lt;/p&gt;\n\n\n\n\nCode\nsoup.find_all('.entry-content h2')\n\n\n[]\n\n\n\n\n3 take picture of url\n\n\n4 bottle_name\n\n\nCode\nbottle_name=soup.find_all('h2',class_=\"product-main__name\")\nbottle_name\n\n\n[&lt;h2 class=\"product-main__name\"&gt;Blended Malt 44 yo 1978 (59,8%, Spheric Spirits 2023, refill sherry butt #6, 331 btl.)&lt;/h2&gt;,\n &lt;h2 class=\"product-main__name\"&gt;Springbank 27 yo 1994 (47,1%, Spheric Spirits 2022, refill sherry hogshead #95, 241 btl.)&lt;/h2&gt;,\n &lt;h2 class=\"product-main__name\"&gt;Islay Malt 25 yo (48,4%, Spheric Spirits 2022, 407 btl.)&lt;/h2&gt;]\n\n\n\n\nCode\nbottle_name2=[]\n\nfor i in bottle_name:\n  name=i.text\n  bottle_name2.append(name)\n\nbottle_name2\n\n\n['Blended Malt 44 yo 1978 (59,8%, Spheric Spirits 2023, refill sherry butt #6, 331 btl.)',\n 'Springbank 27 yo 1994 (47,1%, Spheric Spirits 2022, refill sherry hogshead #95, 241 btl.)',\n 'Islay Malt 25 yo (48,4%, Spheric Spirits 2022, 407 btl.)']\n\n\n\n\n5 bottle_review\n\n\n6 bottle_review_Nose\n\n\n7 bottle_review_Mouth\n\n\n8 bottle_review_Finish\n\n\n9 first score\n\n\n10 all other score\n\n\n11 combine all score\n\n\n12 page_published_date\n\n\n13 page_class\n\n\n14 page_title\n\n\n15 combine all one_page_review\n\n\n16 output\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Other",
      "1 Web scraping on www.whiskynotes.be",
      "One page reveiw"
    ]
  },
  {
    "objectID": "other/1 Web scraping on www.whiskynotes.be/4 whiskynote all year page.html",
    "href": "other/1 Web scraping on www.whiskynotes.be/4 whiskynote all year page.html",
    "title": "All year page",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(rvest)\n\n\n\n\nCode\npackageVersion(\"rvest\")\n\n\n\n1 loop all year page\n\n\nCode\nyear_list=seq(2010,2024)\nyear_list\n\n\n\n\nCode\nurl_list=paste0('https://www.whiskynotes.be/',year_list)\nurl_list\n\n\n\n\nCode\nbottle_list=c()\ntopic_list=c()\ntopic_link_list=c()\nall_year_list_topic=c()\nall_year_list_bottle=c()\n\nfor (i in url_list){\n  year=tail(unlist(strsplit(i, split = \"/\")),1)\n  print(year)\n  print(i)\n  year_ur=i\n  year_page &lt;- read_html(year_ur)\n  bottle001 &lt;- year_page %&gt;% html_elements(\"p\")%&gt;% html_text2()\n  bottle003=unlist(strsplit(bottle001,\"\\n\"))\n  \n  \n  topic001 &lt;- year_page %&gt;% html_elements(\".archive-link\") %&gt;% html_text2()\n  topic_link_001 &lt;- year_page %&gt;%\n    html_elements(css = \".entry-permalink\")%&gt;% html_attr(\"href\")\n\n  year_list_topic=rep(year,length(topic001))\n  year_list_bottle=rep(year,length(bottle003))\n  \n  all_year_list_topic=c(all_year_list_topic,year_list_topic)\n  all_year_list_bottle=c(all_year_list_bottle,year_list_bottle)\n  \n  bottle_list=c(bottle_list,bottle003)\n  topic_list=c(topic_list,topic001)\n  topic_link_list=c(topic_link_list,topic_link_001)\n  \n  Sys.sleep(1)\n  }\n\n\n\n\n2 combine\n\n\nCode\ndata=tibble(topic_list,topic_link_list,all_year_list_topic)\n\n\n\n\nCode\nbottle003=tibble(bottle_list,all_year_list_bottle)\n\n\n\n\n3 output\n\n\nCode\nlibrary(openxlsx)\nlist_of_datasets &lt;- list(\"topic\" = data, \"bottle\" = bottle003)\n\nwrite.xlsx(list_of_datasets, file = \"./output/all year page.xlsx\")\n\n\n\n\n4 reference:\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Other",
      "1 Web scraping on www.whiskynotes.be",
      "All year page"
    ]
  },
  {
    "objectID": "other/1 Web scraping on www.whiskynotes.be/6 web scraping with rvest all second time.html",
    "href": "other/1 Web scraping on www.whiskynotes.be/6 web scraping with rvest all second time.html",
    "title": "All year all topic second time",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(rvest)\n\n\n\n1 one page function\n\n\nCode\none_page_function &lt;- function(review_url){\n  #review_url='https://www.whiskynotes.be/2010/bowmore/bowmore-9y-1997-cigar-malt/'\n  \n #review_url= 'https://www.whiskynotes.be/2013/whisky-bars/whisky-bar-oishii-hasselt/'\n  \n  print(review_url)\n  review_page &lt;- read_html(review_url)\n\n  page_class=review_page  %&gt;% html_elements(\".cat-links a\") %&gt;% html_text2()\n  page_class=str_flatten(page_class,collapse = \"-\")\n  \n  bottle_name=review_page  %&gt;% html_elements(\".entry-content h2\") %&gt;% html_text2()\n  # remove empty\n  bottle_name=bottle_name[nzchar(bottle_name)]\n  # remove space element\n  bottle_name=bottle_name[nchar(bottle_name)&gt;2]\n  \n  \n  # remove ? mark non bottle name element\n  bottle_name=bottle_name[!bottle_name%&gt;% str_detect(\"\\\\?\")]\n  # remove Drams Delivered revisited  non bottle name element\n  bottle_name=bottle_name[ !bottle_name == 'Drams Delivered revisited']\n  print(bottle_name) \n  \n  bottle_review_raw=review_page  %&gt;% html_elements(\"p\") %&gt;% html_text()\n  bottle_review=unlist(strsplit(bottle_review_raw,\"(?= Nose: )\",perl = TRUE))\n  bottle_review=unlist(strsplit(bottle_review,\"(?= Mouth: )\",perl = TRUE))\n   bottle_review=unlist(strsplit(bottle_review,\"(?= Finish: )\",perl = TRUE))\n  \n  bottle_review_Nose=bottle_review[bottle_review %&gt;% str_detect('Nose:|Attractive nose:')]\n  bottle_review_Nose=bottle_review_Nose[nzchar(bottle_review_Nose)]\n\n  bottle_review_Mouth=bottle_review[bottle_review %&gt;% str_detect('Mouth:')]\n  bottle_review_Mouth=bottle_review_Mouth[nzchar(bottle_review_Mouth)]\n\n  bottle_review_Finish=bottle_review[bottle_review %&gt;% str_detect('Finish:')]\n  bottle_review_Finish=bottle_review_Finish[nzchar(bottle_review_Finish)]\n  \n  ########### add dummy score if there is no score review #########\n  # bottle_review_Finish_score=bottle_review[bottle_review %&gt;% str_detect('Finish:|Score:')][-1]\n \n  # # bottle_review_Finish_score2=bottle_review_Finish_score\n  \n  \n  # order=1\n  # for (word in bottle_review_Finish_score){\n  #   print(word)\n  #   print(order)\n  #   print(order%%2)\n  #   print(word %&gt;% str_detect('Score:'))\n  #   print(order%%2==0 & word %&gt;% str_detect('Score:')==FALSE)\n  #   if (order%%2==0 & word %&gt;% str_detect('Score:')==FALSE){\n  #     print('adding add dummy score if there is no score review ')\n  #     bottle_review_Finish_score2=append(bottle_review_Finish_score2,'Score:00/100',order-1)\n  #   }else{\n  #   }\n  #   order=order+1\n  #   }\n################################################\n    \n#\"^[:digit:]+$\" \n################# score  \n  first_bottle_score=review_page  %&gt;% html_elements(\".entry-score\") %&gt;% html_text2()\n\n  bottle_score=review_page  %&gt;% html_elements(\"strong\") %&gt;% html_text2()\n  \n  bottle_score2=bottle_score %&gt;% str_remove(\"100\")%&gt;% str_remove(\"/\") %&gt;% str_remove(\"/100.\") %&gt;% str_remove(\"/100\")%&gt;% str_remove(\"Score:\")%&gt;%str_trim() %&gt;%  str_match(\"^[0-9]{2}$\") %&gt;% as.data.frame() %&gt;% filter(is.na(V1)==FALSE)\n  \n  bottle_score2=bottle_score2 %&gt;% mutate(V1=str_replace(V1,'/100',''))%&gt;% rename(all_page_score=V1) \n\n# if no other score then use first score\n  if(identical(bottle_score, character(0))==TRUE|nrow(bottle_score2)==0){\n    all_page_score=first_bottle_score %&gt;% tibble()%&gt;% rename(all_page_score='.') \n    \n# if no first score then use other score\n  }else if (identical(first_bottle_score, character(0))==TRUE){\n    all_page_score=bottle_score2\n    \n# the other score have same length as bottle.aka the first score appear twices\n    \n  }else if (nrow(bottle_score2)==length(bottle_review_Nose)){\n    all_page_score=bottle_score2\n    \n# if both have first score and other score then combine    \n  }else{\n  #bottle_score=bottle_score %&gt;% str_match('[0-9][0-9]') %&gt;% as.data.frame() %&gt;% filter(is.na(V1)==FALSE)\n  all_page_score=rbind(first_bottle_score,bottle_score2)\n  }\n##############################\n  page_published_date=review_page  %&gt;% html_elements(\".published\") %&gt;% html_text2()\n\n\n  page_title=review_page  %&gt;% html_elements(\".entry-title\") %&gt;% html_text2()\n  \n  if(nrow(all_page_score)!=length(bottle_name)){all_page_score=0}\n  if(length(bottle_review_Nose)!=length(bottle_name)){bottle_review_Nose='no comment'}\n  if(length(bottle_review_Mouth)!=length(bottle_name)){bottle_review_Mouth='no comment'}\n  if(length(bottle_review_Finish)!=length(bottle_name)){bottle_review_Finish='no comment'}\n  \n  \n  \n  one_page_review=tibble(bottle_name,bottle_review_Nose,bottle_review_Mouth,bottle_review_Finish,all_page_score,page_class,page_published_date,page_title,review_url) \n  \n\n  Sys.sleep(runif(n=1, min=0.1, max=0.8))\n  #print(one_page_review)\n  print(dim(one_page_review))\n  #remove(review_page)\n  return(one_page_review)\n}\n\n\n\n\n2 read in all link\n\n\nCode\nlibrary(readxl)\ntopic_link=read_excel('./output/all year page.xlsx',sheet='topic')\n\n\n\n\nCode\nglimpse(topic_link)\n\n\nexclude news\n\n\nCode\nnews=topic_link$topic_link_list %&gt;% str_detect('whisky-news|whisky-bar')\n\n\n\n\nCode\ntopic_link002=topic_link$topic_link_list[!news]\n\n\nexclude no score page:\n\n\nCode\nno_score_page=c(\n\"https://www.whiskynotes.be/2011/blends/bloggers-blend-masterofmalt/\"         \n,\"https://www.whiskynotes.be/2012/glenfarclas/glenfarclas-1968-mytribute-5241/\"\n,\"https://www.whiskynotes.be/2012/ancnoc/ancnoc-distillery-visit/\"             \n,\"https://www.whiskynotes.be/2012/distillery-visits/old-pulteney/\"             \n,\"https://www.whiskynotes.be/2013/blends/cutty-sark-storm-appletiser/\"         \n,\"https://www.whiskynotes.be/2013/glenfarclas/glenfarclas-verticale/\"  \n)\n\n\n\n\n3 read in donload page:\n\n\nCode\nfinish_download=read_excel('./output/all_page_bottle_list_all.xlsx')\n\n\n\n\nCode\nfinish_download_topic_link=unique(finish_download$review_url)\n\n\n\n\nCode\nlength(finish_download_topic_link)\n\n\n\n\nCode\nnon_finish_link=topic_link002 [! topic_link002 %in% c(finish_download_topic_link,no_score_page)]\n\n\n\n\nCode\nlength(non_finish_link)\n\n\n\n\n4 download non download page:\n\n\nCode\nlibrary(openxlsx)\npage=non_finish_link\n\nall_page_review_list=data.frame()\n\nstart_time=Sys.time()\nprint(paste0(\"Start time: \", start_time))\n\nloop_num=0\n\nfor (i in page){\n   tryCatch({\n#############################     \n   loop_num=loop_num+1\n   print(paste0(\"Running loop No.\",which(page==i)))\n         \n   print(paste0(\"current time: \", Sys.time()))\n   \n   output=one_page_function(i) %&gt;% mutate(loop_num=loop_num)\n\n   all_page_review_list=rbind(all_page_review_list,output)\n   \n   \n   print(paste0(\"Used time: \", Sys.time()-start_time))\n   # ouput every 20 page\n   if (loop_num%%20==0){\n      print(paste0(\"#########################. output to excel: \", loop_num))\n      all_page_review_list_total=rbind(finish_download,all_page_review_list)\n      write.xlsx(all_page_review_list_total,'./output/all_page_bottle_list_v2.xlsx')\n     }\n    \n   \n#############################        \n    }, error=function(e){cat(\"ERROR :\",conditionMessage(e), \"\\n\")})\n}\n\nend_time=Sys.time()\nprint(paste0(\"End time: \", end_time))\nprint(paste0(\"total used time: \", end_time-start_time))\n\n\noutput=one_page_function(i) %&gt;% mutate(loop_num=loop_num)\n\nall_page_review_list=rbind(all_page_review_list,output)\n   \nwrite.xlsx(all_page_review_list_total,'./output/all_page_bottle_list_v2.xlsx')\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Other",
      "1 Web scraping on www.whiskynotes.be",
      "All year all topic second time"
    ]
  },
  {
    "objectID": "other/1 Web scraping on www.whiskynotes.be/3 whiskynote one year page.html",
    "href": "other/1 Web scraping on www.whiskynotes.be/3 whiskynote one year page.html",
    "title": "One year page review",
    "section": "",
    "text": "Code\nimport requests\nimport os\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport time\n\n\n\n\nCode\nos.system('pip show beautifulsoup4')\n\n\nWeb scraping on www.whiskynotes.be\n\n1 year page\n\n\nCode\nyear_ur='https://www.whiskynotes.be/2023'\n\n\n\n\n\n2 read in html\n\n\nCode\n# Send an HTTP GET request to the website\nheaders = {'User-Agent': 'My User Agent'}\nresponse = requests.get(year_ur,headers=headers)\n\n\n\n\nCode\n# success code - 200 \nprint(response) \n\n\n\n\nCode\n#print(response.content)\n\n\n\n\nCode\n# Parse the HTML code using BeautifulSoup\nsoup = BeautifulSoup(response.content, 'html.parser')\n\n\n\n\n3 review bottle name on one year\n\n\nCode\nbottle001=soup.find_all('p')\n\n\n\n\nCode\nfor i in bottle001[1:5]:\n  i.get_text()\n\n\n\n\n4 review topic name on one year\n\n\nCode\ntopic001=soup.select('.archive-link')\n\n\n\n\nCode\nfor i in topic001[1:5]:\n  i.get_text()\n\n\n\n\nCode\ntopic_link=soup.select('.entry-permalink')\n\nfor link in topic_link[1:5]:\n  link.get('href')\n\n\n\n\n5 reference:\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Other",
      "1 Web scraping on www.whiskynotes.be",
      "One year page review"
    ]
  },
  {
    "objectID": "other/1 Web scraping on www.whiskynotes.be/7 clean up.html",
    "href": "other/1 Web scraping on www.whiskynotes.be/7 clean up.html",
    "title": "clean up",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(stringr)\nlibrary(openxlsx)\nlibrary(readxl)\nlibrary(janitor)\nlibrary(writexl)\nlibrary(lubridate)\n\n\n\n1 input data\n\n\nCode\ndata001=read_excel('./output/all_page_bottle_list_all.xlsx')\n\n\n\n\n2 clean data\n\n\nCode\ndata002=data001 %&gt;% clean_names()\n\n\n\n\nCode\nglimpse(data002)\n\n\n\n\nCode\ndata003=data002 %&gt;%  mutate(published_date=dmy(page_published_date)\n                            ,year=year(published_date)\n                            ,score=as.numeric(all_page_score)\n                            ,high_score=if_else(score&gt;=90,1,0)\n                             ,score_group=case_when(\n                                        score &gt;= 90 ~ \"1.&gt;=90\",\n                                        score &gt; 85 ~ \"2. &gt;=85\",\n                                          TRUE ~ \"3. &lt;85\")\n                            ,bottle_name=bottle_name%&gt;% str_replace('allt-à-bhainne','Allt-a-Bhainne')%&gt;% str_replace('Allt-a-Bhaine','Allt-a-Bhainne') %&gt;% str_to_lower()\n                            )%&gt;% filter(score&gt;0\n                                        ,score&lt;=100\n                                        ,!page_class %in% c(\"* Armagnac\",\"* Cognac\",\"* Rum\",\"* Armagnac-* Cognac\",\"* Other spirits\",\"* Distillery visits\",\"\")\n                                        ,bottle_review_nose!='no comment'\n                                        ,bottle_review_mouth!='no comment'\n                                        ,bottle_review_finish!='no comment')%&gt;% arrange(score)\n\n\n\nglimpse(data003)\n\n\n\n\nCode\np=ggplot(data003, aes(year,score,group=year)) + geom_boxplot()\np\n\n\n\n\nCode\nsummary001=data003 %&gt;% group_by(year) %&gt;% summarise(bottles=n(),avg_score=mean(score),high=sum(high_score)\n                                                 \n                                                 ,high_pct=sum(high_score)/n()\n                                                 )\nsummary001\n\n\n\n\nCode\nall_distrillery_from_wb=read_xlsx('data/all distrillery from wb.xlsx') %&gt;% mutate(name=Name %&gt;% str_replace('Isle of Jura','jura')  %&gt;% str_replace('Teeling Whiskey Distillery','Teeling') %&gt;% str_replace('Distillery','')%&gt;% str_replace('(Closed)','') %&gt;% str_replace('St. Magdalene','St Magdalene') %&gt;% str_trim()%&gt;% str_to_lower()\n                                                                              ) \n\n\n\n\nCode\nkeywords &lt;- as.character(all_distrillery_from_wb$name)\n\n\n\n\nCode\ndata004=data003[grepl(paste(keywords, collapse=\"|\"), data003$bottle_name),]\n\n\n\n\nCode\ndata005=str_extract_all(tolower(data003$bottle_name),paste(keywords,collapse  = \"|\"))\n\n\nkeep first match\n\n\nCode\ndata006=lapply(data005, `[`, 1)\n\n\n\n\nCode\ndata006[lengths(data006)==0] &lt;- NA\ndata007=unlist(data006)\n\n\n\n\nCode\ndata008=data003 %&gt;% mutate(distillery_name=data007)\n\n\n\n\nCode\nlibrary(\"ggthemes\")\nlibrary(\"scales\")\nlibrary(showtext)\nshowtext_auto()\n\ncoeff &lt;- 5\n\np=ggplot(summary001, aes(year,avg_score)) + geom_line(size=2) +\n  geom_col(aes(year,bottles/ coeff), fill = \"blue\")+scale_y_continuous(\"平均分\",limits=c(0,100), sec.axis = sec_axis(~.*coeff, name = \"评分酒款数量\",breaks = seq(0, 500, by = 100)))+labs(caption = \"2010-01 to 2024-04\")+ ggtitle(\"whiskynote.be\")+ xlim(min=2009, 2025)\np+theme_economist()+scale_x_continuous(breaks=pretty_breaks(n = 20))+scale_y_continuous(breaks=pretty_breaks(n = 20))\n\n\n\n\nCode\nsummary002=data003 %&gt;% group_by(year,score_group) %&gt;% \n  summarise(bottles=n())%&gt;% \n  mutate(score_group=score_group %&gt;% as.factor()) %&gt;% group_by(year) %&gt;% mutate(share=round(bottles/sum(bottles),2))\n\nsummary002\n\n\n\n\nCode\nlibrary(\"ggthemes\")\nlibrary(\"scales\")\nlibrary(showtext)\nshowtext_auto()\n\n\np=ggplot(summary002, aes(y=share, x=year,colour=score_group)) + \n    geom_line()\n\np+theme_economist()+scale_x_continuous(breaks=pretty_breaks(n = 20))+scale_y_continuous(breaks=pretty_breaks(n = 10))\n\n\n\n\n3 reference:\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Other",
      "1 Web scraping on www.whiskynotes.be",
      "clean up"
    ]
  },
  {
    "objectID": "other/1 Web scraping on www.whiskynotes.be/1 whiskynote data.html",
    "href": "other/1 Web scraping on www.whiskynotes.be/1 whiskynote data.html",
    "title": "Whiskynotes.be data",
    "section": "",
    "text": "1 whiskynotes.be data\nhttps://www.whiskynotes.be/\n\nWhiskyNotes is a personal collection of impressions, written while searching for the ultimate single malt whisky. A work in progress, and a continuous exercise for the senses.\nI started it in 2008 while living in Spain for a couple of years. I had discovered whisky a few years earlier but suddenly I was cut off from festivals, shops and whisky friends in my home country. A whisky blog seemed a good way of keeping in touch. It quickly gained a following, first in Belgium but now from all over the world.\n\n\n2 BeautifulSoup package\n\nBeautiful Soup is a Python library for pulling data out of HTML and XML files. It works with your favorite parser to provide idiomatic ways of navigating, searching, and modifying the parse tree. It commonly saves programmers hours or days of work.\n\n\nCode\nimport requests\nimport os\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport time\n\n\n\n\nCode\nos.system('pip show beautifulsoup4')\n\n\nName: beautifulsoup4\nVersion: 4.12.2\nSummary: Screen-scraping library\nHome-page: \nAuthor: \nAuthor-email: Leonard Richardson &lt;leonardr@segfault.org&gt;\nLicense: \nLocation: /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages\nRequires: soupsieve\nRequired-by: nbconvert\n\n\n0\n\n\n\n\n3 reference:\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Other",
      "1 Web scraping on www.whiskynotes.be",
      "Whiskynotes.be data"
    ]
  },
  {
    "objectID": "other/1 Web scraping on www.whiskynotes.be/5 web scraping with rvest all first time.html",
    "href": "other/1 Web scraping on www.whiskynotes.be/5 web scraping with rvest all first time.html",
    "title": "All year all topic first time",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(rvest)\n\n\n\n1 one page function\n\n\nCode\none_page_function &lt;- function(review_url){\n  #review_url='https://www.whiskynotes.be/2010/bowmore/bowmore-9y-1997-cigar-malt/'\n  \n #review_url= 'https://www.whiskynotes.be/2020/laphroaig/williamson-2010-the-whisky-jury/'\n  \n  print(review_url)\n  review_page &lt;- read_html(review_url)\n\n  page_class=review_page  %&gt;% html_elements(\".cat-links a\") %&gt;% html_text2()\n  page_class=str_flatten(page_class,collapse = \"-\")\n  \n  bottle_name=review_page  %&gt;% html_elements(\".entry-content h2\") %&gt;% html_text2()\n  # remove empty\n  bottle_name=bottle_name[nzchar(bottle_name)]\n  # remove space element\n  bottle_name=bottle_name[nchar(bottle_name)&gt;2]\n  \n  \n  # remove ? mark non bottle name element\n  bottle_name=bottle_name[!bottle_name%&gt;% str_detect(\"\\\\?\")]\n  # remove Drams Delivered revisited  non bottle name element\n  bottle_name=bottle_name[ !bottle_name == 'Drams Delivered revisited']\n  print(bottle_name) \n  \n  bottle_review_raw=review_page  %&gt;% html_elements(\"p\") %&gt;% html_text()\n  bottle_review=unlist(strsplit(bottle_review_raw,\"(?= Nose: )\",perl = TRUE))\n  bottle_review=unlist(strsplit(bottle_review,\"(?= Mouth: )\",perl = TRUE))\n   bottle_review=unlist(strsplit(bottle_review,\"(?= Finish: )\",perl = TRUE))\n  \n  bottle_review_Nose=bottle_review[bottle_review %&gt;% str_detect('Nose:|Attractive nose:')]\n  bottle_review_Nose=bottle_review_Nose[nzchar(bottle_review_Nose)]\n\n  bottle_review_Mouth=bottle_review[bottle_review %&gt;% str_detect('Mouth:')]\n  bottle_review_Mouth=bottle_review_Mouth[nzchar(bottle_review_Mouth)]\n\n  bottle_review_Finish=bottle_review[bottle_review %&gt;% str_detect('Finish:')]\n  bottle_review_Finish=bottle_review_Finish[nzchar(bottle_review_Finish)]\n  \n  ########### add dummy score if there is no score review #########\n  # bottle_review_Finish_score=bottle_review[bottle_review %&gt;% str_detect('Finish:|Score:')][-1]\n \n  # # bottle_review_Finish_score2=bottle_review_Finish_score\n  \n  \n  # order=1\n  # for (word in bottle_review_Finish_score){\n  #   print(word)\n  #   print(order)\n  #   print(order%%2)\n  #   print(word %&gt;% str_detect('Score:'))\n  #   print(order%%2==0 & word %&gt;% str_detect('Score:')==FALSE)\n  #   if (order%%2==0 & word %&gt;% str_detect('Score:')==FALSE){\n  #     print('adding add dummy score if there is no score review ')\n  #     bottle_review_Finish_score2=append(bottle_review_Finish_score2,'Score:00/100',order-1)\n  #   }else{\n  #   }\n  #   order=order+1\n  #   }\n################################################\n    \n#\"^[:digit:]+$\" \n################# score  \n  first_bottle_score=review_page  %&gt;% html_elements(\".entry-score\") %&gt;% html_text2()\n\n  bottle_score=review_page  %&gt;% html_elements(\"strong\") %&gt;% html_text2()\n  \n  bottle_score2=bottle_score %&gt;% str_remove(\"100\")%&gt;% str_remove(\"/\") %&gt;% str_remove(\"/100.\") %&gt;% str_remove(\"/100\")%&gt;% str_remove(\"Score:\")%&gt;%str_trim() %&gt;%  str_match(\"^[0-9]{2}$\") %&gt;% as.data.frame() %&gt;% filter(is.na(V1)==FALSE)\n  \n  bottle_score2=bottle_score2 %&gt;% mutate(V1=str_replace(V1,'/100',''))%&gt;% rename(all_page_score=V1) \n\n# if no other score then use first score\n  if(identical(bottle_score, character(0))==TRUE|nrow(bottle_score2)==0){\n    all_page_score=first_bottle_score %&gt;% tibble()%&gt;% rename(all_page_score='.') \n    \n# if no first score then use other score\n  }else if (identical(first_bottle_score, character(0))==TRUE){\n    all_page_score=bottle_score2\n    \n# the other score have same length as bottle.aka the first score appear twices\n    \n  }else if (nrow(bottle_score2)==length(bottle_review_Nose)){\n    all_page_score=bottle_score2\n    \n# if both have first score and other score then combine    \n  }else{\n  #bottle_score=bottle_score %&gt;% str_match('[0-9][0-9]') %&gt;% as.data.frame() %&gt;% filter(is.na(V1)==FALSE)\n  all_page_score=rbind(first_bottle_score,bottle_score2)\n  }\n##############################\n  page_published_date=review_page  %&gt;% html_elements(\".published\") %&gt;% html_text2()\n\n\n  page_title=review_page  %&gt;% html_elements(\".entry-title\") %&gt;% html_text2()\n  \n  if(nrow(all_page_score)!=length(bottle_name)){all_page_score=0}\n  if(length(bottle_review_Nose)!=length(bottle_name)){bottle_review_Nose='no comment'}\n  if(length(bottle_review_Mouth)!=length(bottle_name)){bottle_review_Mouth='no comment'}\n  if(length(bottle_review_Finish)!=length(bottle_name)){bottle_review_Finish='no comment'}\n  \n  \n  \n  one_page_review=tibble(bottle_name,bottle_review_Nose,bottle_review_Mouth,bottle_review_Finish,all_page_score,page_class,page_published_date,page_title,review_url) \n  \n\n  Sys.sleep(runif(n=1, min=0.1, max=0.8))\n  #print(one_page_review)\n  print(dim(one_page_review))\n  #remove(review_page)\n  return(one_page_review)\n}\n\n\n\n\n2 read in all link\n\n\nCode\nlibrary(readxl)\ntopic_link=read_excel('./output/all year page2.xlsx',sheet='topic')\n\n\n\n\nCode\nglimpse(topic_link)\n\n\nexclude news\n\n\nCode\nnews=topic_link$topic_link_list %&gt;% str_detect('/whisky-news/')\n\n\n\n\nCode\ntopic_link002=topic_link$topic_link_list[!news]\n\n\n\n\nCode\nlength(topic_link002)\n\n\n\n\n3 start downlaod all page on first first try\n\n\nCode\n#test=one_page_function('https://www.whiskynotes.be/2020/irish-whiskey/teeling-1996-fill-your-own-teeling-chinkapin-oak-single-pot-still-distillery-exclusive/')%&gt;% mutate(loop_num=3)\n\n\n\n\nCode\nsink(\"log3.txt\", append=FALSE, split=TRUE)  # for screen and log\n\nlibrary(openxlsx)\n\npage=topic_link002\n\n\nall_page_review_list=data.frame()\n\nstart_time=Sys.time()\nprint(paste0(\"Start time: \", start_time))\n\nloop_num=0\n\nfor (i in page){\n   tryCatch({\n#############################     \n   loop_num=loop_num+1\n   print(paste0(\"################ Running loop No.\",which(page==i)))\n         \n   print(paste0(\"current time: \", Sys.time()))\n   \n   output=one_page_function(i) %&gt;% mutate(loop_num=loop_num)\n\n   all_page_review_list=rbind(all_page_review_list,output)\n   \n   print(paste0(\"Used time: \", Sys.time()-start_time))\n   # ouput every 20 page\n   if (loop_num%%20==0){\n      print(paste0(\"############################## output to excel: \", loop_num))\n      write.xlsx(all_page_review_list,'./output/all_page_bottle_list3.xlsx')\n     }\n    \n   \n#############################        \n    }, error=function(e){cat(\"ERROR :\",conditionMessage(e), \"\\n\")})\n}\n\nend_time=Sys.time()\nprint(paste0(\"End time: \", end_time))\nprint(paste0(\"total used time: \", end_time-start_time))\n\nwrite.xlsx(all_page_review_list,'./output/all_page_bottle_list3.xlsx')\n\nsink()\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Other",
      "1 Web scraping on www.whiskynotes.be",
      "All year all topic first time"
    ]
  }
]